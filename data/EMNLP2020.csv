title,abstract,authors,pdf_url,url,label1,label2
Blank Language Models,"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.","['Tianxiao Shen', 'Victor Quach', 'Regina Barzilay', 'Tommi Jaakkola']",https://www.aclweb.org/anthology/2020.emnlp-main.420.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.420/,2,4
Acrostic Poem Generation,"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.","['Rajat Agarwal', 'Katharina Kann']",https://www.aclweb.org/anthology/2020.emnlp-main.94.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.94/,2,
Unsupervised Adaptation of Question Answering Systems via Generative Self-training,"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.","['Steven Rennie', 'Etienne Marcheret', 'Neil Mallinar', 'David Nahamoo', 'Vaibhava Goel']",https://www.aclweb.org/anthology/2020.emnlp-main.87.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.87/,5,
An Imitation Game for Learning Semantic Parsers from User Interaction,"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at https://github.com/sunlab-osu/MISP.","['Ziyu Yao', 'Yiqi Tang', 'Wen-tau Yih', 'Huan Sun', 'Yu Su']",https://www.aclweb.org/anthology/2020.emnlp-main.559.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.559/,8,1
Conversational Semantic Parsing,"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which also set state-of-the-art in ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.","['Armen Aghajanyan', 'Jean Maillard', 'Akshat Shrivastava', 'Keith Diedrick', 'Michael Haeger', 'Haoran Li', 'Yashar Mehdad', 'Veselin Stoyanov', 'Anuj Kumar', 'Mike Lewis', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.408.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.408/,8,1
Probing Task-Oriented Dialogue Representation from Language Models,"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.","['Chien-Sheng Wu', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.409.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.409/,4,5
A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support,"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.","['Ashish Sharma', 'Adam Miner', 'David Atkins', 'Tim Althoff']",https://www.aclweb.org/anthology/2020.emnlp-main.425.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.425/,6,
A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses,"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents’ verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.","['Hisashi Kamezawa', 'Noriki Nishida', 'Nobuyuki Shimizu', 'Takashi Miyazaki', 'Hideki Nakayama']",https://www.aclweb.org/anthology/2020.emnlp-main.267.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.267/,6,9
ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,"Given questions regarding some prototypical situation — such as Name something that people usually do before they leave the house for work? — a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show – Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.","['Michael Boratko', 'Xiang Li', 'Tim O’Gorman', 'Rajarshi Das', 'Dan Le', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.85.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.85/,5,
Interpretation of NLP models through input marginalization,"To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.","['Siwon Kim', 'Jihun Yi', 'Eunji Kim', 'Sungroh Yoon']",https://www.aclweb.org/anthology/2020.emnlp-main.255.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.255/,4,10
Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies,We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.,"['Maryam Aminian', 'Mohammad Sadegh Rasooli', 'Mona Diab']",https://www.aclweb.org/anthology/2020.emnlp-main.663.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.663/,10,
The importance of fillers for text representations of speech transcripts,"While being an essential component of spoken language, fillers (e.g. “um” or “uh”) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two downstream tasks — predicting a speaker’s stance and expressed confidence.","['Tanvi Dinkar', 'Pierre Colombo', 'Matthieu Labeau', 'Chloé Clavel']",https://www.aclweb.org/anthology/2020.emnlp-main.641.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.641/,4,
Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation,"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.","['Ruibo Liu', 'Guangxuan Xu', 'Chenyan Jia', 'Weicheng Ma', 'Lili Wang', 'Soroush Vosoughi']",https://www.aclweb.org/anthology/2020.emnlp-main.726.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.726/,2,10
Improving Neural Topic Models using Knowledge Distillation,"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.","['Alexander Miserlis Hoyle', 'Pranav Goel', 'Philip Resnik']",https://www.aclweb.org/anthology/2020.emnlp-main.137.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.137/,3,
Multi-Unit Transformers for Neural Machine Translation,"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.","['Jianhao Yan', 'Fandong Meng', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.77.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.77/,2,
Weakly Supervised Subevent Knowledge Acquisition,"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.","['Wenlin Yao', 'Zeyu Dai', 'Maitreyi Ramaswamy', 'Bonan Min', 'Ruihong Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.430.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.430/,1,4
Structured Attention for Unsupervised Dialogue Structure Induction,"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.","['Liang Qiu', 'Yizhou Zhao', 'Weiyan Shi', 'Yuan Liang', 'Feng Shi', 'Tao Yuan', 'Zhou Yu', 'Song-chun Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.148.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.148/,1,4
Towards Enhancing Faithfulness for Neural Machine Translation,"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.","['Rongxiang Weng', 'Heng Yu', 'Xiangpeng Wei', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.212.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.212/,2,10
AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","['Taylor Shin', 'Yasaman Razeghi', 'Robert L. Logan IV', 'Eric Wallace', 'Sameer Singh']",https://www.aclweb.org/anthology/2020.emnlp-main.346.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.346/,3,4
Multi-resolution Annotations for Emoji Prediction,"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.","['Weicheng Ma', 'Ruibo Liu', 'Lili Wang', 'Soroush Vosoughi']",https://www.aclweb.org/anthology/2020.emnlp-main.542.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.542/,1,3
LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool,"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for “strong” cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target “weak” alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at https://github.com/google-research-datasets/lareqa.","['Uma Roy', 'Noah Constant', 'Rami Al-Rfou’', 'Aditya Barua', 'Aaron Phillips', 'Yinfei Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.477.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.477/,2,5
Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments,"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims.","['Sungho Jeon', 'Michael Strube']",https://www.aclweb.org/anthology/2020.emnlp-main.604.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.604/,1,
Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art,"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.","['Jing Lu', 'Vincent Ng']",https://www.aclweb.org/anthology/2020.emnlp-main.536.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.536/,4,
"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family – LXMERT – finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT’s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.","['Jaemin Cho', 'Jiasen Lu', 'Dustin Schwenk', 'Hannaneh Hajishirzi', 'Aniruddha Kembhavi']",https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.707/,2,4
HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media,"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.","['Hsin-Yu Chen', 'Cheng-Te Li']",https://www.aclweb.org/anthology/2020.emnlp-main.200.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.200/,4,7
Generationary or “How We Went beyond Word Sense Inventories and Learned to Gloss”,"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.","['Michele Bevilacqua', 'Marco Maru', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.585.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.585/,2,
Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks,"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient — when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.","['Trapit Bansal', 'Rishikesh Jha', 'Tsendsuren Munkhdalai', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.38.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.38/,6,
ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.","['Shiyue Zhang', 'Benjamin Frey', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.43.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.43/,2,10
Imitation Attacks and Defenses for Black-box Machine Translation Systems,"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary’s BLEU score and attack success rate at some cost in the defender’s BLEU and inference speed.","['Eric Wallace', 'Mitchell Stern', 'Dawn Song']",https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.446/,2,10
Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection,"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).","['Shaolei Wang', 'Zhongyuan Wang', 'Wanxiang Che', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.142.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.142/,4,
Semantic Label Smoothing for Sequence to Sequence Problems,"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.","['Michal Lukasik', 'Himanshu Jain', 'Aditya Menon', 'Seungyeon Kim', 'Srinadh Bhojanapalli', 'Felix Yu', 'Sanjiv Kumar']",https://www.aclweb.org/anthology/2020.emnlp-main.405.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.405/,2,
Modularized Transfomer-based Ranking Framework,"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.","['Luyu Gao', 'Zhuyun Dai', 'Jamie Callan']",https://www.aclweb.org/anthology/2020.emnlp-main.342.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.342/,8,
Accurate Word Alignment Induction from Neural Machine Translation,"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","['Yun Chen', 'Yang Liu', 'Guanhua Chen', 'Xin Jiang', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.42.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.42/,4,
Multi-hop Inference for Question-driven Summarization,"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.","['Yang Deng', 'Wenxuan Zhang', 'Wai Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.547.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.547/,2,5
Scaling Hidden Markov Language Models,"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.","['Justin Chiu', 'Alexander M. Rush']",https://www.aclweb.org/anthology/2020.emnlp-main.103.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.103/,4,
Unsupervised Text Style Transfer with Padded Masked Language Models,"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source–target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods’ accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.","['Eric Malmi', 'Aliaksei Severyn', 'Sascha Rothe']",https://www.aclweb.org/anthology/2020.emnlp-main.699.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.699/,3,10
MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification,"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.","['Qianli Ma', 'Zhenxi Lin', 'Jiangyue Yan', 'Zipeng Chen', 'Liuhong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.544.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.544/,4,
Let’s Stop Incorrect Comparisons in End-to-end Relation Extraction!,"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation. We then propose a small empirical study to quantify the most common mistake’s impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05. We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER. This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics. We finally call for unifying the evaluation setting in end-to-end RE.","['Bruno Taillé', 'Vincent Guigue', 'Geoffrey Scoutheeten', 'Patrick Gallinari']",https://www.aclweb.org/anthology/2020.emnlp-main.301.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.301/,4,8
Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages,"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks. To address the issues, we propose a meta graph learning (MGL) method. Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages. Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer. Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.","['Zheng Li', 'Mukul Kumar', 'William Headden', 'Bing Yin', 'Ying Wei', 'Yu Zhang', 'Qiang Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.179.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.179/,2,10
A Preliminary Exploration of GANs for Keyphrase Generation,"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs). For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard benchmark datasets. We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques. Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models. To our knowledge, this is one of the first works that use GANs for keyphrase generation. We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.","['Avinash Swaminathan', 'Haimin Zhang', 'Debanjan Mahata', 'Rakesh Gosangi', 'Rajiv Shah', 'Amanda Stent']",https://www.aclweb.org/anthology/2020.emnlp-main.645.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.645/,2,4
Active Learning for BERT: An Empirical Study,"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.","['Liat Ein Dor', 'Alon Halfon', 'Ariel Gera', 'Eyal Shnarch', 'Lena Dankin', 'Leshem Choshen', 'Marina Danilevsky', 'Ranit Aharonov', 'Yoav Katz', 'Noam Slonim']",https://www.aclweb.org/anthology/2020.emnlp-main.638.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.638/,10,
Context-Aware Answer Extraction in Question Answering,"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases. To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task. With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases. We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.","['Yeon Seonwoo', 'Ji-Hoon Kim', 'Jung-Woo Ha', 'Alice Oh']",https://www.aclweb.org/anthology/2020.emnlp-main.189.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.189/,5,
Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach.","['Dong Zhang', 'Xincheng Ju', 'Junhui Li', 'Shoushan Li', 'Qiaoming Zhu', 'Guodong Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.291.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.291/,10,
Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing,"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.","['Piotr Szymański', 'Kyle Gorman']",https://www.aclweb.org/anthology/2020.emnlp-main.172.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.172/,9,10
HABERTOR: An Efficient and Effective Deep Hatespeech Detector,"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT’s architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.","['Thanh Tran', 'Yifan Hu', 'Changwei Hu', 'Kevin Yen', 'Fei Tan', 'Kyumin Lee', 'Se Rim Park']",https://www.aclweb.org/anthology/2020.emnlp-main.606.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.606/,3,4
Textual Data Augmentation for Efficient Active Learning on Tiny Datasets,"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.","['Husam Quteineh', 'Spyridon Samothrakis', 'Richard Sutcliffe']",https://www.aclweb.org/anthology/2020.emnlp-main.600.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.600/,2,3
Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction,"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven, composed, Ode to Joy>. In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases). Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.","['Patrick Hohenecker', 'Frank Mtumbuka', 'Vid Kocijan', 'Thomas Lukasiewicz']",https://www.aclweb.org/anthology/2020.emnlp-main.690.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.690/,8,
SRLGRN: Semantic Role Labeling Graph Reasoning Network,"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.","['Chen Zheng', 'Parisa Kordjamshidi']",https://www.aclweb.org/anthology/2020.emnlp-main.714.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.714/,3,5
Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT,"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.","['Akshay Smit', 'Saahil Jain', 'Pranav Rajpurkar', 'Anuj Pareek', 'Andrew Y. Ng', 'Matthew Lungren']",https://www.aclweb.org/anthology/2020.emnlp-main.117.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.117/,2,8
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods—including the quantity of gendered words, a dialogue safety classifier, and human assessments—all of which show that our models generate less gendered, but equally engaging chit-chat responses.","['Emily Dinan', 'Angela Fan', 'Adina Williams', 'Jack Urbanek', 'Douwe Kiela', 'Jason Weston']",https://www.aclweb.org/anthology/2020.emnlp-main.656.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.656/,7,
Re-evaluating Evaluation in Text Summarization,"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).","['Manik Bhandari', 'Pranav Narayan Gour', 'Atabak Ashfaq', 'Pengfei Liu', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.751.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.751/,2,10
An information theoretic view on selecting linguistic probes,"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier – or ”probe” – to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either ”the representation being rich in knowledge”, or ”the probe learning the task”, which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the ”good probe” criteria proposed by the two papers, *selectivity* (Hewitt and Liang, 2019) and *information gain* (Pimentel et al., 2020), are equivalent – the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.","['Zining Zhu', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.744.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.744/,1,
An Element-aware Multi-representation Model for Law Article Prediction,"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","['Huilin Zhong', 'Junsheng Zhou', 'Weiguang Qu', 'Yunfei Long', 'Yanhui Gu']",https://www.aclweb.org/anthology/2020.emnlp-main.540.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.540/,4,10
Homophonic Pun Generation with Lexically Constrained Rewriting,"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.","['Zhiwei Yu', 'Hongyu Zang', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.229.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.229/,2,4
On Losses for Modern Language Models,"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP’s effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks – sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant – that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERTBase on the GLUE benchmark using fewer than a quarter of the training tokens.","['Stéphane Aroca-Ouellette', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.403.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.403/,4,10
Named Entity Recognition for Social Media Texts with Semantic Augmentation,"Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.","['Yuyang Nie', 'Yuanhe Tian', 'Xiang Wan', 'Yan Song', 'Bo Dai']",https://www.aclweb.org/anthology/2020.emnlp-main.107.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.107/,7,10
Effectively pretraining a speech translation decoder with Machine Translation data,"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.","['Ashkan Alinejad', 'Anoop Sarkar']",https://www.aclweb.org/anthology/2020.emnlp-main.644.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.644/,2,9
Hate-Speech and Offensive Language Detection in Roman Urdu,"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.","['Hammad Rizwan', 'Muhammad Haroon Shakeel', 'Asim Karim']",https://www.aclweb.org/anthology/2020.emnlp-main.197.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.197/,7,9
Are All Good Word Vector Spaces Isomorphic?,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).","['Ivan Vulić', 'Sebastian Ruder', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.257.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.257/,10,
A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation,"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.","['Ming Wang', 'Yinglin Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.504.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.504/,4,10
AmbigQA: Answering Ambiguous Open-domain Questions,"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.","['Sewon Min', 'Julian Michael', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.466.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.466/,3,5
Text Graph Transformer for Document Classification,"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.","['Haopeng Zhang', 'Jiawei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.668.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.668/,4,
Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.","['Yuncheng Hua', 'Yuan-Fang Li', 'Gholamreza Haffari', 'Guilin Qi', 'Tongtong Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.469.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.469/,5,
Where Are You? Localization from Embodied Dialog,"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans – an Observer and a Locator – complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task – providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer’s location within 3m in unseen buildings, vs. 70.4% for human Locators.","['Meera Hahn', 'Jacob Krantz', 'Dhruv Batra', 'Devi Parikh', 'James Rehg', 'Stefan Lee', 'Peter Anderson']",https://www.aclweb.org/anthology/2020.emnlp-main.59.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.59/,5,
Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product,"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github.com/jd-aig/JAVE.","['Tiangang Zhu', 'Yue Wang', 'Haoran Li', 'Youzheng Wu', 'Xiaodong He', 'Bowen Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.166.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.166/,6,8
Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets,"Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.","['Nedjma Ousidhoum', 'Yangqiu Song', 'Dit-Yan Yeung']",https://www.aclweb.org/anthology/2020.emnlp-main.199.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.199/,3,9
Selection and Generation: Learning towards Multi-Product Advertisement Post Generation,"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic. A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products. Hence, multi-product AD post generation is meaningful and important. We propose a novel end-to-end model named S-MG Net to generate the AD post. Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network). (2) generate a post including selected products via the MGenNet (Multi-Generator Network). Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products. Then, MGenNet generates the description copywriting of each product. Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.","['Zhangming Chan', 'Yuchi Zhang', 'Xiuying Chen', 'Shen Gao', 'Zhiqiang Zhang', 'Dongyan Zhao', 'Rui Yan']",https://www.aclweb.org/anthology/2020.emnlp-main.313.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.313/,2,3
DualTKB: A Dual Learning Bridge between Text and Knowledge Base,"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.","['Pierre Dognin', 'Igor Melnyk', 'Inkit Padhi', 'Cicero dos Santos', 'Payel Das']",https://www.aclweb.org/anthology/2020.emnlp-main.694.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.694/,10,
Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling,"In recent years, there has been an increasing interest in the application of Artificial Intelligence – and especially Machine Learning – to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","['Costanza Conforti', 'Stephanie Hirmer', 'Dai Morgan', 'Marco Basaldella', 'Yau Ben Or']",https://www.aclweb.org/anthology/2020.emnlp-main.677.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.677/,4,6
Pre-Training Transformers as Energy-Based Cloze Models,"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.","['Kevin Clark', 'Minh-Thang Luong', 'Quoc Le', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.20/,9,
Multi-view Story Characterization from Movie Plot Synopses and Reviews,"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events). Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses. Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision. We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.","['Sudipta Kar', 'Gustavo Aguilar', 'Mirella Lapata', 'Thamar Solorio']",https://www.aclweb.org/anthology/2020.emnlp-main.454.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.454/,3,
SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction,"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.","['Xuming Hu', 'Lijie Wen', 'Yusong Xu', 'Chenwei Zhang', 'Philip S. Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.299.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.299/,8,
XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization,"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.","['Alessandro Raganato', 'Tommaso Pasini', 'Jose Camacho-Collados', 'Mohammad Taher Pilehvar']",https://www.aclweb.org/anthology/2020.emnlp-main.584.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.584/,2,10
Beyond [CLS] through Ranking by Generation,"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.","['Cicero dos Santos', 'Xiaofei Ma', 'Ramesh Nallapati', 'Zhiheng Huang', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.134.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.134/,5,8
Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations,"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.","['Arturo Oncevay', 'Barry Haddow', 'Alexandra Birch']",https://www.aclweb.org/anthology/2020.emnlp-main.187.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.187/,1,2
MAVEN: A Massive General Domain Event Detection Dataset,"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.","['Xiaozhi Wang', 'Ziqi Wang', 'Xu Han', 'Wangyi Jiang', 'Rong Han', 'Zhiyuan Liu', 'Juanzi Li', 'Peng Li', 'Yankai Lin', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.129.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.129/,6,10
Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(nˆ6) down to O(nˆ3),"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.",['Caio Corro'],https://www.aclweb.org/anthology/2020.emnlp-main.219.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.219/,1,
Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models,"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC). Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines. As a result, a high Kappa score of 61% is achieved for inter-annotator agreement. Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2). Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.","['Changmao Li', 'Elaine Fisher', 'Rebecca Thomas', 'Steve Pittard', 'Vicki Hertzberg', 'Jinho D. Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.679.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.679/,4,6
Multi-Fact Correction in Abstractive Text Summarization,"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.","['Yue Dong', 'Shuohang Wang', 'Zhe Gan', 'Yu Cheng', 'Jackie Chi Kit Cheung', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.749.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.749/,2,5
Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging,"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.","['Semih Yavuz', 'Kazuma Hashimoto', 'Wenhao Liu', 'Nitish Shirish Keskar', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.412.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.412/,6,4
"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three","Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs. However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available. In this paper, we study ensemble distillation as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles. We validate this framework on two tasks: named-entity recognition and machine translation. We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.","['Steven Reich', 'David Mueller', 'Nicholas Andrews']",https://www.aclweb.org/anthology/2020.emnlp-main.450.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.450/,2,
SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery,"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have a similar likelihood of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities’ infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.","['Jiaming Shen', 'Wenda Qiu', 'Jingbo Shang', 'Michelle Vanni', 'Xiang Ren', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.666.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.666/,1,
Toward Micro-Dialect Identification in Diaglossic and Code-Switched Environments,"Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message. For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models. To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT predicts micro-dialects with 9.9% F1, 76 better than a majority class baseline. Our new language model also establishes new state-of-the-art on several external tasks.","['Muhammad Abdul-Mageed', 'Chiyu Zhang', 'AbdelRahim Elmadany', 'Lyle Ungar']",https://www.aclweb.org/anthology/2020.emnlp-main.472.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.472/,1,6
Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.","['Reina Akama', 'Sho Yokoi', 'Jun Suzuki', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.68.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.68/,1,2
Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training,"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.","['Hai Ye', 'Qingyu Tan', 'Ruidan He', 'Juntao Li', 'Hwee Tou Ng', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.599.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.599/,2,
BERT-enhanced Relational Sentence Ordering Network,"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences. Extensive evaluations are conducted on six public datasets. The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.","['Baiyun Cui', 'Yingming Li', 'Zhongfei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.511.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.511/,10,
Writing Strategies for Science Communication: Data and Computational Analysis,"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies’ use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.","['Tal August', 'Lauren Kim', 'Katharina Reinecke', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.429.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.429/,2,
Text Classification Using Label Names Only: A Language Model Self-Training Approach,"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.","['Yu Meng', 'Yunyi Zhang', 'Jiaxin Huang', 'Chenyan Xiong', 'Heng Ji', 'Chao Zhang', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.724.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.724/,1,3
Zero-Shot Cross-Lingual Transfer with Meta Learning,"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.","['Farhad Nooralahzadeh', 'Giannis Bekoulis', 'Johannes Bjerva', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.368.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.368/,2,3
HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction,"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity. To address this issue, we present a novel nested NER model named HIT. Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary. Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary. Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.","['Yu Wang', 'Yun Li', 'Hanghang Tong', 'Ziye Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.486.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.486/,4,
Training Question Answering Models From Synthetic Data,"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.","['Raul Puri', 'Ryan Spring', 'Mohammad Shoeybi', 'Mostofa Patwary', 'Bryan Catanzaro']",https://www.aclweb.org/anthology/2020.emnlp-main.468.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.468/,2,5
Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models,"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.","['Isabel Papadimitriou', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.554.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.554/,1,
On the Sentence Embeddings from Pre-trained Language Models,"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.","['Bohan Li', 'Hao Zhou', 'Junxian He', 'Mingxuan Wang', 'Yiming Yang', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.733.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.733/,4,10
Biomedical Event Extraction as Sequence Labeling,"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL’s speed and accuracy makes it a viable approach for large-scale real-world scenarios.","['Alan Ramponi', 'Rob van der Goot', 'Rosario Lombardo', 'Barbara Plank']",https://www.aclweb.org/anthology/2020.emnlp-main.431.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.431/,7,8
KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework.","['Wenhu Chen', 'Yu Su', 'Xifeng Yan', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.697.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.697/,2,6
Structure Aware Negative Sampling in Knowledge Graphs,"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.","['Kian Ahrabian', 'Aarash Feizi', 'Yasmin Salehi', 'William L. Hamilton', 'Avishek Joey Bose']",https://www.aclweb.org/anthology/2020.emnlp-main.492.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.492/,4,
Autoregressive Knowledge Distillation through Imitation Learning,"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.","['Alexander Lin', 'Jeremy Wohlwend', 'Howard Chen', 'Tao Lei']",https://www.aclweb.org/anthology/2020.emnlp-main.494.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.494/,2,
NwQM: A neural quality assessment framework for Wikipedia,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.","['Bhanu Prakash Reddy Guda', 'Sasi Bhushan Seelaboyina', 'Soumya Sarkar', 'Animesh Mukherjee']",https://www.aclweb.org/anthology/2020.emnlp-main.674.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.674/,3,
SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness,"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.","['Nathan Ng', 'Kyunghyun Cho', 'Marzyeh Ghassemi']",https://www.aclweb.org/anthology/2020.emnlp-main.97.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.97/,4,10
On the Sparsity of Neural Machine Translation Models,"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.","['Yong Wang', 'Longyue Wang', 'Victor Li', 'Zhaopeng Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.78.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.78/,2,10
The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures,"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.","['Haim Dubossarsky', 'Ivan Vulić', 'Roi Reichart', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.186.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.186/,2,
An Unsupervised Sentence Embedding Method by Mutual Information Maximization,"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.","['Yan Zhang', 'Ruidan He', 'Zuozhu Liu', 'Kwan Hui Lim', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.124.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.124/,6,
Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations,"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.","['Kai Sun', 'Richong Zhang', 'Samuel Mensah', 'Yongyi Mao', 'Xudong Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.304.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.304/,8,
BLEU might be Guilty but References are not Innocent,"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.","['Markus Freitag', 'David Grangier', 'Isaac Caswell']",https://www.aclweb.org/anthology/2020.emnlp-main.5.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.5/,2,
Profile Consistency Identification for Open-domain Dialogue Agents,"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.","['Haoyu Song', 'Yan Wang', 'Weinan Zhang', 'Zhengyu Zhao', 'Ting Liu', 'Xiaojiang Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.539.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.539/,10,
Annotating Temporal Dependency Graphs via Crowdsourcing,"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. The data set is publicly available.","['Jiarui Yao', 'Haoling Qiu', 'Bonan Min', 'Nianwen Xue']",https://www.aclweb.org/anthology/2020.emnlp-main.432.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.432/,10,
DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.","['Bosheng Ding', 'Linlin Liu', 'Lidong Bing', 'Canasai Kruengkrai', 'Thien Hai Nguyen', 'Shafiq Joty', 'Luo Si', 'Chunyan Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.488.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.488/,3,4
Shallow-to-Deep Training for Neural Machine Translation,"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/SDT-Training.","['Bei Li', 'Ziyang Wang', 'Hui Liu', 'Yufan Jiang', 'Quan Du', 'Tong Xiao', 'Huizhen Wang', 'Jingbo Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.72.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.72/,2,
Reading Between the Lines: Exploring Infilling in Visual Narratives,"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.","['Khyathi Raghavi Chandu', 'Ruo-Ping Dong', 'Alan W. Black']",https://www.aclweb.org/anthology/2020.emnlp-main.93.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.93/,2,
Predicting Clinical Trial Results by Implicit Evidence Integration,"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.","['Qiao Jin', 'Chuanqi Tan', 'Mosha Chen', 'Xiaozhong Liu', 'Songfang Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.114.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.114/,6,
Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.","['Jianguo Zhang', 'Kazuma Hashimoto', 'Wenhao Liu', 'Chien-Sheng Wu', 'Yao Wan', 'Philip S. Yu', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.411.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.411/,8,
An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training,"Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.","['Kristjan Arumae', 'Qing Sun', 'Parminder Bhatia']",https://www.aclweb.org/anthology/2020.emnlp-main.394.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.394/,8,
Statistical Power and Translationese in Machine Translation Evaluation,"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.","['Yvette Graham', 'Barry Haddow', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.6.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.6/,2,4
Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks,"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy","['Shubham Toshniwal', 'Sam Wiseman', 'Allyson Ettinger', 'Karen Livescu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.685.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.685/,6,10
DGST: a Dual-Generator Network for Text Style Transfer,"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.","['Xiao Li', 'Guanyi Chen', 'Chenghua Lin', 'Ruizhe Li']",https://www.aclweb.org/anthology/2020.emnlp-main.578.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.578/,2,
An Empirical Study of Generation Order for Machine Translation,"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT’14 English → German and WMT’18 English → Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English → German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.","['William Chan', 'Mitchell Stern', 'Jamie Kiros', 'Jakob Uszkoreit']",https://www.aclweb.org/anthology/2020.emnlp-main.464.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.464/,2,
DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion,"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.","['Zhen Han', 'Peng Chen', 'Yunpu Ma', 'Volker Tresp']",https://www.aclweb.org/anthology/2020.emnlp-main.593.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.593/,4,
Translation Quality Estimation by Jointly Learning to Score and Rank,"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.","['Jingyi Zhang', 'Josef van Genabith']",https://www.aclweb.org/anthology/2020.emnlp-main.205.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.205/,2,10
Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations,"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations. We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets. We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks. Our results indicate a significant improvement over the application of the dense word representations.",['Gábor Berend'],https://www.aclweb.org/anthology/2020.emnlp-main.683.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.683/,6,9
Counterfactual Off-Policy Training for Neural Dialogue Generation,"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.","['Qingfu Zhu', 'Weinan Zhang', 'Ting Liu', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.276.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.276/,2,
"If beam search is the answer, what was the question?","Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","['Clara Meister', 'Ryan Cotterell', 'Tim Vieira']",https://www.aclweb.org/anthology/2020.emnlp-main.170.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.170/,2,5
PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.","['Xinyu Hua', 'Lu Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.57.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.57/,2,10
Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning,"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.","['Lianhui Qin', 'Vered Shwartz', 'Peter West', 'Chandra Bhagavatula', 'Jena D. Hwang', 'Ronan Le Bras', 'Antoine Bosselut', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.58.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.58/,2,10
Compositional Phrase Alignment and Beyond,"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.","['Yuki Arase', 'Jun’ichi Tsujii']",https://www.aclweb.org/anthology/2020.emnlp-main.125.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.125/,8,4
Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data,"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.","['Shachar Rosenman', 'Alon Jacovi', 'Yoav Goldberg']",https://www.aclweb.org/anthology/2020.emnlp-main.302.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.302/,5,8
Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.","['Rujun Han', 'Yichao Zhou', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.461.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.461/,4,8
Enhancing Aspect Term Extraction with Soft Prototypes,"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.","['Zhuang Chen', 'Tieyun Qian']",https://www.aclweb.org/anthology/2020.emnlp-main.164.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.164/,1,8
Extracting Implicitly Asserted Propositions in Argumentation,"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.","['Yohan Jo', 'Jacky Visser', 'Chris Reed', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.2.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.2/,1,3
Learning to Pronounce Chinese Without a Pronunciation Dictionary,"We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.","['Christopher Chu', 'Scot Fang', 'Kevin Knight']",https://www.aclweb.org/anthology/2020.emnlp-main.458.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.458/,9,
Cross Copy Network for Dialogue Generation,"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances’ logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.","['Changzhen Ji', 'Xin Zhou', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Conghui Zhu', 'Tiejun Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.149.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.149/,2,
MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.","['Jonas Pfeiffer', 'Ivan Vulić', 'Iryna Gurevych', 'Sebastian Ruder']",https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.617/,2,5
Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding,"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and information elicitation in such settings, but has been limited to manual review of small corpora. We introduce **Interview**—a large-scale (105K conversations) media dialog dataset collected from news interview transcripts—which allows us to investigate such patterns at scale. We present a dialog model that leverages external knowledge as well as dialog acts via auxiliary losses and demonstrate that our model quantitatively and qualitatively outperforms strong discourse-agnostic baselines for dialog modeling—generating more specific and topical responses in interview-style conversations.","['Bodhisattwa Prasad Majumder', 'Shuyang Li', 'Jianmo Ni', 'Julian McAuley']",https://www.aclweb.org/anthology/2020.emnlp-main.653.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.653/,1,3
Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.","['Nayu Liu', 'Xian Sun', 'Hongfeng Yu', 'Wenkai Zhang', 'Guangluan Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.144.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.144/,2,4
Supervised Seeded Iterated Learning for Interactive Language Learning,"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of in the language-drift translation game.","['Yuchen Lu', 'Soumye Singhal', 'Florian Strub', 'Olivier Pietquin', 'Aaron Courville']",https://www.aclweb.org/anthology/2020.emnlp-main.325.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.325/,2,5
Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization,"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.","['Zhenjie Zhao', 'Evangelos Papalexakis', 'Xiaojuan Ma']",https://www.aclweb.org/anthology/2020.emnlp-main.266.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.266/,4,8
Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact,"Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the tree model. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.",['Yugo Murawaki'],https://www.aclweb.org/anthology/2020.emnlp-main.69.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.69/,4,6
End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems,"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.","['Siamak Shakeri', 'Cicero dos Santos', 'Henghui Zhu', 'Patrick Ng', 'Feng Nan', 'Zhiguo Wang', 'Ramesh Nallapati', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.439.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.439/,2,5
Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise.","['Xiaoan Ding', 'Tianyu Liu', 'Baobao Chang', 'Zhifang Sui', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.657.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.657/,4,
Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions,"The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.","['Ritam Dutt', 'Rishabh Joshi', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.605.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.605/,4,
On the weak link between importance and prunability of attention heads,"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.","['Aakriti Budhraja', 'Madhura Pande', 'Preksha Nema', 'Pratyush Kumar', 'Mitesh M. Khapra']",https://www.aclweb.org/anthology/2020.emnlp-main.260.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.260/,4,10
Detecting Attackable Sentences in Arguments,"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.","['Yohan Jo', 'Seojin Bang', 'Emaad Manzoor', 'Eduard Hovy', 'Chris Reed']",https://www.aclweb.org/anthology/2020.emnlp-main.1.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.1/,3,4
Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity,"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user’s pre-existing knowledge. Assuming a correlation between engagement and user responses such as “liking” messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user’s prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a bert content model by 13 mean reciprocal rank points.","['Pedro Rodriguez', 'Paul A. Crook', 'Seungwhan Moon', 'Zhiguang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.655.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.655/,3,5
Personal Information Leakage Detection in Conversations,"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants. The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation. In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems. The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model. In those cases, a significant number of information leaking utterances can be detected by our models with high precision.","['Qiongkai Xu', 'Lizhen Qu', 'Zeyu Gao', 'Gholamreza Haffari']",https://www.aclweb.org/anthology/2020.emnlp-main.532.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.532/,4,10
Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models,"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.","['Pierangelo Lombardo', 'Alessio Boiardi', 'Luca Colombo', 'Angelo Schiavone', 'Nicolò Tamagnone']",https://www.aclweb.org/anthology/2020.emnlp-main.249.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.249/,6,10
Entities as Experts: Sparse Memory Access with Entity Supervision,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EaE’s entity representations are learned directly from text. We show that EaE’s learned representations capture sufficient knowledge to answer TriviaQA questions such as “Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?”, outperforming an encoder-generator Transformer model with 10x the parameters on this task. According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE’s performance.","['Thibault Févry', 'Livio Baldini Soares', 'Nicholas Fitzgerald', 'Eunsol Choi', 'Tom Kwiatkowski']",https://www.aclweb.org/anthology/2020.emnlp-main.400.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.400/,5,
Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis,"The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like “nothing special”. Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation “food-was” is treated equally as an adjectival complement relation “was-okay” in “food was okay”. To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs. Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information. Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs. Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs. Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.","['Mi Zhang', 'Tieyun Qian']",https://www.aclweb.org/anthology/2020.emnlp-main.286.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.286/,1,3
BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.","['Canwen Xu', 'Wangchunshu Zhou', 'Tao Ge', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.633.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.633/,6,4
Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example—the model’s confidence in the true class, and the variability of this confidence across epochs—obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of “ambiguous” regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are “easy to learn” for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds “hard to learn”; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","['Swabha Swayamdipta', 'Roy Schwartz', 'Nicholas Lourie', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Noah A. Smith', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.746.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.746/,10,6
Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems,"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.","['Jindřich Libovický', 'Alexander Fraser']",https://www.aclweb.org/anthology/2020.emnlp-main.203.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.203/,2,
Asking without Telling: Exploring Latent Ontologies in Contextual Representations,"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.","['Julian Michael', 'Jan A. Botha', 'Ian Tenney']",https://www.aclweb.org/anthology/2020.emnlp-main.552.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.552/,1,3
Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent,"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners’ performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent’s performance. To enable the agent to behave like a learner, we leverage entailment modeling’s capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning.","['Yun-Hsuan Jen', 'Chieh-Yang Huang', 'MeiHua Chen', 'Ting-Hao Huang', 'Lun-Wei Ku']",https://www.aclweb.org/anthology/2020.emnlp-main.312.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.312/,6,
Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.","['Amir Pouran Ben Veyseh', 'Nasim Nouri', 'Franck Dernoncourt', 'Dejing Dou', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.719.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.719/,1,
Towards More Accurate Uncertainty Estimation In Text Classification,"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as “mix-up”, “self-ensembling”, “distinctiveness score”, is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.","['Jianfeng He', 'Xuchao Zhang', 'Shuo Lei', 'Zhiqian Chen', 'Fanglan Chen', 'Abdulaziz Alhamadani', 'Bei Xiao', 'ChangTien Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.671.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.671/,4,
Iterative Domain-Repaired Back-Translation,"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.","['Hao-Ran Wei', 'Zhirui Zhang', 'Boxing Chen', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.474.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.474/,2,
Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations,"The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.","['Jianfei Yu', 'Jing Jiang', 'Ling Min Serena Khoo', 'Hai Leong Chieu', 'Rui Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.108.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.108/,7,
Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.","['Jiaxin Huang', 'Yu Meng', 'Fang Guo', 'Heng Ji', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.568.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.568/,3,
Retrofitting Structure-aware Transformer Language Model for End Tasks,"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.","['Hao Fei', 'Yafeng Ren', 'Donghong Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.168.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.168/,4,
Can Automatic Post-Editing Improve NMT?,"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.","['Shamil Chollampatt', 'Raymond Hendy Susanto', 'Liling Tan', 'Ewa Szymanska']",https://www.aclweb.org/anthology/2020.emnlp-main.217.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.217/,2,
Multi-Dimensional Gender Bias Classification,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","['Emily Dinan', 'Angela Fan', 'Ledell Wu', 'Jason Weston', 'Douwe Kiela', 'Adina Williams']",https://www.aclweb.org/anthology/2020.emnlp-main.23.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.23/,7,
Experience Grounds Language,"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.","['Yonatan Bisk', 'Ari Holtzman', 'Jesse Thomason', 'Jacob Andreas', 'Yoshua Bengio', 'Joyce Chai', 'Mirella Lapata', 'Angeliki Lazaridou', 'Jonathan May', 'Aleksandr Nisnevich', 'Nicolas Pinto', 'Joseph Turian']",https://www.aclweb.org/anthology/2020.emnlp-main.703.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.703/,1,
A Method for Building a Commonsense Inference Dataset based on Basic Events,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a high-performance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.","['Kazumasa Omura', 'Daisuke Kawahara', 'Sadao Kurohashi']",https://www.aclweb.org/anthology/2020.emnlp-main.192.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.192/,10,
Fortifying Toxic Speech Detectors Against Veiled Toxicity,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.","['Xiaochuang Han', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.622.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.622/,7,
Revealing the Myth of Higher-Order Inference in Coreference Resolution,"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.","['Liyan Xu', 'Jinho D. Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.686.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.686/,1,
Semantic Role Labeling as Syntactic Dependency Parsing,"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.","['Tianze Shi', 'Igor Malioutov', 'Ozan İrsoy']",https://www.aclweb.org/anthology/2020.emnlp-main.610.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.610/,1,
"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40,000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.","['AmirAli Bagher Zadeh', 'Yansheng Cao', 'Simon Hessner', 'Paul Pu Liang', 'Soujanya Poria', 'Louis-Philippe Morency']",https://www.aclweb.org/anthology/2020.emnlp-main.141.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.141/,10,
Which *BERT? A Survey Organizing Contextualized Encoders,"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.","['Patrick Xia', 'Shijie Wu', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.608.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.608/,4,
Adversarial Attack and Defense of Structured Prediction Models,"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.","['Wenjuan Han', 'Liwen Zhang', 'Yong Jiang', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.182.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.182/,4,
An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks,"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.","['Lifu Tu', 'Tianyu Liu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.449.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.449/,4,
COD3S: Diverse Generation with Discrete Semantic Signatures,"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.","['Nathaniel Weir', 'João Sedoc', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.421.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.421/,2,
Benchmarking Meaning Representations in Neural Semantic Parsing,"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work’s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose , a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets × four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.","['Jiaqi Guo', 'Qian Liu', 'Jian-Guang Lou', 'Zhenwen Li', 'Xueqing Liu', 'Tao Xie', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.118.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.118/,1,
From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers,"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.","['Anne Lauscher', 'Vinit Ravishankar', 'Ivan Vulić', 'Goran Glavaš']",https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.363/,2,
Improving AMR Parsing with Sequence-to-Sequence Pre-training,"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.","['Dongqin Xu', 'Junhui Li', 'Muhua Zhu', 'Min Zhang', 'Guodong Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.196.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.196/,8,
CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","['Nikita Nangia', 'Clara Vania', 'Rasika Bhalerao', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.154.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.154/,10,
DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset,"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.","['Lijie Wang', 'Ao Zhang', 'Kun Wu', 'Ke Sun', 'Zhenghua Li', 'Hua Wu', 'Min Zhang', 'Haifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.562.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.562/,10,
Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays,"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.","['Wei Song', 'Ziyao Song', 'Ruiji Fu', 'Lizhen Liu', 'Miaomiao Cheng', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.225.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.225/,3,
Causal Inference of Script Knowledge,"When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.","['Noah Weber', 'Rachel Rudinger', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.612.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.612/,8,
Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble,"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as “black boxes”. We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.","['Peerat Limkonchotiwat', 'Wannaphong Phatthiyaphaibun', 'Raheem Sarwar', 'Ekapol Chuangsuwanich', 'Sarana Nutanong']",https://www.aclweb.org/anthology/2020.emnlp-main.315.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.315/,4,
Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling,"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by ‘composing’ word representations of the first and last words in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.","['Diego Marcheggiani', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.322.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.322/,1,
Template Guided Text Generation for Task-Oriented Dialogue,"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.","['Mihir Kale', 'Abhinav Rastogi']",https://www.aclweb.org/anthology/2020.emnlp-main.527.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.527/,2,
An Analysis of Natural Language Inference Benchmarks through the Lens of Negation,"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.","['Md Mosharaf Hossain', 'Venelin Kovatchev', 'Pranoy Dutta', 'Tiffany Kao', 'Elizabeth Wei', 'Eduardo Blanco']",https://www.aclweb.org/anthology/2020.emnlp-main.732.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.732/,4,
Pareto Probing: Trading Off Accuracy for Complexity,"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe’s performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.","['Tiago Pimentel', 'Naomi Saphra', 'Adina Williams', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.254.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.254/,4,
Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder,"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.","['Wenyue Zhang', 'Xiaoli Li', 'Yang Li', 'Suge Wang', 'Deyu Li', 'Jian Liao', 'Jianxing Zheng']",https://www.aclweb.org/anthology/2020.emnlp-main.307.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.307/,3,
Analyzing Individual Neurons in Pre-trained Language Models,"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.","['Nadir Durrani', 'Hassan Sajjad', 'Fahim Dalvi', 'Yonatan Belinkov']",https://www.aclweb.org/anthology/2020.emnlp-main.395.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.395/,4,
End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning,"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.","['Zixiang Ding', 'Rui Xia', 'Jianfei Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.290.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.290/,3,
Simulated multiple reference training improves low-resource machine translation,"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser’s distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation.","['Huda Khayrallah', 'Brian Thompson', 'Matt Post', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.7.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.7/,2,
A Spectral Method for Unsupervised Multi-Document Summarization,"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.","['Kexiang Wang', 'Baobao Chang', 'Zhifang Sui']",https://www.aclweb.org/anthology/2020.emnlp-main.32.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.32/,2,
Efficient One-Pass End-to-End Entity Linking for Questions,"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.","['Belinda Z. Li', 'Sewon Min', 'Srinivasan Iyer', 'Yashar Mehdad', 'Wen-tau Yih']",https://www.aclweb.org/anthology/2020.emnlp-main.522.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.522/,5,
Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness,"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.","['Stefan Larson', 'Anthony Zheng', 'Anish Mahendran', 'Rishi Tekriwal', 'Adrian Cheung', 'Eric Guldan', 'Kevin Leach', 'Jonathan K. Kummerfeld']",https://www.aclweb.org/anthology/2020.emnlp-main.650.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.650/,4,
Scene Restoring for Narrative Machine Reading Comprehension,"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.","['Zhixing Tian', 'Yuanzhe Zhang', 'Kang Liu', 'Jun Zhao', 'Yantao Jia', 'Zhicheng Sheng']",https://www.aclweb.org/anthology/2020.emnlp-main.247.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.247/,5,
Dissecting Span Identification Tasks with Performance Prediction,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks’ properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.","['Sean Papay', 'Roman Klinger', 'Sebastian Padó']",https://www.aclweb.org/anthology/2020.emnlp-main.396.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.396/,1,
Pretrained Language Model Embryology: The Birth of ALBERT,"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.","['Cheng-Han Chiang', 'Sung-Feng Huang', 'Hung-Yi Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.553/,4,
Investigating representations of verb bias in neural language models,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker’s choice of construction is known to depend on multiple factors, including the choice of main verb – a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.","['Robert Hawkins', 'Takateru Yamakoshi', 'Thomas L. Griffiths', 'Adele Goldberg']",https://www.aclweb.org/anthology/2020.emnlp-main.376.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.376/,1,
Neural Deepfake Detection with Factual Structure of Text,"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.","['Wanjun Zhong', 'Duyu Tang', 'Zenan Xu', 'Ruize Wang', 'Nan Duan', 'Ming Zhou', 'Jiahai Wang', 'Jian Yin']",https://www.aclweb.org/anthology/2020.emnlp-main.193.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.193/,6,
Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection,"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies. In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments. We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection. Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.","['Jingfeng Yang', 'Diyi Yang', 'Zhaoran Ma']",https://www.aclweb.org/anthology/2020.emnlp-main.113.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.113/,2,
Semantic Evaluation for Text-to-SQL with Distilled Test Suites,"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.","['Ruiqi Zhong', 'Tao Yu', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.29.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.29/,1,
Modeling Content Importance for Summarization with Pre-trained Language Models,"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.","['Liqiang Xiao', 'Lu Wang', 'Hao He', 'Yaohui Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.293.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.293/,2,
EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering,"We propose EXAMS – a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.","['Momchil Hardalov', 'Todor Mihaylov', 'Dimitrina Zlatkova', 'Yoan Dinkov', 'Ivan Koychev', 'Preslav Nakov']",https://www.aclweb.org/anthology/2020.emnlp-main.438.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.438/,5,
Unsupervised Commonsense Question Answering with Self-Talk,"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as “what is the definition of...” to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.","['Vered Shwartz', 'Peter West', 'Ronan Le Bras', 'Chandra Bhagavatula', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.373.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.373/,5,
The Grammar of Emergent Languages,"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.","['Oskar van der Wal', 'Silvan de Boer', 'Elia Bruni', 'Dieuwke Hupkes']",https://www.aclweb.org/anthology/2020.emnlp-main.270.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.270/,1,
An Empirical Investigation of Contextualized Number Prediction,"We conduct a large scale empirical investigation of contextualized number prediction in running text. Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value within a sentence. We experiment with novel combinations of contextual encoders and output distributions over the real number line. Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures. We evaluate these models on two numeric datasets in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection. We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.","['Taylor Berg-Kirkpatrick', 'Daniel Spokoyny']",https://www.aclweb.org/anthology/2020.emnlp-main.385.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.385/,6,
Direct Segmentation Models for Streaming Speech Translation,"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account. This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk. An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.","['Javier Iranzo-Sánchez', 'Adrià Giménez Pastor', 'Joan Albert Silvestre-Cerdà', 'Pau Baquero-Arnal', 'Jorge Civera Saiz', 'Alfons Juan']",https://www.aclweb.org/anthology/2020.emnlp-main.206.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.206/,9,
Generating Fact Checking Briefs,"Fact checking at scale is difficult—while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs — in particular QABriefs — increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.","['Angela Fan', 'Aleksandra Piktus', 'Fabio Petroni', 'Guillaume Wenzek', 'Marzieh Saeidi', 'Andreas Vlachos', 'Antoine Bordes', 'Sebastian Riedel']",https://www.aclweb.org/anthology/2020.emnlp-main.580.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.580/,2,
Keep it Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit,"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.","['Amrith Krishna', 'Ashim Gupta', 'Deepak Garasangi', 'Pavankumar Satuluri', 'Pawan Goyal']",https://www.aclweb.org/anthology/2020.emnlp-main.388.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.388/,1,
Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders,"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.","['Jue Wang', 'Wei Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.133.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.133/,1,
"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions","We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.","['Xiang Zhou', 'Yixin Nie', 'Hao Tan', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.659.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.659/,10,
Knowledge Graph Alignment with Entity-Pair Embedding,"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations. Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.","['Zhichun Wang', 'Jinjian Yang', 'Xiaoju Ye']",https://www.aclweb.org/anthology/2020.emnlp-main.130.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.130/,8,
Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model,"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.","['Bugeun Kim', 'Kyung Seo Ki', 'Donggeon Lee', 'Gahgene Gweon']",https://www.aclweb.org/anthology/2020.emnlp-main.308.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.308/,6,
Unsupervised Discovery of Implicit Gender Bias,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.","['Anjalie Field', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.44.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.44/,7,
Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions,"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","['Arya D. McCarthy', 'Adina Williams', 'Shijia Liu', 'David Yarowsky', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.456.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.456/,1,
GLUCOSE: GeneraLized and COntextualized Story Explanations,"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of ~670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE’s rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans’ mental models.","['Nasrin Mostafazadeh', 'Aditya Kalyanpur', 'Lori Moon', 'David Buchanan', 'Lauren Berkowitz', 'Or Biran', 'Jennifer Chu-Carroll']",https://www.aclweb.org/anthology/2020.emnlp-main.370.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.370/,2,
"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.","['Zhiyuan Liu', 'Yixin Cao', 'Liangming Pan', 'Juanzi Li', 'Zhiyuan Liu', 'Tat-Seng Chua']",https://www.aclweb.org/anthology/2020.emnlp-main.515.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.515/,8,
Adversarial Semantic Collisions,"We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.","['Congzheng Song', 'Alexander M. Rush', 'Vitaly Shmatikov']",https://www.aclweb.org/anthology/2020.emnlp-main.344.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.344/,4,
MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.","['Andreas Rücklé', 'Jonas Pfeiffer', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.194.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.194/,5,
MLSUM: The Multilingual Summarization Corpus,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages – namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.","['Thomas Scialom', 'Paul-Alexis Dray', 'Sylvain Lamprier', 'Benjamin Piwowarski', 'Jacopo Staiano']",https://www.aclweb.org/anthology/2020.emnlp-main.647.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.647/,10,
Don’t Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings,"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.","['Phillip Keung', 'Yichao Lu', 'Julian Salazar', 'Vikas Bhardwaj']",https://www.aclweb.org/anthology/2020.emnlp-main.40.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.40/,2,
How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task. The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.","['Henry Elder', 'Alexander O’Connor', 'Jennifer Foster']",https://www.aclweb.org/anthology/2020.emnlp-main.230.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.230/,2,
Table Fact Verification with Structure-Aware Transformer,"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.","['Hongzhi Zhang', 'Yingyao Wang', 'Sirui Wang', 'Xuezhi Cao', 'Fuzheng Zhang', 'Zhongyuan Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.126.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.126/,6,
Global-to-Local Neural Networks for Document-Level Relation Extraction,"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.","['Difeng Wang', 'Wei Hu', 'Ermei Cao', 'Weijian Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.303.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.303/,8,
Help! Need Advice on Identifying Advice,"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research.","['Venkata Subrahmanyan Govindarajan', 'Benjamin Chen', 'Rebecca Warholic', 'Katrin Erk', 'Junyi Jessy Li']",https://www.aclweb.org/anthology/2020.emnlp-main.427.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.427/,7,
Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.","['Xiaomian Kang', 'Yang Zhao', 'Jiajun Zhang', 'Chengqing Zong']",https://www.aclweb.org/anthology/2020.emnlp-main.175.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.175/,2,
Stepwise Extractive Summarization and Planning with Structured Transformers,"We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.","['Shashi Narayan', 'Joshua Maynez', 'Jakub Adamek', 'Daniele Pighin', 'Blaz Bratanic', 'Ryan McDonald']",https://www.aclweb.org/anthology/2020.emnlp-main.339.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.339/,2,
Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT,"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.","['Rik van Noord', 'Antonio. Toral', 'Johan Bos']",https://www.aclweb.org/anthology/2020.emnlp-main.371.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.371/,1,
Investigating African-American Vernacular English in Transformer-Based Text Generation,"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to text corpora availability. We investigate the performance of GPT-2 on AAVE text by creating a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating syntactic structure and AAVE- or SAE-specific language for each pair. We evaluate each sample and its GPT-2 generated text with pretrained sentiment classifiers and find that while AAVE text results in more classifications of negative sentiment than SAE, the use of GPT-2 generally increases occurrences of positive sentiment for both. Additionally, we conduct human evaluation of AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall quality.","['Sophie Groenwold', 'Lily Ou', 'Aesha Parekh', 'Samhita Honnavalli', 'Sharon Levy', 'Diba Mirza', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.473.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.473/,2,
Please Mind the Root: Decoding Arborescences for Dependency Parsing,"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.","['Ran Zmigrod', 'Tim Vieira', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.390.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.390/,1,
SubjQA: A Dataset for Subjectivity and Review Comprehension,"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.","['Johannes Bjerva', 'Nikita Bhutani', 'Behzad Golshan', 'Wang-Chiew Tan', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.442.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.442/,10,
Look at the First Sentence: Position Bias in Question Answering,"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.","['Miyoung Ko', 'Jinhyuk Lee', 'Hyunjae Kim', 'Gangwoo Kim', 'Jaewoo Kang']",https://www.aclweb.org/anthology/2020.emnlp-main.84.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.84/,5,
MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.","['Anthony Chen', 'Gabriel Stanovsky', 'Sameer Singh', 'Matt Gardner']",https://www.aclweb.org/anthology/2020.emnlp-main.528.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.528/,10,
MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge,"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles. Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue. In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task. We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve). Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.","['Sopan Khosla', 'Shikhar Vashishth', 'Jill Fain Lehman', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.626.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.626/,8,
Connecting the Dots: Event Graph Schema Induction with Path Language Modeling,"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.","['Manling Li', 'Qi Zeng', 'Ying Lin', 'Kyunghyun Cho', 'Heng Ji', 'Jonathan May', 'Nathanael Chambers', 'Clare Voss']",https://www.aclweb.org/anthology/2020.emnlp-main.50.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.50/,6,
AnswerFact: Fact Checking in Product Question Answering,"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.","['Wenxuan Zhang', 'Yang Deng', 'Jing Ma', 'Wai Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.188.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.188/,5,
Surprisal Predicts Code-Switching in Chinese-English Bilingual Text,"Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese. The model includes known control variables together with word surprisal and word entropy. We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors. We also found sentence length to be a significant predictor, which has been related to sentence complexity. We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language. We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.","['Jesús Calvillo', 'Le Fang', 'Jeremy Cole', 'David Reitter']",https://www.aclweb.org/anthology/2020.emnlp-main.330.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.330/,2,
Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction,"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models. Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport. Experimental results on MUSE and VecMap datasets show significant improvement of our models. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed method.","['Xu Zhao', 'Zihao Wang', 'Hao Wu', 'Yong Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.238.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.238/,2,
"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow","We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.","['Li Zhang', 'Qing Lyu', 'Chris Callison-Burch']",https://www.aclweb.org/anthology/2020.emnlp-main.374.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.374/,5,
INSPIRED: Toward Sociable Recommendation Dialog Systems,"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories.","['Shirley Anugrah Hayati', 'Dongyeop Kang', 'Qingxiaoyang Zhu', 'Weiyan Shi', 'Zhou Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.654.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.654/,5,
Recurrent Event Network: Autoregressive Structure Inferenceover Temporal Knowledge Graphs,"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.","['Woojeong Jin', 'Meng Qu', 'Xisen Jin', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.541.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.541/,4,
CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.","['Ahmed El-Kishky', 'Vishrav Chaudhary', 'Francisco Guzmán', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.480.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.480/,10,
Coarse-to-Fine Query Focused Multi-Document Summarization,"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.","['Yumo Xu', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.296/,2,
A State-independent and Time-evolving Network for Early Rumor Detection in Social Media,"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event’s life cycle. Such coarse-grained methods failed to capture the event’s unique features in different states. To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation. Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events. For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction. This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection. Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems. We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.","['Rui Xia', 'Kaizhou Xuan', 'Jianfei Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.727.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.727/,7,
"BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations","Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.","['Aina Garí Soler', 'Marianna Apidianaki']",https://www.aclweb.org/anthology/2020.emnlp-main.598.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.598/,1,
The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual APIs,"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.","['Nitish Shirish Keskar', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",https://www.aclweb.org/anthology/2020.emnlp-main.501.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.501/,2,
RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling,"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.","['Jun Quan', 'Shian Zhang', 'Qian Cao', 'Zizhong Li', 'Deyi Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.67.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.67/,5,
X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset,"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.","['Angel Daza', 'Anette Frank']",https://www.aclweb.org/anthology/2020.emnlp-main.321.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.321/,10,
Probing Pretrained Language Models for Lexical Semantics,"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.","['Ivan Vulić', 'Edoardo Maria Ponti', 'Robert Litschko', 'Goran Glavaš', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.586/,1,
Mention Extraction and Linking for SQL Query Generation,"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.","['Jianqiang Ma', 'Zeyu Yan', 'Shuai Pang', 'Yang Zhang', 'Jianping Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.563.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.563/,8,
Solving Historical Dictionary Codes with a Neural Language Model,"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1% of the cipher-word tokens correctly.","['Christopher Chu', 'Raphael Valenti', 'Kevin Knight']",https://www.aclweb.org/anthology/2020.emnlp-main.471.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.471/,6,
Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.","['Yanlin Feng', 'Xinyue Chen', 'Bill Yuchen Lin', 'Peifeng Wang', 'Jun Yan', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.99.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.99/,5,
Multi-task Learning for Multilingual Neural Machine Translation,"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.","['Yiren Wang', 'ChengXiang Zhai', 'Hany Hassan']",https://www.aclweb.org/anthology/2020.emnlp-main.75.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.75/,2,
Learning from Context or Names? An Empirical Study on Neural Relation Extraction,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.","['Hao Peng', 'Tianyu Gao', 'Xu Han', 'Yankai Lin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.298.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.298/,8,
Seq2Edits: Sequence Transduction Using Span-level Edit Operations,"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","['Felix Stahlberg', 'Shankar Kumar']",https://www.aclweb.org/anthology/2020.emnlp-main.418.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.418/,4,
Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC contains over 98K explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: “X is a Y” AND “Y has Z” IMPLIES “X has Z”). We find that generalized chains maintain performance while also being more robust to certain perturbations.","['Harsh Jhamtani', 'Peter Clark']",https://www.aclweb.org/anthology/2020.emnlp-main.10.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.10/,5,
Pre-training Entity Relation Encoder with Intra-span and Inter-span Information,"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).","['Yijun Wang', 'Changzhi Sun', 'Yuanbin Wu', 'Junchi Yan', 'Peng Gao', 'Guotong Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.132.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.132/,8,
Calibration of Pre-trained Transformers,"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.","['Shrey Desai', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.21.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.21/,4,
Generating Dialogue Responses from a Semantic Latent Space,"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.","['Wei-Jen Ko', 'Avik Ray', 'Yilin Shen', 'Hongxia Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.352.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.352/,5,
Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people’s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","['Haochen Liu', 'Wentao Wang', 'Yiqi Wang', 'Hui Liu', 'Zitao Liu', 'Jiliang Tang']",https://www.aclweb.org/anthology/2020.emnlp-main.64.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.64/,2,
An Empirical Study of Pre-trained Transformers for Arabic Information Extraction,"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.","['Wuwei Lan', 'Yang Chen', 'Wei Xu', 'Alan Ritter']",https://www.aclweb.org/anthology/2020.emnlp-main.382.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.382/,8,
A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression,"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution – Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.","['Mingming Sun', 'Wenyue Hua', 'Zoey Liu', 'Xin Wang', 'Kangjie Zheng', 'Ping Li']",https://www.aclweb.org/anthology/2020.emnlp-main.167.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.167/,8,
TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue,"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.","['Chien-Sheng Wu', 'Steven C.H. Hoi', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.66.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.66/,5,
Learning from Task Descriptions,"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model’s ability to solve each task. Moreover, the dataset’s structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.","['Orion Weller', 'Nicholas Lourie', 'Matt Gardner', 'Matthew Peters']",https://www.aclweb.org/anthology/2020.emnlp-main.105.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.105/,8,
Question Directed Graph Attention Network for Numerical Reasoning over Text,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.","['Kunlong Chen', 'Weidi Xu', 'Xingyi Cheng', 'Zou Xiaochuan', 'Yuyu Zhang', 'Le Song', 'Taifeng Wang', 'Yuan Qi', 'Wei Chu']",https://www.aclweb.org/anthology/2020.emnlp-main.549.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.549/,6,
UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues,"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.","['Hung Le', 'Doyen Sahoo', 'Chenghao Liu', 'Nancy Chen', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.146.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.146/,5,
Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding,"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.","['Samson Tan', 'Shafiq Joty', 'Lav Varshney', 'Min-Yen Kan']",https://www.aclweb.org/anthology/2020.emnlp-main.455.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.455/,1,
Data and Representation for Turkish Natural Language Inference,"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.","['Emrah Budur', 'Rıza Özçelik', 'Tunga Güngör', 'Christopher Potts']",https://www.aclweb.org/anthology/2020.emnlp-main.662.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.662/,8,
Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction,"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner’s perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.","['Tara Safavi', 'Danai Koutra', 'Edgar Meij']",https://www.aclweb.org/anthology/2020.emnlp-main.667.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.667/,4,
XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.","['Edoardo Maria Ponti', 'Goran Glavaš', 'Olga Majewska', 'Qianchu Liu', 'Ivan Vulić', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.185.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.185/,10,
AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network,"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.","['Xinyu Wang', 'Yong Jiang', 'Nguyen Bach', 'Tao Wang', 'Zhongqiang Huang', 'Fei Huang', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.485.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.485/,4,
Fˆ2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax,"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, Fˆ2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. Fˆ2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.","['Byung-Ju Choi', 'Jimin Hong', 'David Park', 'Sang Wan Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.737.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.737/,2,
PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction,"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more proactive and powerful (“_He pursues his dream of being a doctor_”). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","['Xinyao Ma', 'Maarten Sap', 'Hannah Rashkin', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.602.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.602/,2,
How Much Knowledge Can You Pack Into the Parameters of a Language Model?,"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.","['Adam Roberts', 'Colin Raffel', 'Noam Shazeer']",https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.437/,4,
Hierarchical Graph Network for Multi-hop Question Answering,"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.","['Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Rohit Pillai', 'Shuohang Wang', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.710.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.710/,5,
Position-Aware Tagging for Aspect Sentiment Triplet Extraction,"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.","['Lu Xu', 'Hao Li', 'Wei Lu', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.183.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.183/,3,
Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze,"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.","['Ece Takmaz', 'Sandro Pezzelle', 'Lisa Beinborn', 'Raquel Fernández']",https://www.aclweb.org/anthology/2020.emnlp-main.377.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.377/,6,
RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark – Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.","['Tatiana Shavrina', 'Alena Fenogenova', 'Emelyanov Anton', 'Denis Shevelev', 'Ekaterina Artemova', 'Valentin Malykh', 'Vladislav Mikhailov', 'Maria Tikhonova', 'Andrey Chertok', 'Andrey Evlampiev']",https://www.aclweb.org/anthology/2020.emnlp-main.381.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.381/,10,
What do Models Learn from Question Answering Datasets?,"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter","['Priyanka Sen', 'Amir Saffari']",https://www.aclweb.org/anthology/2020.emnlp-main.190.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.190/,5,
Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.","['Nils Reimers', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.365.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.365/,2,
A Bilingual Generative Transformer for Semantic Sentence Embedding,"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.","['John Wieting', 'Graham Neubig', 'Taylor Berg-Kirkpatrick']",https://www.aclweb.org/anthology/2020.emnlp-main.122.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.122/,2,
KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks","['Fabio Massimo Zanzotto', 'Andrea Santilli', 'Leonardo Ranaldi', 'Dario Onorati', 'Pierfrancesco Tommasino', 'Francesca Fallucchi']",https://www.aclweb.org/anthology/2020.emnlp-main.18.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.18/,1,
Multi-Step Inference for Reasoning Over Paragraphs,"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.","['Jiangming Liu', 'Matt Gardner', 'Shay B. Cohen', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.245.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.245/,8,
Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots,"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.","['Yuanmeng Yan', 'Keqing He', 'Hong Xu', 'Sihong Liu', 'Fanyu Meng', 'Min Hu', 'Weiran Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.490.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.490/,1,
Modeling the Music Genre Perception across Language-Bound Cultures,"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.","['Elena V. Epure', 'Guillaume Salha', 'Manuel Moussallam', 'Romain Hennequin']",https://www.aclweb.org/anthology/2020.emnlp-main.386.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.386/,6,
Automatic Extraction of Rules Governing Morphological Agreement,"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/","['Aditi Chaudhary', 'Antonios Anastasopoulos', 'Adithya Pratapa', 'David R. Mortensen', 'Zaid Sheikh', 'Yulia Tsvetkov', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.422.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.422/,1,
Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.","['Brian Thompson', 'Matt Post']",https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.8/,2,
Word Frequency Does Not Predict Grammatical Knowledge in Language Models,"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks. Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.","['Charles Yu', 'Ryan Sie', 'Nicolas Tedeschi', 'Leon Bergen']",https://www.aclweb.org/anthology/2020.emnlp-main.331.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.331/,1,
EmoTag1200: Understanding the Association between Emojis and Emotions,"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available.","['Abu Awal Md Shoeb', 'Gerard de Melo']",https://www.aclweb.org/anthology/2020.emnlp-main.720.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.720/,10,
An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing,"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.","['Martin Schmitt', 'Sahand Sharifzadeh', 'Volker Tresp', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.577.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.577/,2,
On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment,"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers’ generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.","['Zirui Wang', 'Zachary C. Lipton', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.359/,2,
PRover: Proof Generation for Interpretable Reasoning over Rules,"Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for “depth 5”, indicating significant scope for future work.","['Swarnadeep Saha', 'Sayan Ghosh', 'Shashank Srivastava', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.9.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.9/,8,
BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance,"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model’s performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression","['Jianquan Li', 'Xiaokang Liu', 'Honghong Zhao', 'Ruifeng Xu', 'Min Yang', 'Yaohong Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.242.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.242/,4,
Assessing Phrasal Representation and Composition in Transformers,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.","['Lang Yu', 'Allyson Ettinger']",https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.397/,1,
Transformer Based Multi-Source Domain Adaptation,"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple source domains and must make predictions on a domain for which no labelled data has been seen. Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training, to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their performance, suggesting that large transformer-based models are already relatively robust across domains. Additionally, we show that mixture of experts leads to significant performance improvements by comparing several variants of mixing functions, including one novel metric based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective metrics for mixing their predictions.","['Dustin Wright', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.639.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.639/,4,
HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training,"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.","['Linjie Li', 'Yen-Chun Chen', 'Yu Cheng', 'Zhe Gan', 'Licheng Yu', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.161.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.161/,4,
With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation,"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.","['Bianca Scarlini', 'Tommaso Pasini', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.285.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.285/,1,
Less is More: Attention Supervision with Counterfactuals for Text Classification,"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.","['Seungtaek Choi', 'Haeju Park', 'Jinyoung Yeo', 'Seung-won Hwang']",https://www.aclweb.org/anthology/2020.emnlp-main.543.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.543/,4,
Be More with Less: Hypergraph Attention Networks for Inductive Text Classification,"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model – hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.","['Kaize Ding', 'Jianling Wang', 'Jundong Li', 'Dingcheng Li', 'Huan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.399.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.399/,4,
TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion,"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.","['Jiapeng Wu', 'Meng Cao', 'Jackie Chi Kit Cheung', 'William L. Hamilton']",https://www.aclweb.org/anthology/2020.emnlp-main.462.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.462/,4,
Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation,"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.","['Wenxiang Jiao', 'Xing Wang', 'Shilin He', 'Irwin King', 'Michael Lyu', 'Zhaopeng Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.176.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.176/,2,
Sound Natural: Content Rephrasing in Dialog Systems,"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user. For example, for queries like ‘ask my wife if she can pick up the kids’ or ‘remind me to take my pills’, we need to rephrase the content to ‘can you pick up the kids’ and ‘take your pills’. In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query. We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it. We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model","['Arash Einolghozati', 'Anchit Gupta', 'Keith Diedrick', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.414.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.414/,5,
UDapter: Language Adaptation for Truly Universal Dependency Parsing,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.","['Ahmet Üstün', 'Arianna Bisazza', 'Gosse Bouma', 'Gertjan Van Noord']",https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.180/,1,
Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages,"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.","['Michael A. Hedderich', 'David Adelani', 'Dawei Zhu', 'Jesujoba Alabi', 'Udia Markus', 'Dietrich Klakow']",https://www.aclweb.org/anthology/2020.emnlp-main.204.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.204/,2,
Inference Strategies for Machine Translation with Conditional Masking,"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks.","['Julia Kreutzer', 'George Foster', 'Colin Cherry']",https://www.aclweb.org/anthology/2020.emnlp-main.465.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.465/,2,
Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems,"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols’ semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.","['Jinghui Qin', 'Lihui Lin', 'Xiaodan Liang', 'Rumin Zhang', 'Liang Lin']",https://www.aclweb.org/anthology/2020.emnlp-main.309.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.309/,6,1
Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models,"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.","['Thuy Vu', 'Dinh Phung', 'Gholamreza Haffari']",https://www.aclweb.org/anthology/2020.emnlp-main.497.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.497/,4,
Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis,"The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation. To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper. Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling. Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.","['Chenggong Gong', 'Jianfei Yu', 'Rui Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.572.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.572/,3,
Self-Paced Learning for Neural Machine Translation,"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.","['Yu Wan', 'Baosong Yang', 'Derek F. Wong', 'Yikai Zhou', 'Lidia S. Chao', 'Haibo Zhang', 'Boxing Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.80.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.80/,2,
Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","['Francisco Vargas', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.232.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.232/,4,
"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.","['Yun He', 'Ziwei Zhu', 'Yin Zhang', 'Qin Chen', 'James Caverlee']",https://www.aclweb.org/anthology/2020.emnlp-main.372.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.372/,6,4
Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks,"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.","['Yufan Zhao', 'Can Xu', 'Wei Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.279.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.279/,2,
Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs,"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.",['Marius Pasca'],https://www.aclweb.org/anthology/2020.emnlp-main.503.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.503/,1,
Named Entity Recognition Only from Word Embeddings,"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features. However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.","['Ying Luo', 'Hai Zhao', 'Junlang Zhan']",https://www.aclweb.org/anthology/2020.emnlp-main.723.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.723/,4,
Exploring Contextualized Neural Language Models for Temporal Dependency Parsing,"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.","['Hayley Ross', 'Jonathon Cai', 'Bonan Min']",https://www.aclweb.org/anthology/2020.emnlp-main.689.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.689/,4,
Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes,"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes. The classification is carried out by mapping text embeddings to the word graph embeddings of the classes. Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved. In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.","['Paulo Cavalin', 'Victor Henrique Alves Ribeiro', 'Ana Appel', 'Claudio Pinhanez']",https://www.aclweb.org/anthology/2020.emnlp-main.324.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.324/,1,4
Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks,"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.","['Viet Dac Lai', 'Tuan Ngo Nguyen', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.435.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.435/,4,
Utility is in the Eye of the User: A Critique of NLP Leaderboards,"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards – in their current form – can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model’s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","['Kawin Ethayarajh', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.393.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.393/,4,
FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction,"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.","['Dianbo Sui', 'Yubo Chen', 'Jun Zhao', 'Yantao Jia', 'Yuantao Xie', 'Weijian Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.165/,4,
The role of context in neural pitch accent detection in English,"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.","['Elizabeth Nielsen', 'Mark Steedman', 'Sharon Goldwater']",https://www.aclweb.org/anthology/2020.emnlp-main.642.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.642/,9,
Monolingual Adapters for Zero-Shot Neural Machine Translation,"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.","['Jerin Philip', 'Alexandre Bérard', 'Matthias Gallé', 'Laurent Besacier']",https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.361/,2,
VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls,"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.","['Ramit Sawhney', 'Piyush Khanna', 'Arshiya Aggarwal', 'Taru Jain', 'Puneet Mathur', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.643.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.643/,6,
GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems,"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.","['Lishan Huang', 'Zheng Ye', 'Jinghui Qin', 'Liang Lin', 'Xiaodan Liang']",https://www.aclweb.org/anthology/2020.emnlp-main.742.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.742/,10,
Predicting Reference: What do Language Models Learn about Discourse Models?,"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.","['Shiva Upadhye', 'Leon Bergen', 'Andrew Kehler']",https://www.aclweb.org/anthology/2020.emnlp-main.70.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.70/,10,
Interpretable Multi-dataset Evaluation for Named Entity Recognition,"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval","['Jinlan Fu', 'Pengfei Liu', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.489.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.489/,10,
Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning,"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.","['Yuning Mao', 'Yanru Qu', 'Yiqing Xie', 'Xiang Ren', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.136.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.136/,2,
CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.","['Tara Safavi', 'Danai Koutra']",https://www.aclweb.org/anthology/2020.emnlp-main.669.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.669/,10,
Inquisitive Question Generation for High Level Text Comprehension,"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.","['Wei-Jen Ko', 'Te-yuan Chen', 'Yiyan Huang', 'Greg Durrett', 'Junyi Jessy Li']",https://www.aclweb.org/anthology/2020.emnlp-main.530.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.530/,10,
ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.","['Liying Cheng', 'Dekun Wu', 'Lidong Bing', 'Yan Zhang', 'Zhanming Jie', 'Wei Lu', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.90.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.90/,10,
F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","['Hendrik Schuff', 'Heike Adel', 'Ngoc Thang Vu']",https://www.aclweb.org/anthology/2020.emnlp-main.575.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.575/,5,
Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness,"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.","['Hyunwoo Kim', 'Byeongchang Kim', 'Gunhee Kim']",https://www.aclweb.org/anthology/2020.emnlp-main.65.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.65/,5,
A matter of framing: The impact of linguistic formalism on probing results,"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.","['Ilia Kuznetsov', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.13.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.13/,1,
What Can We Learn from Collective Human Opinions on Natural Language Inference Data?,"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in αNLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.","['Yixin Nie', 'Xiang Zhou', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.734.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.734/,10,
A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT,"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.","['Masaaki Nagata', 'Katsuki Chousa', 'Masaaki Nishino']",https://www.aclweb.org/anthology/2020.emnlp-main.41.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.41/,2,
Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning,"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.","['Hanlu Wu', 'Tengfei Ma', 'Lingfei Wu', 'Tariro Manyumwa', 'Shouling Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.294.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.294/,10,
Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning,"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to guide a sophisticated walk-based reinforcement learning (RL) model. An alternate approach is to use traditional symbolic methods (e.g., rule induction), which achieve good performance but can be hard to generalize due to the limitation of symbolic representation. In this paper, we propose RuleGuider, which leverages high-quality rules generated by symbolic-based methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets shows that RuleGuider clearly improves the performance of walk-based models without losing interpretability.","['Deren Lei', 'Gangrong Jiang', 'Xiaotao Gu', 'Kexuan Sun', 'Yuning Mao', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.688.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.688/,4,
Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding,"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.","['Jiaming Shen', 'Heng Ji', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.22.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.22/,1,
Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.","['Chengyu Wang', 'Minghui Qiu', 'Jun Huang', 'Xiaofeng He']",https://www.aclweb.org/anthology/2020.emnlp-main.250.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.250/,4,
A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis,"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.","['Zehui Dai', 'Cheng Peng', 'Huajie Chen', 'Yadong Ding']",https://www.aclweb.org/anthology/2020.emnlp-main.565.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.565/,3,
Quantifying Intimacy in Language,"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.","['Jiaxin Pei', 'David Jurgens']",https://www.aclweb.org/anthology/2020.emnlp-main.428.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.428/,7,
Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.","['Xiangji Zeng', 'Yunliang Li', 'Yuchen Zhai', 'Yin Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.590.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.590/,1,
Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation,"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.","['Minki Kang', 'Moonsu Han', 'Sung Ju Hwang']",https://www.aclweb.org/anthology/2020.emnlp-main.493.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.493/,10,
ConjNLI: Natural Language Inference Over Conjunctive Sentences,"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions (“and”, “or”, “but”, “nor”) with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions.","['Swarnadeep Saha', 'Yixin Nie', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.661.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.661/,1,
Condolence and Empathy in Online Communities,"Offering condolence is a natural reaction to hearing someone’s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal—trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.","['Naitian Zhou', 'David Jurgens']",https://www.aclweb.org/anthology/2020.emnlp-main.45.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.45/,7,10
Multi-Stage Pre-training for Low-Resource Domain Adaptation,"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.","['Rong Zhang', 'Revanth Gangi Reddy', 'Md Arafat Sultan', 'Vittorio Castelli', 'Anthony Ferritto', 'Radu Florian', 'Efsun Sarioglu Kayi', 'Salim Roukos', 'Avirup Sil', 'Todd Ward']",https://www.aclweb.org/anthology/2020.emnlp-main.440.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.440/,10,4
Knowledge Association with Hyperbolic Knowledge Graph Embeddings,"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.","['Zequn Sun', 'Muhao Chen', 'Wei Hu', 'Chengming Wang', 'Jian Dai', 'Wei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.460.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.460/,10,
Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task,"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.","['Dongyeop Kang', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.529.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.529/,1,
CancerEmo: A Dataset for Fine-Grained Emotion Detection,"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce CancerEmo, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.","['Tiberiu Sosea', 'Cornelia Caragea']",https://www.aclweb.org/anthology/2020.emnlp-main.715.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.715/,10,
TeaForN: Teacher-Forcing with N-grams,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.","['Sebastian Goodman', 'Nan Ding', 'Radu Soricut']",https://www.aclweb.org/anthology/2020.emnlp-main.702.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.702/,4,
BAE: BERT-based Adversarial Examples for Text Classification,"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.","['Siddhant Garg', 'Goutham Ramakrishnan']",https://www.aclweb.org/anthology/2020.emnlp-main.498.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.498/,1,
SetConv: A New Approach for Learning from Imbalanced Data,"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.","['Yang Gao', 'Yi-Fan Li', 'Yu Lin', 'Charu Aggarwal', 'Latifur Khan']",https://www.aclweb.org/anthology/2020.emnlp-main.98.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.98/,4,
AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue,"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.","['Jaehun Jung', 'Bokyung Son', 'Sungwon Lyu']",https://www.aclweb.org/anthology/2020.emnlp-main.280.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.280/,5,
Keep CALM and Explore: Language Models for Action Generation in Text-based Games,"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame.","['Shunyu Yao', 'Rohan Rao', 'Matthew Hausknecht', 'Karthik Narasimhan']",https://www.aclweb.org/anthology/2020.emnlp-main.704.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.704/,6,
RethinkCWS: Is Chinese Word Segmentation a Solved Task?,"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user’s models: https://github.com/neulab/InterpretEval","['Jinlan Fu', 'Pengfei Liu', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.457.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.457/,10,
Task-oriented Domain-specific Meta-Embedding for Text Classification,"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains. Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem. In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings. We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.","['Xin Wu', 'Yi Cai', 'Yang Kai', 'Tao Wang', 'Qing Li']",https://www.aclweb.org/anthology/2020.emnlp-main.282.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.282/,1,
Improving Word Sense Disambiguation with Translations,"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation. Since our approach is language independent, we perform WSD experiments on several languages. The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD. To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.","['Yixing Luan', 'Bradley Hauer', 'Lili Mou', 'Grzegorz Kondrak']",https://www.aclweb.org/anthology/2020.emnlp-main.332.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.332/,2,4
STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering,"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.","['Hrituraj Singh', 'Sumit Shekhar']",https://www.aclweb.org/anthology/2020.emnlp-main.264.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.264/,5,6
Design Challenges in Low-resource Cross-lingual Entity Linking,"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia’s interlanguage links and thus suffer when the foreign language’s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.","['Xingyu Fu', 'Weijia Shi', 'Xiaodong Yu', 'Zian Zhao', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.521.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.521/,10,2
COMETA: A Corpus for Medical Entity Linking in the Social Media,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman’s language. Meanwhile, there is a growing need for applications that can understand the public’s voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.","['Marco Basaldella', 'Fangyu Liu', 'Ehsan Shareghi', 'Nigel Collier']",https://www.aclweb.org/anthology/2020.emnlp-main.253.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.253/,10,
Suicidal Risk Detection for Military Personnel,"We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q&A site, we collect posts containing military-relevant content written by active-duty military personnel. We then annotate the posts with two groups of experts: military experts and mental health experts. Our dataset includes 2,791 posts with 13,955 corresponding expert annotations of suicidal risk levels, and this dataset is available to researchers who consent to research ethics agreement. Using various fine-tuned state-of-the-art language models, we predict the level of suicide risk, reaching .88 F1 score for classifying the risks.","['Sungjoon Park', 'Kiwoong Park', 'Jaimeen Ahn', 'Alice Oh']",https://www.aclweb.org/anthology/2020.emnlp-main.198.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.198/,7,10
Modularized Syntactic Neural Networks for Sentence Classification,"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each node of a syntax tree is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation. We design a tree-parallel mini-batch strategy for efficient training and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.","['Haiyan Wu', 'Ying Liu', 'Shaoyun Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.222.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.222/,1,4
CHARM: Inferring Personal Attributes from Conversations,"Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.","['Anna Tigunova', 'Andrew Yates', 'Paramita Mirza', 'Gerhard Weikum']",https://www.aclweb.org/anthology/2020.emnlp-main.434.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.434/,4,8
Cross-lingual Spoken Language Understanding with Regularized Representation Alignment,"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.","['Zihan Liu', 'Genta Indra Winata', 'Peng Xu', 'Zhaojiang Lin', 'Pascale Fung']",https://www.aclweb.org/anthology/2020.emnlp-main.587.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.587/,2,
Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.","['Alex Warstadt', 'Yian Zhang', 'Xiaocheng Li', 'Haokun Liu', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.16.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.16/,10,
Uncertainty-Aware Semantic Augmentation for Neural Machine Translation,"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.","['Xiangpeng Wei', 'Heng Yu', 'Yue Hu', 'Rongxiang Weng', 'Luxi Xing', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.216.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.216/,8,2
Human-centric dialog training via offline reinforcement learning,"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.","['Natasha Jaques', 'Judy Hanwen Shen', 'Asma Ghandeharioun', 'Craig Ferguson', 'Agata Lapedriza', 'Noah Jones', 'Shixiang Gu', 'Rosalind Picard']",https://www.aclweb.org/anthology/2020.emnlp-main.327.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.327/,5,
Double Graph Based Reasoning for Document-level Relation Extraction,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.","['Shuang Zeng', 'Runxin Xu', 'Baobao Chang', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.127.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.127/,8,
An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels,"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.","['Ilias Chalkidis', 'Manos Fergadiotis', 'Sotiris Kotitsas', 'Prodromos Malakasiotis', 'Nikolaos Aletras', 'Ion Androutsopoulos']",https://www.aclweb.org/anthology/2020.emnlp-main.607.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.607/,10,
Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.","['Ramy Eskander', 'Smaranda Muresan', 'Michael Collins']",https://www.aclweb.org/anthology/2020.emnlp-main.391.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.391/,2,4
IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system’s performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.","['James Ferguson', 'Matt Gardner', 'Hannaneh Hajishirzi', 'Tushar Khot', 'Pradeep Dasigi']",https://www.aclweb.org/anthology/2020.emnlp-main.86.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.86/,10,
Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The “omni-directional” BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.","['Brielen Madureira', 'David Schlangen']",https://www.aclweb.org/anthology/2020.emnlp-main.26.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.26/,10,
Parsing Gapping Constructions Based on Grammatical and Semantic Roles,A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.,"['Yoshihide Kato', 'Shigeki Matsubara']",https://www.aclweb.org/anthology/2020.emnlp-main.218.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.218/,1,
Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses,"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.","['Prathyusha Jwalapuram', 'Shafiq Joty', 'Youlin Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.177.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.177/,2,
Intrinsic Evaluation of Summarization Datasets,"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.","['Rishi Bommasani', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.649/,10,
Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing,"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.","['Xilun Chen', 'Asish Ghoshal', 'Yashar Mehdad', 'Luke Zettlemoyer', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.413.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.413/,1,4
SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge,"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.","['Pei Ke', 'Haozhe Ji', 'Siyang Liu', 'Xiaoyan Zhu', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.567.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.567/,3,1
"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation","In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.","['Yaobo Liang', 'Nan Duan', 'Yeyun Gong', 'Ning Wu', 'Fenfei Guo', 'Weizhen Qi', 'Ming Gong', 'Linjun Shou', 'Daxin Jiang', 'Guihong Cao', 'Xiaodong Fan', 'Ruofei Zhang', 'Rahul Agrawal', 'Edward Cui', 'Sining Wei', 'Taroon Bharti', 'Ying Qiao', 'Jiun-Hung Chen', 'Winnie Wu', 'Shuguang Liu', 'Fan Yang', 'Daniel Campos', 'Rangan Majumder', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.484.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.484/,10,
"Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis","Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect’s sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models’ performance on ARTS by up to 32.85%. Our code and new test set are available at https://github.com/zhijing-jin/ARTS_TestSet","['Xiaoyu Xing', 'Zhijing Jin', 'Di Jin', 'Bingning Wang', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.292.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.292/,10,3
Neural Conversational QA: Learning to Reason vs Exploiting Patterns,"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based program, built to exploit these patterns, had comparative performance to that of the neural models. In this paper we share our findings about the four types of patterns in the ShARC corpus and how the neural models exploit them. Motivated by the above findings, we create and share a modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.","['Nikhil Verma', 'Abhishek Sharma', 'Dhiraj Madan', 'Danish Contractor', 'Harshit Kumar', 'Sachindra Joshi']",https://www.aclweb.org/anthology/2020.emnlp-main.589.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.589/,10,5
Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis,"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.","['Yuncong Li', 'Cunxiang Yin', 'Sheng-hua Zhong', 'Xu Pan']",https://www.aclweb.org/anthology/2020.emnlp-main.287.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.287/,3,
On the Role of Supervision in Unsupervised Constituency Parsing,"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Few-shot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.","['Haoyue Shi', 'Karen Livescu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.614.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.614/,4,
AxCell: Automatic Extraction of Results from Machine Learning Papers,"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.","['Marcin Kardas', 'Piotr Czapla', 'Pontus Stenetorp', 'Sebastian Ruder', 'Sebastian Riedel', 'Ross Taylor', 'Robert Stojnic']",https://www.aclweb.org/anthology/2020.emnlp-main.692.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.692/,8,6
The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection,"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.","['Zibo Lin', 'Deng Cai', 'Yan Wang', 'Xiaojiang Liu', 'Haitao Zheng', 'Shuming Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.741.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.741/,10,5
Knowledge-Grounded Dialogue Generation with Pre-trained Language Models,"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.","['Xueliang Zhao', 'Wei Wu', 'Can Xu', 'Chongyang Tao', 'Dongyan Zhao', 'Rui Yan']",https://www.aclweb.org/anthology/2020.emnlp-main.272.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.272/,5,
Evaluating the Factual Consistency of Abstractive Text Summarization,"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.","['Wojciech Kryściński', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",https://www.aclweb.org/anthology/2020.emnlp-main.750.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.750/,10,2
MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models,"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","['Peng Xu', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Raul Puri', 'Pascale Fung', 'Animashree Anandkumar', 'Bryan Catanzaro']",https://www.aclweb.org/anthology/2020.emnlp-main.226.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.226/,2,
SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning,"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.","['Tsu-Jui Fu', 'Xin Wang', 'Scott Grafton', 'Miguel Eckstein', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.357.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.357/,6,4
Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data,"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.","['Lingkai Kong', 'Haoming Jiang', 'Yuchen Zhuang', 'Jie Lyu', 'Tuo Zhao', 'Chao Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.102.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.102/,4,
Exploring Semantic Capacity of Terms,"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.","['Jie Huang', 'Zilong Wang', 'Kevin Chang', 'Wen-Mei Hwu', 'JinJun Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.684.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.684/,1,
Attention Is All You Need for Chinese Word Segmentation,"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.","['Sufeng Duan', 'Hai Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.317.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.317/,4,1
Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News,"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users’ consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters’ followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.","['Nguyen Vo', 'Kyumin Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.621.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.621/,7,6
Parallel Interactive Networks for Multi-Domain Dialogue State Generation,"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.","['Junfan Chen', 'Richong Zhang', 'Yongyi Mao', 'Jie Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.151.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.151/,5,
The Multilingual Amazon Reviews Corpus,"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.","['Phillip Keung', 'Yichao Lu', 'György Szarvas', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.369.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.369/,2,4
Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.","['Loitongbam Gyanendro Singh', 'Anasua Mitra', 'Sanasam Ranbir Singh']",https://www.aclweb.org/anthology/2020.emnlp-main.718.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.718/,3,
Visually Grounded Continual Learning of Compositional Phrases,"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work.","['Xisen Jin', 'Junyi Du', 'Arka Sadhu', 'Ram Nevatia', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.158.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.158/,7,4
Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.","['Miguel Ballesteros', 'Rishita Anubhai', 'Shuai Wang', 'Nima Pourdamghani', 'Yogarshi Vyas', 'Jie Ma', 'Parminder Bhatia', 'Kathleen McKeown', 'Yaser Al-Onaizan']",https://www.aclweb.org/anthology/2020.emnlp-main.436.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.436/,4,1
How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.","['Nicola De Cao', 'Michael Sejr Schlichtkrull', 'Wilker Aziz', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.262.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.262/,4,
Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.","['Mengjie Zhao', 'Tao Lin', 'Fei Mi', 'Martin Jaggi', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.174/,4,
Incremental Event Detection via Knowledge Consolidation Networks,"Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.","['Pengfei Cao', 'Yubo Chen', 'Jun Zhao', 'Taifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.52.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.52/,8,
Inducing Target-Specific Latent Structures for Aspect Sentiment Classification,"Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our model can complement supervised syntactic features with latent semantic dependencies. Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.","['Chenhua Chen', 'Zhiyang Teng', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.451.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.451/,3,4
TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization,"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as ‘London’ and ‘Paris’ with different expressions: “the major cities”, “the capital cities” and “two European cities”. Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.","['Clément Jumel', 'Annie Louis', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.646.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.646/,8,
TernaryBERT: Distillation-aware Ultra-low Bit BERT,"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.","['Wei Zhang', 'Lu Hou', 'Yichun Yin', 'Lifeng Shang', 'Xiao Chen', 'Xin Jiang', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.37.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.37/,4,
Generating Label Cohesive and Well-Formed Adversarial Claims,"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.","['Pepa Atanasova', 'Dustin Wright', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.256.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.256/,4,
Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.","['Xiaoxiao Guo', 'Mo Yu', 'Yupeng Gao', 'Chuang Gan', 'Murray Campbell', 'Shiyu Chang']",https://www.aclweb.org/anthology/2020.emnlp-main.624.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.624/,6,2
PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.","['Hannah Rashkin', 'Asli Celikyilmaz', 'Yejin Choi', 'Jianfeng Gao']",https://www.aclweb.org/anthology/2020.emnlp-main.349.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.349/,2,4
Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme,"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26% (p<0.001) in F1 measure.","['Chaofa Yuan', 'Chuang Fan', 'Jianzhu Bao', 'Ruifeng Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.289.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.289/,8,3
Re-examining the Role of Schema Linking in Text-to-SQL,"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking. We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider despite its structural simplicity. Many remaining errors are attributable to corpus noise. This suggests schema linking is the crux for the current text-to-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks.","['Wenqiang Lei', 'Weixin Wang', 'Zhixin Ma', 'Tian Gan', 'Wei Lu', 'Min-Yen Kan', 'Tat-Seng Chua']",https://www.aclweb.org/anthology/2020.emnlp-main.564.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.564/,10,
Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors,"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.","['Sida Gao', 'Matthew R. Gormley']",https://www.aclweb.org/anthology/2020.emnlp-main.406.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.406/,4,
Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization,"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.","['Jiaao Chen', 'Diyi Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.336.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.336/,2,4
Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering,"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","['Yuxiang Wu', 'Sebastian Riedel', 'Pasquale Minervini', 'Pontus Stenetorp']",https://www.aclweb.org/anthology/2020.emnlp-main.244.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.244/,5,
Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification,"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.","['Yunjie Ji', 'Hao Liu', 'Bolei He', 'Xinyan Xiao', 'Hua Wu', 'Yanhua Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.570.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.570/,3,
Dynamic Data Selection and Weighting for Iterative Back-Translation,"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.","['Zi-Yi Dou', 'Antonios Anastasopoulos', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.475.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.475/,2,
TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4-way classification is 60%. This is a dramatic drop of 35% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines will be made freely available.","['Wanqiu Long', 'Bonnie Webber', 'Deyi Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.223.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.223/,10,5
Local Additivity Based Data Augmentation for Semi-supervised NER,"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA","['Jiaao Chen', 'Zhenghui Wang', 'Ran Tian', 'Zichao Yang', 'Diyi Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.95.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.95/,8,
Chapter Captor: Text Segmentation in Novels,"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.","['Charuta Pethe', 'Allen Kim', 'Steven Skiena']",https://www.aclweb.org/anthology/2020.emnlp-main.672.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.672/,6,
Learning Adaptive Segmentation Policy for Simultaneous Translation,"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.","['Ruiqing Zhang', 'Chuanqiang Zhang', 'Zhongjun He', 'Hua Wu', 'Haifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.178.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.178/,2,
Train No Evil: Selective Masking for Task-Guided Pre-Training,"Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient. The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.","['Yuxian Gu', 'Zhengyan Zhang', 'Xiaozhi Wang', 'Zhiyuan Liu', 'Maosong Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.566.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.566/,4,
Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification,"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.","['Prithviraj Sen', 'Marina Danilevsky', 'Yunyao Li', 'Siddhartha Brahma', 'Matthias Boehm', 'Laura Chiticariu', 'Rajasekar Krishnamurthy']",https://www.aclweb.org/anthology/2020.emnlp-main.345.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.345/,1,4
A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving,"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on local features while neglecting global information. To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph. Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations. Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information. Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.","['Qinzhuo Wu', 'Qi Zhang', 'Jinlan Fu', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.579.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.579/,6,
IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation,"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.","['Yitao Cai', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.560.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.560/,10,
Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model’s representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model’s expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.","['Bang An', 'Jie Lyu', 'Zhenyi Wang', 'Chunyuan Li', 'Changwei Hu', 'Fei Tan', 'Ruiyi Zhang', 'Yifan Hu', 'Changyou Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.17.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.17/,4,
"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source wide-coverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.","['Valentina Pyatkin', 'Ayal Klein', 'Reut Tsarfaty', 'Ido Dagan']",https://www.aclweb.org/anthology/2020.emnlp-main.224.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.224/,5,
Efficient Meta Lifelong-Learning with Limited Memory,"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.","['Zirui Wang', 'Sanket Vaibhav Mehta', 'Barnabás Poczós', 'Jaime G. Carbonell']",https://www.aclweb.org/anthology/2020.emnlp-main.39.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.39/,4,
Conversational Semantic Parsing for Dialog State Tracking,"We consider a new perspective on dialog state tracking (DST), the task of estimating a user’s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to ~20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.","['Jianpeng Cheng', 'Devang Agrawal', 'Héctor Martínez Alonso', 'Shruti Bhargava', 'Joris Driesen', 'Federico Flego', 'Dain Kaplan', 'Dimitri Kartsaklis', 'Lin Li', 'Dhivya Piraviperumal', 'Jason D. Williams', 'Hong Yu', 'Diarmuid Ó Séaghdha', 'Anders Johannsen']",https://www.aclweb.org/anthology/2020.emnlp-main.651.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.651/,5,
Non-Autoregressive Machine Translation with Latent Alignments,"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates. In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline. Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model. On the competitive WMT’14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.","['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Mohammad Norouzi']",https://www.aclweb.org/anthology/2020.emnlp-main.83.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.83/,2,
A Streaming Approach For Efficient Batched Beam Search,"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines’ BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.","['Kevin Yang', 'Violet Yao', 'John DeNero', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.366.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.366/,4,
Incorporating a Local Translation Mechanism into Non-autoregressive Translation,"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.","['Xiang Kong', 'Zhisong Zhang', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.79.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.79/,2,
Exploring Logically Dependent Multi-task Learning with Causal Inference,"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks. Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks. In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models. We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors. In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL. We conduct experiments on two English datasets and one Chinese dataset. Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions’ consistency.","['Wenqing Chen', 'Jidong Tian', 'Liqiang Xiao', 'Hao He', 'Yaohui Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.173.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.173/,4,
AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data,"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.","['Silei Xu', 'Sina Semnani', 'Giovanni Campagna', 'Monica Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.31.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.31/,10,5
Querying Across Genres for Medical Claims in News,"We present a query-based biomedical information retrieval task across two vastly different genres – newswire and research literature – where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.","['Chaoyuan Zuo', 'Narayan Acharya', 'Ritwik Banerjee']",https://www.aclweb.org/anthology/2020.emnlp-main.139.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.139/,6,
Improving Low Compute Language Modeling with In-Domain Embedding Initialisation,"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","['Charles Welch', 'Rada Mihalcea', 'Jonathan K. Kummerfeld']",https://www.aclweb.org/anthology/2020.emnlp-main.696.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.696/,4,
Reactive Supervision: A New Method for Collecting Sarcasm Data,"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.","['Boaz Shmueli', 'Lun-Wei Ku', 'Soumya Ray']",https://www.aclweb.org/anthology/2020.emnlp-main.201.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.201/,7,10
Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings,"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We release these dictionaries to the research community. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60%. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.","['Naoki Otani', 'Satoru Ozaki', 'Xingyuan Zhao', 'Yucen Li', 'Micaelah St Johns', 'Lori Levin']",https://www.aclweb.org/anthology/2020.emnlp-main.360.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.360/,2,
Introducing a New Dataset for Event Detection in Cybersecurity Texts,"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.","['Hieu Man Duc Trong', 'Duc-Trong Le', 'Amir Pouran Ben Veyseh', 'Thuat Nguyen', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.433.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.433/,6,
"Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts","Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.","['Ece Takmaz', 'Mario Giulianelli', 'Sandro Pezzelle', 'Arabella Sinclair', 'Raquel Fernández']",https://www.aclweb.org/anthology/2020.emnlp-main.353.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.353/,9,2
Simultaneous Machine Translation with Visual Context,"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.","['Ozan Caglayan', 'Julia Ive', 'Veneta Haralampieva', 'Pranava Swaroop Madhyastha', 'Loïc Barrault', 'Lucia Specia']",https://www.aclweb.org/anthology/2020.emnlp-main.184.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.184/,2,
Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.","['Suzanna Sia', 'Ayush Dalmia', 'Sabrina J. Mielke']",https://www.aclweb.org/anthology/2020.emnlp-main.135.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.135/,4,
Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.","['Eleftheria Briakou', 'Marine Carpuat']",https://www.aclweb.org/anthology/2020.emnlp-main.121.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.121/,2,
"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision","Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named “vokenization” that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call “vokens”). The “vokenizer” is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.","['Hao Tan', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.162.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.162/,9,
VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling,"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, “Cambridge” and the first non-English corpus “Robert”, which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.","['Machel Reid', 'Edison Marrese-Taylor', 'Yutaka Matsuo']",https://www.aclweb.org/anthology/2020.emnlp-main.513.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.513/,2,6
COMET: A Neural Framework for MT Evaluation,"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.","['Ricardo Rei', 'Craig Stewart', 'Ana C Farinha', 'Alon Lavie']",https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.213/,2,
Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.","['Chris Kedzie', 'Kathleen McKeown']",https://www.aclweb.org/anthology/2020.emnlp-main.419.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.419/,4,2
Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning,"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.","['Ye Liu', 'Sheng Zhang', 'Rui Song', 'Suo Feng', 'Yanghua Xiao']",https://www.aclweb.org/anthology/2020.emnlp-main.693.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.693/,8,
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation","AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.","['Yan Zhang', 'Zhijiang Guo', 'Zhiyang Teng', 'Wei Lu', 'Shay B. Cohen', 'Zuozhu Liu', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.169.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.169/,8,
Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88% novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.","['Tuhin Chakrabarty', 'Smaranda Muresan', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.524.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.524/,2,6
Generating Radiology Reports via Memory-driven Transformer,"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.","['Zhihong Chen', 'Yan Song', 'Tsung-Hui Chang', 'Xiang Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.112/,6,
RNNs can generate bounded hierarchical languages with optimal memory,"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-(k,m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use O(km⁄2) memory (hidden units) to generate these languages. We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.","['John Hewitt', 'Michael Hahn', 'Surya Ganguli', 'Percy Liang', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.156.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.156/,4,
Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction,"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding. Effective designs encode within the layout and formatting signals that point to where the important information can be found. In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection. Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these features function within the model.","['Yansen Wang', 'Zhen Fan', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.140.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.140/,8,9
doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset,"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.","['Song Feng', 'Hui Wan', 'Chulaka Gunasekara', 'Siva Patel', 'Sachindra Joshi', 'Luis Lastras']",https://www.aclweb.org/anthology/2020.emnlp-main.652.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.652/,10,5
Evaluating and Characterizing Human Rationales,"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using “fidelity curves” to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.","['Samuel Carton', 'Anirudh Rathore', 'Chenhao Tan']",https://www.aclweb.org/anthology/2020.emnlp-main.747.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.747/,10,
Understanding Neural Abstractive Summarization Models via Uncertainty,"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model’s token-level predictions. For two strong pre-trained models, PEGASUS and BART on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder’s uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model’s next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.","['Jiacheng Xu', 'Shrey Desai', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.508.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.508/,10,
Online Back-Parsing for AMR-to-Text Generation,"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.","['Xuefeng Bai', 'Linfeng Song', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.92.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.92/,8,2
Towards Debiasing NLU Models from Unknown Biases,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.","['Prasetya Ajie Utama', 'Nafise Sadat Moosavi', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.613.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.613/,4,
Precise Task Formalization Matters in Winograd Schema Evaluations,"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalization—the combination of input specification, loss function, and reuse of pretrained parameters—by users of the dataset, rather than improvements in the pretrained model’s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model’s extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.","['Haokun Liu', 'William Huang', 'Dhara Mungra', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.664.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.664/,10,
Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,"Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at https://github.com/JiaweiSheng/FAAN.","['Jiawei Sheng', 'Shu Guo', 'Zhenyu Chen', 'Juwei Yue', 'Lihong Wang', 'Tingwen Liu', 'Hongbo Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.131.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.131/,8,
Coreferential Reasoning Learning for Language Representation,"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.","['Deming Ye', 'Yankai Lin', 'Jiaju Du', 'Zhenghao Liu', 'Peng Li', 'Maosong Sun', 'Zhiyuan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.582.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.582/,1,
Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network,"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others’ responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.","['Sihan Wang', 'Kaijie Zhou', 'Kunfeng Lai', 'Jianping Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.278.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.278/,5,
Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection,"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.","['Ruize Wang', 'Duyu Tang', 'Nan Duan', 'Wanjun Zhong', 'Zhongyu Wei', 'Xuan-Jing Huang', 'Daxin Jiang', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.320.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.320/,6,
Learning to Represent Image and Text with Denotation Graph,"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.","['Bowen Zhang', 'Hexiang Hu', 'Vihan Jain', 'Eugene Ie', 'Fei Sha']",https://www.aclweb.org/anthology/2020.emnlp-main.60.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.60/,9,
Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT,"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.","['Alexandra Chronopoulou', 'Dario Stojanovski', 'Alexander Fraser']",https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.214/,2,
Incorporating Behavioral Hypotheses for Query Generation,"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.","['Ruey-Cheng Chen', 'Chia-Jung Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.251.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.251/,4,
Coding Textual Inputs Boosts the Accuracy of Neural Networks,"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding. Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs. Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging. The source code is available at https://github.com/abdulrafae/coding_nmt.","['Abdul Rafae Khan', 'Jia Xu', 'Weiwei Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.104.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.104/,8,
PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation,"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context. This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.","['Bin Bi', 'Chenliang Li', 'Chen Wu', 'Ming Yan', 'Wei Wang', 'Songfang Huang', 'Fei Huang', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.700.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.700/,4,
MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision,"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.","['Patrick Huber', 'Giuseppe Carenini']",https://www.aclweb.org/anthology/2020.emnlp-main.603.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.603/,1,
Multilingual AMR-to-Text Generation,"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics. We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.","['Angela Fan', 'Claire Gardent']",https://www.aclweb.org/anthology/2020.emnlp-main.231.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.231/,2,
Message Passing for Hyper-Relational Knowledge Graphs,"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.","['Mikhail Galkin', 'Priyansh Trivedi', 'Gaurav Maheshwari', 'Ricardo Usbeck', 'Jens Lehmann']",https://www.aclweb.org/anthology/2020.emnlp-main.596.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.596/,10,
Semi-supervised New Event Type Induction and Event Detection,"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.","['Lifu Huang', 'Heng Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.53.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.53/,8,
Do sequence-to-sequence VAEs learn global features of sentences?,"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.","['Tom Bosc', 'Pascal Vincent']",https://www.aclweb.org/anthology/2020.emnlp-main.350.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.350/,4,
Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis,"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.","['Yao-Hung Hubert Tsai', 'Martin Ma', 'Muqiao Yang', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency']",https://www.aclweb.org/anthology/2020.emnlp-main.143.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.143/,9,
What time is it? Temporal Analysis of Novels,"Recognizing the flow of time in a story is a crucial aspect of understanding it. Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hour. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.","['Allen Kim', 'Charuta Pethe', 'Steven Skiena']",https://www.aclweb.org/anthology/2020.emnlp-main.730.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.730/,6,
MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering,"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.","['Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral.', 'Yezhou Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.63.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.63/,4,
TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as “what happened before/after [some event]?” We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.","['Qiang Ning', 'Hao Wu', 'Rujun Han', 'Nanyun Peng', 'Matt Gardner', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.88.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.88/,10,
Incremental Neural Coreference Resolution in Constant Memory,"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity’s representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3% relative loss in F1 on OntoNotes 5.0.","['Patrick Xia', 'João Sedoc', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.695.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.695/,4,
Exploring the Role of Argument Structure in Online Debate Persuasion,"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the best predictive performance in assessing the persuasiveness of the arguments on online debates.","['Jialu Li', 'Esin Durmus', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.716.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.716/,3,
Quantitative argument summarization and beyond: Cross-domain key point analysis,"When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data. Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.","['Roy Bar-Haim', 'Yoav Kantor', 'Lilach Eden', 'Roni Friedman', 'Dan Lahav', 'Noam Slonim']",https://www.aclweb.org/anthology/2020.emnlp-main.3.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.3/,3,
Towards Interpretable Reasoning over Paragraph Effects in Situation,"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step “black box” model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.","['Mucheng Ren', 'Xiubo Geng', 'Tao Qin', 'He-Yan Huang', 'Daxin Jiang']",https://www.aclweb.org/anthology/2020.emnlp-main.548.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.548/,1,
Better Highlighting: Creating Sub-Sentence Summary Highlights,"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.","['Sangwoo Cho', 'Kaiqiang Song', 'Chen Li', 'Dong Yu', 'Hassan Foroosh', 'Fei Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.509.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.509/,2,
META: Metadata-Empowered Weak Supervision for Text Classification,"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","['Dheeraj Mekala', 'Xinyang Zhang', 'Jingbo Shang']",https://www.aclweb.org/anthology/2020.emnlp-main.670.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.670/,4,
MIME: MIMicking Emotions for Empathetic Response Generation,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.","['Navonil Majumder', 'Pengfei Hong', 'Shanshan Peng', 'Jiankun Lu', 'Deepanway Ghosal', 'Alexander Gelbukh', 'Rada Mihalcea', 'Soujanya Poria']",https://www.aclweb.org/anthology/2020.emnlp-main.721.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.721/,2,
Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering,"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.","['Zujie Liang', 'Weitao Jiang', 'Haifeng Hu', 'Jiaying Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.265.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.265/,5,
Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph,"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.","['Xin Lv', 'Xu Han', 'Lei Hou', 'Juanzi Li', 'Zhiyuan Liu', 'Wei Zhang', 'Yichi Zhang', 'Hao Kong', 'Suhui Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.459.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.459/,4,
MovieChats: Chat like Humans in a Closed Domain,"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work","['Hui Su', 'Xiaoyu Shen', 'Zhou Xiao', 'Zheng Zhang', 'Ernie Chang', 'Cheng Zhang', 'Cheng Niu', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.535.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.535/,5,
Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models,"Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.","['Ethan Wilcox', 'Peng Qian', 'Richard Futrell', 'Ryosuke Kohita', 'Roger Levy', 'Miguel Ballesteros']",https://www.aclweb.org/anthology/2020.emnlp-main.375.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.375/,10,
Improving Multilingual Models with Language-Clustered Vocabularies,"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.","['Hyung Won Chung', 'Dan Garrette', 'Kiat Chuan Tan', 'Jason Riesa']",https://www.aclweb.org/anthology/2020.emnlp-main.367.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.367/,2,
"Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics","This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers’ responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chinese microblog Sina Weibo with over 13 thousand trending topics, emotion votes in 24 fine-grained types from massive participants, and user comments to allow context understanding. In experiments, we examine baseline performance to predict a topic’s possible social emotions in a multilabel classification setting. The results show that a seq2seq model with user comment modeling performs the best, even surpassing human prediction. More analyses shed light on the effects of emotion types, topic description lengths, contexts from user comments, and the limited capacity of the existing models.","['Keyang Ding', 'Jing Li', 'Yuji Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.106.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.106/,7,
A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?,"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist. And therefore it raises a critical question of whether previous creditable approaches can still work well when facing these challenges. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. To further verify our conclusions, we also construct a new open NER dataset that focuses on entity types with weaker name regularity and lower mention coverage to verify our conclusion. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is critical for the models to generalize to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained encoders.","['Hongyu Lin', 'Yaojie Lu', 'Jialong Tang', 'Xianpei Han', 'Le Sun', 'Zhicheng Wei', 'Nicholas Jing Yuan']",https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.592/,10,
Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!,"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.","['Jack Hessel', 'Lillian Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.62.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.62/,10,
Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training,"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.","['Joe Stacey', 'Pasquale Minervini', 'Haim Dubossarsky', 'Sebastian Riedel', 'Tim Rocktäschel']",https://www.aclweb.org/anthology/2020.emnlp-main.665.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.665/,4,
Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements,"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,860 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.","['Yang Li', 'Gang Li', 'Luheng He', 'Jingjie Zheng', 'Hong Li', 'Zhiwei Guan']",https://www.aclweb.org/anthology/2020.emnlp-main.443.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.443/,6,
Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach,"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.","['Bowen Tan', 'Lianhui Qin', 'Eric Xing', 'Zhiting Hu']",https://www.aclweb.org/anthology/2020.emnlp-main.510.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.510/,2,
Multilevel Text Alignment with Cross-Document Attention,"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.","['Xuhui Zhou', 'Nikolaos Pappas', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.407.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.407/,2,
Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.","['Dayiheng Liu', 'Yeyun Gong', 'Jie Fu', 'Yu Yan', 'Jiusheng Chen', 'Jiancheng Lv', 'Nan Duan', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.467.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.467/,5,
Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.","['Weishi Wang', 'Steven C.H. Hoi', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.533.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.533/,5,
Generating Diverse Translation from Model Distribution with Dropout,"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.","['Xuanfu Wu', 'Yang Feng', 'Chenze Shao']",https://www.aclweb.org/anthology/2020.emnlp-main.82.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.82/,2,
A Diagnostic Study of Explainability Techniques for Text Classification,"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.","['Pepa Atanasova', 'Jakob Grue Simonsen', 'Christina Lioma', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.263.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.263/,4,
BERT-ATTACK: Adversarial Attack Against BERT Using BERT,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.","['Linyang Li', 'Ruotian Ma', 'Qipeng Guo', 'Xiangyang Xue', 'Xipeng Qiu']",https://www.aclweb.org/anthology/2020.emnlp-main.500.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.500/,4,
ToTTo: A Controlled Table-To-Text Generation Dataset,"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.","['Ankur Parikh', 'Xuezhi Wang', 'Sebastian Gehrmann', 'Manaal Faruqui', 'Bhuwan Dhingra', 'Diyi Yang', 'Dipanjan Das']",https://www.aclweb.org/anthology/2020.emnlp-main.89.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.89/,10,
STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.","['Nader Akoury', 'Shufan Wang', 'Josh Whiting', 'Stephen Hood', 'Nanyun Peng', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.525.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.525/,2,
Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder,"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts. Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics. We observe that our model can highly improve short text topic modeling performance. Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.","['Xiaobao Wu', 'Chunping Li', 'Yan Zhu', 'Yishu Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.138.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.138/,4,
"Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations","Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn’s existing contents. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.","['Lingzhi Wang', 'Jing Li', 'Xingshan Zeng', 'Haisong Zhang', 'Kam-Fai Wong']",https://www.aclweb.org/anthology/2020.emnlp-main.538.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.538/,5,
Exploiting Sentence Order in Document Alignment,"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala–English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.","['Brian Thompson', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.483.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.483/,2,
DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.","['Valentin Hofmann', 'Janet Pierrehumbert', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.316.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.316/,4,
Affective Event Classification with Discourse-enhanced Self-training,"Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective polarity to event phrases. First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base. Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data. The key idea is to exploit event phrases that occur with a coreferent sentiment expression. The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier’s predictions and the polarities of the event’s coreferent sentiment expressions. Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.","['Yuan Zhuang', 'Tianyu Jiang', 'Ellen Riloff']",https://www.aclweb.org/anthology/2020.emnlp-main.452.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.452/,4,
ALICE: Active Learning with Contrastive Natural Language Explanations,"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model’s structure. We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.","['Weixin Liang', 'James Zou', 'Zhou Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.355.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.355/,8,
OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction,"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost. On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time. Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.","['Keshav Kolluru', 'Vaibhav Adlakha', 'Samarth Aggarwal', 'Mausam', 'Soumen Chakrabarti']",https://www.aclweb.org/anthology/2020.emnlp-main.306.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.306/,8,
Methods for Numeracy-Preserving Word Embeddings,"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.","['Dhanasekar Sundararaman', 'Shijing Si', 'Vivek Subramanian', 'Guoyin Wang', 'Devamanyu Hazarika', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.384.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.384/,8,
Constrained Fact Verification for FEVER,"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","['Adithya Pratapa', 'Sai Muralidhar Jayanthi', 'Kavya Nerella']",https://www.aclweb.org/anthology/2020.emnlp-main.629.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.629/,6,
BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues,"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.","['Hung Le', 'Doyen Sahoo', 'Nancy Chen', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.145.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.145/,9,
Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation,"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.","['Dana Ruiter', 'Josef van Genabith', 'Cristina España-Bonet']",https://www.aclweb.org/anthology/2020.emnlp-main.202.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.202/,2,
Translation Artifacts in Cross-lingual Transfer Learning,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.","['Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre']",https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.618/,2,
Semantically Inspired AMR Alignment for the Portuguese Language,"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.","['Rafael Anchiêta', 'Thiago Pardo']",https://www.aclweb.org/anthology/2020.emnlp-main.123.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.123/,8,
Language Model Prior for Low-Resource Neural Machine Translation,"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.","['Christos Baziotis', 'Barry Haddow', 'Alexandra Birch']",https://www.aclweb.org/anthology/2020.emnlp-main.615.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.615/,2,
A Simple Approach to Learning Unsupervised Multilingual Embeddings,"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space. In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques. We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.","['Pratik Jawanpuria', 'Mayank Meghwanshi', 'Bamdev Mishra']",https://www.aclweb.org/anthology/2020.emnlp-main.240.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.240/,2,
De-Biased Court’s View Generation with Causality,"Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court’s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.","['Yiquan Wu', 'Kun Kuang', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Jun Xiao', 'Yueting Zhuang', 'Luo Si', 'Fei Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.56.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.56/,6,
Pre-training for Abstractive Document Summarization by Reinstating Source Text,"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.","['Yanyan Zou', 'Xingxing Zhang', 'Wei Lu', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.297.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.297/,2,
BioMegatron: Larger Biomedical Domain Language Model,"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].","['Hoo-Chang Shin', 'Yang Zhang', 'Evelina Bakhturina', 'Raul Puri', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Raghav Mani']",https://www.aclweb.org/anthology/2020.emnlp-main.379.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.379/,10,
Towards Interpreting BERT for Reading Comprehension Based QA,"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.","['Sahana Ramnath', 'Preksha Nema', 'Deep Sahni', 'Mitesh M. Khapra']",https://www.aclweb.org/anthology/2020.emnlp-main.261.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.261/,4,
Entity Enhanced BERT Pre-training for Chinese NER,"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.","['Chen Jia', 'Yuefeng Shi', 'Qinrong Yang', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.518.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.518/,8,
Alignment-free Cross-lingual Semantic Role Labeling,"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings. The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence. It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.","['Rui Cai', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.319.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.319/,1,
On the Reliability and Validity of Detecting Approval of Political Actors in Tweets,"Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on social media. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from tweets.","['Indira Sen', 'Fabian Flöck', 'Claudia Wagner']",https://www.aclweb.org/anthology/2020.emnlp-main.110.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.110/,10,3
Learning to Fuse Sentences with Transformers for Summarization,"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning. In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer’s performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.","['Logan Lebanoff', 'Franck Dernoncourt', 'Doo Soon Kim', 'Lidan Wang', 'Walter Chang', 'Fei Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.338.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.338/,2,
COGS: A Compositional Generalization Challenge Based on Semantic Interpretation,"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.","['Najoung Kim', 'Tal Linzen']",https://www.aclweb.org/anthology/2020.emnlp-main.731.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.731/,10,
Aspect Sentiment Classification with Aspect-Specific Opinion Spans,"Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans. In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs. Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.","['Lu Xu', 'Lidong Bing', 'Wei Lu', 'Fei Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.288.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.288/,3,
PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge,"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.","['Yun He', 'Zhuoer Wang', 'Yin Zhang', 'Ruihong Huang', 'James Caverlee']",https://www.aclweb.org/anthology/2020.emnlp-main.611.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.611/,10,
Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations,"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.","['Ramit Sawhney', 'Shivam Agarwal', 'Arnav Wadhwa', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.676.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.676/,6,
Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning,"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.","['Yi Yang', 'Arzoo Katiyar']",https://www.aclweb.org/anthology/2020.emnlp-main.516.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.516/,4,
XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques,"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.","['Rexhina Blloshmi', 'Rocco Tripodi', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.195.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.195/,2,
Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.","['Yosi Mass', 'Haggai Roitman']",https://www.aclweb.org/anthology/2020.emnlp-main.343.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.343/,4,
Text Segmentation by Cross Segment Attention,"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.","['Michal Lukasik', 'Boris Dadachev', 'Kishore Papineni', 'Gonçalo Simões']",https://www.aclweb.org/anthology/2020.emnlp-main.380.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.380/,1,
Sub-Instruction Aware Vision-and-Language Navigation,"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent’s performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.","['Yicong Hong', 'Cristian Rodriguez', 'Qi Wu', 'Stephen Gould']",https://www.aclweb.org/anthology/2020.emnlp-main.271.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.271/,6,
What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding,"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Hence, we carry out an empirical study on position embedding of mainstream pre-trained Transformers mainly focusing on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings by feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future works to choose the suitable positional encoding function for specific tasks given the application property.","['Yu-An Wang', 'Yun-Nung Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.555.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.555/,10,
MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems,"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.","['Zhaojiang Lin', 'Andrea Madotto', 'Genta Indra Winata', 'Pascale Fung']",https://www.aclweb.org/anthology/2020.emnlp-main.273.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.273/,5,
What is More Likely to Happen Next? Video-and-Language Future Event Prediction,"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work.","['Jie Lei', 'Licheng Yu', 'Tamara Berg', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.706.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.706/,10,9
Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks,"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.","['Yuanhe Tian', 'Yan Song', 'Fei Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.487.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.487/,1,
Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.","['Xiuyi Chen', 'Fandong Meng', 'Peng Li', 'Feilong Chen', 'Shuang Xu', 'Bo Xu', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.275.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.275/,5,2
SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search,"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.","['Sean MacAvaney', 'Arman Cohan', 'Nazli Goharian']",https://www.aclweb.org/anthology/2020.emnlp-main.341.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.341/,6,
Multi-turn Response Selection using Dialogue Dependency Relations,"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.","['Qi Jia', 'Yizhu Liu', 'Siyu Ren', 'Kenny Zhu', 'Haifeng Tang']",https://www.aclweb.org/anthology/2020.emnlp-main.150.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.150/,1,5
Information-Theoretic Probing with Minimum Description Length,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates “the amount of effort” needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.","['Elena Voita', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.14/,8,
“What Do You Mean by That?” A Parser-Independent Interactive Approach for Enhancing Text-to-SQL,"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users’ natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.","['Yuntao Li', 'Bei Chen', 'Qian Liu', 'Yan Gao', 'Jian-Guang Lou', 'Yan Zhang', 'Dongmei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.561.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.561/,6,
Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,"Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account. In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT). However, graph-based neural networks do not take sequential information into account. In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure. Accordingly, our RGAT model can capture both the speaker dependency and the sequential information. Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations. In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.","['Taichi Ishiwatari', 'Yuki Yasuda', 'Taro Miyazaki', 'Jun Goto']",https://www.aclweb.org/anthology/2020.emnlp-main.597.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.597/,1,
Event Extraction as Machine Reading Comprehension,"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.","['Jian Liu', 'Yubo Chen', 'Kang Liu', 'Wei Bi', 'Xiaojiang Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.128.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.128/,8,
Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions,"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.","['Bodhisattwa Prasad Majumder', 'Harsh Jhamtani', 'Taylor Berg-Kirkpatrick', 'Julian McAuley']",https://www.aclweb.org/anthology/2020.emnlp-main.739.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.739/,5,
Routing Enforced Generative Model for Recipe Generation,"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.","['Zhiwei Yu', 'Hongyu Zang', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.311.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.311/,6,
More Bang for Your Buck: Natural Perturbation for Robust Question Answering,"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.","['Daniel Khashabi', 'Tushar Khot', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.12.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.12/,10,5
Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube,"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively “easy:” speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are “grounded” and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.","['Jack Hessel', 'Zhenhai Zhu', 'Bo Pang', 'Radu Soricut']",https://www.aclweb.org/anthology/2020.emnlp-main.709.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.709/,10,9
ETC: Encoding Long and Structured Inputs in Transformers,"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a “Contrastive Predictive Coding” (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.","['Joshua Ainslie', 'Santiago Ontanon', 'Chris Alberti', 'Vaclav Cvicek', 'Zachary Fisher', 'Philip Pham', 'Anirudh Ravula', 'Sumit Sanghai', 'Qifan Wang', 'Li Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.19/,4,
Unsupervised stance detection for arguments from consequences,"Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.","['Jonathan Kobbe', 'Ioana Hulpuş', 'Heiner Stuckenschmidt']",https://www.aclweb.org/anthology/2020.emnlp-main.4.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.4/,3,7
Distilling Multiple Domains for Neural Machine Translation,"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.","['Anna Currey', 'Prashant Mathur', 'Georgiana Dinu']",https://www.aclweb.org/anthology/2020.emnlp-main.364.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.364/,2,
On the importance of pre-training data volume for compact language models,"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.","['Vincent Micheli', 'Martin d’Hoffschmidt', 'François Fleuret']",https://www.aclweb.org/anthology/2020.emnlp-main.632.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.632/,4,
Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles,"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results—using several state-of-the-art models trained on the Multi-XScience dataset—reveal that Multi-XScience is well suited for abstractive models.","['Yao Lu', 'Yue Dong', 'Laurent Charlin']",https://www.aclweb.org/anthology/2020.emnlp-main.648.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.648/,10,2
X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.","['Zhengbao Jiang', 'Antonios Anastasopoulos', 'Jun Araki', 'Haibo Ding', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.479/,2,4
Coarse-to-Fine Pre-training for Named Entity Recognition,"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.","['Xue Mengge', 'Bowen Yu', 'Zhenyu Zhang', 'Tingwen Liu', 'Yue Zhang', 'Bin Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.514.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.514/,4,
Gradient-guided Unsupervised Lexically Constrained Text Generation,"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",['Lei Sha'],https://www.aclweb.org/anthology/2020.emnlp-main.701.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.701/,2,
"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation","News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.","['Dayiheng Liu', 'Yeyun Gong', 'Yu Yan', 'Jie Fu', 'Bo Shao', 'Daxin Jiang', 'Jiancheng Lv', 'Nan Duan']",https://www.aclweb.org/anthology/2020.emnlp-main.505.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.505/,2,
Plug and Play Autoencoders for Conditional Text Generation,"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.","['Florian Mai', 'Nikolaos Pappas', 'Ivan Montero', 'Noah A. Smith', 'James Henderson']",https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.491/,2,
“I’d rather just go to bed”: Understanding Indirect Answers,"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret ‘I’m starving.’ in response to ‘Hungry?’, even without direct cue words such as ‘yes’ and ‘no’. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today’s systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus ‘Circa’ with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes.","['Annie Louis', 'Dan Roth', 'Filip Radlinski']",https://www.aclweb.org/anthology/2020.emnlp-main.601.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.601/,5,
Joint Constrained Learning for Event-Event Relation Extraction,"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.","['Haoyu Wang', 'Muhao Chen', 'Hongming Zhang', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.51.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.51/,8,
Friendly Topic Assistant for Transformer Based Abstractive Summarization,"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.","['Zhengjue Wang', 'Zhibin Duan', 'Hao Zhang', 'Chaojie Wang', 'Long Tian', 'Bo Chen', 'Mingyuan Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.35.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.35/,2,
Deconstructing word embedding algorithms,"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.","['Kian Kenyon-Dean', 'Edward Newell', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.681.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.681/,4,
SLM: Learning a Discourse Language Representation with Sentence Unshuffling,"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.","['Haejun Lee', 'Drew A. Hudson', 'Kangwook Lee', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.120.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.120/,4,
Identifying Elements Essential for BERT’s Multilinguality,"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.","['Philipp Dufter', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.358.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.358/,4,
When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models,"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.","['Changlong Yu', 'Jialong Han', 'Peifeng Wang', 'Yangqiu Song', 'Hongming Zhang', 'Wilfred Ng', 'Shuming Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.502.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.502/,1,
Debiasing knowledge graph embeddings,"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.","['Joseph Fisher', 'Arpit Mittal', 'Dave Palfrey', 'Christos Christodoulopoulos']",https://www.aclweb.org/anthology/2020.emnlp-main.595.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.595/,4,
Interactive Refinement of Cross-Lingual Word Embeddings,"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.","['Michelle Yuan', 'Mozhi Zhang', 'Benjamin Van Durme', 'Leah Findlater', 'Jordan Boyd-Graber']",https://www.aclweb.org/anthology/2020.emnlp-main.482.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.482/,2,
Adversarial Self-Supervised Data-Free Distillation for Text Classification,"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge. Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.","['Xinyin Ma', 'Yongliang Shen', 'Gongfan Fang', 'Chen Chen', 'Chenghao Jia', 'Weiming Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.499.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.499/,10,
VD-BERT: A Unified Vision and Dialog Transformer with BERT,"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.","['Yue Wang', 'Shafiq Joty', 'Michael Lyu', 'Irwin King', 'Caiming Xiong', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.269.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.269/,5,9
Identifying Exaggerated Language,"While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.","['Li Kong', 'Chuanyi Li', 'Jidong Ge', 'Bin Luo', 'Vincent Ng']",https://www.aclweb.org/anthology/2020.emnlp-main.571.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.571/,10,1
“You are grounded!”: Latent Name Artifacts in Pre-trained Language Models,"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.","['Vered Shwartz', 'Rachel Rudinger', 'Oyvind Tafjord']",https://www.aclweb.org/anthology/2020.emnlp-main.556.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.556/,10,
Learning Structured Representations of Entity Names using ActiveLearning and Weak Supervision,"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.","['Kun Qian', 'Poornima Chozhiyath Raman', 'Yunyao Li', 'Lucian Popa']",https://www.aclweb.org/anthology/2020.emnlp-main.517.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.517/,8,
Neural Topic Modeling with Cycle-Consistent Adversarial Training,"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.","['Xuemeng Hu', 'Rui Wang', 'Deyu Zhou', 'Yuxuan Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.725.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.725/,8,
SLURP: A Spoken Language Understanding Resource Package,"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.","['Emanuele Bastianelli', 'Andrea Vanzo', 'Pawel Swietojanski', 'Verena Rieser']",https://www.aclweb.org/anthology/2020.emnlp-main.588.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.588/,10,9
Speakers Fill Lexical Semantic Gaps with Context,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear—resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this—one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. 𝜌 = 0.40 in English). We then test our main hypothesis—that a word’s lexical ambiguity should negatively correlate with its contextual uncertainty—and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.","['Tiago Pimentel', 'Rowan Hall Maudslay', 'Damian Blasi', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.328.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.328/,1,
Token-level Adaptive Training for Neural Machine Translation,"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.","['Shuhao Gu', 'Jinchao Zhang', 'Fandong Meng', 'Yang Feng', 'Wanying Xie', 'Jie Zhou', 'Dong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.76.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.76/,2,
Scalable Zero-shot Entity Linking with Dense Entity Retrieval,"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.","['Ledell Wu', 'Fabio Petroni', 'Martin Josifoski', 'Sebastian Riedel', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.519.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.519/,8,
Lifelong Language Knowledge Distillation,"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.","['Yung-Sung Chuang', 'Shang-Yu Su', 'Yun-Nung Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.233.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.233/,4,
New Protocols and Negative Results for Textual Entailment Data Collection,"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit *pre-filled* text boxes, reduce previously observed issues with annotation artifacts.","['Samuel Bowman', 'Jennimaria Palomaki', 'Livio Baldini Soares', 'Emily Pitler']",https://www.aclweb.org/anthology/2020.emnlp-main.658.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.658/,10,
Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection,"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.","['Adam Tsakalidis', 'Maria Liakata']",https://www.aclweb.org/anthology/2020.emnlp-main.682.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.682/,1,
Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications,"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.","['Matthew Khoury', 'Rumen Dangovski', 'Longwu Ou', 'Preslav Nakov', 'Yichen Shen', 'Li Jing']",https://www.aclweb.org/anthology/2020.emnlp-main.640.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.640/,4,
Sparse Text Generation,"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: 𝜖-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.","['Pedro Henrique Martins', 'Zita Marinho', 'André F. T. Martins']",https://www.aclweb.org/anthology/2020.emnlp-main.348.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.348/,10,2
Multi-Stage Pre-training for Automated Chinese Essay Scoring,"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..","['Wei Song', 'Kai Zhang', 'Ruiji Fu', 'Lizhen Liu', 'Ting Liu', 'Miaomiao Cheng']",https://www.aclweb.org/anthology/2020.emnlp-main.546.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.546/,6,
"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation","Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.","['Tahmid Hasan', 'Abhik Bhattacharjee', 'Kazi Samin', 'Masum Hasan', 'Madhusudan Basak', 'M. Sohel Rahman', 'Rifat Shahriyar']",https://www.aclweb.org/anthology/2020.emnlp-main.207.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.207/,10,2
Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning,"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT – a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.","['Tsvetomila Mihaylova', 'Vlad Niculae', 'André F. T. Martins']",https://www.aclweb.org/anthology/2020.emnlp-main.171.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.171/,4,
Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.","['Ruipeng Jia', 'Yanan Cao', 'Hengzhu Tang', 'Fang Fang', 'Cong Cao', 'Shi Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.295.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.295/,2,
ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention,"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.","['José Manuel Gómez-Pérez', 'Raúl Ortega']",https://www.aclweb.org/anthology/2020.emnlp-main.441.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.441/,5,9
Event Extraction by Answering (Almost) Natural Questions,"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).","['Xinya Du', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.49.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.49/,8,5
"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.","['Ieva Staliūnaitė', 'Ignacio Iacobacci']",https://www.aclweb.org/anthology/2020.emnlp-main.573.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.573/,4,5
APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning,"Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multitask learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.","['Liying Cheng', 'Lidong Bing', 'Qian Yu', 'Wei Lu', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.569.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.569/,3,
Conditional Causal Relationships between Emotions and Causes in Texts,"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.","['Xinhong Chen', 'Qing Li', 'Jianping Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.252.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.252/,3,
Distilling Structured Knowledge for Text-Based Relational Reasoning,"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.","['Jin Dong', 'Marc-Antoine Rondeau', 'William L. Hamilton']",https://www.aclweb.org/anthology/2020.emnlp-main.551.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.551/,4,
Discontinuous Constituent Parsing as Sequence Labeling,"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.","['David Vilares', 'Carlos Gómez-Rodríguez']",https://www.aclweb.org/anthology/2020.emnlp-main.221.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.221/,1,
PathQG: Neural Question Generation from Facts,"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.","['Siyuan Wang', 'Zhongyu Wei', 'Zhihao Fan', 'Zengfeng Huang', 'Weijian Sun', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.729.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.729/,2,
LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.","['Ikuya Yamada', 'Akari Asai', 'Hiroyuki Shindo', 'Hideaki Takeda', 'Yuji Matsumoto']",https://www.aclweb.org/anthology/2020.emnlp-main.523.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.523/,1,
Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms – greedy search, beam search, top-k sampling, and nucleus sampling – are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.","['Sean Welleck', 'Ilia Kulikov', 'Jaedeok Kim', 'Richard Yuanzhe Pang', 'Kyunghyun Cho']",https://www.aclweb.org/anthology/2020.emnlp-main.448.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.448/,4,
MedDialog: Large-scale Medical Dialogue Datasets,"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets – MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System","['Guangtao Zeng', 'Wenmian Yang', 'Zeqian Ju', 'Yue Yang', 'Sicheng Wang', 'Ruisi Zhang', 'Meng Zhou', 'Jiaqi Zeng', 'Xiangyu Dong', 'Ruoyu Zhang', 'Hongchao Fang', 'Penghui Zhu', 'Shu Chen', 'Pengtao Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.743.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.743/,10,
Improving Text Generation with Student-Forcing Optimal Transport,"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.","['Jianqiao Li', 'Chunyuan Li', 'Guoyin Wang', 'Hao Fu', 'Yuhchen Lin', 'Liqun Chen', 'Yizhe Zhang', 'Chenyang Tao', 'Ruiyi Zhang', 'Wenlin Wang', 'Dinghan Shen', 'Qian Yang', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.735.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.735/,2,
To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints,"State of the art research for date-time entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.","['Barun Patra', 'Chala Fufa', 'Pamela Bhattacharya', 'Charles C. Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.678.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.678/,6,1
Pre-training Mention Representations in Coreference Models,"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.","['Yuval Varkel', 'Amir Globerson']",https://www.aclweb.org/anthology/2020.emnlp-main.687.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.687/,4,
Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples,"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.","['Lihao Wang', 'Xiaoqing Zheng']",https://www.aclweb.org/anthology/2020.emnlp-main.228.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.228/,4,
Reformulating Unsupervised Style Transfer as Paraphrase Generation,"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input’s meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.","['Kalpesh Krishna', 'John Wieting', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.55.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.55/,2,
Grounded Compositional Outputs for Adaptive Language Modeling,"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.","['Nikolaos Pappas', 'Phoebe Mulcaire', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.96.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.96/,1,
On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.","['Jonathan Pilault', 'Raymond Li', 'Sandeep Subramanian', 'Christopher Pal']",https://www.aclweb.org/anthology/2020.emnlp-main.748.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.748/,2,
With Little Power Comes Great Responsibility,"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.","['Dallas Card', 'Peter Henderson', 'Urvashi Khandelwal', 'Robin Jia', 'Kyle Mahowald', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.745.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.745/,10,
Explainable Automated Fact-Checking for Public Health Claims,"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.","['Neema Kotonya', 'Francesca Toni']",https://www.aclweb.org/anthology/2020.emnlp-main.623.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.623/,6,
Word Rotator’s Distance,"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover’s distance (optimal transport), which we refer to as word rotator’s distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines. The source code is avaliable at https://github.com/eumesy/wrd","['Sho Yokoi', 'Ryo Takahashi', 'Reina Akama', 'Jun Suzuki', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.236.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.236/,8,
Word class flexibility: A deep contextualized approach,"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.","['Bai Li', 'Guillaume Thomas', 'Yang Xu', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.71.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.71/,1,
A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning,"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.","['Yichi Zhang', 'Zhijian Ou', 'Min Hu', 'Junlan Feng']",https://www.aclweb.org/anthology/2020.emnlp-main.740.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.740/,5,4
Denoising Relation Extraction from Document-level Distant Supervision,"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.","['Chaojun Xiao', 'Yuan Yao', 'Ruobing Xie', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun', 'Fen Lin', 'Leyu Lin']",https://www.aclweb.org/anthology/2020.emnlp-main.300.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.300/,8,4
Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers,"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.","['Hanjie Chen', 'Yangfeng Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.347.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.347/,4,6
Dialogue Response Ranking Training with Large-Scale Human Feedback Data,"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.","['Xiang Gao', 'Yizhe Zhang', 'Michel Galley', 'Chris Brockett', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.28.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.28/,5,7
Program Enhanced Fact Verification with Verbalization and Graph Attention Network,"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.","['Xiaoyu Yang', 'Feng Nie', 'Yufei Feng', 'Quan Liu', 'Zhigang Chen', 'Xiaodan Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.628.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.628/,8,4
Measuring Information Propagation in Literary Social Networks,"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.","['Matthew Sims', 'David Bamman']",https://www.aclweb.org/anthology/2020.emnlp-main.47.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.47/,10,7
Authorship Attribution for Neural Text Generation,"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the “author” of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T1 and T2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/ 302zWdz","['Adaku Uchendu', 'Thai Le', 'Kai Shu', 'Dongwon Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.673.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.673/,2,
Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs,"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations. Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.","['Jueqing Lu', 'Lan Du', 'Ming Liu', 'Joanna Dipnall']",https://www.aclweb.org/anthology/2020.emnlp-main.235.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.235/,8,4
FIND: Human-in-the-Loop Debugging Deep Text Classifiers,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND – a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).","['Piyawat Lertvittayakumjorn', 'Lucia Specia', 'Francesca Toni']",https://www.aclweb.org/anthology/2020.emnlp-main.24.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.24/,10,
Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks,"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.","['Denis Emelin', 'Ivan Titov', 'Rico Sennrich']",https://www.aclweb.org/anthology/2020.emnlp-main.616.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.616/,2,
Explainable Clinical Decision Support from Text,"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.","['Jinyue Feng', 'Chantal Shaib', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.115.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.115/,8,
Do Explicit Alignments Robustly Improve Multilingual Encoders?,"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.","['Shijie Wu', 'Mark Dredze']",https://www.aclweb.org/anthology/2020.emnlp-main.362.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.362/,10,2
CapWAP: Image Captioning with a Purpose,"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs—a natural expression of information need—from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.","['Adam Fisch', 'Kenton Lee', 'Ming-Wei Chang', 'Jonathan H. Clark', 'Regina Barzilay']",https://www.aclweb.org/anthology/2020.emnlp-main.705.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.705/,6,5
Uncertainty-Aware Label Refinement for Sequence Labeling,"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.","['Tao Gui', 'Jiacheng Ye', 'Qi Zhang', 'Zhengyan Li', 'Zichu Fei', 'Yeyun Gong', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.181.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.181/,8,4
Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks,"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.","['Chengyue Jiang', 'Yinggong Zhao', 'Shanbo Chu', 'Libin Shen', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.258.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.258/,4,1
Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking,"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.","['Yexiang Wang', 'Yi Guo', 'Siqi Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.243.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.243/,5,
Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text,"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.","['Dongfang Li', 'Baotian Hu', 'Qingcai Chen', 'Weihua Peng', 'Anqi Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.111.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.111/,5,8
Deep Weighted MaxSAT for Aspect-based Opinion Extraction,"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.","['Meixi Wu', 'Wenya Wang', 'Sinno Jialin Pan']",https://www.aclweb.org/anthology/2020.emnlp-main.453.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.453/,8,4
Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification. Our source code is available at https://github.com/ShyamSubramanian/HESM.","['Shyam Subramanian', 'Kyumin Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.627.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.627/,8,
Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games,"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model’s action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.","['Subhajit Chaudhury', 'Daiki Kimura', 'Kartik Talamadupula', 'Michiaki Tatsubori', 'Asim Munawar', 'Ryuki Tachibana']",https://www.aclweb.org/anthology/2020.emnlp-main.241.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.241/,4,6
Within-Between Lexical Relation Classification,"We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.","['Oren Barkan', 'Avi Caciularu', 'Ido Dagan']",https://www.aclweb.org/anthology/2020.emnlp-main.284.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.284/,8,1
Social Media Attributions in the Context of Water Crisis,"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of attribution tie detection of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34% on attribution detection and 81.37% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.","['Rupak Sarkar', 'Sayantan Mahinder', 'Hirak Sarkar', 'Ashiqur KhudaBukhsh']",https://www.aclweb.org/anthology/2020.emnlp-main.109.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.109/,7,6
Compressive Summarization with Plausibility and Salience Modeling,"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.","['Shrey Desai', 'Jiacheng Xu', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.507.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.507/,2,
What Have We Achieved on Text Summarization?,"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.","['Dandan Huang', 'Leyang Cui', 'Sen Yang', 'Guangsheng Bao', 'Kun Wang', 'Jun Xie', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.33.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.33/,2,10
Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation,"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.","['Pei Zhang', 'Boxing Chen', 'Niyu Ge', 'Kai Fan']",https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.81/,2,10
Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited.","['Wenpeng Yin', 'Nazneen Fatema Rajani', 'Dragomir Radev', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.660.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.660/,8,4
A Simple Yet Strong Pipeline for HotpotQA,"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named , performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.","['Dirk Groeneveld', 'Tushar Khot', 'Mausam', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.711/,5,
Entity Linking in 100 Languages,"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.","['Jan A. Botha', 'Zifei Shan', 'Dan Gillick']",https://www.aclweb.org/anthology/2020.emnlp-main.630.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.630/,8,2
Incomplete Utterance Rewriting as Semantic Segmentation,"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.","['Qian Liu', 'Bei Chen', 'Jian-Guang Lou', 'Bin Zhou', 'Dongmei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.227.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.227/,2,
LOGAN: Local Group Bias Detection by Clustering,"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.","['Jieyu Zhao', 'Kai-Wei Chang']",https://www.aclweb.org/anthology/2020.emnlp-main.155.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.155/,7,4
Improving Bilingual Lexicon Induction for Low Frequency Words,"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.","['Jiaji Huang', 'Xingyu Cai', 'Kenneth Church']",https://www.aclweb.org/anthology/2020.emnlp-main.100.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.100/,1,
Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering,"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.","['Pratyay Banerjee', 'Chitta Baral.']",https://www.aclweb.org/anthology/2020.emnlp-main.11.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.11/,5,4
Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents,"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as “kitchen” and “bedroom”, and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating “granite” with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.","['Gregory Yauney', 'Jack Hessel', 'David Mimno']",https://www.aclweb.org/anthology/2020.emnlp-main.160.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.160/,6,4
Structured Pruning of Large Language Models,"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.","['Ziheng Wang', 'Jeremy Wohlwend', 'Tao Lei']",https://www.aclweb.org/anthology/2020.emnlp-main.496.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.496/,4,1
Conversational Document Prediction to Assist Customer Care Agents,"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs. We study the task of predicting the documents that customer care agents can use to facilitate users’ needs. We also introduce a new public dataset which supports the aforementioned problem. Using this dataset and two others, we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task. Additionally, we analyze the practicality of such systems in terms of inference time complexity. Our show that an hybrid IR+DL approach provides the best of both worlds.","['Jatin Ganhotra', 'Haggai Roitman', 'Doron Cohen', 'Nathaniel Mills', 'Chulaka Gunasekara', 'Yosi Mass', 'Sachindra Joshi', 'Luis Lastras', 'David Konopnicki']",https://www.aclweb.org/anthology/2020.emnlp-main.25.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.25/,6,10
Learning VAE-LDA Models with Rounded Reparameterization Trick,"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.","['Runzhi Tian', 'Yongyi Mao', 'Richong Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.101.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.101/,4,
An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.","['Bhargavi Paranjape', 'Mandar Joshi', 'John Thickstun', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.153.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.153/,4,
Cold-start Active Learning through Self-supervised Language Modeling,"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.","['Michelle Yuan', 'Hsuan-Tien Lin', 'Jordan Boyd-Graber']",https://www.aclweb.org/anthology/2020.emnlp-main.637.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.637/,4,
Facilitating the Communication of Politeness through Fine-Grained Paraphrasing,"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.","['Liye Fu', 'Susan Fussell', 'Cristian Danescu-Niculescu-Mizil']",https://www.aclweb.org/anthology/2020.emnlp-main.416.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.416/,2,
Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems,"The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool.","['Jan Milan Deriu', 'Don Tuggener', 'Pius von Däniken', 'Jon Ander Campos', 'Álvaro Rodrigo', 'Thiziri Belkacem', 'Aitor Soroa', 'Eneko Agirre', 'Mark Cieliebak']",https://www.aclweb.org/anthology/2020.emnlp-main.326.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.326/,10,5
GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems,"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.","['Shiquan Yang', 'Rui Zhang', 'Sarah Erfani']",https://www.aclweb.org/anthology/2020.emnlp-main.147.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.147/,5,8
Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.","['Goro Kobayashi', 'Tatsuki Kuribayashi', 'Sho Yokoi', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.574.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.574/,2,4
SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.","['Rongzhi Zhang', 'Yue Yu', 'Chao Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.691.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.691/,4,10
Analogous Process Structure Induction for Sub-event Sequence Prediction,"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.","['Hongming Zhang', 'Muhao Chen', 'Haoyu Wang', 'Yangqiu Song', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.119.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.119/,8,
CSP:Code-Switching Pre-training for Neural Machine Translation,"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.","['Zhen Yang', 'Bojie Hu', 'Ambyera Han', 'Shen Huang', 'Qi Ju']",https://www.aclweb.org/anthology/2020.emnlp-main.208.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.208/,2,
A Dataset for Tracking Entities in Open Domain Procedural Text,"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI, a high-quality (91.5% coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel model architectures.","['Niket Tandon', 'Keisuke Sakaguchi', 'Bhavana Dalvi', 'Dheeraj Rajagopal', 'Peter Clark', 'Michal Guerquin', 'Kyle Richardson', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.520.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.520/,10,
Fact or Fiction: Verifying Scientific Claims,"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.","['David Wadden', 'Shanchuan Lin', 'Kyle Lo', 'Lucy Lu Wang', 'Madeleine van Zuylen', 'Arman Cohan', 'Hannaneh Hajishirzi']",https://www.aclweb.org/anthology/2020.emnlp-main.609.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.609/,10,
Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models,"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as “neural knowledge bases” via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).","['Bill Yuchen Lin', 'Seyeon Lee', 'Rahul Khanna', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.557.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.557/,6,8
Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers,"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese→English, Turkish→English, and English→German directions. Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.","['Yimeng Wu', 'Peyman Passban', 'Mehdi Rezagholizadeh', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.74.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.74/,2,4
Neural Topic Modeling by Incorporating Document Relationship Graph,"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.","['Deyu Zhou', 'Xuemeng Hu', 'Rui Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.310.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.310/,4,8
CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval,"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url]. We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.","['Shuo Sun', 'Kevin Duh']",https://www.aclweb.org/anthology/2020.emnlp-main.340.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.340/,10,
Analyzing Redundancy in Pretrained Transformer Models,"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.","['Fahim Dalvi', 'Hassan Sajjad', 'Nadir Durrani', 'Yonatan Belinkov']",https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.398/,10,
Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation,"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers’ robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs – dialog response generation and user simulation, where our model outperforms previous strong baselines.","['Kang Min Yoo', 'Hanbit Lee', 'Franck Dernoncourt', 'Trung Bui', 'Walter Chang', 'Sang-goo Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.274.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.274/,5,10
Tackling the Low-resource Challenge for Canonical Segmentation,"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches out-perform existing ones on all languages by up to 11.4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly low-resource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages.","['Manuel Mager', 'Özlem Çetinoğlu', 'Katharina Kann']",https://www.aclweb.org/anthology/2020.emnlp-main.423.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.423/,4,1
Digital Voicing of Silent Speech,"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.","['David Gaddy', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.445.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.445/,9,
Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.","['Mengyun Chen', 'Tao Ge', 'Xingxing Zhang', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.581.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.581/,2,10
Towards Modeling Revision Requirements in wikiHow Instructions,"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.","['Irshad Bhat', 'Talita Anthonio', 'Michael Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.675.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.675/,10,4
Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.","['Simon Flachs', 'Ophélie Lacroix', 'Helen Yannakoudakis', 'Marek Rei', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.680.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.680/,10,
Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.","['Zehui Lin', 'Xiao Pan', 'Mingxuan Wang', 'Xipeng Qiu', 'Jiangtao Feng', 'Hao Zhou', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.210.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.210/,2,
IGT2P: From Interlinear Glossed Texts to Paradigms,"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.","['Sarah Moeller', 'Ling Liu', 'Changbing Yang', 'Katharina Kann', 'Mans Hulden']",https://www.aclweb.org/anthology/2020.emnlp-main.424.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.424/,1,
Is Graph Structure Necessary for Multi-hop Question Answering?,"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for textual multi-hop reasoning. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.","['Nan Shao', 'Yiming Cui', 'Ting Liu', 'Shijin Wang', 'Guoping Hu']",https://www.aclweb.org/anthology/2020.emnlp-main.583.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.583/,5,10
Intrinsic Probing through Dimension Selection,"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","['Lucas Torroba Hennigen', 'Adina Williams', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.15.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.15/,1,2
Embedding Words in Non-Vector Space with Unsupervised Graph Learning,"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.","['Max Ryabinin', 'Sergei Popov', 'Liudmila Prokhorenkova', 'Elena Voita']",https://www.aclweb.org/anthology/2020.emnlp-main.594.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.594/,4,
Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments.","['Alexander Ku', 'Peter Anderson', 'Roma Patel', 'Eugene Ie', 'Jason Baldridge']",https://www.aclweb.org/anthology/2020.emnlp-main.356.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.356/,6,2
HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification,"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.","['Wenshuo Yang', 'Jiyi Li', 'Fumiyo Fukumoto', 'Yanming Ye']",https://www.aclweb.org/anthology/2020.emnlp-main.545.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.545/,4,
Multilingual Offensive Language Identification with Cross-lingual Embeddings,"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources. We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish. Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.","['Tharindu Ranasinghe', 'Marcos Zampieri']",https://www.aclweb.org/anthology/2020.emnlp-main.470.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.470/,7,2
Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings,"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.","['Yue Wang', 'Jing Li', 'Michael Lyu', 'Irwin King']",https://www.aclweb.org/anthology/2020.emnlp-main.268.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.268/,7,8
Understanding Procedural Text using Interactive Entity Networks,"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned. In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking. In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions. Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes. We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.","['Jizhi Tang', 'Yansong Feng', 'Dongyan Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.591.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.591/,2,
Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning,"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent’s actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating commonsense captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset “Video-to-Commonsense (V2C)” that contains \sim9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.","['Zhiyuan Fang', 'Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral.', 'Yezhou Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.61.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.61/,6,5
Exploring and Predicting Transferability across NLP Tasks,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.","['Tu Vu', 'Tong Wang', 'Tsendsuren Munkhdalai', 'Alessandro Sordoni', 'Adam Trischler', 'Andrew Mattarella-Micke', 'Subhransu Maji', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.635.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.635/,10,
Towards Better Context-aware Lexical Semantics:Adjusting Contextualized Representations through Static Anchors,"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors. Our method requires only another pre-trained model and no labeled data is needed. We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.","['Qianchu Liu', 'Diana McCarthy', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.333.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.333/,4,1
SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling,"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77). In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.","['Di Wu', 'Liang Ding', 'Fan Lu', 'Jian Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.152.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.152/,3,
Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations,"Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.","['Emily Allaway', 'Kathleen McKeown']",https://www.aclweb.org/anthology/2020.emnlp-main.717.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.717/,10,4
Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation,"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.","['Mehrad Moradshahi', 'Giovanni Campagna', 'Sina Semnani', 'Silei Xu', 'Monica Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.481.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.481/,2,5
Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media,"In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.","['Shamik Roy', 'Dan Goldwasser']",https://www.aclweb.org/anthology/2020.emnlp-main.620.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.620/,7,6
Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation,"We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.","['Robert Munro', 'Alex (Carmen) Morrison']",https://www.aclweb.org/anthology/2020.emnlp-main.157.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.157/,3,10
Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution,"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.","['Yordan Yordanov', 'Oana-Maria Camburu', 'Vid Kocijan', 'Thomas Lukasiewicz']",https://www.aclweb.org/anthology/2020.emnlp-main.402.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.402/,7,10
Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning,"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.","['Wanyun Cui', 'Guangyu Zheng', 'Wei Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.444.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.444/,6,9
Social Chemistry 101: Learning to Reason about Social and Moral Norms,"Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives. For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct, such as “It is expected that you report crimes.” We present SOCIAL CHEMISTRY, a new conceptual formalism to study people’s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","['Maxwell Forbes', 'Jena D. Hwang', 'Vered Shwartz', 'Maarten Sap', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.48.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.48/,7,10
Partially-Aligned Data-to-Text Generation with Distant Supervision,"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data’s supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.","['Zihao Fu', 'Bei Shi', 'Wai Lam', 'Lidong Bing', 'Zhiyuan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.738.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.738/,8,4
Zero-Shot Crosslingual Sentence Simplification,"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.","['Jonathan Mallinson', 'Rico Sennrich', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.415.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.415/,2,
Fast semantic parsing with well-typedness guarantees,"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.","['Matthias Lindemann', 'Jonas Groschwitz', 'Alexander Koller']",https://www.aclweb.org/anthology/2020.emnlp-main.323.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.323/,1,
Sequence-Level Mixed Sample Data Augmentation,"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.","['Demi Guo', 'Yoon Kim', 'Alexander M. Rush']",https://www.aclweb.org/anthology/2020.emnlp-main.447.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.447/,4,
A Simple and Effective Model for Answering Multi-span Questions,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.","['Elad Segal', 'Avia Efrat', 'Mor Shoham', 'Amir Globerson', 'Jonathan Berant']",https://www.aclweb.org/anthology/2020.emnlp-main.248.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.248/,5,2
MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding,"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.","['Qinxin Wang', 'Hao Tan', 'Sheng Shen', 'Michael Mahoney', 'Zhewei Yao']",https://www.aclweb.org/anthology/2020.emnlp-main.159.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.159/,6,
Content Planning for Neural Story Generation with Aristotelian Rescoring,"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle’s Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.","['Seraphina Goldfarb-Tarrant', 'Tuhin Chakrabarty', 'Ralph Weischedel', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.351.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.351/,2,
Are “Undocumented Workers” the Same as “Illegal Aliens”? Disentangling Denotation and Connotation in Vector Spaces,"In politics, neologisms are frequently invented for partisan objectives. For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., “immigrants” vs. “aliens”, “estate tax” vs. “death tax”) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.","['Albert Webson', 'Zhizhong Chen', 'Carsten Eickhoff', 'Ellie Pavlick']",https://www.aclweb.org/anthology/2020.emnlp-main.335.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.335/,7,4
Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations,"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models’ performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.","['Wanrong Zhu', 'Xin Wang', 'Pradyumna Narayana', 'Kazoo Sone', 'Sugato Basu', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.708.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.708/,10,
Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of ‘the doctor removed his mask’ is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","['Ana Valeria González', 'Maria Barrett', 'Rasmus Hvingelby', 'Kellie Webster', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.209.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.209/,2,
Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.","['Weijie Yu', 'Chen Xu', 'Jun Xu', 'Liang Pang', 'Xiaopeng Gao', 'Xiaozhao Wang', 'Ji-Rong Wen']",https://www.aclweb.org/anthology/2020.emnlp-main.239.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.239/,4,1
PyMT5: multi-mode translation of natural language and Python code with transformers,"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.","['Colin Clement', 'Dawn Drain', 'Jonathan Timcheck', 'Alexey Svyatkovskiy', 'Neel Sundaresan']",https://www.aclweb.org/anthology/2020.emnlp-main.728.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.728/,2,10
Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model,"Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.","['Jun Yen Leung', 'Guy Emerson', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.329.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.329/,2,
OCR Post Correction for Endangered Language Texts,"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.","['Shruti Rijhwani', 'Antonios Anastasopoulos', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.478.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.478/,10,2
"When BERT Plays the Lottery, All Tickets Are Winning","Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful. We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.","['Sai Prasanna', 'Anna Rogers', 'Anna Rumshisky']",https://www.aclweb.org/anthology/2020.emnlp-main.259.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.259/,4,
A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media,"Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user’s historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.","['Ramit Sawhney', 'Harshit Joshi', 'Saumya Gandhi', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.619.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.619/,7,6
Contrastive Distillation on Intermediate Representations for Language Model Compression,"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.","['Siqi Sun', 'Zhe Gan', 'Yuwei Fang', 'Yu Cheng', 'Shuohang Wang', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.36.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.36/,4,
Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.","['Haozhe Ji', 'Pei Ke', 'Shaohan Huang', 'Furu Wei', 'Xiaoyan Zhu', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.54.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.54/,2,5
Unsupervised Question Decomposition for Question Answering,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.","['Ethan Perez', 'Patrick Lewis', 'Wen-tau Yih', 'Kyunghyun Cho', 'Douwe Kiela']",https://www.aclweb.org/anthology/2020.emnlp-main.713.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.713/,5,
Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character’s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.","['Victor Martinez', 'Krishna Somandepalli', 'Yalda Tehranian-Uhls', 'Shrikanth Narayanan']",https://www.aclweb.org/anthology/2020.emnlp-main.387.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.387/,3,7
Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.","['Chunyuan Li', 'Xiang Gao', 'Yuan Li', 'Baolin Peng', 'Xiujun Li', 'Yizhe Zhang', 'Jianfeng Gao']",https://www.aclweb.org/anthology/2020.emnlp-main.378.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.378/,4,
Some Languages Seem Easier to Parse Because Their Treebanks Leak,"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.",['Anders Søgaard'],https://www.aclweb.org/anthology/2020.emnlp-main.220.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.220/,1,
Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.","['Reuben Tan', 'Bryan Plummer', 'Kate Saenko']",https://www.aclweb.org/anthology/2020.emnlp-main.163.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.163/,10,7
Learning a Cost-Effective Annotation Policy for Question Answering,"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.","['Bernhard Kratzwald', 'Stefan Feuerriegel', 'Huan Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.246.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.246/,5,
Semantic Role Labeling Guided Multi-turn Dialogue ReWriter,"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.","['Kun Xu', 'Haochen Tan', 'Linfeng Song', 'Han Wu', 'Haisong Zhang', 'Linqi Song', 'Dong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.537.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.537/,1,4
LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space,"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.","['Muhammad Tasnim Mohiuddin', 'M Saiful Bari', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.215.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.215/,2,
Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments suggest that there hasn’t been much progress in multi-hop QA in the reading comprehension setting. For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning (19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction.","['Harsh Trivedi', 'Niranjan Balasubramanian', 'Tushar Khot', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.712/,5,
Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation,"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).","['Jason Lee', 'Raphael Shu', 'Kyunghyun Cho']",https://www.aclweb.org/anthology/2020.emnlp-main.73.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.73/,2,
A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization,"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.","['Jinghui Yan', 'Yining Wang', 'Lu Xiang', 'Yu Zhou', 'Chengqing Zong']",https://www.aclweb.org/anthology/2020.emnlp-main.116.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.116/,8,
Dense Passage Retrieval for Open-Domain Question Answering,"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.","['Vladimir Karpukhin', 'Barlas Oguz', 'Sewon Min', 'Patrick Lewis', 'Ledell Wu', 'Sergey Edunov', 'Danqi Chen', 'Wen-tau Yih']",https://www.aclweb.org/anthology/2020.emnlp-main.550.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.550/,5,2
Grounded Adaptation for Zero-shot Executable Semantic Parsing,"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.","['Victor Zhong', 'Mike Lewis', 'Sida I. Wang', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.558.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.558/,8,
Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation,"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.","['Maximiliana Behnke', 'Kenneth Heafield']",https://www.aclweb.org/anthology/2020.emnlp-main.211.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.211/,2,
Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training,"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.","['Wanwei He', 'Min Yang', 'Rui Yan', 'Chengming Li', 'Ying Shen', 'Ruifeng Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.281.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.281/,8,4
Understanding the Difficulty of Training Transformers,"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance","['Liyuan Liu', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.463/,4,1
Regularizing Dialogue Generation by Imitating Implicit Scenarios,"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.","['Shaoxiong Feng', 'Xuancheng Ren', 'Hongshen Chen', 'Bin Sun', 'Kan Li', 'Xu Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.534.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.534/,5,
We Can Detect Your Bias: Predicting the Political Ideology of News Articles,"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology –left, center, or right–, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.","['Ramy Baly', 'Giovanni Da San Martino', 'James Glass', 'Preslav Nakov']",https://www.aclweb.org/anthology/2020.emnlp-main.404.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.404/,10,7
Unsupervised Parsing via Constituency Tests,"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.","['Steven Cao', 'Nikita Kitaev', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.389.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.389/,1,
Visually Grounded Compound PCFGs,"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs).","['Yanpeng Zhao', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.354.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.354/,6,1
Disentangle-based Continual Graph Representation Learning,"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human’s ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models. The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.","['Xiaoyu Kou', 'Yankai Lin', 'Shaobo Liu', 'Peng Li', 'Jie Zhou', 'Yan Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.237.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.237/,4,10
Form2Seq : A Framework for Higher-Order Form Structure Extraction,"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures. We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms. To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task. We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation. Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines. Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.","['Milan Aggarwal', 'Hiresh Gupta', 'Mausoom Sarkar', 'Balaji Krishnamurthy']",https://www.aclweb.org/anthology/2020.emnlp-main.314.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.314/,8,
On the Ability and Limitations of Transformers to Recognize Formal Languages,"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.","['Satwik Bhattamishra', 'Kabir Ahuja', 'Navin Goyal']",https://www.aclweb.org/anthology/2020.emnlp-main.576.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.576/,4,2
A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation,"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model. Besides, we utilize a transfer learning method to improve the performance of OOV words. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our method achieves the state-of-the-art performances on all datasets. Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.","['Kaiyu Huang', 'Degen Huang', 'Zhuang Liu', 'Fengran Mo']",https://www.aclweb.org/anthology/2020.emnlp-main.318.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.318/,1,
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.","['Tianlu Wang', 'Xuezhi Wang', 'Yao Qin', 'Ben Packer', 'Kang Li', 'Jilin Chen', 'Alex Beutel', 'Ed Chi']",https://www.aclweb.org/anthology/2020.emnlp-main.417.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.417/,2,
An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets,"Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators’ attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.","['Gregory Spell', 'Brian Guay', 'Sunshine Hillygus', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.46.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.46/,3,7
Q-learning with Language Model for Edit-based Unsupervised Summarization,"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.","['Ryosuke Kohita', 'Akifumi Wachi', 'Yang Zhao', 'Ryuki Tachibana']",https://www.aclweb.org/anthology/2020.emnlp-main.34.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.34/,4,2
Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.","['Sanyuan Chen', 'Yutai Hou', 'Yiming Cui', 'Wanxiang Che', 'Ting Liu', 'Xiangzhan Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.634/,4,
UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.","['Jian Guan', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.736.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.736/,10,
Modeling Protagonist Emotions for Emotion-Aware Storytelling,"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.","['Faeze Brahman', 'Snigdha Chaturvedi']",https://www.aclweb.org/anthology/2020.emnlp-main.426.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.426/,3,2
Towards Persona-Based Empathetic Conversational Models,"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.","['Peixiang Zhong', 'Chen Zhang', 'Hao Wang', 'Yong Liu', 'Chunyan Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.531.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.531/,7,
Cross-Thought for Sentence Encoder Pre-training,"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.","['Shuohang Wang', 'Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Yu Cheng', 'Jingjing Liu', 'Jing Jiang']",https://www.aclweb.org/anthology/2020.emnlp-main.30.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.30/,8,5
"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching","Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.","['Sangwhan Moon', 'Naoaki Okazaki']",https://www.aclweb.org/anthology/2020.emnlp-main.631.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.631/,6,4
Revisiting Modularized Multilingual NMT to Meet Industrial Demands,"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.","['Sungwon Lyu', 'Bokyung Son', 'Kichang Yang', 'Jaekyoung Bae']",https://www.aclweb.org/anthology/2020.emnlp-main.476.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.476/,2,
T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack,"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models. Our code is publicly available at https://github.com/AI-secure/T3/.","['Boxin Wang', 'Hengzhi Pei', 'Boyuan Pan', 'Qian Chen', 'Shuohang Wang', 'Bo Li']",https://www.aclweb.org/anthology/2020.emnlp-main.495.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.495/,3,5
Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data,"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.","['Rongsheng Zhang', 'Yinhe Zheng', 'Jianzhi Shao', 'Xiaoxi Mao', 'Yadong Xi', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.277.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.277/,10,5
Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning,"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.","['Tao Shen', 'Yi Mao', 'Pengcheng He', 'Guodong Long', 'Adam Trischler', 'Weizhu Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.722.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.722/,4,8
Few-Shot Learning for Opinion Summarization,"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.","['Arthur Bražinskas', 'Mirella Lapata', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.337.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.337/,2,
To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging,"Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.","['Kasturi Bhattacharjee', 'Miguel Ballesteros', 'Rishita Anubhai', 'Smaranda Muresan', 'Jie Ma', 'Faisal Ladhak', 'Yaser Al-Onaizan']",https://www.aclweb.org/anthology/2020.emnlp-main.636.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.636/,2,8
End-to-End Slot Alignment and Recognition for Cross-Lingual NLU,"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.","['Weijia Xu', 'Batool Haider', 'Saab Mansour']",https://www.aclweb.org/anthology/2020.emnlp-main.410.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.410/,2,10
Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols,"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.","['Prachi Jain', 'Sushant Rathi', 'Mausam', 'Soumen Chakrabarti']",https://www.aclweb.org/anthology/2020.emnlp-main.305.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.305/,10,
Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading,"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern.","['Yifan Gao', 'Chien-Sheng Wu', 'Jingjing Li', 'Shafiq Joty', 'Steven C.H. Hoi', 'Caiming Xiong', 'Irwin King', 'Michael Lyu']",https://www.aclweb.org/anthology/2020.emnlp-main.191.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.191/,5,
TNT: Text Normalization based Pre-training of Transformers for Content Moderation,"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.","['Fei Tan', 'Yifan Hu', 'Changwei Hu', 'Keqian Li', 'Kevin Yen']",https://www.aclweb.org/anthology/2020.emnlp-main.383.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.383/,1,4
Online Conversation Disentanglement with Pointer Networks,"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.","['Tao Yu', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.512.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.512/,5,4
Factual Error Correction for Abstractive Summarization Models,"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.","['Meng Cao', 'Yue Dong', 'Jiapeng Wu', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.506.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.506/,2,
POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training,"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research.","['Yizhe Zhang', 'Guoyin Wang', 'Chunyuan Li', 'Zhe Gan', 'Chris Brockett', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.698.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.698/,2,
Augmented Natural Language for Generative Sequence Labeling,"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results. Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.","['Ben Athiwaratkun', 'Cicero dos Santos', 'Jason Krone', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.27.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.27/,8,
Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models,"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.","['Alexander Terenin', 'Måns Magnusson', 'Leif Jonsson']",https://www.aclweb.org/anthology/2020.emnlp-main.234.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.234/,8,
DORB: Dynamically Optimizing Multiple Rewards with Bandits,"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.","['Ramakanth Pasunuru', 'Han Guo', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.625.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.625/,4,
Substance over Style: Document-Level Targeted Content Transfer,"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model’s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.","['Allison Hegel', 'Sudha Rao', 'Asli Celikyilmaz', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.526.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.526/,2,
Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders,"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.","['Andrew Drozdov', 'Subendhu Rongali', 'Yi-Pei Chen', 'Tim O’Gorman', 'Mohit Iyyer', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.392.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.392/,1,4
Small but Mighty: New Benchmarks for Split and Rephrase,"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable syntactic cues caused by its automatic generation process. Taking advantage of such cues, we show that even a simple rule-based model can perform on par with the state-of-the-art model. To remedy such limitations, we collect and release two crowdsourced benchmark datasets. We not only make sure that they contain significantly more diverse syntax, but also carefully control for their quality according to a well-defined set of criteria. While no satisfactory automatic metric exists, we apply fine-grained manual evaluation based on these criteria using crowdsourcing, showing that our datasets better represent the task and are significantly more challenging for the models.","['Li Zhang', 'Huaiyu Zhu', 'Siddhartha Brahma', 'Yunyao Li']",https://www.aclweb.org/anthology/2020.emnlp-main.91.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.91/,2,10
Don’t Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation,"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.","['Daniel Loureiro', 'Jose Camacho-Collados']",https://www.aclweb.org/anthology/2020.emnlp-main.283.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.283/,8,10
Compositional Demographic Word Embeddings,"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.","['Charles Welch', 'Jonathan K. Kummerfeld', 'Verónica Pérez-Rosas', 'Rada Mihalcea']",https://www.aclweb.org/anthology/2020.emnlp-main.334.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.334/,7,10
OnionBot: A System for Collaborative Computational Cooking,"  An unsolved challenge in cooking automation is designing for shared kitchen workspaces. In particular, robots struggle with dexterity in the unstructured and dynamic kitchen environment. We propose that human-machine collaboration can be achieved without robotic manipulation. We describe a novel system design using computer vision to inform intelligent cooking interventions. This human-centered approach does not require actuators and promotes dynamic, natural collaboration. We show that automation that assists user-led actions can offer meaningful cooking assistance and can generate the image databases needed for fully autonomous robotic systems of the future. We provide an open source implementation of our work and encourage the research community to build upon it. ","Bennet Cobley, David Boyle",,,11,
Above Surface Interaction for Multiscale Navigation in Mobile Virtual   Reality,"  Virtual Reality enables the exploration of large information spaces. In physically constrained spaces such as airplanes or buses, controller-based or mid-air interaction in mobile Virtual Reality can be challenging. Instead, the input space on and above touch-screen enabled devices such as smartphones or tablets could be employed for Virtual Reality interaction in those spaces.   In this context, we compared an above surface interaction technique with traditional 2D on-surface input for navigating large planar information spaces such as maps in a controlled user study (n = 20). We find that our proposed above surface interaction technique results in significantly better performance and user preference compared to pinch-to-zoom and drag-to-pan when navigating planar information spaces. ",Tim Menzner and Travis Gesslein and Alexander Otte and Jens Grubert,,,11,
Quick Question: Interrupting Users for Microtasks with Reinforcement   Learning,"  Human attention is a scarce resource in modern computing. A multitude of microtasks vie for user attention to crowdsource information, perform momentary assessments, personalize services, and execute actions with a single touch. A lot gets done when these tasks take up the invisible free moments of the day. However, an interruption at an inappropriate time degrades productivity and causes annoyance. Prior works have exploited contextual cues and behavioral data to identify interruptibility for microtasks with much success. With Quick Question, we explore use of reinforcement learning (RL) to schedule microtasks while minimizing user annoyance and compare its performance with supervised learning. We model the problem as a Markov decision process and use Advantage Actor Critic algorithm to identify interruptible moments based on context and history of user interactions. In our 5-week, 30-participant study, we compare the proposed RL algorithm against supervised learning methods. While the mean number of responses between both methods is commensurate, RL is more effective at avoiding dismissal of notifications and improves user experience over time. ","Bo-Jhang Ho, Bharathan Balaji, Mehmet Koseoglu, Sandeep Sandha, Siyou   Pei, Mani Srivastava",,,11,
Designing for the Long Tail of Machine Learning,"  Recent technical advances has made machine learning (ML) a promising component to include in end user facing systems. However, user experience (UX) practitioners face challenges in relating ML to existing user-centered design processes and how to navigate the possibilities and constraints of this design space. Drawing on our own experience, we characterize designing within this space as navigating trade-offs between data gathering, model development and designing valuable interactions for a given model performance. We suggest that the theoretical description of how machine learning performance scales with training data can guide designers in these trade-offs as well as having implications for prototyping. We exemplify the learning curve's usage by arguing that a useful pattern is to design an initial system in a bootstrap phase that aims to exploit the training effect of data collected at increasing orders of magnitude. ",Martin Lindvall and Jesper Molin,,,11,
Towards Measuring Adversarial Twitter Interactions against Candidates in   the US Midterm Elections,"  Adversarial interactions against politicians on social media such as Twitter have significant impact on society. In particular they disrupt substantive political discussions online, and may discourage people from seeking public office. In this study, we measure the adversarial interactions against candidates for the US House of Representatives during the run-up to the 2018 US general election. We gather a new dataset consisting of 1.7 million tweets involving candidates, one of the largest corpora focusing on political discourse. We then develop a new technique for detecting tweets with toxic content that are directed at any specific candidate.Such technique allows us to more accurately quantify adversarial interactions towards political candidates. Further, we introduce an algorithm to induce candidate-specific adversarial terms to capture more nuanced adversarial interactions that previous techniques may not consider toxic. Finally, we use these techniques to outline the breadth of adversarial interactions seen in the election, including offensive name-calling, threats of violence, posting discrediting information, attacks on identity, and adversarial message repetition. ","Yiqing Hua, Thomas Ristenpart, Mor Naaman",,,11,
Investigating usability of mobile health applications in Bangladesh,"  Background: Lack of usability can be a major barrier for the rapid adoption of mobile services. Therefore, the purpose of this paper is to investigate the usability of Mobile Health applications in Bangladesh.   Method: We followed a 3-stage approach in our research. First, we conducted a keyword-based application search in the popular app stores. We followed the affinity diagram approach and clustered the found applications into nine groups. Second, we randomly selected four apps from each group (36 apps in total) and conducted a heuristic evaluation. Finally, we selected the highest downloaded app from each group and conducted user studies with 30 participants.   Results: We found 61% usability problems are catastrophe or major in nature from heuristic inspection. The most (21%) violated heuristic is aesthetic and minimalist design. The user studies revealed low System Usability Scale (SUS) scores for those apps that had a high number of usability problems based on the heuristic evaluation. Thus, the results of heuristic evaluation and user studies complement each other.   Conclusion: Overall, the findings suggest that the usability of the mobile health apps in Bangladesh is not satisfactory in general and could be a potential barrier for wider adoption of mobile health services. ","Muhammad Nazrul Islam, Md. Mahboob Karim, Toki Tahmid Inan, A. K. M.   Najmul Islam",,,11,
3D Augmented Reality Tangible User Interface using Commodity Hardware,"  During the last years, the emerging field of Augmented and Virtual Reality (AR-VR) has seen tremendous growth. An interface that has also become very popular for the AR systems is the tangible interface or passive-haptic interface. Specifically, an interface where users can manipulate digital information with input devices that are physical objects. This work presents a low cost Augmented Reality system with a tangible interface that offers interaction between the real and the virtual world. The system estimates in real-time the 3D position of a small colored ball (input device), it maps it to the 3D virtual world and then uses it to control the AR application that runs in a mobile device. Using the 3D position of our ""input"" device, it allows us to implement more complicated interactivity compared to a 2D input device. Finally, we present a simple, fast and robust algorithm that can estimate the corners of a convex quadrangle. The proposed algorithm is suitable for the fast registration of markers and significantly improves performance compared to the state of the art. ",Dimitrios Chamzas and Konstantinos Moustakas,,,11,
VisMaker: a Question-Oriented Visualization Recommender System for Data   Exploration,"  The increasingly rapid growth of data production and the consequent need to explore data to obtain answers to the most varied questions have promoted the development of tools to facilitate the manipulation and construction of data visualizations. However, building useful data visualizations is not a trivial task: it may involve a large number of subtle decisions that require experience from their designer. In this paper, we present VisMaker, a visualization recommender tool that uses a set of rules to present visualization recommendations organized and described through questions, in order to facilitate the understanding of the recommendations and assisting the visual exploration process. We carried out two studies comparing our tool with Voyager 2 and analyzed some aspects of the use of tools. We collected feedback from participants to identify the advantages and disadvantages of our recommendation approach. As a result, we gathered comments to help improve the development of tools in this domain. ",Raul de Ara\'ujo Lima and Simone Diniz Junqueira Barbosa,,,11,
"On-the-fly Detection of User Engagement Decrease in Spontaneous   Human-Robot Interaction, International Journal of Social Robotics, 2019","  In this paper, we consider the detection of a decrease of engagement by users spontaneously interacting with a socially assistive robot in a public space. We first describe the UE-HRI dataset that collects spontaneous Human-Robot Interactions following the guidelines provided by the Affective Computing research community to collect data ""in-the-wild"". We then analyze the users' behaviors, focusing on proxemics, gaze, head motion, facial expressions and speech during interactions with the robot. Finally, we investigate the use of deep learning techniques (Recurrent and Deep Neural Networks) to detect user engagement decrease in realtime. The results of this work highlight, in particular, the relevance of taking into account the temporal dynamics of a user's behavior. Allowing 1 to 2 seconds as buffer delay improves the performance of taking a decision on user engagement. ","Atef Ben Youssef, Giovanna Varni, Slim Essid, Chlo\'e Clavel",,,11,
Keeping Community in the Loop: Understanding Wikipedia Stakeholder   Values for Machine Learning-Based Systems,"  On Wikipedia, sophisticated algorithmic tools are used to assess the quality of edits and take corrective actions. However, algorithms can fail to solve the problems they were designed for if they conflict with the values of communities who use them. In this study, we take a Value-Sensitive Algorithm Design approach to understanding a community-created and -maintained machine learning-based algorithm called the Objective Revision Evaluation System (ORES)---a quality prediction system used in numerous Wikipedia applications and contexts. Five major values converged across stakeholder groups that ORES (and its dependent applications) should: (1) reduce the effort of community maintenance, (2) maintain human judgement as the final authority, (3) support differing peoples' differing workflows, (4) encourage positive engagement with diverse editor groups, and (5) establish trustworthiness of people and algorithms within the community. We reveal tensions between these values and discuss implications for future research to improve algorithms like ORES. ","C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren   Terveen, Haiyi Zhu",,,11,
Towards better social crisis data with HERMES: Hybrid sensing for   EmeRgency ManagEment System,"  People involved in mass emergencies increasingly publish information-rich contents in online social networks (OSNs), thus acting as a distributed and resilient network of human sensors. In this work, we present HERMES, a system designed to enrich the information spontaneously disclosed by OSN users in the aftermath of disasters. HERMES leverages a mixed data collection strategy, called hybrid crowdsensing, and state-of-the-art AI techniques. Evaluated in real-world emergencies, HERMES proved to increase: (i) the amount of the available damage information; (ii) the density (up to 7x) and the variety (up to 18x) of the retrieved geographic information; (iii) the geographic coverage (up to 30%) and granularity. ","Marco Avvenuti, Salvatore Bellomo, Stefano Cresci, Leonardo Nizzoli,   Maurizio Tesconi",,,11,
Learning to Ignore: A Case Study of Organization-Wide Bulk Email   Effectiveness,"  Bulk email is a primary communication channel within organizations, with all-company messages and regular newsletters serving as a mechanism for making employees aware of policies, events, and other needed messages. Ineffective communication could result in substantial wasted employee time and lack of awareness or compliance. Previous studies on organizational emails focused mostly on recipients. However, organizational bulk email systems are a multi-stakeholder problem including recipients, communicators, and organization itself. Thus we study the effectiveness, experience, practice, and assessments of organizational bulk email systems from different stakeholders' perspectives.   We conducted a quantitative study within a large organization to understand the extent to which the recipients retained the messages from the organizational bulk emails they received.   We conducted a qualitative study with communicators and recipients within the organization. We delved into the bulk email distributing mechanisms of the communicators, the reading behaviors of recipients, and the bulk emails' values from communicators and recipients' points of view.   We found that the recipients were not retaining most of the messages from the bulk emails though they opened a large number of them. The tools for designing and distributing organizational bulk email for communicators were very limited. The assessments on bulk emails' values and effectiveness were different between communicators and recipients. We discussed possible solutions that could improve organizational bulk emails' designing and distributing mechanisms. ","Ruoyan Kong, Haiyi Zhu and Joseph A. Konstan",,,11,
Investigating Social Haptic Illusions for Tactile Stroking (SHIFTS),"  A common and effective form of social touch is stroking on the forearm. We seek to replicate this stroking sensation using haptic illusions. This work compares two methods that provide sequential discrete stimulation: sequential normal indentation and sequential lateral skin-slip using discrete actuators. Our goals are to understand which form of stimulation more effectively creates a continuous stroking sensation, and how many discrete contact points are needed. We performed a study with 20 participants in which they rated sensations from the haptic devices on continuity and pleasantness. We found that lateral skin-slip created a more continuous sensation, and decreasing the number of contact points decreased the continuity. These results inform the design of future wearable haptic devices and the creation of haptic signals for effective social communication. ","Cara M. Nunez, Bryce N. Huerta, Allison M. Okamura, and Heather   Culbertson",,,11,
Designing for Employee Voice,"  Employee voice and workplace democracy have a positive impact on employee wellbeing and the performance of organizations. In this paper, we conducted interviews with employees to identify facilitators and inhibitors for the voice within the workplace and a corresponding set of appropriate qualities: Civility, Validity, Safety and Egalitarianism. We then operationalised these qualities as a set of design goals - Assured Anonymity, Constructive Moderation, Adequate Slowness and Controlled Access - in the design and development of a secure anonymous employee voice system. Our novel take on the Enterprise Social Network aims to foster good citizenship whilst also promoting frank yet constructive discussion. We reflect on a two-week deployment of our system, the diverse range of candid discussions that emerged around important workplace issues and the potential for change within the host organization. We conclude by reflecting on the ways in which our approach shaped the discourse and supported the creation of a trusted environment for employee voice. ","Dinislam Abdulgalimov, Reuben Kirkham, James Nicholson, Vasilis   Vlachokyriakos, Pam Briggs, Patrick Olivier",,,11,
Context-Dependent Implicit Authentication for Wearable Device User,"  As market wearables are becoming popular with a range of services, including making financial transactions, accessing cars, etc. that they provide based on various private information of a user, security of this information is becoming very important. However, users are often flooded with PINs and passwords in this internet of things (IoT) world. Additionally, hard-biometric, such as facial or finger recognition, based authentications are not adaptable for market wearables due to their limited sensing and computation capabilities. Therefore, it is a time demand to develop a burden-free implicit authentication mechanism for wearables using the less-informative soft-biometric data that are easily obtainable from the market wearables. In this work, we present a context-dependent soft-biometric-based wearable authentication system utilizing the heart rate, gait, and breathing audio signals. From our detailed analysis, we find that a binary support vector machine (SVM) with radial basis function (RBF) kernel can achieve an average accuracy of $0.94 \pm 0.07$, $F_1$ score of $0.93 \pm 0.08$, an equal error rate (EER) of about $0.06$ at a lower confidence threshold of 0.52, which shows the promise of this work. ",William Cheung and Sudip Vhaduri,,,11,
A Human-Grounded Evaluation Benchmark for Local Explanations of Machine   Learning,"  Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a human attention benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to evaluate model saliency explanations obtained using Grad-cam and LIME techniques. We demonstrate our benchmark's utility for quantitative evaluation of model explanations by comparing it with human subjective ratings and ground-truth single-layer segmentation masks evaluations. Our study results show that our threshold agnostic evaluation method with the human attention baseline is more effective than single-layer object segmentation masks to ground truth. Our experiments also reveal user biases in the subjective rating of model saliency explanations. ",Sina Mohseni and Jeremy E. Block and Eric D. Ragan,,,11,
Brain-Computer Interface in Virtual Reality,"  We study the performance of brain computer interface (BCI) system in a virtual reality (VR) environment and compare it to 2D regular displays. First, we design a headset that consists of three components: a wearable electroencephalography (EEG) device, a VR headset and an interface. Recordings of brain and behavior from human subjects, performing a wide variety of tasks using our device are collected. The tasks consist of object rotation or scaling in VR using either mental commands or facial expression (smile and eyebrow movement). Subjects are asked to repeat similar tasks on regular 2D monitor screens. The performance in 3-D virtual reality environment is considerably higher compared to the to the 2D screen. Particularly, the median number of success rate across trials for VR setting is double of that for the 2D setting (8 successful command in VR setting compared to 4 successful command in 2D screen in 1 minute trials). Our results suggest that the design of future BCI systems can remarkably benefit from the VR setting. ","Reza Abbasi-Asl, Mohammad Keshavarzi, and Dorian Yao Chan",,,11,
Subtle Sensing: Detecting Differences in the Flexibility of Virtually   Simulated Molecular Objects,"  During VR demos we have performed over last few years, many participants (in the absence of any haptic feedback) have commented on their perceived ability to 'feel' differences between simulated molecular objects. The mechanisms for such 'feeling' are not entirely clear: observing from outside VR, one can see that there is nothing physical for participants to 'feel'. Here we outline exploratory user studies designed to evaluate the extent to which participants can distinguish quantitative differences in the flexibility of VR-simulated molecular objects. The results suggest that an individual's capacity to detect differences in molecular flexibility is enhanced when they can interact with and manipulate the molecules, as opposed to merely observing the same interaction. Building on these results, we intend to carry out further studies investigating humans' ability to sense quantitative properties of VR simulations without haptic technology. ","Rhoslyn Roebuck Williams, Xan Varcoe, Becca R. Glowacki, Ella M. Gale,   Alexander Jamieson-Binnie, David R. Glowacki",,,11,
Familiarization tours for first-time users of highly automated cars:   Comparing the effects of virtual environments with different levels of   interaction fidelity,"  Research in aviation and driving has highlighted the importance of training as an effective approach to reduce the costs associated with the supervisory role of the human in automated systems. However, only a few studies have investigated the effect of pre-trip familiarization tours on highly automated driving. In the present study, a driving simulator experiment compared the effectiveness of four familiarization groups, control, video, low fidelity virtual reality (VR), and high fidelity VR on automation trust and driving performance in several critical and non-critical transition tasks. The results revealed the positive impact of familiarization tours on trust, takeover, and handback performance at the first time of measurement. Takeover quality only improved when practice was presented in high-fidelity VR. After three times of exposure to transition requests, trust and transition performance of all groups converged to those of the high fidelity VR group, demonstrating that: a) experiencing automation failures during the training may reduce costs associated with first failures in highly automated driving; b) the VR tour with high level of interaction fidelity is superior to other types of familiarization tour, and c) uneducated and less-educated drivers learn about automation by experiencing it. Knowledge resulting from this research could help develop cost-effective familiarization tours for highly automated vehicles in dealerships and car rental centers. ","Mahdi Ebnali, Richard Lamb, Razieh Fathi",,,11,
"Printmaking, Puzzles, and Studio Closets: Using Artistic Metaphors to   Reimagine the User Interface for Designing Immersive Visualizations","  We, as a society, need artists to help us interpret and explain science, but what does an artist's studio look like when today's science is built upon the language of large, increasingly complex data? This paper presents a data visualization design interface that lifts the barriers for artists to engage with actively studied, 3D multivariate datasets. To accomplish this, the interface must weave together the need for creative artistic processes and the challenging constraints of real-time, data-driven 3D computer graphics. The result is an interface for a technical process, but technical in the way artistic printmaking is technical, not in the sense of computer scripting and programming. Using metaphor, computer graphics algorithms and shader program parameters are reimagined as tools in an artist's printmaking studio. These artistic metaphors and language are merged with a puzzle-piece approach to visual programming and matching iconography. Finally, artists access the interface using a web browser, making it possible to design immersive multivariate data visualizations that can be displayed in VR and AR environments using familiar drawing tablets and touch screens. We report on insights from the interdisciplinary design of the interface and early feedback from artists. ","Bridger Herman, Francesca Samsel, Annie Bares, Seth Johnson, Greg   Abram, Daniel F. Keefe",,,11,
Medial Axis Isoperimetric Profiles,"  Recently proposed as a stable means of evaluating geometric compactness, the isoperimetric profile of a planar domain measures the minimum perimeter needed to inscribe a shape with prescribed area varying from 0 to the area of the domain. While this profile has proven valuable for evaluating properties of geographic partitions, existing algorithms for its computation rely on aggressive approximations and are still computationally expensive. In this paper, we propose a practical means of approximating the isoperimetric profile and show that for domains satisfying a ""thick neck"" condition, our approximation is exact. For more general domains, we show that our bound is still exact within a conservative regime and is otherwise an upper bound. Our method is based on a traversal of the medial axis which produces efficient and robust results. We compare our technique with the state-of-the-art approximation to the isoperimetric profile on a variety of domains and show significantly tighter bounds than were previously achievable. ","Paul Zhang, Daryl Deford, Justin Solomon",,,11,
Oriented Convex Containers of Polygons -- II,"  We define an 'oriented convex region' as a convex region with a direction of symmetry. An earlier article had touched upon isosceles triangles, rectangles and ellipses. Here, we examine some more possible oriented containers - semicircles, segments of circles and sectors - and raise some questions. ",R Nandakumar,,,11,
Framework for $\exists \mathbb{R}$-Completeness of Two-Dimensional   Packing Problems,"  We show that many natural two-dimensional packing problems are algorithmically equivalent to finding real roots of multivariate polynomials. A two-dimensional packing problem is defined by the type of pieces, containers, and motions that are allowed. The aim is to decide if a given set of pieces can be placed inside a given container. The pieces must be placed so that they are pairwise interior-disjoint, and only motions of the allowed type can be used to move them there. We establish a framework which enables us to show that for many combinations of allowed pieces, containers, and motions, the resulting problem is $\exists\mathbb R$-complete. This means that the problem is equivalent (under polynomial time reductions) to deciding whether a given system of polynomial equations and inequalities with integer coefficients has a real solution. We consider packing problems where only translations are allowed as the motions, and problems where arbitrary rigid motions are allowed, i.e., both translations and rotations. When rotations are allowed, we show that the following combinations of allowed pieces and containers are $\exists\mathbb R$-complete:   $\bullet$ simple polygons, each of which has at most 8 corners, into a square,   $\bullet$ convex objects bounded by line segments and hyperbolic curves into a square,   $\bullet$ convex polygons into a container bounded by line segments and hyperbolic curves.   Restricted to translations, we show that the following combinations are $\exists\mathbb R$-complete:   $\bullet$ objects bounded by segments and hyperbolic curves into a square,   $\bullet$ convex polygons into a container bounded by segments and hyperbolic curves. ","Mikkel Abrahamsen, Tillmann Miltzow, Nadja Seiferth",,,11,
"Bregman Voronoi Diagrams: Properties, Algorithms and Applications","  The Voronoi diagram of a finite set of objects is a fundamental geometric structure that subdivides the embedding space into regions, each region consisting of the points that are closer to a given object than to the others. We may define many variants of Voronoi diagrams depending on the class of objects, the distance functions and the embedding space. In this paper, we investigate a framework for defining and building Voronoi diagrams for a broad class of distance functions called Bregman divergences. Bregman divergences include not only the traditional (squared) Euclidean distance but also various divergence measures based on entropic functions. Accordingly, Bregman Voronoi diagrams allow to define information-theoretic Voronoi diagrams in statistical parametric spaces based on the relative entropy of distributions. We define several types of Bregman diagrams, establish correspondences between those diagrams (using the Legendre transformation), and show how to compute them efficiently. We also introduce extensions of these diagrams, e.g. k-order and k-bag Bregman Voronoi diagrams, and introduce Bregman triangulations of a set of points and their connexion with Bregman Voronoi diagrams. We show that these triangulations capture many of the properties of the celebrated Delaunay triangulation. Finally, we give some applications of Bregman Voronoi diagrams which are of interest in the context of computational geometry and machine learning. ","Frank Nielsen, Jean-Daniel Boissonnat, Richard Nock",,,11,
On homotopy continuation based singularity distance computations for   3-RPR manipulators,"  It is known that parallel manipulators suffer from singular configurations. Evaluating the distance between a given configuration to the closest singular one is of interest for industrial applications (e.g.\ singularity-free path planning). For parallel manipulators of Stewart-Gough type, geometric meaningful distance measures are known, which are used for the computation of the singularity distance as the global minimizer of an optimization problem. In the case of hexapods and linear pentapods the critical points of the corresponding polynomial Lagrange function cannot be found by the Grobner basis method due to the degree and number of unknowns. But this polynomial system of equations can be solved by software tools of numerical algebraic geometry relying on homotopy continuation. To gain experiences for the treatment of the mentioned spatial manipulators, this paper attempts to find minimal multi-homogeneous Bezout numbers for the homotopy continuation based singularity distance computation with respect to various algebraic motion representations of planar Euclidean/equiform kinematics. The homogenous and non-homogenous representations under study are compared and discussed based on the 3-RPR manipulator. ",Aditya Kapilavai and Georg Nawratil,,,11,
Application of Mean Curvature Flow for Surface Parametrizations,"  This is an expository article describing the conformalized mean curvature flow, originally introduced by Kazhdan, Solomon, and Ben-Chen. We are interested in applying mean curvature flow to surface parametrizations. We discuss our own implementation of their algorithm and some limitations. ",Ka Wai Wong,,,11,
Obtaining a Canonical Polygonal Schema from a Greedy Homotopy Basis with   Minimal Mesh Refinement,"  Any closed manifold of genus g can be cut open to form a topological disk and then mapped to a regular polygon with 4g sides. This construction is called the canonical polygonal schema of the manifold, and is a key ingredient for many applications in graphics and engineering, where a parameterization between two shapes with same topology is often needed. The sides of the 4g-gon define on the manifold a system of loops, which all intersect at a single point and are disjoint elsewhere. Computing a shortest system of loops of this kind is NP-hard. A computationally tractable alternative consists in computing a set of shortest loops that are not fully disjoint in polynomial time, using the greedy homotopy basis algorithm proposed by Erickson and Whittlesey, and then detach them in post processing via mesh refinement. Despite this operation is conceptually simple, known refinement strategies do not scale well for high genus shapes, triggering a mesh growth that may exceed the amount of memory available in modern computers, leading to failures. In this paper we study various local refinement operators to detach cycles in a system of loops, and show that there are important differences between them, both in terms of mesh complexity and preservation of the original surface. We ultimately propose two novel refinement approaches: the former minimizes the number of new elements in the mesh, possibly at the cost of a deviation from the input geometry. The latter allows to trade mesh complexity for geometric accuracy, bounding deviation from the input surface. Both strategies are trivial to implement, and experiments confirm that they allow to realize canonical polygonal schemas even for extremely high genus shapes where previous methods fail. ",Marco Livesu,,,11,
Approximating the packedness of polygonal curves,  In 2012 Driemel et al. \cite{DBLP:journals/dcg/DriemelHW12} introduced the concept of $c$-packed curves as a realistic input model. In the case when $c$ is a constant they gave a near linear time $(1+\varepsilon)$-approximation algorithm for computing the Fr\'echet distance between two $c$-packed polygonal curves. Since then a number of papers have used the model.   In this paper we consider the problem of computing the smallest $c$ for which a given polygonal curve in $\mathbb{R}^d$ is $c$-packed. We present two approximation algorithms. The first algorithm is a $2$-approximation algorithm and runs in $O(dn^2 \log n)$ time. In the case $d=2$ we develop a faster algorithm that returns a $(6+\varepsilon)$-approximation and runs in $O((n/\varepsilon^3)^{4/3} polylog (n/\varepsilon)))$ time.   We also implemented the first algorithm and computed the approximate packedness-value for 16 sets of real-world trajectories. The experiments indicate that the notion of $c$-packedness is a useful realistic input model for many curves and trajectories. ,"Joachim Gudmundsson, Yuan Sha and Sampson Wong",,,11,
The Parameterized Hardness of Art Gallery Problems,"  Given a simple polygon $\mathcal{P}$ on $n$ vertices, two points $x,y$ in $\mathcal{P}$ are said to be visible to each other if the line segment between $x$ and $y$ is contained in $\mathcal{P}$. The Point Guard Art Gallery problem asks for a minimum set $S$ such that every point in $\mathcal{P}$ is visible from a point in $S$. The Vertex Guard Art Gallery problem asks for such a set $S$ subset of the vertices of $\mathcal{P}$. A point in the set $S$ is referred to as a guard. For both variants, we rule out any $f(k)n^{o(k / \log k)}$ algorithm, where $k := |S|$ is the number of guards, for any computable function $f$, unless the Exponential Time Hypothesis fails. These lower bounds almost match the $n^{O(k)}$ algorithms that exist for both problems. ",\'Edouard Bonnet and Tillmann Miltzow,,,11,
Minimum bounded chains and minimum homologous chains in embedded   simplicial complexes,"  We study two optimization problems on simplicial complexes with homology over $\mathbb{Z}_2$, the minimum bounded chain problem: given a $d$-dimensional complex $\mathcal{K}$ embedded in $\mathbb{R}^{d+1}$ and a null-homologous $(d-1)$-cycle $C$ in $\mathcal{K}$, find the minimum $d$-chain with boundary $C$, and the minimum homologous chain problem: given a $(d+1)$-manifold $\mathcal{M}$ and a $d$-chain $D$ in $\mathcal{M}$, find the minimum $d$-chain homologous to $D$. We show strong hardness results for both problems even for small values of $d$; $d = 2$ for the former problem, and $d=1$ for the latter problem. We show that both problems are APX-hard, and hard to approximate within any constant factor assuming the unique games conjecture. On the positive side, we show that both problems are fixed parameter tractable with respect to the size of the optimal solution. Moreover, we provide an $O(\sqrt{\log \beta_d})$-approximation algorithm for the minimum bounded chain problem where $\beta_d$ is the $d$th Betti number of $\mathcal{K}$. Finally, we provide an $O(\sqrt{\log n_{d+1}})$-approximation algorithm for the minimum homologous chain problem where $n_{d+1}$ is the number of $d$-simplices in $\mathcal{M}$. ","Glencora Borradaile, William Maxwell, Amir Nayyeri",,,11,
Quadrilateral Mesh Generation III: Optimizing Singularity Configuration   Based on Abel-Jacobi Theory,  This work proposes a rigorous and practical algorithm for generating meromorphic quartic differentials for the purpose of quad-mesh generation. The work is based on the Abel-Jacobi theory of algebraic curve. The algorithm pipeline can be summarized as follows: calculate the homology group; compute the holomorphic differential group; construct the period matrix of the surface and Jacobi variety; calculate the Abel-Jacobi map for a given divisor; optimize the divisor to satisfy the Abel-Jacobi condition by an integer programming; compute the flat Riemannian metric with cone singularities at the divisor by Ricci flow; isometric immerse the surface punctured at the divisor onto the complex plane and pull back the canonical holomorphic differential to the surface to obtain the meromorphic quartic differential; construct the motor-graph to generate the resulting T-Mesh. The proposed method is rigorous and practical. The T-mesh results can be applied for constructing T-Spline directly. The efficiency and efficacy of the proposed algorithm are demonstrated by experimental results. ,"Xiaopeng Zheng, Yiming Zhu, Na Lei, Zhongxuan Luo, Xianfeng Gu",,,11,
Dots & Polygons,"  We present a new game, Dots & Polygons, played on a planar point set. Players take turns connecting two points, and when a player closes a (simple) polygon, the player scores its area. We show that deciding whether the game can be won from a given state, is NP-hard. We do so by a reduction from vertex-disjoint cycle packing in cubic planar graphs, including a self-contained reduction from planar 3-Satisfiability to this cycle-packing problem. This also provides a simple proof of the NP-hardness of the related game Dots & Boxes. For points in convex position, we discuss a greedy strategy for Dots & Polygons. ","Kevin Buchin, Mart Hagedoorn, Irina Kostitsyna, Max van Mulken, Jolan   Rensen, Leo van Schooten",,,11,
Affine invariant triangulations,"  We study affine invariant 2D triangulation methods. That is, methods that produce the same triangulation for a point set $S$ for any (unknown) affine transformation of $S$. Our work is based on a method by Nielson [A characterization of an affine invariant triangulation. Geom. Mod, 191-210. Springer, 1993] that uses the inverse of the covariance matrix of $S$ to define an affine invariant norm, denoted $A_{S}$, and an affine invariant triangulation, denoted ${DT}_{A_{S}}[S]$. We revisit the $A_{S}$-norm from a geometric perspective, and show that ${DT}_{A_{S}}[S]$ can be seen as a standard Delaunay triangulation of a transformed point set based on $S$. We prove that it retains all of its well-known properties such as being 1-tough, containing a perfect matching, and being a constant spanner of the complete geometric graph of $S$. We show that the $A_{S}$-norm extends to a hierarchy of related geometric structures such as the minimum spanning tree, nearest neighbor graph, Gabriel graph, relative neighborhood graph, and higher order versions of these graphs. In addition, we provide different affine invariant sorting methods of a point set $S$ and of the vertices of a polygon $P$ that can be combined with known algorithms to obtain other affine invariant triangulation methods of $S$ and of $P$. ","Prosenjit Bose, Pilar Cano, Rodrigo I. Silveira",,,11,
Geometric secluded paths and planar satisfiability,"  We consider paths with low \emph{exposure} to a 2D polygonal domain, i.e., paths which are seen as little as possible; we differentiate between \emph{integral} exposure (when we care about how long the path sees every point of the domain) and \emph{0/1} exposure (just counting whether a point is seen by the path or not). For the integral exposure, we give a PTAS for finding the minimum-exposure path between two given points in the domain; for the 0/1 version, we prove that in a simple polygon the shortest path has the minimum exposure, while in domains with holes the problem becomes NP-hard. We also highlight connections of the problem to minimum satisfiability and settle hardness of variants of planar min- and max-SAT. ","Kevin Buchin, Valentin Polishchuk, Leonid Sedov, Roman Voronov",,,11,
$\epsilon$-net Induced Lazy Witness Complexes on Graphs,"  Computation of persistent homology of simplicial representations such as the Rips and the C\v{e}ch complexes do not efficiently scale to large point clouds. It is, therefore, meaningful to devise approximate representations and evaluate the trade-off between their efficiency and effectiveness. The lazy witness complex economically defines such a representation using only a few selected points, called landmarks.   Topological data analysis traditionally considers a point cloud in a Euclidean space. In many situations, however, data is available in the form of a weighted graph. A graph along with the geodesic distance defines a metric space. This metric space of a graph is amenable to topological data analysis.   We discuss the computation of persistent homologies on a weighted graph. We present a lazy witness complex approach leveraging the notion of $\epsilon$-net that we adapt to weighted graphs and their geodesic distance to select landmarks. We show that the value of the $\epsilon$ parameter of the $\epsilon$-net provides control on the trade-off between choice and number of landmarks and the quality of the approximate simplicial representation.   We present three algorithms for constructing an $\epsilon$-net of a graph. We comparatively and empirically evaluate the efficiency and effectiveness of the choice of landmarks that they induce for the topological data analysis of different real-world graphs. ","Naheed Anjum Arafat, Debabrota Basu and St\'ephane Bressan",,,11,
Shortest Secure Path in a Voronoi Diagram,"  We investigate the problem of computing the shortest secure path in a Voronoi diagram. Here, a path is secure if it is a sequence of touching Voronoi cells, where each Voronoi cell in the path has a uniform cost of being secured. Importantly, we allow inserting new sites, which in some cases leads to significantly shorter paths. We present an $O(n \log n)$ time algorithm for solving this problem in the plane, which uses a dynamic additive weighted Voronoi diagram to compute this path. The algorithm is an interesting combination of the continuous and discrete Dijkstra algorithms. We also implemented the algorithm using CGAL. ",Sariel Har-Peled and Rajgopal Varadharajan,,,11,
A Generalization of Self-Improving Algorithms,"  Ailon et al. [SICOMP'11] proposed self-improving algorithms for sorting and Delaunay triangulation (DT) when the input instances $x_1,\cdots,x_n$ follow some unknown \emph{product distribution}. That is, $x_i$ comes from a fixed unknown distribution $\mathsf{D}_i$, and the $x_i$'s are drawn independently. After spending $O(n^{1+\varepsilon})$ time in a learning phase, the subsequent expected running time is $O((n+ H)/\varepsilon)$, where $H \in \{H_\mathrm{S},H_\mathrm{DT}\}$, and $H_\mathrm{S}$ and $H_\mathrm{DT}$ are the entropies of the distributions of the sorting and DT output, respectively. In this paper, we allow dependence among the $x_i$'s under the \emph{group product distribution}. There is a hidden partition of $[1,n]$ into groups; the $x_i$'s in the $k$-th group are fixed unknown functions of the same hidden variable $u_k$; and the $u_k$'s are drawn from an unknown product distribution. We describe self-improving algorithms for sorting and DT under this model when the functions that map $u_k$ to $x_i$'s are well-behaved. After an $O(\mathrm{poly}(n))$-time training phase, we achieve $O(n + H_\mathrm{S})$ and $O(n\alpha(n) + H_\mathrm{DT})$ expected running times for sorting and DT, respectively, where $\alpha(\cdot)$ is the inverse Ackermann function. ",Siu-Wing Cheng and Man-Kwun Chiu and Kai Jin and Man Ting Wong,,,11,
An Integer-Linear Program for Bend-Minimization in Ortho-Radial Drawings,"  An ortho-radial grid is described by concentric circles and straight-line spokes emanating from the circles' center. An ortho-radial drawing is the analog of an orthogonal drawing on an ortho-radial grid. Such a drawing has an unbounded outer face and a central face that contains the origin. Building on the notion of an ortho-radial representation (Barth et al., SoCG, 2017), we describe an integer-linear program (ILP) for computing bend-free ortho-radial representations with a given embedding and fixed outer and central face. Using the ILP as a building block, we introduce a pruning technique to compute bend-optimal ortho-radial drawings with a given embedding and a fixed outer face, but freely choosable central face. Our experiments show that, in comparison with orthogonal drawings using the same embedding and the same outer face, the use of ortho-radial drawings reduces the number of bends by 43.8% on average. Further, our approach allows us to compute ortho-radial drawings of embedded graphs such as the metro system of Beijing or London within seconds. ","Benjamin Niedermann, Ignaz Rutter",,,11,
Rectilinear Link Diameter and Radius in a Rectilinear Polygonal Domain,"  We study the computation of the diameter and radius under the rectilinear link distance within a rectilinear polygonal domain of $n$ vertices and $h$ holes. We introduce a \emph{graph of oriented distances} to encode the distance between pairs of points of the domain. This helps us transform the problem so that we can search through the candidates more efficiently. Our algorithm computes both the diameter and the radius in $\min \{\,O(n^\omega), O(n^2 + nh \log h + \chi^2)\,\}$ time, where $\omega<2.373$ denotes the matrix multiplication exponent and $\chi\in \Omega(n)\cap O(n^2)$ is the number of edges of the graph of oriented distances. We also provide a faster algorithm for computing the diameter that runs in $O(n^2 \log n)$ time. ","Elena Arseneva, Man-Kwun Chiu, Matias Korman, Aleksandar Markovic,   Yoshio Okamoto, Aur\'elien Ooms, Andr\'e van Renssen, Marcel Roeloffzen",,,11,
The Very Best of Perfect Non-crossing Matchings,"  Given a set of points in the plane, we are interested in matching them with straight line segments. We focus on perfect (all points are matched) non-crossing (no two edges intersect) matchings. Apart from the well known MinMax variation, where the length of the longest edge is minimized, we extend work by looking into different optimization variants such as MaxMin, MinMin, and MaxMax. We consider both the monochromatic and bichromatic versions of these problems and provide efficient algorithms for various input point configurations. ","Ioannis Mantas, Marko Savi\'c, Hendrik Schrezenmaier",,,11,
On the Support Recovery of Jointly Sparse Gaussian Sources using Sparse   Bayesian Learning,"  In this work, we provide non-asymptotic, probabilistic guarantees for successful recovery of the common nonzero support of jointly sparse Gaussian sources in the multiple measurement vector (MMV) problem. The support recovery problem is formulated as the Type-II maximum likelihood (ML) estimation of the variance hyperparameters of a joint sparsity inducing Gaussian prior on the source signals. We derive conditions under which the resulting nonconvex constrained optimization perfectly recovers the nonzero support of a joint-sparse Gaussian source ensemble with arbitrarily high probability. The support error probability decays exponentially with the number of MMVs at a rate that depends on the smallest restricted singular value and the nonnegative null space property of the self Khatri-Rao product of the sensing matrix. Our support consistency guarantee for the constrained Type-II ML solution extends to any global solution of the multiple sparse Bayesian learning (M-SBL) optimization whose nonzero coefficients lie inside a bounded interval. Our analysis confirms that nonzero supports of size as high as O($m^2$) are recoverable from $m$ measurements per sparse vector. For the case of noiseless measurements, we show that a single MMV is sufficient for perfect recovery of the $k$-sparse support by M-SBL, provided all subsets of $k + 1$ columns of the sensing matrix are linearly independent. ",Saurabh Khanna and Chandra R. Murthy,,,11,
Deep Learning for Massive MIMO Channel State Acquisition and Feedback,"  Massive multiple-input multiple-output (MIMO) systems are a main enabler of the excessive throughput requirements in 5G and future generation wireless networks as they can serve many users simultaneously with high spectral and energy efficiency. To achieve this, massive MIMO systems require accurate and timely channel state information (CSI), which is acquired by a training process that involves pilot transmission, CSI estimation and feedback. This training process incurs a training overhead, which scales with the number of antennas, users and subcarriers. Reducing this training overhead in massive MIMO systems has been a major topic of research since the emergence of the concept. Recently, deep learning (DL)-based approaches for massive MIMO training have been proposed and showed significant improvements compared to traditional techniques. This paper provides an overview of how neural networks (NNs) can be used in the training process of massive MIMO systems to improve the performance by reducing the CSI acquisition overhead and to reduce complexity. ","Mahdi Boloursaz Mashhadi, and Deniz G\""und\""uz",,,11,
Reconfigurable ULAs for Line-of-Sight MIMO Transmission,"  This paper establishes an upper bound on the capacity of line-of-sight multiantenna channels over all possible antenna arrangements and shows that uniform linear arrays (ULAs) with an SNR-dependent rotation of transmitter or receiver can closely approach such capacity---and in fact achieve it at low and high SNR, and asymptotically in the numbers of antennas. Then, as an alternative to mechanically rotating ULAs, we propose to electronically select among multiple ULAs having a radial disposition at either transmitter or receiver, and we bound the shortfall from capacity as a function of the number of such ULAs. With only three ULAs, properly angled, 96% of the capacity can be achieved. Finally, we further introduce reduced-complexity precoders and linear receivers that capitalize on the structure of the channels spawned by these configurable ULA architectures. ","Heedong Do, Namyoon Lee, Angel Lozano",,,11,
Reconfigurable Intelligent Surface Assisted Device-to-Device   Communications,"  With the evolution of the 5G, 6G and beyond, device-to-device (D2D) communication has been developed as an energy-, and spectrum-efficient solution. In cellular network, D2D links need to share the same spectrum resources with the cellular link. A reconfigurable intelligent surface (RIS) can reconfigure the phase shifts of elements and create favorable beam steering, which can mitigate aggravated interference caused by D2D links. In this paper, we study a RIS-assisted single cell uplink communication network scenario, where the cellular link and multiple D2D links utilize direct propagation and reflecting one-hop propagation. The problem of maximizing the total system rate is formulated by jointly optimizing transmission powers of all links and discrete phase shifts of all elements. The formulated problem is an NP-hard mixed integer non-convex non-linear problem. To obtain practical solutions, we capitalize on alternating maximization and the problem is decomposed into two sub-problems. For the power allocation, the problem is a difference of concave functions (DC) problem, which is solved with the gradient descent method. For the phase shift, a local search algorithm with lower complexity is utilized. Simulation results show that deploying RIS and optimizing the phase shifts have a significant effect on mitigating D2D network interference. ","Yali Chen, Bo Ai, Hongliang Zhang, Yong Niu, Lingyang Song, Zhu Han,   H. Vincent Poor",,,11,
Third-Order Asymptotics of Variable-Length Compression Allowing Errors,"  This study investigates the fundamental limits of variable-length compression in which prefix-free constraints are not imposed (i.e., one-to-one codes are studied) and non-vanishing error probabilities are permitted. Due in part to a crucial relation between the variable-length and fixed-length compression problems, our analysis requires a careful and refined analysis of the fundamental limits of fixed-length compression in the setting where the error probabilities are allowed to approach either zero or one polynomially in the blocklength. To obtain the refinements, we employ tools from moderate deviations and strong large deviations. Finally, we provide the third-order asymptotics for the problem of variable-length compression with non-vanishing error probabilities. We show that unlike several other information-theoretic problems in which the third-order asymptotics are known, for the problem of interest here, the third-order term depends on the permissible error probability. ",Yuta Sakai and Vincent Y. F. Tan,,,11,
Gelfand numbers related to structured sparsity and Besov space   embeddings with small mixed smoothness,"  We consider the problem of determining the asymptotic order of the Gelfand numbers of mixed-(quasi-)norm embeddings $\ell^b_p(\ell^d_q) \hookrightarrow \ell^b_r(\ell^d_u)$ given that $p \leq r$ and $q \leq u$, with emphasis on cases with $p\leq 1$ and/or $q\leq 1$. These cases turn out to be related to structured sparsity. We obtain sharp bounds in a number of interesting parameter constellations. Our new matching bounds for the Gelfand numbers of the embeddings of $\ell_1^b(\ell_2^d)$ and $\ell_2^b(\ell_1^d)$ into $\ell_2^b(\ell_2^d)$ imply optimality assertions for the recovery of block-sparse and sparse-in-levels vectors, respectively. In addition, we apply the sharp estimates for $\ell^b_p(\ell^d_q)$-spaces to obtain new two-sided estimates for the Gelfand numbers of multivariate Besov space embeddings in regimes of small mixed smoothness. It turns out that in some particular cases these estimates show the same asymptotic behaviour as in the univariate situation. In the remaining cases they differ at most by a $\log\log$ factor from the univariate bound. ","Sjoerd Dirksen, Tino Ullrich",,,11,
Hardware Impaired Ambient Backscatter NOMA Systems: Reliability and   Security,"  Non-orthogonal multiple access (NOMA) and ambient backscatter communication have been envisioned as two promising technologies for the Internet-of-things due to their high spectral efficiency and energy efficiency. Motivated by this fact, we consider an ambient backscatter NOMA system in the presence of a malicious eavesdropper. Under some realistic assumptions of residual hardware impairments (RHIs), channel estimation errors (CEEs) and imperfect successive interference cancellation (ipSIC), we investigate the physical layer security (PLS) of the ambient backscatter NOMA systems focusing on reliability and security. In order to further improve the security of the considered system, an artificial noise scheme is proposed where the radio frequency (RF) source acts as a jammer that transmits interference signal to the legitimate receivers and eavesdropper. On this basis, the analytical expressions for the outage probability (OP) and the intercept probability (IP) are derived. To gain more insights, the asymptotic analysis and diversity orders for the OP in the high signal-to-noise ratio (SNR) regime are carried out, and the asymptotic behaviors of the IP in the high main-to-eavesdropper ratio (MER) region are explored as well. Numerical results show that: 1) RHIs, CEEs and ipSIC have negative effects on the OP but positive effects on the IP; 2) Compared with CEEs, RHIs have a more serious impact on the reliability and security of the considered system; 3) There exists a trade-off between reliability and security, and this trade-off can be optimized by reducing the power coefficient of the artificial noise or increasing the interfering factor of readers; 4) There are error floors for the OP due to the CEEs and the reflection coefficient; 5) As MER grows large, the security for Rnand Rf is improved, while the security for T is reduced. ","Xingwang Li, Mengle Zhao, Ming Zeng, Shahid Mumtaz, Varun G Menon,   Zhiguo Ding, Octavia A. Dobre",,,11,
Online Estimation and Adaptation for Random Access with Successive   Interference Cancellation,"  This paper proposes an adaptive transmission algorithm for slotted random access systems supporting the successive interference cancellation (SIC) at the access point (AP). When multiple users transmit packets simultaneously in a slot, owing to the SIC technique, the AP is able to decode them through SIC resolve procedures (SRPs), which may occupy multiple consequent slots. While such an SRP could potentially improve the system throughput, how to fully exploit this capability in practical systems is still questionable. In particular, the number of active users contending for the channel varies over time which complicates the algorithm design. By fully exploiting the potential of SIC, the proposed algorithm is designed to maximize the system throughput and minimize the access delay. For this purpose, an online estimation is introduced to estimate the number of active users in real-time and controls their transmissions accordingly. It is shown that the throughput of the proposed algorithm can reach up to 0.693 packets/slot under practical assumptions, which is the first result achieving the throughput limit proved by Yu-Giannakis. It is further shown that the system throughput of 0.559 packets/slot (80.6$\%$ of the throughput limit) is still achievable when the SIC capability is restricted by two. ",Sang-Woon Jeon and Hu Jin,,,11,
Quantization Games on Social Networks and Language Evolution,"  We consider a strategic network quantizer design setting where agents must balance fidelity in representing their local source distributions against their ability to successfully communicate with other connected agents. We study the problem as a network game and show existence of Nash equilibrium quantizers. For any agent, under Nash equilibrium, the word representing a given partition region is the conditional expectation of the mixture of local and social source probability distributions within the region. Since having knowledge of the original source of information in the network may not be realistic, we show that under certain conditions, the agents need not know the source origin and yet still settle on a Nash equilibrium using only the observed sources. Further, the network may converge to equilibrium through a distributed version of the Lloyd-Max algorithm. In contrast to traditional results in the evolution of language, we find several vocabularies may coexist in the Nash equilibrium, with each individual having exactly one of these vocabularies. The overlap between vocabularies is high for individuals that communicate frequently and have similar local sources. Finally, we argue that error in translation along a chain of communication does not grow if and only if the chain consists of agents with shared vocabulary. Numerical results are given. ","Ankur Mani, Lav R. Varshney, and Alex (Sandy) Pentland",,,11,
Recursive Optimization of Finite Blocklength Allocation to Mitigate   Age-of-Information Outage,"  As an emerging metric for the timeliness of information delivery, Age-of-Information (AoI) raises a special interest in the research area of tolerance-critical communications, wherein sufficiently short blocklength is usually adopted as an essential requirement. However, the interplay between AoI and finite blocklength is scantly treated. This paper studies the occurrence of high AoI, i.e., AoI outage, in TDMA systems with respect to the blocklength allocation among users. A Markov Decision Process model is set up for the problem, which enables a static state analysis, and therewith a policy iteration approach to improve the AoI robustness is proposed. The burstiness of outages is also analyzed to provide additional insights into this problem in the finite blocklength (FBL) regime. It is shown that, different from average AoI optimizations, a risk-sensitive approach is significantly beneficial for AoI outage optimizations, on account of the FBL regime. ","Bin Han, Zhiyuan Jiang, Yao Zhu, and Hans D. Schotten",,,11,
Rate Adaptation and Latency in Heterogeneous IoT Networks,"  This paper studies the effect of rate adaptation in time slotted Internet of things (IoT) networks. For a given time slot duration and packets size, rate adaptation necessitates packet fragmentation to fit the time slot duration. Accounting for the quality and time resolution of the underlying traffic, this paper characterizes the tradeoff between transmission rate and packet latency in IoT networks. Using tools from stochastic geometry and queueing theory, a novel mathematical framework is developed for static and dynamic rate adaptation schemes. The results show that there is an optimal static rate that minimizes latency, which depends on the network parameters. Furthermore, the dynamic rate is shown to be resilient to different variations in the network parameters without sacrificing packet latency. ",Hesham ElSawy,,,11,
Upper Bounds on the Generalization Error of Private Algorithms,"  In this work, we study the generalization capability of algorithms from an information-theoretic perspective. It has been shown that the expected generalization error of an algorithm is bounded from above by a function of the relative entropy between the conditional probability distribution of the algorithm's output hypothesis, given the dataset with which it was trained, and its marginal probability distribution. We build upon this fact and introduce a mathematical formulation to obtain upper bounds on this relative entropy. We then develop a strategy using this formulation, based on the method of types and typicality, to find explicit upper bounds on the generalization error of stable algorithms, i.e., algorithms that produce similar output hypotheses given similar input datasets. In particular, we show the bounds obtained with this strategy for the case of $\epsilon$-DP and $\mu$-GDP algorithms. ","Borja Rodr\'iguez-G\'alvez, Germ\'an Bassi, and Mikael Skoglund",,,11,
Coded Caching with Demand Privacy: Constructions for Lower   Subpacketization and Generalizations,"  Coded caching is a technique where we utilize multi-casting opportunities to reduce rate in cached networks. One limitation of coded caching schemes is that they reveal the demands of all users to their peers. In this work, we consider coded caching schemes that assure privacy for user demands. We focus on reducing subpacketization in such schemes. For the 2-user, 2-file case, we propose a new linear demand-private scheme with the lowest possible subpacketization. This is done by presenting the scheme explicitly and proving impossibility results under lower subpacketization. We then propose new construction schemes for placement delivery arrays. This includes direct as well as lifting constructions. Coded caching schemes based on these can achieve lower subpacketization. A new notion of privacy with security is introduced which combines demand privacy and content security and schemes to achieve the same are proposed. Additionally, when only partial privacy is required, we show that subpacketization can be significantly reduced when there are a large number of files. ","V R Aravind, Pradeep Kiran Sarvepalli and Andrew Thangaraj",,,11,
Location-Aware Pilot Allocation in Multi-Cell Multi-User Massive MIMO   Networks,"  We propose a location-aware pilot allocation algorithm for a massive multiple-input multiple-output (MIMO) network with high-mobility users, where the wireless channels are subject to Rician fading. Pilot allocation in massive MIMO is a hard combinatorial problem and depends on the locations of users. As such, it is highly complex to achieve the optimal pilot allocation in real-time for a network with high-mobility users. Against this background, we propose a low-complexity pilot allocation algorithm, which exploits the behavior of line-of-sight (LOS) interference among the users and allocate the same pilot sequence to the users with small LOS interference. Our examination demonstrates that our proposed algorithm significantly outperforms the existing algorithms, even with localization errors. Specifically, for the system considered in this work, our proposed algorithm provides up to 37.26% improvement in sum spectral efficiency (SE) and improves the sum SE of the worst interference-affected users by up to 2.57 bits/sec/Hz, as compared to the existing algorithms. ","Noman Akbar, Shihao Yan, Nan Yang, and Jinhong Yuan",,,11,
Codes over Trees,"  In graph theory, a tree is one of the more popular families of graphs with a wide range of applications in computer science as well as many other related fields. While there are several distance measures over the set of all trees, we consider here the one which defines the so-called tree distance, defined by the minimum number of edit operations, of removing and adding edges, in order to change one tree into another. From a coding theoretic perspective, codes over the tree distance are used for the correction of edge erasures and errors. However, studying this distance measure is important for many other applications that use trees and properties on their locality and the number of neighbor trees. Under this paradigm, the largest size of code over trees with a prescribed minimum tree distance is investigated. Upper bounds on these codes as well as code constructions are presented. A significant part of our study is dedicated to the problem of calculating the size of the ball of trees of a given radius. These balls are not regular and thus we show that while the star tree has asymptotically the smallest size of the ball, the maximum is achieved for the line tree. ",Lev Yohananov and Eitan yaakobi,,,11,
Principal Component Analysis Based Broadband Hybrid Precoding for   Millimeter-Wave Massive MIMO Systems,"  Hybrid analog-digital precoding is challenging for broadband millimeter-wave (mmWave) massive MIMO systems, since the analog precoder is frequency-flat but the mmWave channels are frequency-selective. In this paper, we propose a principal component analysis (PCA)-based broadband hybrid precoder/combiner design, where both the fully-connected array and partially-connected subarray (including the fixed and adaptive subarrays) are investigated. Specifically, we first design the hybrid precoder/combiner for fully-connected array and fixed subarray based on PCA, whereby a low-dimensional frequency-flat precoder/combiner is acquired based on the optimal high-dimensional frequency-selective precoder/combiner. Meanwhile, the near-optimality of our proposed PCA approach is theoretically proven. Moreover, for the adaptive subarray, a low-complexity shared agglomerative hierarchical clustering algorithm is proposed to group the antennas for the further improvement of spectral efficiency (SE) performance. Besides, we theoretically prove that the proposed antenna grouping algorithm is only determined by the slow time-varying channel parameters in the large antenna limit. Simulation results demonstrate the superiority of the proposed solution over state-of-the-art schemes in SE, energy efficiency (EE), bit-error-rate performance, and the robustness to time-varying channels. Our work reveals that the EE advantage of adaptive subarray over fully-connected array is obvious for both active and passive antennas, but the EE advantage of fixed subarray only holds for passive antennas. ","Yiwei Sun, Zhen Gao, Hua Wang, Byonghyo Shim, Guan Gui, Guoqiang Mao,   and Fumiyuki Adachi",,,11,
User-Number Threshold-based Base Station On/Off Control for Maximizing   Coverage Probability,"  In this study, we investigate the operation of user-number threshold-based base station (BS) on/off control, in which the BS turns off when the number of active users is less than a specific threshold value. This paper presents a space-based analysis of the BS on/off control system to which a stochastic geometric approach is applied. In particular, we derive the approximated closed-form expression of the coverage probability of a homogeneous network (HomNet) with the user-number threshold-based on/off control. Moreover, the optimal user-number threshold for maximizing the coverage probability is analytically derived. In addition to HomNet, we also derive the overall coverage probability and the optimal user-number thresholds for a heterogeneous network (HetNet). The results show that HetNet, the analysis of which seems intractable, can be analyzed in the form of a linear combination of HomNets with weighted densities. In addition, the optimal user-number threshold of each tier is obtained independently of other tiers. The modeling and analysis presented in this paper are not only limited to the case of user-number threshold-based on/off control, but also applicable to other novel on/off controls with minor modifications. Finally, by comparing with the simulated results, the theoretical contributions of this study are validated. ",Jung-Hoon Noh and Seong-Jun Oh,,,11,
Secure list decoding,"  We propose a new concept of secure list decoding. While the conventional list decoding requires that the list contains the transmitted message, secure list decoding requires the following additional security conditions. The first additional security condition is the impossibility of the correct decoding, i.e., the receiver cannot uniquely identify the transmitted message even though the transmitted message is contained in the list. This condition can be trivially satisfied when the transmission rate is larger than the channel capacity. The other additional security condition is the impossibility for the sender to estimate another element of the decoded list except for the transmitted message. This protocol can be used for anonymous auction, which realizes the anonymity for bidding. ",Masahito Hayashi,,,11,
Chaos-Based Multicarrier VLC Modulator With Compensation of LED   Nonlinearity,"  The massive deployment of light-emitting diode (LED) lightning infrastructure has opened the opportunity to reuse it as visible light communication (VLC) to leverage the current RF spectrum crisis in indoor scenarios. One of the main problems in VLC is the limited dynamic range of LEDs and their nonlinear response, which may lead to a severe degradation in the communication, and more specifically in the bit error rate (BER). This is aggravated by the extensive usage of multicarrier multiplexing, based on optical orthogonal frequency division multiplexing (O-OFDM), characterized by a high peak-to-average power ratio. Here, we present a chaos-based coded modulation (CCM) setup specifically adapted to the LED nonlinearities. It replaces the usual modulation, while keeping the multicarrier O-OFDM structure unchanged. First, we obtain a semi-analytical bound for the bit error probability, taking into account the LED nonlinear response. The bound results particularly tight for the range of signal-to-noise ratio of interest. Then, we propose a method to design the modulator based on optimization techniques. The objective function is the semi-analytical bound, and the optimization is applied to a parameterization of the CCM conjugation function. This appropriately shapes the chaotic waveform and leads to BER improvements that outperform classical counterparts under ideal predistortion. ","Francisco J. Escribano, Jos\'e S\'aez-Landete and Alexandre Wagemakers",,,11,
On the Energy Self-Sustainability of IoT via Distributed Compressed   Sensing,"  This paper advocates the use of the distributed compressed sensing (DCS) paradigm to deploy energy harvesting (EH) Internet of Thing (IoT) devices for energy self-sustainability. We consider networks with signal/energy models that capture the fact that both the collected signals and the harvested energy of different devices can exhibit correlation. We provide theoretical analysis on the performance of both the classical compressive sensing (CS) approach and the proposed distributed CS (DCS)-based approach to data acquisition for EH IoT. Moreover, we perform an in-depth comparison of the proposed DCS-based approach against the distributed source coding (DSC) system. These performance characterizations and comparisons embody the effect of various system phenomena and parameters including signal correlation, EH correlation, network size, and energy availability level. Our results unveil that, the proposed approach offers significant increase in data gathering capability with respect to the CS-based approach, and offers a substantial reduction of the mean-squared error distortion with respect to the DSC system. ","Wei Chen, Nikos Deligiannis, Yiannis Andreopoulos, Ian J. Wassell",,,11,
The Web is Still Small After More Than a Decade,"  Understanding web co-location is essential for various reasons. For instance, it can help one to assess the collateral damage that denial-of-service attacks or IP-based blocking can cause to the availability of co-located web sites. However, it has been more than a decade since the first study was conducted in 2007. The Internet infrastructure has changed drastically since then, necessitating a renewed study to comprehend the nature of web co-location.   In this paper, we conduct an empirical study to revisit web co-location using datasets collected from active DNS measurements. Our results show that the web is still small and centralized to a handful of hosting providers. More specifically, we find that more than 60% of web sites are co-located with at least ten other web sites---a group comprising less popular web sites. In contrast, 17.5% of mostly popular web sites are served from their own servers.   Although a high degree of web co-location could make co-hosted sites vulnerable to DoS attacks, our findings show that it is an increasing trend to co-host many web sites and serve them from well-provisioned content delivery networks (CDN) of major providers that provide advanced DoS protection benefits. Regardless of the high degree of web co-location, our analyses of popular block lists indicate that IP-based blocking does not cause severe collateral damage as previously thought. ","Nguyen Phong Hoang, Arian Akhavan Niaki, Michalis Polychronakis,   Phillipa Gill",,,11,
Edge-Computing-Enabled Smart Cities: A Comprehensive Survey,"  Recent years have disclosed a remarkable proliferation of compute-intensive applications in smart cities. Such applications continuously generate enormous amounts of data which demand strict latency-aware computational processing capabilities. Although edge computing is an appealing technology to compensate for stringent latency related issues, its deployment engenders new challenges. In this survey, we highlight the role of edge computing in realizing the vision of smart cities. First, we analyze the evolution of edge computing paradigms. Subsequently, we critically review the state-of-the-art literature focusing on edge computing applications in smart cities. Later, we categorize and classify the literature by devising a comprehensive and meticulous taxonomy. Furthermore, we identify and discuss key requirements, and enumerate recently reported synergies of edge computing enabled smart cities. Finally, several indispensable open challenges along with their causes and guidelines are discussed, serving as future research directions. ","Latif U. Khan, Ibrar Yaqoob, Nguyen H. Tran, S. M. Ahsan Kazmi, Tri   Nguyen Dang, Choong Seon Hong",,,11,
Analyzing Mobility-Traffic Correlations in Large WLAN Traces: Flutes vs.   Cellos,"  Two major factors affecting mobile network performance are mobility and traffic patterns. Simulations and analytical-based performance evaluations rely on models to approximate factors affecting the network. Hence, the understanding of mobility and traffic is imperative to the effective evaluation and efficient design of future mobile networks. Current models target either mobility or traffic, but do not capture their interplay. Many trace-based mobility models have largely used pre-smartphone datasets (e.g., AP-logs), or much coarser granularity (e.g., cell-towers) traces. This raises questions regarding the relevance of existing models, and motivates our study to revisit this area. In this study, we conduct a multidimensional analysis, to quantitatively characterize mobility and traffic spatio-temporal patterns, for laptops and smartphones, leading to a detailed integrated mobility-traffic analysis. Our study is data-driven, as we collect and mine capacious datasets (with 30TB, 300k devices) that capture all of these dimensions. The investigation is performed using our systematic (FLAMeS) framework. Overall, dozens of mobility and traffic features have been analyzed. The insights and lessons learnt serve as guidelines and a first step towards future integrated mobility-traffic models. In addition, our work acts as a stepping-stone towards a richer, more-realistic suite of mobile test scenarios and benchmarks. ","Babak Alipour, Leonardo Tonetto, Aaron Ding, Roozbeh Ketabi, J\""org   Ott, Ahmed Helmy",,,11,
"Topology Management, Multi-Path Routing, and Link Scheduling for mmW WMN   Backhaul","  Mobile backhaul system based on a wireless mesh network using point-to-point millimetre wave links is a promising solution for dense 5G small cell deployments. While mmW radio technology can provide the sufficient capacity, the management of transport delays over multiple wireless hops is challenging especially if TDD backhaul radios are used. Earlier, we have proposed the Self-Optimizing WMN concept and presented routing and link scheduling principles that can be used for backhaul nodes with single radio unit. In this paper, we are extending the concept to support backhaul nodes that can have multiple radio units with own beam steering antennas covering non-overlapping sectors. The proposed system is based on dividing the task in separate phases. In the first phase, an active network topology is created by selecting a suitable subset of all available links. In the next step, the routing information and transmission sets are generated. Finally, the link schedule is optimized by finding an optimal ordering of transmission sets. In this paper, we are proposing a feedback loop from transmission set generation to topology management. We show that this feedback loop removes efficiently ""troublesome"" links from the active topology making it easier to find optimal link schedules. ","Kari Sepp\""anen, Pekka J. Wainio",,,11,
LSQ: Load Balancing in Large-Scale Heterogeneous Systems with Multiple   Dispatchers,"  Nowadays, the efficiency and even the feasibility of traditional load-balancing policies are challenged by the rapid growth of cloud infrastructure and the increasing levels of server heterogeneity. In such heterogeneous systems with many load-balancers, traditional solutions, such as JSQ, incur a prohibitively large communication overhead and detrimental incast effects due to herd behavior. Alternative low-communication policies, such as JSQ(d) and the recently proposed JIQ, are either unstable or provide poor performance.   We introduce the Local Shortest Queue (LSQ) family of load balancing algorithms. In these algorithms, each dispatcher maintains its own, local, and possibly outdated view of the server queue lengths, and keeps using JSQ on its local view. A small communication overhead is used infrequently to update this local view. We formally prove that as long as the error in these local estimates of the server queue lengths is bounded in expectation, the entire system is strongly stable. Finally, in simulations, we show how simple and stable LSQ policies exhibit appealing performance and significantly outperform existing low-communication policies, while using an equivalent communication budget. In particular, our simple policies often outperform even JSQ due to their reduction of herd behavior. We further show how, by relying on smart servers (i.e., advanced pull-based communication), we can further improve performance and lower communication overhead. ","Shay Vargaftik, Isaac Keslassy, Ariel Orda",,,11,
Real-world Video Adaptation with Reinforcement Learning,"  Client-side video players employ adaptive bitrate (ABR) algorithms to optimize user quality of experience (QoE). We evaluate recently proposed RL-based ABR methods in Facebook's web-based video streaming platform. Real-world ABR contains several challenges that requires customized designs beyond off-the-shelf RL algorithms -- we implement a scalable neural network architecture that supports videos with arbitrary bitrate encodings; we design a training method to cope with the variance resulting from the stochasticity in network conditions; and we leverage constrained Bayesian optimization for reward shaping in order to optimize the conflicting QoE objectives. In a week-long worldwide deployment with more than 30 million video streaming sessions, our RL approach outperforms the existing human-engineered ABR algorithms. ","Hongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell,   Yuandong Tian, Mohammad Alizadeh, Eytan Bakshy",,,11,
Wireless sensor networks simulators and testbeds,"  Wireless sensor networks (WSNs) have emerged as one of the most promising technologies for the current era. Researchers have studied them for several years ago, but more work still needed to be made since open opportunities to integrate new technologies are added to this field. One challenging task is WSN deployment. Yet, this is done by real deployment with testbeds platforms or by simulation tools when real deployment could be costly and time-consuming. In this paper, we review the implementation and evaluation process in WSNs. We then describe relevant testbeds and simulation tools, and their features. Lastly, we conduct an experimentation study using these testbeds and simulations to highlight their pro and cons. As a use case, we implement a localization protocol. This work gives clarity to future-work for better implementation in order to improve reliability, accuracy and time consumed. ","Souhila Silmi, Zouina Doukha, Rebiha Kemcha and Samira Moussaoui",,,11,
Threats and Corrective Measures for IoT Security with Observance to   Cybercrime,"  Internet of Things (IoT) is the utmost assuring framework to facilitate human life with quality and comfort. IoT has contributed significantly in numerous application areas. The stormy expansion of smart devices and their credence for data transfer on wireless mechanics boosts their susceptibility to cyber-attacks. Consequently, the rate of cybercrimes is increasing day by day. Hence, the study of IoT security threats and possible corrective measures can benefit the researchers to identify appropriate solutions to deal with various challenges in cybercrime investigation. IoT forensics plays a vital role in cybercrime investigation. This review paper presents an overview of the IoT framework consisting of IoT architecture, protocols, and technologies. Various security issues at each layer and corrective measures are also discussed in detail. This paper also presents the role of IoT forensics in cybercrime investigation in various domains like smart homes, smart cities, automated vehicles, healthcare, etc. Along with the role of advanced technologies like Artificial Intelligence, Machine Learning, Cloud computing, Edge computing, Fog computing, and Blockchain technology in cybercrime investigation are also discussed. At last, the various open research challenges in the area of IoT to assist cybercrime investigation are explained, which provide a new direction to the researchers to work further. ","Sita Rani, Aman Kataria, Smarajit Ghosh, Vinod Karar, Takshi Gupta,   Kyungroul Lee, and Chang Choi",,,11,
IEEE 802.15.3d: First Standardization Efforts for Sub-Terahertz Band   Communications towards 6G,"  With the ratification of the IEEE 802.15.3d amendment to the 802.15.3, a first step has been made to standardize consumer wireless communications in the sub-THz frequency band. The IEEE 802.15.3d offers switched point-to-point connectivity with the data rates of 100\,Gbit/s and higher at distances ranging from tens of centimeters up to a few hundred meters. In this article, we provide a detailed introduction to the IEEE 802.15.3d and the key design principles beyond the developed standard. We particularly describe the target applications and usage scenarios, as well as the specifics of the IEEE 802.15.3d physical and medium access layers. Later, we present the results of the initial performance evaluation of IEEE 802.15.3d wireless communications. The obtained first-order performance predictions show non-incremental benefits compared to the characteristics of the fifth-generation wireless systems, thus paving the way towards the six-generation (6G) THz networks. We conclude the article by outlining the further standardization and regulatory activities on wireless networking in the THz frequency band. ","Vitaly Petrov and Thomas K\""urner and and Iwao Hosako",,,11,
Enhancing Routing Security in IoT: Performance Evaluation of RPL Secure   Mode under Attacks,"  As the Routing Protocol for Low Power and Lossy Networks (RPL) became the standard for routing in the Internet of Things (IoT) networks, many researchers had investigated the security aspects of this protocol. However, no work (to the best of our knowledge) has investigated the use of the security mechanisms included in RPL standard, mainly because there was no implementation for these features in any IoT operating systems yet. A partial implementation of RPL security mechanisms was presented recently for the Contiki operating system (by Perazzo et al.), which provided us with an opportunity to examine RPL security mechanisms. In this paper, we investigate the effects and challenges of using RPL security mechanisms under common routing attacks. First, a comparison of RPL performance, with and without its security mechanisms, under four routing attacks (Blackhole, Selective-Forward, Neighbor, and Wormhole attacks) is conducted using several metrics (e.g., average data packet delivery rate, average data packet delay, average power consumption, etc.). This comparison is performed using two commonly used Radio Duty-Cycle protocols. Secondly, and based on the observations from this comparison, we propose two techniques that could reduce the effects of such attacks, without having added security mechanisms for RPL. An evaluation of these techniques shows improved performance of RPL under the investigated attacks, except for the Wormhole attack. ","Ahmed Raoof, Ashraf Matrawy, Chung-Horng Lung",,,11,
How To Dimension Radio Resources When Users Are Distributed on Roads   Modeled by Poisson Line Process,"  Resources dimensioning aims at finding the number of radio resources required to carry a forecast data traffic at a target users Quality of Services (QoS). The present paper attempts to provide a new approach of radio resources dimensioning considering the congestion probability, qualified as a relevant metric for QoS evaluation. Users are assumed to be distributed according to a linear Poisson Point Process (PPP) in a random system of roads modeled by Poisson Line Process (PLP) instead of the widely-used spatial PPP. We derive the analytical expression of the congestion probability for analyzing its behavior as a function of network parameters. Finally we show how to dimension radio resources by setting a value of the congestion probability, often targeted by the operator, in order to find the relation between the necessary resources and the forecast data traffic expressed in terms of cell throughput. Different numerical results are presented to justify this dimensioning approach. ","Jalal Rachad (LTCI), Ridha Nasri, Laurent Decreusefond (LTCI)",,,11,
Network Slicing for Service-Oriented Networks with Flexible Routing and   Guaranteed E2E Latency,"  Network function virtualization is a promising technology to simultaneously support multiple services with diverse characteristics and requirements in the fifth generation and beyond networks. In practice, each service consists of a predetermined sequence of functions, called service function chain (SFC), running on a cloud environment. To make different service slices work properly in harmony, it is crucial to select the cloud nodes to deploy the functions in the SFC and flexibly route the flow of the services such that these functions are processed in sequence, the end-to-end (E2E) latency constraints of all services are guaranteed, and all resource constraints are respected. In this paper, we propose a new (mixed binary linear program) formulation of the above network slicing problem that optimizes the system energy efficiency while jointly considers the resource budget, functional instantiation, flow routing, and E2E latency requirement. Numerical results show the advantage of the proposed formulation compared to the existing ones. ","Wei-Kun Chen, Ya-Feng Liu, Antonio De Domenico, Zhi-Quan Luo",,,11,
Deep reinforcement learning for RAN optimization and control,"  Due to the high variability of the traffic in the radio access network (RAN), fixed network configurations are not flexible to achieve the optimal performance. Our vendors provide several settings of the eNodeB to optimize the RAN performance, such as media access control scheduler, loading balance, etc. But the detailed mechanisms of the eNodeB configurations are usually very complicated and not disclosed, not to mention the large KPIs space needed to be considered. These make constructing simulator, offline tuning, or rule-based solutions difficult. We aim to build an intelligent controller without strong assumption or domain knowledge about the RAN and can run for 24/7 without supervision. To achieve this goal, we first build a closed-loop control testbed RAN in a lab environment with one eNodeB provided by one of the largest wireless vendors and four smartphones. Next, we build a double Q network agent that is trained with the live feedbacks of the key performance indicators from the RAN. Our work proved the effectiveness of applying deep reinforcement learning to improve network performance in a real RAN network environment. ","Yu Chen, Jie Chen, Ganesh Krishnamurthi, Huijing Yang, Huahui Wang,   Wenjie Zhao",,,11,
From ATM to MPLS and QCI: The Evolution of Differentiated QoS Standards   and Implications for 5G Network Slicing,"  The networking community continues to create new technologies and update existing ones to improve the quality, reliability, and ""tailorability"" of data networks. However, whenever Internet service providers attempt to productize ""tailorability"" and sell it explicitly to end customers as a premium service over best effort connectivity, they either fail to overcome net neutrality concerns or struggle to gain market traction. For this article, we focus only on those networking protocols, technologies, or standards whose goal is to offer tailored connectivity to paying customers on a public network and refer to them as differentiated QoS (D-QoS) standards. This article makes two contributions. First, it explores the techno-economic market trajectory of D-QoS standards to understand the factors that determine success. In doing this, we acknowledge that while there is wide variation and dissimilarity in their underlying technical properties, the expectation and goal for all D-QoS standards is that they will be used to provide guaranteed connection for which customers could be prepared to pay. As such, we consider Layer 2/3 technologies (e.g., ATM, frame relay, MPLS), signaling technologies (e.g., RSVP), data packet markers (e.g., IP ToS, DiffServ, WME, QCI), and end-to-end separation solutions (e.g., leased lines, network slicing) as a single cohort and analyze them together. Second, by exploring the parallels with 5G network slicing, we argue that despite its inherent technical differences with other D-QoS standards, the commercial performance of network slicing may end up resembling that of previous D-QoS standards. Consequently, we seek to learn lessons from previous D-QoS attempts and suggest that enterprise-focused 5G slices, running within a single service provider's domain and with binding service level agreements, will have the highest chance of success in the short/medium term. ",Emeka Obiodu and Nishanth Sastry,,,11,
STDPG: A Spatio-Temporal Deterministic Policy Gradient Agent for Dynamic   Routing in SDN,"  Dynamic routing in software-defined networking (SDN) can be viewed as a centralized decision-making problem. Most of the existing deep reinforcement learning (DRL) agents can address it, thanks to the deep neural network (DNN)incorporated. However, fully-connected feed-forward neural network (FFNN) is usually adopted, where spatial correlation and temporal variation of traffic flows are ignored. This drawback usually leads to significantly high computational complexity due to large number of training parameters. To overcome this problem, we propose a novel model-free framework for dynamic routing in SDN, which is referred to as spatio-temporal deterministic policy gradient (STDPG) agent. Both the actor and critic networks are based on identical DNN structure, where a combination of convolutional neural network (CNN) and long short-term memory network (LSTM) with temporal attention mechanism, CNN-LSTM-TAM, is devised. By efficiently exploiting spatial and temporal features, CNNLSTM-TAM helps the STDPG agent learn better from the experience transitions. Furthermore, we employ the prioritized experience replay (PER) method to accelerate the convergence of model training. The experimental results show that STDPG can automatically adapt for current network environment and achieve robust convergence. Compared with a number state-ofthe-art DRL agents, STDPG achieves better routing solutions in terms of the average end-to-end delay. ","Juan Chen, Zhiwen Xiao, Huanlai Xing, Penglin Dai, Shouxi Luo,   Muhammad Azhar Iqbal",,,11,
A Vision of Self-Evolving Network Management for Future Intelligent   Vertical HetNet,"  Future integrated terrestrial-aerial-satellite networks will have to exhibit some unprecedented characteristics for the provision of both communications and computation services, and security for a tremendous number of devices with very broad and demanding requirements in an almost-ubiquitous manner. Although 3GPP introduced the concept of self-organization networks (SONs) in 4G and 5G documents to automate network management, even this progressive concept will face several challenges as it may not be sufficiently agile in coping with the immense levels of complexity, heterogeneity, and mobility in the envisioned beyond-5G integrated networks. In the presented vision, we discuss how future integrated networks can be intelligently and autonomously managed to efficiently utilize resources, reduce operational costs, and achieve the targeted Quality of Experience (QoE). We introduce the novel concept of self-evolving networks (SENs) framework, which utilizes artificial intelligence, enabled by machine learning (ML) algorithms, to make future integrated networks fully intelligent and automated with respect to the provision, adaptation, optimization, and management aspects of networking, communications, and computation. To envisage the concept of SEN in future integrated networks, we use the Intelligent Vertical Heterogeneous Network (I-VHetNet) architecture as our reference. The paper discusses five prominent communications and computation scenarios where SEN plays the main role in providing automated network management. Numerical results provide an insight on how the SEN framework improves the performance of future integrated networks. The paper presents the leading enablers and examines the challenges associated with the application of SEN concept in future integrated networks. ","Tasneem Darwish, Gunes Karabulut Kurt, Halim Yanikomeroglu, Gamini   Senarath, and Peiying Zhu",,,11,
Fast and Reliable WiFi Fingerprint Collection for Indoor Localization,"  Fingerprinting is a popular indoor localization technique since it can utilize existing infrastructures (e.g., access points). However, its site survey process is a labor-intensive and time-consuming task, which limits the application of such systems in practice. In this paper, motivated by the availability of advanced sensing capabilities in smartphones, we propose a fast and reliable fingerprint collection method to reduce the time and labor required for site survey. The proposed method uses a landmark graph-based method to automatically associate the collected fingerprints, which does not require active user participation. We will show that besides fast fingerprint data collection, the proposed method results in accurate location estimate compared to the state-of-the-art methods. Experimental results show that the proposed method is an order of magnitude faster than the manual fingerprint collection method, and using the radio map generated by our method achieves a much better accuracy compared to the existing methods. ","Fuqiang Gu, Milad Ramezani, Kourosh Khoshelham, Xiaoping Zheng, Ruiqin   Zhou and Jianga Shang",,,11,
RDSP: Rapidly Deployable Wireless Ad Hoc System for Post-Disaster   Management,"  In post-disaster scenarios, such as after floods, earthquakes, and in war zones, the cellular communication infrastructure may be destroyed or seriously disrupted. In such emergency scenarios, it becomes very important for first aid responders to communicate with other rescue teams in order to provide feedback to both the central office and the disaster survivors. To address this issue, rapidly deployable systems are required to re-establish connectivity and assist users and first responders in the region of incident. In this work, we describe the design, implementation, and evaluation of a rapidly deployable system for first response applications in post-disaster situations, named RDSP. The proposed system helps early rescue responders and victims by sharing their location information to remotely located servers by utilizing a novel routing scheme. This novel routing scheme consists of the Dynamic ID Assignment (DIA) algorithm and the Minimum Maximum Neighbor (MMN) algorithm. The DIA algorithm is used by relay devices to dynamically select their IDs on the basis of all the available IDs of networks. Whereas, the MMN algorithm is used by the client and relay devices to dynamically select their next neighbor relays for the transmission of messages. The RDSP contains three devices; the client device sends the victim's location information to the server, the relay device relays information between client and server device, the server device receives messages from the client device to alert the rescue team. We deployed and evaluated our system in the outdoor environment of the university campus. The experimental results show that the RDSP system reduces the message delivery delay and improves the message delivery ratio with lower communication overhead. ","Ajmal Khan, Adnan Munir, Zeeshan Kaleem, Farman Ullah, Muhammad Bilal,   Lewis Nkenyereye, Shahen Shah, Long D. Nguyen, S. M. Riazul Islam and   Kyung-Sup Kwak",,,11,
Cocktail: Cost-efficient and Data Skew-aware Online In-Network   Distributed Machine Learning for Intelligent 5G and Beyond,"  To facilitate the emerging applications in the 5G networks and beyond, mobile network operators will provide many powerful control functionalities such as RAN slicing and resource scheduling. These control functionalities generally comprise a series of prediction tasks such as channel state information prediction, cellular traffic prediction and user mobility prediction which will be enabled by machine learning (ML) techniques. However, training the ML models offline is inefficient, due to the excessive overhead for forwarding the huge volume of data samples from cellular networks to remote ML training clouds. Thanks to the promising edge computing paradigm, we advocate cooperative online in-network ML training across edge clouds. To alleviate the data skew issue caused by the capacity heterogeneity and dynamics of edge clouds while avoiding excessive overhead, we propose Cocktail, a cost-efficient and data skew-aware online in-network distributed machine learning framework. We build a comprehensive model and formulate an online data scheduling problem to optimize the framework cost while reconciling the data skew from both short-term and long-term perspective. We exploit the stochastic gradient descent to devise an online asymptotically optimal algorithm. As its core building block, we propose optimal policies based on novel graph constructions to respectively solve two subproblems. We also improve the proposed online algorithm with online learning for fast convergence of in-network ML training. A small-scale testbed and large-scale simulations validate the superior performance of our framework. ","Lingjun Pu, Xu Chen, Ruilin Yun, Xinjing Yuan, Pan Zhou, Jingdong Xu",,,11,
All that Glitters is not Bitcoin -- Unveiling the Centralized Nature of   the BTC (IP) Network,"  Blockchains are typically managed by peer-to-peer (P2P) networks providing the support and substrate to the so-called distributed ledger (DLT), a replicated, shared, and synchronized data structure, geographically spread across multiple nodes. The Bitcoin (BTC) blockchain is by far the most well known DLT, used to record transactions among peers, based on the BTC digital currency. In this paper, we focus on the network side of the BTC P2P network, analyzing its nodes from a purely network measurements-based approach. We present a BTC crawler able to discover and track the BTC P2P network through active measurements, and use it to analyze its main properties. Through the combined analysis of multiple snapshots of the BTC network as well as by using other publicly available data sources on the BTC network and DLT, we unveil the BTC P2P network, locate its active nodes, study their performance, and track the evolution of the network over the past two years. Among other relevant findings, we show that (i) the size of the BTC network has remained almost constant during the last 12 months - since the major BTC price drop in early 2018, (ii) most of the BTC P2P network resides in US and EU countries, and (iii) despite this western network locality, most of the mining activity and corresponding revenue is controlled by major mining pools located in China. By additionally analyzing the distribution of BTC coins among independent BTC entities (i.e., single BTC addresses or groups of BTC addresses controlled by the same actor), we also conclude that (iv) BTC is very far from being the decentralized and uncontrolled system it is so much advertised to be, with only 4.5% of all the BTC entities holding about 85% of all circulating BTC coins. ","Sami Ben Mariem, Pedro Casas, Matteo Romiti, Benoit Donnet, Rainer   St\""utz, Bernhard Haslhofer",,,11,
A Research Agenda on Pediatric Chest X-Ray: Is Deep Learning Still in   Childhood?,"  Several reasons explain the significant role that chest X-rays play on supporting clinical analysis and early disease detection in pediatric patients, such as low cost, high resolution, low radiation levels, and high availability. In the last decade, Deep Learning (DL) has been given special attention from the computer-aided diagnosis research community, outperforming the state of the art of many techniques, including those applied to pediatric chest X-rays (PCXR). Due to this increasing interest, much high-quality secondary research has also arisen, overviewing machine learning and DL algorithms on medical imaging and PCXR, in particular. However, these secondary studies follow different guidelines, hampering their reproduction or improvement by third-parties regarding the identified trends and gaps. This paper proposes a ""deep radiography"" of primary research on DL techniques applied in PCXR images. We elaborated on a Systematic Literature Mapping (SLM) protocol, including automatic search on six sources for studies published from January 1, 2010, to May 20, 2020, and selection criteria utilized on a hundred research papers. As a result, this paper categorizes twenty-six relevant studies and provides a research agenda highlighting limitations, gaps, and trends for further investigations on DL usage in PCXR images. Besides the fact that there is no systematic mapping study on this research topic, to the best of authors' knowledge, this work organizes the process of finding and selecting relevant studies and data gathering and synthesis in a reproducible way. ","Afonso U. Fonseca, Gabriel S. Vieira, Fabr\'izzio A. A. M. N. Soares,   and Renato F. Bulc\~ao-Neto",,,11,
Reconfigurable Computing Applied to Latency Reduction for the Tactile   Internet,"  Tactile internet applications allow robotic devices to be remotely controlled over a communication medium with an unnoticeable time delay. In a bilateral communication, the acceptable round trip latency is usually in the order of 1ms up to 10ms depending on the application requirements. It is estimated that 70% of the total latency is generated by the communication network, and the remaining 30% is produced by master and slave devices. Thus, this paper aims to propose a strategy to reduce 30% of the total latency that is produced by such devices. The strategy is to apply reconfigurable computation using FPGAs to minimize the execution time of device-associated algorithms. With this in mind, this work presents a hardware reference model for modules that implement nonlinear positioning and force calculations as well as a tactile system formed by two robotic manipulators. In addition to presenting the implementation details, simulations and experimental tests are performed in order to validate the proposed model. Results associated with the FPGA sampling rate, throughput, latency, and post-synthesis occupancy area are analyzed. ","Jos\'e C. V. S. Junior, Matheus F. Torquato, Toktam Mahmoodi, Mischa   Dohler and Marcelo A. C. Fernandes",,,11,
"Artificial Buildings: Safety, Complexity and a Quantifiable Measure of   Beauty","  A place to live is one of the most crucial necessities for all living organisms since the advent of life on planet Earth. The nature of homes has changed considerably over time. At the very early stages, human begins lived in natural places such as caves. Later on, they started to use their intelligence to build places with special purposes. Nowadays, modern technologies such as robotics and artificial intelligence have made their ways into the construction process and opened up a whole new area of opportunities and concerns that may be of interest to both technologists and philosophers. In this article, I review the evolution of buildings from fully natural to fully artificial and discuss philosophical thoughts that a fully automated construction technology may raise. I elaborate on the safety concerns of a fully automated architectural process. Then, I'll borrow Kolmogorov complexity from algorithmic information theory to define a complexity measure for buildings. The proposed measure is then used to provide a quantifiable measure of beauty. ",Arash Mehrjou,,,11,
Current Practices in the Information Collection for Enterprise   Architecture Management,"  The digital transformation influences business models, processes, and enterprise IT landscape as a whole. Therefore, business-IT alignment is becoming more important than ever before. Enterprise architecture management (EAM) is designed to support and improve this business-IT alignment. The success of EAM crucially depends on the information available about a company's enterprise architecture, such as infrastructure components, applications, and business processes. This paper discusses the results of a qualitative expert survey with 26 experts in the field of EAM. The goal of this survey was to highlight current practices in the information collection for EAM and identify relevant information from enterprise-external data sources. The results provide a comprehensive overview of collected and utilized information in the industry, including an assessment of the relevance of such information. Furthermore, the results highlight challenges in practice and point out investments that organizations plan in the field of EAM. ","Robert Ehrensperger, Clemens Sauerwein and Ruth Breu",,,11,
White Paper on Business of 6G,"  Developing products, services and vertical applications for the future digitized society in the 6G era requires a multidisciplinary approach and a re-definition of how we create, deliver and consume network resources, data and services for both communications and sensing purposes. This development will change and disrupt the traditional business models and ecosystem roles of digital service providers, as well as open the market for key stakeholders in the 6G era like digital service operators, cloud operators and resource brokers. White paper discusses unprecedented opportunities of enabling and empowering multiple stakeholders to have a more active participation in the future 6G ecosystem via novel sustainable open ecosystemic business models with flexible integration of long tail services with tailored performance attributes. This research adopts a qualitative scenario planning method and portrays three scenario themes resulting in a total of 12 scenarios for the futures of the 6G business. By focusing on key trends, their interactions, and irreducible uncertainties, scenario building generates perspectives for the futures within which alternative 6G business strategies were developed and assessed for a traditional incumbent mobile network operator and a novel 6G digital service provider stemming from redefined sustainable economics. Value-capture in the 6G era requires understanding the dynamics of platforms and ecosystems. Results indicate that, to reach some of the preferred futures, we should pay attention to the privacy and security issues related to business and regulation needs; public/governmental, corporate, community and user(s) perspectives to and aims of governance; ecosystem configuration related to users, decentralized business models and platforms; user empowerment; and the role of location-specificity of services. ","Seppo Yrjola, Petri Ahokangas, Marja Matinmikko-Blue, Risto Jurva,   Vivek Kant, Pasi Karppinen, Marianne Kinnula, Harilaos Koumaras, Mika   Rantakokko, Volker Ziegler, Abhishek Thakur, Hans-Jurgen Zepernick",,,11,
Duty to Delete on Non-Volatile Memory,  We firstly suggest new cache policy applying the duty to delete invalid cache data on Non-volatile Memory (NVM). This cache policy includes generating random data and overwriting the random data into invalid cache data. Proposed cache policy is more economical and effective regarding perfect deletion of data. It is ensure that the invalid cache data in NVM is secure against malicious hackers. ,Na-Young Ahn and Dong Hoon Lee,,,11,
"The Separator, a Two-Phase Oil and Water Gravity CPS Separator Testbed","  Industrial Control Systems (ICS) are evolving with advances in new technology. The addition of wireless sensors and actuators and new control techniques means that engineering practices from communication systems are being integrated into those used for control systems. The two are engineered in very different ways. Neither engineering approach is capable of accounting for the subtle interactions and interdependence that occur when the two are combined. This paper describes our first steps to bridge this gap, and push the boundaries of both computer communication system and control system design. We present The Separator testbed, a Cyber-Physical testbed enabling our search for a suitable way to engineer systems that combine both computer networks and control systems. ","Michael Breza, Laksh Bhatia, Ivana Tomic, Anqi Fu, Waqas Ikram,   Valentinos Kongezos, Julie A. McCann",,,11,
Software-Based Monitoring and Analysis of a USB Host Controller Subject   to Electrostatic Discharge,"  Observing, understanding, and mitigating the effects of failure in embedded systems is essential for building dependable control systems. We develop a software-based monitoring methodology to further this goal. This methodology can be applied to any embedded system peripheral and allows the system to operate normally while the monitoring software is running. We use software to instrument the operating system kernel and record indicators of system behavior. By comparing those indicators against baseline indicators of normal system operation, faults can be detected and appropriate action can be taken.   We implement this methodology to detect faults caused by electrostatic discharge in a USB host controller. As indicators, we select specific control registers that provide a manifestation of the internal execution of the host controller. Analysis of the recorded register values reveals differences in system execution when the system is subject to interference. %We also develop a classifier capable of predicting whether or not the system's behavior is being affected by such shocks. This improved understanding of system behavior may lead to better hardware and software mitigation of electrostatic discharge and assist in root-cause analysis and repair of failures. ","Natasha Jarus, Antonio Sabatini, Pratik Maheshwari, Sahra Sedigh   Sarvestani",,,11,
Deep Learning-Based FPGA Function Block Detection Method using an   Image-Coded Representation of Bitstream,"  Examining field-programmable gate array (FPGA) bitstream is found to help detect known function blocks, which offers assistance and insight to analyze the circuit's system function. Our goal is to detect one or more than one function block in FPGA design from a complete bitstream by utilizing the latest deep learning techniques, which do not require manually designing features. To this end, in this paper, we propose a deep learning-based FPGA function block detection method by transforming the bitstream into a three-channel color image. In specific, we first analyze the format of the bitstream to find the mapping relationship between the configuration bits and configurable logic blocks. Next, an image-coded representation of bitstream is proposed suitable for deep learning processing. This bitstream-to-image transformation takes into account of the adjacency nature of the programmable logic as well as high degree of redundancy of configuration information. With the color images transformed from bitstreams as the training dataset, a deep learning-based object detection algorithm is applied for generating the function block detection results. The effects of EDA tools, input size of the deep neural network, and the data arrangement of representation on the detection accuracy are explored. The Xilinx Zynq-7000 SoCs and Xilinx Zynq UltraScale+ MPSoCs are adopted to verify the proposed method, and the results show that the mean Average Precision (IoU=0.5) for 10 function blocks is as high as 97.72% for YOLOv3 detector. ","Minzhen Chen and Peng Liu (Zhejiang University, Hangzhou, China)",,,11,
Testability of Reversible Iterative Logic Arrays,"  Iterative Logic Arrays (ILAs) are ideal as VLSI sub-systems because of their regular structure and its close resemblance with FPGAs (Field Programmable Gate Arrays). Reversible circuits are of interest in the design of very low power circuits where energy loss implied by high frequency switching is not of much consideration. Reversibility is essential for Quantum Computing. This paper examines the testability of Reversible Iterative Logic Arrays (ILAs) composed of reversible k-CNOT gates. For certain ILAs it is possible to find a test set whose size remains constant irrespective of the size of the ILA, while for others it varies with array size. Former type of ILAs is known as Constant-Testable, i.e. C-Testable. It has been shown that Reversible Logic Arrays are C-Testable and size of test set is equal to number of entries in cells truth table implying that the reversible ILAs are also Optimal-Testable, i.e. O-Testable. Uniform-Testability, i.e. U-Testability has been defined and Reversible Heterogeneous ILAs have been characterized as U-Testable. The test generation problem has been shown to be related to certain properties of cycles in a set of graphs derived from cell truth table. By careful analysis of these cycles an efficient test generation technique that can be easily converted to an ATPG program has been presented for both 1-D and 2D ILAs. The same algorithms can be easily extended for n-Dimensional Reversible ILAs. ",Avik Chakraborty,,,11,
Effect of NBTI/PBTI Aging and Process Variations on Write Failures in   MOSFET and FinFET Flip-Flops,"  The assessment of noise margins and the related probability of failure in digital cells has growingly become essential, as nano-scale CMOS and FinFET technologies are confronting reliability issues caused by aging mechanisms, such as NBTI, and variability in process parameters. The influence of such phenomena is particularly associated to the Write Noise Margins (WNM) in memory elements, since a wrong stored logic value can result in an upset of the system state. In this work, we calculated and compared the effect of process variations and NBTI aging over the years on the actual WNM of various CMOS and FinFET based flip-flop cells. The massive transistor-level Monte Carlo simulations produced both nominal (i.e. mean) values and associated standard deviations of the WNM of the chosen flip-flops. This allowed calculating the consequent write failure probability as a function of an input voltage shift on the flip-flop cells, and assessing a comparison for robustness among different circuit topologies and technologies. ","Usman Khalid, Antonio Mastrandrea, Mauro Olivieri",,,11,
Towards Leveraging End-of-Life Tools as an Asset: Value Co-Creation   based on Deep Learning in the Machining Industry,"  Sustainability is the key concept in the management of products that reached their end-of-life. We propose that end-of-life products have -- besides their value as recyclable assets -- additional value for producer and consumer. We argue this is especially true for the machining industry, where we illustrate an automatic characterization of worn cutting tools to foster value co-creation between tool manufacturer and tool user (customer) in the future. In the work at hand, we present a deep-learning-based computer vision system for the automatic classification of worn tools regarding flank wear and chipping. The resulting Matthews Correlation Coefficient of 0.878 and 0.644 confirms the feasibility of our system based on the VGG-16 network and Gradient Boosting. Based on these first results we derive a research agenda which addresses the need for a more holistic tool characterization by semantic segmentation and assesses the perceived business impact and usability by different user groups. ","Jannis Walk, Niklas K\""uhl and Jonathan Sch\""afer",,,11,
Proceedings of the VI International Workshop on Locational Analysis and   Related Problems,"  The International Workshop on Locational Analysis and Related Problems will take place during November 25-27, 2015 in Barcelona (Spain). It is organized by the Spanish Location Network and Location Group GELOCA (SEIO). GELOCA is a working group on location belonging to the Statistics and Operations Research Spanish Society. The Spanish Location Network is a group of more than 140 researchers distributed into 16 nodes corresponding to several Spanish universities. The Network has been funded by the Spanish Government. Every year, the Network organizes a meeting to promote the communication among its members and between them and other researchers, and to contribute to the development of the location field and related problems. Previous meetings took place in Sevilla (October 1-3, 2014), Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), Las Palmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010). The topics of interest are location analysis and related problems. This includes location, routing, networks, transportation and logistics models; exact and heuristic solution methods, and computational geometry, among others. ","Maria Albareda-Sambola, Luisa I. Mart\'inez-Merino, Antonio M.   Rodr\'iguez-Ch\'ia (Editors)",,,11,
Non-linearity identification for construction workers'   personality-safety behaviour predictive relationship using neural network and   linear regression modelling,"  The prediction of workers' safety behaviour can help identify vulnerable workers who intend to undertake unsafe behaviours and be useful in the design of management practices to minimise the occurrence of accidents. The latest literature has evidenced that there is within-population diversity that leads people's intended safety behaviours in the workplace, which are found to vary among individuals as a function of their personality traits. In this study, an innovative forecasting model, which employs neural network algorithms, is developed to numerically simulate the predictive relationship between construction workers' personality traits and their intended safety behaviour. The data-driven nature of neural network enabled a reliable estimate of the relationship, which allowed this research to find that a nonlinear effect exists in the relationship. This research has practical implications. The neural network developed is shown to have highly satisfactory prediction accuracy and is thereby potentially useful for assisting project decision-makers to assess how prone workers are to carry out unsafe behaviours in the workplace. ","Yifan Gao, Vicente A. Gonzalez, Tak Wing Yiu, and Guillermo   Cabrera-Guerrerod",,,11,
Efficient Metastability Characterization for Schmitt-Triggers,"  Despite their attractiveness as metastability filters, Schmitt-Triggers can suffer from metastability themselves. Therefore, in the selection or construction of a suitable Schmitt-Trigger implementation, it is a necessity to accurately determine the metastable behavior. Only then one is able to compare different designs and thus guide proper optimizations, and only then one can assess the potential for residual metastable upsets. However, while the state of the art provides a lot of research and practical characterization approaches for flip-flops, comparatively little is known about Schmitt-Trigger characterization. Unlike the flip-flop with its single metastable point, the Schmitt-Trigger exhibits a whole range of metastable points depending on the input voltage. Thus the task of characterization gets much more challenging.   In this paper we present different approaches to determine the metastable behavior of Schmitt-Triggers using novel methods and mechanisms. We compare their accuracy and runtime by applying them to three common circuit implementations. The achieved results are then used to reason about the metastable behavior of the chosen designs which turns out to be problematic in some cases. Overall the approaches proposed in this paper are generic and can be extended beyond the Schmitt-Trigger, i.e., to efficiently characterize metastable states in other circuits as well. ","J\""urgen Maier and Andreas Steininger",,,11,
"NDE 4.0: Digital Twin, Semantics, Interfaces, Networking, Feedback, New   Markets and Integration into the Industrial Internet of Things","  The industrial revolution is divided into three phases by historians: The invention of the steam engine (mechanization), electricity (mass production) and the microelectric revolution (automation). There was a similar development in non-destructive evaluation: tools such as lenses or stethoscopes allowed the human senses to be sharpened, the conversion of waves makes the invisible visible and thus offers a ""look"" into the components and finally automation, digitization and reconstruction. During the entire industrial development NDE was decisively responsible for the quality and thus for the success of the manufactured goods. Industry is now talking about a fourth revolution: The informatization, digitization and networking of industrial production. As always, NDE will be critical to the success of this fourth revolution by providing the database needed for feedback in a networked production environment. For NDE, this will lead to change. The test results must be made available to a networked production environment in such a way that they can be evaluated for feedback loops, the testability must be considered in the design and the reliability of the test statements will become increasingly important. This publication presents first an orientation to NDE 4.0, including the development of Industry and NDE, a definition of its revolutions, a collection of several current-day challenges of NDE, and a discussion whether and how those can be solved with NDE 4.0. Second this publication presents concepts on how NDE can be integrated into Industry 4.0 landscapes: The Reference Architecture Model Industry 4.0 (RAMI 4.0) shows the complete Industry 4.0 space and allows every Industry 4.0 standard and interface to be located. The Industry 4.0 Asset Administration Shell (AAS) implements the digital twin and is the interface between Industry 4.0 communication and the physical device. The ... ",Johannes Vrana,,,11,
Does Cascading Schmitt-Trigger Stages Improve the Metastable Behavior?,"  Schmitt-Trigger stages are the method of choice for robust discretization of input voltages with excessive transition times or significant noise. However, they may suffer from metastability. Based on the experience that the cascading of flip-flop stages yields a dramatic improvement of their overall metastability hardness, in this paper we elaborate on the question whether the cascading of Schmitt-Trigger stages can obtain a similar gain. We perform a theoretic analysis that is backed up by an existing metastability model for a single Schmitt-Trigger stage and elaborate some claims about the behavior of a Schmitt-Trigger cascade. These claims suggest that the occurrence of metastability is indeed reduced from the first stage to the second which suggests an improvement. On the downside, however, it becomes clear that metastability can still not be completely ruled out, and in some cases the behavior of the cascade may be less beneficial for a given application, e.g. by introducing seemingly acausal transitions. We validate our findings by extensive HSPICE simulations in which we directly cover our most important claims. ","Andreas Steininger and Robert Najvirt and J\""urgen Maier",,,11,
JXES: JSON Support for the XES Event Log Standard,"  Process mining assumes the existence of an event log where each event refers to a case, an activity, and a point in time. XES is an XML based IEEE approved standard format for event logs supported by most of the process mining tools. JSON (JavaScript Object Notation) is a lightweight data interchange format. In this paper, we present JXES, the JSON standard for the event logs and also provide implementation in ProM for importing and exporting event logs in JSON format using 4 different parsers. The evaluation results show notable performance differences between the different parsers (Simple JSON, Jackson, GSON, Jsoninter). ","Madhavi Bangalore Shankara Narayana, Hossameldin Khalifa, Wil van der   Aalst",,,11,
Visual-Based Analysis of Classification Measures with Applications to   Imbalanced Data,"  With a plethora of available classification performance measures, choosing the right metric for the right task requires careful thought. To make this decision in an informed manner, one should study and compare general properties of candidate measures. However, analysing measures with respect to complete ranges of their domain values is a difficult and challenging task. In this study, we attempt to support such analyses with a specialized visualization technique, which operates in a barycentric coordinate system using a 3D tetrahedron. Additionally, we adapt this technique to the context of imbalanced data and put forward a set of properties which should be taken into account when selecting a classification performance measure. As a result, we compare 22 popular measures and show important differences in their behaviour. Moreover, for parametric measures such as the F$_{\beta}$ and IBA$_\alpha$(G-mean), we analytically derive parameter thresholds that change measure properties. Finally, we provide an online visualization tool that can aid the analysis of complete domain ranges of performance measures. ","Dariusz Brzezinski, Jerzy Stefanowski, Robert Susmaga, Izabela   Szcz\k{e}ch",,,11,
Roof Age Determination for the Automated Site-Selection of Rooftop Solar,"  Rooftop solar is one of the most promising tools for drawing down greenhouse gas (GHG) emissions and is cost-competitive with fossil fuels in many areas of the world today. One of the most important criteria for determining the suitability of a building for rooftop solar is the current age of its roof. The reason for this is simple -- because rooftop solar installations are long-lived, the roof needs to be new enough to last for the lifetime of the solar array or old enough to justify being replaced. In this paper we present a data-driven method for determining the age of a roof from historical satellite imagery, which removes one of the last obstacles to a fully automated pipeline for rooftop solar site selection. We estimate that a full solution to this problem would reduce customer acquisition costs for rooftop solar by $\sim$20\%, leading to an additional $\sim$750 megatons of CO$_2$ displaced between 2020 and 2050. ","Chris Heinrich, Michael Laskin, Simas Glinskis, Evert van Nieuwenburg",,,11,
Testing the Agreement of Trees with Internal Labels,"  The input to the agreement problem is a collection $P = \{T_1, T_2, \dots , T_k\}$ of phylogenetic trees, called input trees, over partially overlapping sets of taxa. The question is whether there exists a tree $T$, called an agreement tree, whose taxon set is the union of the taxon sets of the input trees, such that for each $i \in \{1, 2, \dots , k\}$, the restriction of $T$ to the taxon set of $T_i$ is isomorphic to $T_i$. We give a $O(n k (\sum_{i \in [k]} d_i + \log^2(nk)))$ algorithm for a generalization of the agreement problem in which the input trees may have internal labels, where $n$ is the total number of distinct taxa in $P$, $k$ is the number of trees in $P$, and $d_i$ is the maximum number of children of a node in $T_i$. ",David Fern\'andez-Baca and Lei Liu,,,11,
Robust Algorithms for TSP and Steiner Tree,"  Robust optimization is a widely studied area in operations research, where the algorithm takes as input a range of values and outputs a single solution that performs well for the entire range. Specifically, a robust algorithm aims to minimize regret, defined as the maximum difference between the solution's cost and that of an optimal solution in hindsight once the input has been realized. For graph problems in P, such as shortest path and minimum spanning tree, robust polynomial-time algorithms that obtain a constant approximation on regret are known. In this paper, we study robust algorithms for minimizing regret in NP-hard graph optimization problems, and give constant approximations on regret for the classical traveling salesman and Steiner tree problems. ","Arun Ganesh, Bruce M. Maggs, Debmalya Panigrahi",,,11,
"Simple, Deterministic, Constant-Round Coloring in the Congested Clique","  We settle the complexity of the $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems in the CONGESTED CLIQUE model by presenting a simple deterministic algorithm for both problems running in a constant number of rounds. This matches the complexity of the recent breakthrough randomized constant-round $(\Delta+1)$-list coloring algorithm due to Chang et al. (PODC'19), and significantly improves upon the state-of-the-art $O(\log \Delta)$-round deterministic $(\Delta+1)$-coloring bound of Parter (ICALP'18).   A remarkable property of our algorithm is its simplicity. Whereas the state-of-the-art randomized algorithms for this problem are based on the quite involved local coloring algorithm of Chang et al. (STOC'18), our algorithm can be described in just a few lines. At a high level, it applies a careful derandomization of a recursive procedure which partitions the nodes and their respective palettes into separate bins. We show that after $O(1)$ recursion steps, the remaining uncolored subgraph within each bin has linear size, and thus can be solved locally by collecting it to a single node. This algorithm can also be implemented in the Massively Parallel Computation (MPC) model provided that each machine has linear (in $n$, the number of nodes in the input graph) space.   We also show an extension of our algorithm to the MPC regime in which machines have sublinear space: we present the first deterministic $(\Delta+1)$-list coloring algorithm designed for sublinear-space MPC, which runs in $O(\log \Delta + \log\log n)$ rounds. ","Artur Czumaj, Peter Davies, Merav Parter",,,11,
Semi-Streaming Bipartite Matching in Fewer Passes and Less Space,"  We provide algorithms with improved pass and space complexities for approximately solving linear programs, optimal transport, bipartite matching, and more in the semi-streaming model. For instance, we provide a (randomized) algorithm computing a maximum cardinality matching in an unweighted bipartite graph in $O(\log^2 n \cdot \epsilon^{-1})$ passes, using $O(n \log^2 n \cdot \epsilon^{-1})$ auxiliary memory. This marks the first improvements to the $O(\log\log\epsilon^{-1} \cdot \epsilon^{-2})$ pass, $O(n \log\log\epsilon^{-1}\cdot \epsilon^{-2})$-space algorithms of [AG13] when $\epsilon$ is moderately small.   To obtain our results, we give an $O(n)$ space deterministic semi-streaming algorithm for approximating the value of linear programs (in the form of box-simplex games), based on low-space implementations of [She17, JST19]. We further give a general sampling procedure for explicitly forming a fractional solution in low space, yielding improved semi-streaming guarantees for optimal transport and, in some regimes, maximum weighted matching. Finally, we improve the space complexity of our maximum cardinality matching method using an implicit implementation of the random walk rounding of [GKK10] via custom turnstile samplers. ","Yujia Jin, Aaron Sidford, Kevin Tian",,,11,
Improved Distortion and Spam Resistance for PageRank,"  Ranking functions are an integral component of web searching, and are thus subject to link spamming. Quite a lot of practical and theoretical knowledge has accumulated about which types of ranking functions are susceptible to which types of spam attacks, but there is no general characterization of the class of spam attacks nor of the susceptibility of a ranking function to arbitrary spam attacks.   Our first contribution is a characterization of the class of spam attacks and a definition of the spam resistance of a ranking function against all possible link-spam attacks. We show how this formal notion of spam resistance matches the intuition of practitioners. This definition allows us to evaluate novel ranking functions.   A natural way to resist spam is to design a ranking function that heavily favors trusted nodes, which are nodes known not be compromised by the spammer. However, this introduces local distortions into the ranking function. We formalize this notion of distortion and show that it matches the intuitive notion used by practitioners. We know of no ranking functions in the literature that combine high spam resistance and low distortion; we consider several well-known ranking functions (including the original form of PageRank) and show that they fail on one or both counts.   Finally, we introduce a new form of PageRank known as Min-PPR. We prove that Min-PPR has low distortion and high spam resistance. A secondary benefit is that Min-PPR comes with an explicit cost function on nodes that shows how important they are to the spammer; thus a ranker can focus their spam-detection capacity on these vulnerable nodes. Both Min-PPR and its associated cost function are straightforward to compute. ","Lucas Farach-Colton, Martin Farach-Colton, Leslie Ann Goldberg, John   Lapinskas, Reut Levi, Moti Medina and Miguel A. Mosteiro",,,11,
Reduced-Rank Regression with Operator Norm Error,"  A common data analysis task is the reduced-rank regression problem: $$\min_{\textrm{rank-}k \ X} \|AX-B\|,$$ where $A \in \mathbb{R}^{n \times c}$ and $B \in \mathbb{R}^{n \times d}$ are given large matrices and $\|\cdot\|$ is some norm. Here the unknown matrix $X \in \mathbb{R}^{c \times d}$ is constrained to be of rank $k$ as it results in a significant parameter reduction of the solution when $c$ and $d$ are large. In the case of Frobenius norm error, there is a standard closed form solution to this problem and a fast algorithm to find a $(1+\varepsilon)$-approximate solution. However, for the important case of operator norm error, no closed form solution is known and the fastest known algorithms take singular value decomposition time.   We give the first randomized algorithms for this problem running in time $$(\text{nnz}{(A)} + \text{nnz}{(B)} + c^2) \cdot k/\varepsilon^{1.5} + (n+d)k^2/\epsilon + c^{\omega},$$ up to a polylogarithmic factor involving condition numbers, matrix dimensions, and dependence on $1/\varepsilon$. Here $\text{nnz}{(M)}$ denotes the number of non-zero entries of a matrix $M$, and $\omega$ is the exponent of matrix multiplication. As both (1) spectral low rank approximation ($A = B$) and (2) linear system solving ($m = n$ and $d = 1$) are special cases, our time cannot be improved by more than a $1/\varepsilon$ factor (up to polylogarithmic factors) without a major breakthrough in linear algebra. Interestingly, known techniques for low rank approximation, such as alternating minimization or sketch-and-solve, provably fail for this problem. Instead, our algorithm uses an existential characterization of a solution, together with Krylov methods, low degree polynomial approximation, and sketching-based preconditioning. ",Praneeth Kacham and David P. Woodruff,,,11,
A Scaling Algorithm for Weighted $f$-Factors in General Graphs,"  We study the maximum weight perfect $f$-factor problem on any general simple graph $G=(V,E,w)$ with positive integral edge weights $w$, and $n=|V|$, $m=|E|$. When we have a function $f:V\rightarrow \mathbb{N}_+$ on vertices, a perfect $f$-factor is a generalized matching so that every vertex $u$ is matched to $f(u)$ different edges. The previous best algorithms on this problem have running time $O(m f(V))$ [Gabow 2018] or $\tilde{O}(W(f(V))^{2.373}))$ [Gabow and Sankowski 2013], where $W$ is the maximum edge weight, and $f(V)=\sum_{u\in V}f(u)$. In this paper, we present a scaling algorithm for this problem with running time $\tilde{O}(mn^{2/3}\log W)$. Previously this bound is only known for bipartite graphs [Gabow and Tarjan 1989]. The running time of our algorithm is independent of $f(V)$, and consequently it first breaks the $\Omega(mn)$ barrier for large $f(V)$ even for the unweighted $f$-factor problem in general graphs. ","Ran Duan, Haoqing He, Tianyi Zhang",,,11,
Online Allocation of Reusable Resources via Algorithms Guided by Fluid   Approximations,"  We consider the problem of online allocation (matching and assortments) of reusable resources where customers arrive sequentially in an adversarial fashion and allocated resources are used or rented for a stochastic duration that is drawn independently from known distributions. Focusing on the case of large inventory, we give an algorithm that is $(1-1/e)$ competitive for general usage distributions. At the heart of our result is the notion of a relaxed online algorithm that is only subjected to fluid approximations of the stochastic elements in the problem. The output of this algorithm serves as a guide for the final algorithm. This leads to a principled approach for seamlessly addressing stochastic elements (such as reusability, customer choice, and combinations thereof) in online resource allocation problems, that may be useful more broadly. ","Vineet Goyal, Garud Iyengar, Rajan Udwani",,,11,
"Multi-Perspective, Simultaneous Embedding","  We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions. ",Md Iqbal Hossain and Vahan Huroyan and Stephen Kobourov and Raymundo   Navarrete,,,11,
Near-Optimal Entrywise Sampling of Numerically Sparse Matrices,"  Many real-world data sets are sparse or almost sparse. One method to measure this for a matrix $A\in \mathbb{R}^{n\times n}$ is the \emph{numerical sparsity}, denoted $\mathsf{ns}(A)$, defined as the minimum $k\geq 1$ such that $\|a\|_1/\|a\|_2 \leq \sqrt{k}$ for every row and every column $a$ of $A$. This measure of $a$ is smooth and is clearly only smaller than the number of non-zeros in the row/column $a$. The seminal work of Achlioptas and McSherry [2007] has put forward the question of approximating an input matrix $A$ by entrywise sampling. More precisely, the goal is to quickly compute a sparse matrix $\tilde{A}$ satisfying $\|A - \tilde{A}\|_2 \leq \epsilon \|A\|_2$ (i.e., additive spectral approximation) given an error parameter $\epsilon>0$. The known schemes sample and rescale a small fraction of entries from $A$. We propose a scheme that sparsifies an almost-sparse matrix $A$ -- it produces a matrix $\tilde{A}$ with $O(\epsilon^{-2}\mathsf{ns}(A) \cdot n\ln n)$ non-zero entries with high probability. We also prove that this upper bound on $\mathsf{nnz}(\tilde{A})$ is \emph{tight} up to logarithmic factors. Moreover, our upper bound improves when the spectrum of $A$ decays quickly (roughly replacing $n$ with the stable rank of $A$). Our scheme can be implemented in time $O(\mathsf{nnz}(A))$ when $\|A\|_2$ is given. Previously, a similar upper bound was obtained by Achlioptas et. al [2013] but only for a restricted class of inputs that does not even include symmetric or covariance matrices. Finally, we demonstrate two applications of these sampling techniques, to faster approximate matrix multiplication, and to ridge regression by using sparse preconditioners. ","Vladimir Braverman, Robert Krauthgamer, Aditya Krishnan and Shay Sapir",,,11,
Localized Topological Simplification of Scalar Data,"  This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field f and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to f and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed -- from a combinatorial perspective -- to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to x36. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging. ","Jonas Lukasczyk, Christoph Garth, Ross Maciejewski, and Julien Tierny",,,11,
Regular Partitions and Their Use in Structural Pattern Recognition,"  Recent years are characterized by an unprecedented quantity of available network data which are produced at an astonishing rate by an heterogeneous variety of interconnected sensors and devices. This high-throughput generation calls for the development of new effective methods to store, retrieve, understand and process massive network data. In this thesis, we tackle this challenge by introducing a framework to summarize large graphs based on Szemer\'edi's Regularity Remma (RL), which roughly states that any sufficiently large graph can almost entirely be partitioned into a bounded number of random-like bipartite graphs. The partition resulting from the RL gives rise to a summary, which inherits many of the essential structural properties of the original graph. We first extend an heuristic version of the RL to improve its efficiency and its robustness. We use the proposed algorithm to address graph-based clustering and image segmentation tasks. In the second part of the thesis, we introduce a new heuristic algorithm which is characterized by an improvement of the summary quality both in terms of reconstruction error and of noise filtering. We use the proposed heuristic to address the graph search problem defined under a similarity measure. Finally, we study the linkage among the regularity lemma, the stochastic block model and the minimum description length. This study provide us a principled way to develop a graph decomposition algorithm based on stochastic block model which is fitted using likelihood maximization. ",Marco Fiorucci,,,11,
Spectral Independence in High-Dimensional Expanders and Applications to   the Hardcore Model,"  We say a probability distribution $\mu$ is spectrally independent if an associated correlation matrix has a bounded largest eigenvalue for the distribution and all of its conditional distributions. We prove that if $\mu$ is spectrally independent, then the corresponding high dimensional simplicial complex is a local spectral expander. Using a line of recent works on mixing time of high dimensional walks on simplicial complexes \cite{KM17,DK17,KO18,AL19}, this implies that the corresponding Glauber dynamics mixes rapidly and generates (approximate) samples from $\mu$.   As an application, we show that natural Glauber dynamics mixes rapidly (in polynomial time) to generate a random independent set from the hardcore model up to the uniqueness threshold. This improves the quasi-polynomial running time of Weitz's deterministic correlation decay algorithm \cite{Wei06} for estimating the hardcore partition function, also answering a long-standing open problem of mixing time of Glauber dynamics \cite{LV97,LV99,DG00,Vig01,EHSVY16}. ","Nima Anari, Kuikui Liu, Shayan Oveis Gharan",,,11,
Flexible Graph Connectivity: Approximating Network Design Problems   Between 1- and 2-connectivity,"  Graph connectivity and network design problems are among the most fundamental problems in combinatorial optimization. The minimum spanning tree problem, the two edge-connected spanning subgraph problem (2-ECSS) and the tree augmentation problem (TAP) are all examples of fundamental well-studied network design tasks that postulate different initial states of the network and different assumptions on the reliability of network components. In this paper we motivate and study \emph{Flexible Graph Connectivity} (FGC), a problem that mixes together both the modeling power and the complexities of all aforementioned problems and more. In a nutshell, FGC asks to design a connected network, while allowing to specify different reliability levels for individual edges. While this non-uniform nature of the problem makes it appealing from the modeling perspective, it also renders most existing algorithmic tools for dealing with network design problems unfit for approximating FGC.   In this paper we develop a general algorithmic approach for approximating FGC that yields approximation algorithms with ratios that are very close to the best known bounds for many special cases, such as 2-ECSS and TAP. Our algorithm and analysis combine various techniques including a weight-scaling algorithm, a charging argument that uses a variant of exchange bijections between spanning trees and a factor revealing min-max-min optimization problem. ","David Adjiashvili, Felix Hommelsheim and Moritz M\""uhlenthaler",,,11,
On the complexity of optimally modifying graphs representing spatial   correlation in areal unit count data,"  Lee and Meeks recently demonstrated that improved inference for areal unit count data can be achieved by carrying out modifications to a graph representing spatial correlations; specifically, they delete edges of the planar graph derived from border-sharing between geographic regions in order to maximise a specific objective function. In this paper we address the computational complexity of the associated graph optimisation problem, demonstrating that it cannot be solved in polynomial time unless P = NP. ",Duncan Lee and Kitty Meeks,,,11,
Efficient Approximation Schemes for Stochastic Probing and Prophet   Problems,"  Our main contribution is a general framework to design efficient polynomial time approximation schemes (EPTAS) for fundamental classes of stochastic combinatorial optimization problems. Given an error parameter $\epsilon>0$, such algorithmic schemes attain a $(1+\epsilon)$-approximation in only $t(\epsilon)\cdot poly(n)$ time, where $t(\cdot)$ is some function that depends only on $\epsilon$. Technically speaking, our approach relies on presenting tailor-made reductions to a newly-introduced multi-dimensional extension of the Santa Claus problem [Bansal-Sviridenko, STOC'06]. Even though the single-dimensional problem is already known to be APX-Hard, we prove that an EPTAS can be designed under certain structural assumptions, which hold for our applications.   To demonstrate the versatility of our framework, we obtain an EPTAS for the adaptive ProbeMax problem as well as for its non-adaptive counterpart; in both cases, state-of-the-art approximability results have been inefficient polynomial time approximation schemes (PTAS) [Chen et al., NIPS'16; Fu et al., ICALP'18]. Turning our attention to selection-stopping settings, we further derive an EPTAS for the Free-Order Prophets problem [Agrawal et al., EC'20] and for its cost-driven generalization, Pandora's Box with Commitment [Fu et al., ICALP'18]. These results improve on known PTASes for their adaptive variants, and constitute the first non-trivial approximations in the non-adaptive setting. ",Danny Segev and Sahil Singla,,,11,
The nearest-colattice algorithm,"  In this work, we exhibit a hierarchy of polynomial time algorithms solving approximate variants of the Closest Vector Problem (CVP). Our first contribution is a heuristic algorithm achieving the same distance tradeoff as HSVP algorithms, namely $\approx   \beta^{\frac{n}{2\beta}}\textrm{covol}(\Lambda)^{\frac{1}{n}}$ for a random lattice $\Lambda$ of rank $n$. Compared to the so-called Kannan's embedding technique, our algorithm allows using precomputations and can be used for efficient batch CVP instances. This implies that some attacks on lattice-based signatures lead to very cheap forgeries, after a precomputation. Our second contribution is a proven reduction from approximating the closest vector with a factor $\approx n^{\frac32}\beta^{\frac{3n}{2\beta}}$ to the Shortest Vector Problem (SVP) in dimension $\beta$. ","Thomas Espitau, Paul Kirchner",,,11,
"An Improved Cutting Plane Method for Convex Optimization, Convex-Concave   Games and its Applications","  Given a separation oracle for a convex set $K \subset \mathbb{R}^n$ that is contained in a box of radius $R$, the goal is to either compute a point in $K$ or prove that $K$ does not contain a ball of radius $\epsilon$. We propose a new cutting plane algorithm that uses an optimal $O(n \log (\kappa))$ evaluations of the oracle and an additional $O(n^2)$ time per evaluation, where $\kappa = nR/\epsilon$.   $\bullet$ This improves upon Vaidya's $O( \text{SO} \cdot n \log (\kappa) + n^{\omega+1} \log (\kappa))$ time algorithm [Vaidya, FOCS 1989a] in terms of polynomial dependence on $n$, where $\omega < 2.373$ is the exponent of matrix multiplication and $\text{SO}$ is the time for oracle evaluation.   $\bullet$ This improves upon Lee-Sidford-Wong's $O( \text{SO} \cdot n \log (\kappa) + n^3 \log^{O(1)} (\kappa))$ time algorithm [Lee, Sidford and Wong, FOCS 2015] in terms of dependence on $\kappa$.   For many important applications in economics, $\kappa = \Omega(\exp(n))$ and this leads to a significant difference between $\log(\kappa)$ and $\mathrm{poly}(\log (\kappa))$. We also provide evidence that the $n^2$ time per evaluation cannot be improved and thus our running time is optimal.   A bottleneck of previous cutting plane methods is to compute leverage scores, a measure of the relative importance of past constraints. Our result is achieved by a novel multi-layered data structure for leverage score maintenance, which is a sophisticated combination of diverse techniques such as random projection, batched low-rank update, inverse maintenance, polynomial interpolation, and fast rectangular matrix multiplication. Interestingly, our method requires a combination of different fast rectangular matrix multiplication algorithms. ","Haotian Jiang, Yin Tat Lee, Zhao Song, Sam Chiu-wai Wong",,,11,
Faster Parallel Algorithm for Approximate Shortest Path,"  We present the first $m\,\text{polylog}(n)$ work, $\text{polylog}(n)$ time algorithm in the PRAM model that computes $(1+\epsilon)$-approximate single-source shortest paths on weighted, undirected graphs. This improves upon the breakthrough result of Cohen~[JACM'00] that achieves $O(m^{1+\epsilon_0})$ work and $\text{polylog}(n)$ time. While most previous approaches, including Cohen's, leveraged the power of hopsets, our algorithm builds upon the recent developments in \emph{continuous optimization}, studying the shortest path problem from the lens of the closely-related \emph{minimum transshipment} problem. To obtain our algorithm, we demonstrate a series of near-linear work, polylogarithmic-time reductions between the problems of approximate shortest path, approximate transshipment, and $\ell_1$-embeddings, and establish a recursive algorithm that cycles through the three problems and reduces the graph size on each cycle. As a consequence, we also obtain faster parallel algorithms for approximate transshipment and $\ell_1$-embeddings with polylogarithmic distortion. The minimum transshipment algorithm in particular improves upon the previous best $m^{1+o(1)}$ work sequential algorithm of Sherman~[SODA'17].   To improve readability, the paper is almost entirely self-contained, save for several staple theorems in algorithms and combinatorics. ",Jason Li,,,11,
A Simple Deterministic Algorithm for Edge Connectivity,"  We show a deterministic algorithm for computing edge connectivity of a simple graph with $m$ edges in $m^{1+o(1)}$ time. Although the fastest deterministic algorithm by Henzinger, Rao, and Wang [SODA'17] has a faster running time of $O(m\log^{2}m\log\log m)$, we believe that our algorithm is conceptually simpler. The key tool for this simplication is the expander decomposition. We exploit it in a very straightforward way compared to how it has been previously used in the literature. ",Thatchaphol Saranurak,,,11,
Drawing Shortest Paths in Geodetic Graphs,"  Motivated by the fact that in a space where shortest paths are unique, no two shortest paths meet twice, we study a question posed by Greg Bodwin: Given a geodetic graph $G$, i.e., an unweighted graph in which the shortest path between any pair of vertices is unique, is there a philogeodetic drawing of $G$, i.e., a drawing of $G$ in which the curves of any two shortest paths meet at most once? We answer this question in the negative by showing the existence of geodetic graphs that require some pair of shortest paths to cross at least four times. The bound on the number of crossings is tight for the class of graphs we construct. Furthermore, we exhibit geodetic graphs of diameter two that do not admit a philogeodetic drawing. ","Sabine Cornelsen, Maximilian Pfister, Henry F\""orster, Martin   Gronemann, Michael Hoffmann, Stephen Kobourov, Thomas Schneck",,,11,
Book Embeddings of Graph Products,"  A $k$-stack layout (also called a $k$-page book embedding) of a graph consists of a total order of the vertices, and a partition of the edges into $k$ sets of non-crossing edges with respect to the vertex order. The stack number (book thickness, page number) of a graph is the minimum $k$ such that it admits a $k$-stack layout. A $k$-queue layout is defined similarly, except that no two edges in a single set may be nested.   It was recently proved that graphs of various non-minor-closed classes are subgraphs of the strong product of a path and a graph with bounded treewidth. Motivated by this decomposition result, we explore stack layouts of graph products. We show that the stack number is bounded for the strong product of a path and (i) a graph of bounded pathwidth or (ii) a bipartite graph of bounded treewidth and bounded degree. The results are obtained via a novel concept of simultaneous stack-queue layouts, which may be of independent interest. ",Sergey Pupyrev,,,11,
An Embellished Account of Agafonov's Proof of Agafonov's Theorem,"  We give an account of Agafonov's original proof of his eponymous theorem. The original proof was only reported in Russian in a journal not widely available, and the work most commonly cited in western literature is instead the english translation of a summary version containing no proofs. The account contains some embellishments to Agafonov's original arguments, made in the interest of clarity, and provides some historical context to Agafonov's work. ","Thomas Seiller (CNRS, LIPN), Jakob Simonsen (DIKU)",,,11,
The Graphs of Stably Matchable Pairs,"  We study the graphs formed from instances of the stable matching problem by connecting pairs of elements with an edge when there exists a stable matching in which they are matched. Our results include the NP-completeness of recognizing these graphs, an exact recognition algorithm that is singly exponential in the number of edges of the given graph, and an algorithm whose time is linear in the number of vertices of the graph but exponential in a polynomial of its carving width. We also provide characterizations of graphs of stably matchable pairs that belong to certain classes of graphs, and of the lattices of stable matchings that can have graphs in these classes. ",David Eppstein,,,11,
"Critical sets, crowns, and local maximum independent sets","  A set $S\subseteq V(G)$ is independent (or stable) if no two vertices from $S$ are adjacent, and by $\mathrm{Ind}(G)$ we mean the set of all independent sets of $G$.   A set $A\in\mathrm{Ind}(G)$ is critical (and we write $A\in CritIndep(G)$) if $\left\vert A\right\vert -\left\vert N(A)\right\vert =\max\{\left\vert I\right\vert -\left\vert N(I)\right\vert :I\in \mathrm{Ind}(G)\}$, where $N(I)$ denotes the neighborhood of $I$.   If $S\in\mathrm{Ind}(G)$ and there is a matching from $N(S)$ into $S$, then $S$ is a crown, and we write $S\in Crown(G)$.   Let $\Psi(G)$ be the family of all local maximum independent sets of graph $G$, i.e., $S\in\Psi(G)$ if $S$ is a maximum independent set in the subgraph induced by $S\cup N(S)$.   In this paper we show that $CritIndep(G)\subseteq Crown(G)$ $\subseteq\Psi(G)$ are true for every graph. In addition, we present some classes of graphs where these families coincide and form greedoids or even more general set systems that we call augmentoids. ",Vadim E. Levit and Eugen Mandrescu,,,11,
Nonleaf Patterns in Trees: Protected Nodes and Fine Numbers,"  A closed-form formula is derived for the number of occurrences of matches of a multiset of patterns among all ordered (plane-planted) trees with a given number of edges. A pattern looks like a tree, with internal nodes and leaves, but also contain components that match subtrees or sequences of subtrees. This result extends previous versatile tree-pattern enumeration formulae to incorporate components that are only allowed to match nonleaf subtrees and provides enumerations of trees by the number of protected (shortest outgoing path has two or more edges) or unprotected nodes. ",Nachum Dershowitz,,,11,
Growth of bilinear maps,"  For a bilinear map $*:\mathbb R^d\times \mathbb R^d\to \mathbb R^d$ of nonnegative coefficients and a vector $s\in \mathbb R^d$ of positive entries, among an exponentially number of ways combining $n$ instances of $s$ using $n-1$ applications of $*$ for a given $n$, we are interested in the largest entry over all the resulting vectors. An asymptotic behavior is that the $n$-th root of this largest entry converges to a growth rate $\lambda$ when $n$ tends to infinity. In this paper, we prove the existence of this limit by a special structure called linear pattern. We also pose a question on the possibility of a relation between the structure and whether $\lambda$ is algebraic. ",Vuong Bui,,,11,
New Integrality Gap Results for the Firefighters Problem on Trees,"  The firefighter problem is NP-hard and admits a $(1-1/e)$ approximation based on rounding the canonical LP. In this paper, we first show a matching integrality gap of $(1-1/e+\epsilon)$ on the canonical LP. This result relies on a powerful combinatorial gadget that can be used to prove integrality gap results for many problem settings. We also consider the canonical LP augmented with simple additional constraints (as suggested by Hartke). We provide several evidences that these constraints improve the integrality gap of the canonical LP: (i) Extreme points of the new LP are integral for some known tractable instances and (ii) A natural family of instances that are bad for the canonical LP admits an improved approximation algorithm via the new LP. We conclude by presenting a $5/6$ integrality gap instance for the new LP. ",Parinya Chalermsook and Daniel Vaz,,,11,
Enumerating minimal dominating sets in $K_t$-free graphs and variants,"  It is a long-standing open problem whether the minimal dominating sets of a graph can be enumerated in output-polynomial time. In this paper we investigate this problem in graph classes defined by forbidding an induced subgraph. In particular, we provide output-polynomial time algorithms for $K_t$-free graphs and variants. This answers a question of Kant\'e et al. about enumeration in bipartite graphs. ","Marthe Bonamy, Oscar Defrain, Marc Heinrich, Jean-Florent Raymond, and   Micha{\l} Pilipczuk",,,11,
On the similarity between ranking vectors in the pairwise comparison   method,"  There are many priority deriving methods for pairwise comparison matrices. It is known that when these matrices are consistent all these methods result in the same priority vector. However, when they are inconsistent, the results may vary. The presented work formulates an estimation of the difference between priority vectors in the two most popular ranking methods: the eigenvalue method and the geometric mean method. The estimation provided refers to the inconsistency of the pairwise comparison matrix. Theoretical considerations are accompanied by Montecarlo experiments showing the discrepancy between the values of both methods. ",Konrad Ku{\l}akowski and Ji\v{r}\'i Mazurek and Micha{\l} Strada,,,11,
The Discrete Cosine Transform over Prime Finite Fields,"  This paper examines finite field trigonometry as a tool to construct trigonometric digital transforms. In particular, by using properties of the k-cosine function over GF(p), the Finite Field Discrete Cosine Transform (FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths that are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined when p is a Mersenne prime. In this instance blocklengths that are powers of two are possible and radix-2 fast algorithms can be used to compute the transform. ","M.M. Campello de Souza, H.M. de Oliveira, R.M. Campello de Souza and   M.M. Vasconcelos",,,11,
Temporal Cliques Admit Sparse Spanners,"  Let $G=(V,E)$ be an undirected graph on $n$ vertices and $\lambda:E\to 2^{\mathbb{N}}$ a mapping that assigns to every edge a non-empty set of integer labels (times). Such a graph is {\em temporally connected} if a path exists with non-decreasing times from every vertex to every other vertex. In a seminal paper, Kempe, Kleinberg, and Kumar~\cite{KKK02} asked whether, given such a temporal graph, a {\em sparse} subset of edges always exists whose labels suffice to preserve temporal connectivity---a {\em temporal spanner}. Axiotis and Fotakis~\cite{AF16} answered negatively by exhibiting a family of $\Theta(n^2)$-dense temporal graphs which admit no temporal spanner of density $o(n^2)$. In this paper, we give the first positive answer as to the existence of $o(n^2)$-sparse spanners in a dense class of temporal graphs, by showing that if $G$ is a complete graph, then one can always find a temporal spanner of density $O(n \log n)$. The proofs are constructive. ","Arnaud Casteigts, Joseph G. Peters, Jason Schoeters",,,11,
On the diameter of the polytope of the stable marriage with ties,"  The stable marriage problem with ties is a well-studied and interesting problem in game theory. We are given a set of men and a set of women. Each individual has a preference ordering on the opposite group, which can possibly contain ties. A stable marriage is given by a matching between men and women for which there is no blocking pair, i.e., a men and a women who strictly prefer each other to their current partner in the matching.   In this paper, we study the diameter of the polytope given by the convex hull of characteristic vectors of stable marriages, in the setting with ties. We prove an upper bound of $\lfloor \frac{n}{3}\rfloor$ on the diameter, where $n$ is the total number of men and women, and give a family of instances for which the bound holds tight. Our result generalizes the bound on the diameter of the standard stable marriage polytope (i.e., the well-known polytope that describes the setting without ties), developed previously in the literature. ",Felix Bauckholt and Laura Sanit\`a,,,11,
Eccentricity terrain of $\delta$-hyperbolic graphs,"  A graph $G=(V,E)$ is $\delta$-hyperbolic if for any four vertices $u,v,w,x$, the two larger of the three distance sums $d(u,v)+d(w,x)$, $d(u,w)+d(v,x)$, and $d(u,x)+d(v,w)$ differ by at most $2\delta \geq 0$. Recent work shows that many real-world graphs have small hyperbolicity $\delta$. This paper describes the eccentricity terrain of a $\delta$-hyperbolic graph. The eccentricity function $e_G(v)=\max\{d(v,u) : u \in V\}$ partitions the vertex set of $G$ into eccentricity layers $C_{k}(G) = \{v \in V : e(v)=rad(G)+k\}$, $k \in \mathbb{N}$, where $rad(G)=\min\{e_G(v): v\in V\}$ is the radius of $G$. The paper studies the eccentricity layers of vertices along shortest paths, identifying such terrain features as hills, plains, valleys, terraces, and plateaus. It introduces the notion of $\beta$-pseudoconvexity, which implies Gromov's $\epsilon$-quasiconvexity, and illustrates the abundance of pseudoconvex sets in $\delta$-hyperbolic graphs. In particular, it shows that all sets $C_{\leq k}(G)=\{v\in V : e_G(v) \leq rad(G) + k\}$, $k\in \mathbb{N}$, are $(2\delta-1)$-pseudoconvex. Additionally, several bounds on the eccentricity of a vertex are obtained which yield a few approaches to efficiently approximating all eccentricities. An $O(\delta |E|)$ time eccentricity approximation $\hat{e}(v)$, for all $v\in V$, is presented that uses distances to two mutually distant vertices and satisfies $e_G(v)-2\delta \leq \hat{e}(v) \leq {e_G}(v)$. It also shows existence of two eccentricity approximating spanning trees $T$, one constructible in $O(\delta |E|)$ time and the other in $O(|E|)$ time, which satisfy ${e}_G(v) \leq e_T(v) \leq {e}_G(v)+4\delta+1$ and ${e}_G(v) \leq e_T(v) \leq {e}_G(v)+6\delta$, respectively. Thus, the eccentricity terrain of a tree gives a good approximation (up-to an additive error $O(\delta))$ of the eccentricity terrain of a $\delta$-hyperbolic graph. ",Feodor F. Dragan and Heather M. Guarnera,,,11,
Integrality of Linearizations of Polynomials over Binary Variables using   Additional Monomials,"  Polynomial optimization problems over binary variables can be expressed as integer programs using a linearization with extra monomials in addition to those arising in the given polynomial. We characterize when such a linearization yields an integral relaxation polytope, generalizing work by Del Pia and Khajavirad (SIAM Journal on Optimization, 2018) and Buchheim, Crama and Rodr\'iguez-Heck (European Journal of Operations Research, 2019). We also present an algorithm that finds these extra monomials for a given polynomial to yield an integral relaxation polytope or determines that no such set of extra monomials exists. In the former case, our approach yields an algorithm to solve the given polynomial optimization problem as a compact LP, and we complement this with a purely combinatorial algorithm. ",Christopher Hojny and Marc E. Pfetsch and Matthias Walter,,,11,
On motifs in colored graphs,"  One of the most important concepts in biological network analysis is that of network motifs, which are patterns of interconnections that occur in a given network at a frequency higher than expected in a random network. In this work we are interested in searching and inferring network motifs in a class of biological networks that can be represented by vertex-colored graphs. We show the computational complexity for many problems related to colorful topological motifs and present efficient algorithms for special cases. We also present a probabilistic strategy to detect highly frequent motifs in vertex-colored graphs. Experiments on real data sets show that our algorithms are very competitive both in efficiency and in quality of the solutions. ","Diego P Rubert, Eloi Araujo, Marco A Stefanes, Jens Stoye, F\'abio V   Martinez",,,11,
A Polyhedral Study for the Cubic Formulation of the Unconstrained   Traveling Tournament Problem,"  We consider the unconstrained traveling tournament problem, a sports timetabling problem that minimizes traveling of teams. Since its introduction about 20 years ago, most research was devoted to modeling and reformulation approaches. In this paper we carry out a polyhedral study for the cubic integer programming formulation by establishing the dimension of the integer hull as well as of faces induced by model inequalities. Moreover, we introduce a new class of inequalities and show that they are facet-defining. Finally, we evaluate the impact of these inequalities on the linear programming bounds. ",Marije Siemann and Matthias Walter,,,11,
Matroidal Approximations of Independence Systems,"  Milgrom (2017) has proposed a heuristic for determining a maximum weight basis of an independence system ${\mathcal I}$ given that we want an approximation guarantee only for sets in a prescribed ${\mathcal O}\subseteq {\mathcal I}$. This ${\mathcal O}$ reflects prior knowledge of the designer about the location of the optimal basis. The heuristic is based on finding an `inner matroid', one contained in the independence system. We show that even in the case ${\mathcal O}={\mathcal I}$ of zero additional knowledge the worst-case performance of this new heuristic can be better than that of the classical greedy algorithm. ","Sven de Vries, Rakesh V. Vohra",,,11,
"$2$-Layer $k$-Planar Graphs: Density, Crossing Lemma, Relationships, and   Pathwidth","  The $2$-layer drawing model is a well-established paradigm to visualize bipartite graphs. Several beyond-planar graph classes have been studied under this model. Surprisingly, however, the fundamental class of $k$-planar graphs has been considered only for $k=1$ in this context. We provide several contributions that address this gap in the literature. First, we show tight density bounds for the classes of $2$-layer $k$-planar graphs with $k\in\{2,3,4,5\}$. Based on these results, we provide a Crossing Lemma for $2$-layer $k$-planar graphs, which then implies a general density bound for $2$-layer $k$-planar graphs. We prove this bound to be almost optimal with a corresponding lower bound construction. Finally, we study relationships between $k$-planarity and $h$-quasiplanarity in the $2$-layer model and show that $2$-layer $k$-planar graphs have pathwidth at most $k+1$. ","Patrizio Angelini, Giordano Da Lozzo, Henry F\""orster and Thomas   Schneck",,,11,
Criteria for the numerical constant recognition,"  The need for recognition of numerical (decimal, floating-point) constants in terms of elementary functions emerges in many areas of experimental mathematics, numerical analysis, computer algebra systems, model building, approximation and data compression. However, existing solutions are plagued by lack of any criteria distinguishing between random formula, matching literally decimal expansion (i.e. approximation) and probable ""exact"" (or at least probable) expression match in the sense of Occam's razor. In particular, convincing STOP criteria for search were never developed. In article, such a criteria, working in statistical sense, are provided. Recognition process can be viewed as (1) enumeration of all formulas in order of increasing Kolmogorov complexity (2) random process with appropriate statistical distribution (3) compression of a decimal string. All three approaches are remarkably consistent, and provide essentially the same limit for practical depth of search. Tested unique formulas count must not exceed 1/sigma, where sigma is relative numerical error of the target constant. Beyond that, further search is pointless, because, in the view of approach (1), number of equivalent expressions within error bounds grows exponentially; in view of (2), probability of random match approaches 1; in view of (3) compression ratio much smaller than 1. ",Andrzej Odrzywolek,,,11,
On the Complexity of Branching Proofs,"  We consider the task of proving integer infeasibility of a bounded convex $K$ in $\mathbb{R}^n$ using a general branching proof system. In a general branching proof, one constructs a branching tree by adding an integer disjunction $\mathbf{a} \mathbf{x} \leq b$ or $\mathbf{a} \mathbf{x} \geq b+1$, $\mathbf{a} \in \mathbb{Z}^n$, $b \in \mathbb{Z}$, at each node, such that the leaves of the tree correspond to empty sets (i.e., $K$ together with the inequalities picked up from the root to leaf is empty). Recently, Beame et al (ITCS 2018), asked whether the bit size of the coefficients in a branching proof, which they named stabbing planes (SP) refutations, for the case of polytopes derived from SAT formulas, can be assumed to be polynomial in $n$. We resolve this question by showing that any branching proof can be recompiled so that the integer disjunctions have coefficients of size at most $(n R)^{O(n^2)}$, where $R \in \mathbb{N}$ such that $K \in R \mathbb{B}_1^n$, while increasing the number of nodes in the branching tree by at most a factor $O(n)$. As our second contribution, we show that Tseitin formulas, an important class of infeasible SAT instances, have quasi-polynomial sized cutting plane (CP) refutations, disproving the conjecture that Tseitin formulas are (exponentially) hard for CP. As our final contribution, we give a simple family of polytopes in $[0,1]^n$ requiring branching proofs of length $2^n/n$. ",Daniel Dadush and Samarth Tiwari,,,11,
"A class of examples demonstrating that P is different from NP in the ""P   vs NP"" problem","  The CMI Millennium ""P vs NP Problem"" can be resolved e.g. if one shows at least one counterexample to the conjecture ""P is equal to NP"". A certain class of problems being such counterexamples is formulated. This implies the rejection of the hypothesis ""P is equal to NP"" for any conditions satisfying the formulation of the problem. Thus, the solution ""P is different from NP"" of the problem is proved. The class of counterexamples can be interpreted as any quantum superposition of any finite set of quantum states. The Kochen-Specker theorem is involved. Any fundamentally random choice among a finite set of alternatives belong to NP, but not to P. The conjecture that the set complement of P to NP can be described by that kind of choice is formulated exhaustively. ",Vasil Penchev,,,11,
On the directed tile assembly systems at temperature 1,"  We show here that a model called directed self-assembly at temperature 1 is unable to do complex computations like the ones of a Turing machine. Since this model can be seen as a generalization of finite automata to 2D languages, a logical approach is to proceed in two steps. The first one is to develop a 2D pumping lemma and the second one is to use this pumping lemma to classify the different types of possible computation.   Previously, Meunier at al have proven a pumping lemma and Doty et al, assuming the existence of a pumping lemma, have classified the different types of terminal assembly. Thus the combination of these two papers solves the directed temperature 1 conjecture ... but in an imperfect way. Indeed, since the work of Doty et al is anterior to the pumping lemma of Meunier et al, the authors assumed a different and stronger pumping lemma. Nevertheless, all the demonstrations made in Doty et al still hold with the pumping lemma of Meunier et al.   In this paper, we harmonize the notations between these two articles in order to clearly solve the directed temperature 1 conjecture. We are also able to give an optimal description of the bi-periodic structures which may appear in some tile assembly system. ",Pierre-\'Etienne Meunier and Damien Regnault,,,11,
Trace Reconstruction Problems in Computational Biology,"  The problem of reconstructing a string from its error-prone copies, the trace reconstruction problem, was introduced by Vladimir Levenshtein two decades ago. While there has been considerable theoretical work on trace reconstruction, practical solutions have only recently started to emerge in the context of two rapidly developing research areas: immunogenomics and DNA data storage. In immunogenomics, traces correspond to mutated copies of genes, with mutations generated naturally by the adaptive immune system. In DNA data storage, traces correspond to noisy copies of DNA molecules that encode digital data, with errors being artifacts of the data retrieval process. In this paper, we introduce several new trace generation models and open questions relevant to trace reconstruction for immunogenomics and DNA data storage, survey theoretical results on trace reconstruction, and highlight their connections to computational biology. Throughout, we discuss the applicability and shortcomings of known solutions and suggest future research directions. ","Vinnu Bhardwaj, Pavel A. Pevzner, Cyrus Rashtchian, Yana Safonova",,,11,
'Target Set Selection' on Graphs of Bounded Vertex Cover Number,"  Given a simple, undirected graph $G$ with a threshold function $\tau:V(G) \rightarrow \mathbb{N}$, the \textsc{Target Set Selection} (TSS) Problem is about choosing a minimum cardinality set, say $S \subseteq V(G)$, such that starting a diffusion process with $S$ as its seed set will eventually result in activating all the nodes in $G$. We have the following results on the TSS Problem:   - It was shown by Nichterlein et al. [Social Network Analysis and Mining, 2013] that it is possible to compute an optimal sized target set in $O(2^{(2^{t}+1)t}\cdot m)$ time, where $m$ and $t$ denote the number of edges and the cardinality of a minimum vertex cover, respectively, of the graph under consideration. We improve this result by designing an algorithm that computes an optimal sized target set in $2^{O(t\log t)}n^{O(1)}$ time, where $n$ denotes the number of vertices of the graph under consideration.   - We show that the TSS Problem on bipartite graphs does not admit an approximation algorithm with a performance guarantee asymptotically better than $O(\log n_{min})$, where $n_{min}$ is the cardinality of the smaller bipartition, unless $P=NP$. Chen et al. [SIDMA, 2009] %[On the Approximability of Influence in Social Networks. SIAM Journal on Discrete Mathematics, 23(3):1400-1415, 2009] had shown that the TSS Problem on general graphs does not admit an approximation algorithm with a performance guarantee asymptotically better than $O(2^{\log^{1 - \epsilon} n})$, where $n$ is the number of vertices of the graph under consideration, unless $NP \subseteq DTIME(n^{polylog(n)})$. ","Suman Banerjee, Rogers Mathew, and Fahad Panolan",,,11,
"Special-case Algorithms for Blackbox Radical Membership, Nullstellensatz   and Transcendence Degree","  Radical membership testing, and the special case of Hilbert's Nullstellensatz (HN), is a fundamental computational algebra problem. It is NP-hard; and has a famous PSPACE algorithm due to effective Nullstellensatz bounds. We identify a useful case of these problems where practical algorithms, and improved bounds, could be given, when the transcendence degree $r$ of the input polynomials is smaller than the number of variables $n$. If $d$ is the degree bound on the input polynomials, then we solve radical membership (even if input polynomials are blackboxes) in around $d^r$ time. The prior best was $> d^n$ time (always, $d^n\ge d^r$). Also, we significantly improve effective Nullstellensatz degree-bound, when $r\ll n$. Structurally, our proof shows that these problems reduce to the case of $r+1$ polynomials of transcendence degree $\ge r$. This input instance (corresponding to none or a unique annihilator) is at the core of HN's hardness. Our proof methods invoke basic algebraic-geometry. ","Abhibhav Garg, Nitin Saxena",,,11,
"Counting Homomorphisms to $K_4$-minor-free Graphs, modulo 2","  We study the problem of computing the parity of the number of homomorphisms from an input graph $G$ to a fixed graph $H$. Faben and Jerrum [ToC'15] introduced an explicit criterion on the graph $H$ and conjectured that, if satisfied, the problem is solvable in polynomial time and, otherwise, the problem is complete for the complexity class $\oplus\mathrm{P}$ of parity problems. We verify their conjecture for all graphs $H$ that exclude the complete graph on $4$ vertices as a minor. Further, we rule out the existence of a subexponential-time algorithm for the $\oplus\mathrm{P}$-complete cases, assuming the randomised Exponential Time Hypothesis. Our proofs introduce a novel method of deriving hardness from globally defined substructures of the fixed graph $H$. Using this, we subsume all prior progress towards resolving the conjecture (Faben and Jerrum [ToC'15]; G\""obel, Goldberg and Richerby [ToCT'14,'16]). As special cases, our machinery also yields a proof of the conjecture for graphs with maximum degree at most $3$, as well as a full classification for the problem of counting list homomorphisms, modulo $2$. ","Jacob Focke, Leslie Ann Goldberg, Marc Roth and Stanislav \v{Z}ivn\'y",,,11,
A Quadratic Lower Bound for Algebraic Branching Programs and Formulas,"  We show that any Algebraic Branching Program (ABP) computing the polynomial $\sum_{i = 1}^n x_i^n$ has at least $\Omega(n^2)$ vertices. This improves upon the lower bound of $\Omega(n\log n)$, which follows from the classical result of Baur and Strassen [Str73, BS83], and extends the results in [K19], which showed a quadratic lower bound for \emph{homogeneous} ABPs computing the same polynomial.   Our proof relies on a notion of depth reduction which is reminiscent of similar statements in the context of matrix rigidity, and shows that any small enough ABP computing the polynomial $\sum_{i=1}^n x_i^n$ can be depth reduced to essentially a homogeneous ABP of the same size which computes the polynomial $\sum_{i = 1}^n x_i^n + \epsilon(x_1, \ldots, x_n)$, for a structured ""error polynomial"" $\epsilon(x_1, \ldots, x_n)$. To complete the proof, we then observe that the lower bound in [K19] is robust enough and continues to hold for all polynomials $\sum_{i = 1}^n x_i^n + \epsilon(x_1, \ldots, x_n)$, where $\epsilon(x_1, \ldots, x_n)$ has the appropriate structure.   We also use our ideas to show an $\Omega(n^2)$ lower bound of the size of algebraic formulas computing the elementary symmetric polynomial of degree $0.1n$ on $n$ variables. This is a slight improvement upon the prior best known formula lower bound (proved for a different polynomial) of $\Omega(n^2/\log n)$ [Nec66, K85, SY10]. Interestingly, this lower bound is asymptotically better than $n^2/\log n$, the strongest lower bound that can be proved using previous methods. This lower bound also matches the upper bound, due to Ben-Or, who showed that elementary symmetric polynomials can be computed by algebraic formula (in fact depth-$3$ formula) of size $O(n^2)$. Prior to this work, Ben-Or's construction was known to be optimal only for algebraic formulas of depth-$3$ [SW01]. ","Prerona Chatterjee, Mrinal Kumar, Adrian She, Ben Lee Volk",,,11,
A Multistage View on 2-Satisfiability,"  We study $q$-SAT in the multistage model, focusing on the linear-time solvable 2-SAT. Herein, given a sequence of $q$-CNF fomulas and a non-negative integer $d$, the question is whether there is a sequence of satisfying truth assignments such that for every two consecutive truth assignments, the number of variables whose values changed is at most $d$. We prove that Multistage 2-SAT is NP-hard even in quite restricted cases. Moreover, we present parameterized algorithms (including kernelization) for Multistage 2-SAT and prove them to be asymptotically optimal. ",Till Fluschnik,,,11,
Sum of squares bounds for the ordering principle,"  In this paper, we analyze the sum of squares hierarchy (SOS) on the ordering principle on $n$ elements. We prove that degree $O(\sqrt{n}log(n))$ SOS can prove the ordering principle. We then show that this upper bound is essentially tight by proving that for any $\epsilon > 0$, SOS requires degree $\Omega(n^{\frac{1}{2} - \epsilon})$ to prove the ordering principle on $n$ elements. ",Aaron Potechin,,,11,
Efficient List-Decoding with Constant Alphabet and List Sizes,"  We present an explicit and efficient algebraic construction of capacity-achieving list decodable codes with both constant alphabet and constant list sizes. More specifically, for any $R \in (0,1)$ and $\epsilon>0$, we give an algebraic construction of an infinite family of error-correcting codes of rate $R$, over an alphabet of size $(1/\epsilon)^{O(1/\epsilon^2)}$, that can be list decoded from a $(1-R-\epsilon)$-fraction of errors with list size at most $\exp(\mathrm{poly}(1/\epsilon))$. Moreover, the codes can be encoded in time $\mathrm{poly}(1/\epsilon, n)$, the output list is contained in a linear subspace of dimension at most $\mathrm{poly}(1/\epsilon)$, and a basis for this subspace can be found in time $\mathrm{poly}(1/\epsilon, n)$. Thus, both encoding and list decoding can be performed in fully polynomial-time $\mathrm{poly}(1/\epsilon, n)$, except for pruning the subspace and outputting the final list which takes time $\exp(\mathrm{poly}(1/\epsilon))\cdot\mathrm{poly}(n)$.   Our codes are quite natural and structured. Specifically, we use algebraic-geometric (AG) codes with evaluation points restricted to a subfield, and with the message space restricted to a (carefully chosen) linear subspace. Our main observation is that the output list of AG codes with subfield evaluation points is contained in an affine shift of the image of a block-triangular-Toeplitz (BTT) matrix, and that the list size can potentially be reduced to a constant by restricting the message space to a BTT evasive subspace, which is a large subspace that intersects the image of any BTT matrix in a constant number of points. We further show how to explicitly construct such BTT evasive subspaces, based on the explicit subspace designs of Guruswami and Kopparty (Combinatorica, 2016), and composition. ","Zeyu Guo, Noga Ron-Zewi",,,11,
From Functional Nondeterministic Transducers to Deterministic Two-Tape   Automata,"  The question whether P = NP revolves around the discrepancy between active production and mere verification by Turing machines. In this paper, we examine the analogous problem for finite transducers and automata. Every nondeterministic finite transducer defines a binary relation associating input words with output words that are computed and accepted by the transducer. Functional transducers are those for which the relation is a function. We characterize finite-valued, functional, and unambiguous nondeterministic transducers whose relations can be verified by a deterministic two-tape automaton, show how to construct such an automaton if one exists, and prove the undecidability of the criterion. ",Elisabet Burjons and Fabian Frei and Martin Raszyk,,,11,
Bare quantum simultaneity versus classical interactivity in   communication complexity,"  A relational bipartite communication problem is presented that has an efficient quantum simultaneous-messages protocol, but no efficient classical two-way protocol. ",Dmitry Gavinsky,,,11,
Explicit Designs and Extractors,"  We give significantly improved explicit constructions of three related pseudorandom objects.   1. Extremal designs: An $(n,r,s)$-design, or $(n,r,s)$-partial Steiner system, is an $r$-uniform hypergraph over $n$ vertices with pairwise hyperedge intersections of size $<s$. For all constants $r\geq s\in\mathbb{N}$ with $r$ even, we explicitly construct $(n,r,s)$-designs $(G_n)_{n\in\mathbb{N}}$ with independence number $\alpha(G_n)\leq O(n^{\frac{2(r-s)}{r}})$. This gives the first derandomization of a result by R\""odl and \v{S}inajov\'a (Random Structures & Algorithms, 1994).   2. Extractors for adversarial sources: By combining our designs with leakage-resilient extractors (Chattopadhyay et al., FOCS 2020), we establish a new, simple framework for extracting from adversarial sources of locality $0$. As a result, we obtain significantly improved low-error extractors for these sources. For any constant $\delta>0$, we extract from $(N,K,n,$ polylog$(n))$-adversarial sources of locality $0$, given just $K\geq N^\delta$ good sources. The previous best result (Chattopadhyay et al., STOC 2020) required $K\geq N^{1/2+o(1)}$.   3. Extractors for small-space sources: Using a known reduction to adversarial sources, we immediately obtain improved low-error extractors for space $s$ sources over $n$ bits that require entropy $k\geq n^{1/2+\delta}\cdot s^{1/2-\delta}$, whereas the previous best result (Chattopadhyay et al., STOC 2020) required $k\geq n^{2/3+\delta}\cdot s^{1/3-\delta}$. On the other hand, using a new reduction from small-space sources to affine sources, we obtain near-optimal extractors for small-space sources in the polynomial error regime. Our extractors require just $k\geq s\cdot\log^Cn$ entropy for some constant $C$, which is an exponential improvement over the previous best result, which required $k\geq s^{1.1}\cdot2^{\log^{0.51}n}$ (Chattopadhyay and Li, STOC 2016). ","Eshan Chattopadhyay, Jesse Goodman",,,11,
Eliminating Intermediate Measurements in Space-Bounded Quantum   Computation,"  A foundational result in the theory of quantum computation known as the ""principle of safe storage"" shows that it is always possible to take a quantum circuit and produce an equivalent circuit that makes all measurements at the end of the computation. While this procedure is time efficient, meaning that it does not introduce a large overhead in the number of gates, it uses extra ancillary qubits and so is not generally space efficient. It is quite natural to ask whether it is possible to defer measurements to the end of a quantum computation without increasing the number of ancillary qubits.   We give an affirmative answer to this question by exhibiting a procedure to eliminate all intermediate measurements that is simultaneously space-efficient and time-efficient. A key component of our approach, which may be of independent interest, involves showing that the well-conditioned versions of many standard linear-algebraic problems may be solved by a quantum computer in less space than seems possible by a classical computer. ","Bill Fefferman (University of Chicago), Zachary Remscrim (MIT)",,,11,
The PSPACE-hardness of understanding neural circuits,"  In neuroscience, an important aspect of understanding the function of a neural circuit is to determine which, if any, of the neurons in the circuit are vital for the biological behavior governed by the neural circuit. A similar problem is to determine whether a given small set of neurons may be enough for the behavior to be displayed, even if all other neurons in the circuit are deactivated. Such a subset of neurons forms what is called a degenerate circuit for the behavior being studied.   Recent advances in experimental techniques have provided researchers with tools to activate and deactivate subsets of neurons with a very high resolution, even in living animals. The data collected from such experiments may be of the following form: when a given subset of neurons is deactivated, is the behavior under study observed? This setting leads to the algorithmic question of determining the minimal vital or degenerate sets of neurons when one is given as input a description of the neural circuit. The algorithmic problem entails both figuring out which subsets of neurons should be perturbed (activated/deactivated), and then using the data from those perturbations to determine the minimal vital or degenerate sets. Given the large number of possible perturbations, and the recurrent nature of neural circuits, the possibility of a combinatorial explosion in such an approach has been recognized in the biology and the neuroscience literature.   In this paper, we prove that the problems of finding minimal or minimum-size degenerate sets, and of finding the set of vital neurons, of a neural circuit given as input, are in fact PSPACE-hard. More importantly, we prove our hardness results by showing that a simpler problem, that of simulating such neural circuits, is itself PSPACE-hard. ","Vidya Sagar Sharma, Piyush Srivastava",,,11,
The Computational Complexity of Angry Birds,"  The physics-based simulation game Angry Birds has been heavily researched by the AI community over the past five years, and has been the subject of a popular AI competition that is currently held annually as part of a leading AI conference. Developing intelligent agents that can play this game effectively has been an incredibly complex and challenging problem for traditional AI techniques to solve, even though the game is simple enough that any human player could learn and master it within a short time. In this paper we analyse how hard the problem really is, presenting several proofs for the computational complexity of Angry Birds. By using a combination of several gadgets within this game's environment, we are able to demonstrate that the decision problem of solving general levels for different versions of Angry Birds is either NP-hard, PSPACE-hard, PSPACE-complete or EXPTIME-hard. Proof of NP-hardness is by reduction from 3-SAT, whilst proof of PSPACE-hardness is by reduction from True Quantified Boolean Formula (TQBF). Proof of EXPTIME-hardness is by reduction from G2, a known EXPTIME-complete problem similar to that used for many previous games such as Chess, Go and Checkers. To the best of our knowledge, this is the first time that a single-player game has been proven EXPTIME-hard. This is achieved by using stochastic game engine dynamics to effectively model the real world, or in our case the physics simulator, as the opponent against which we are playing. These proofs can also be extended to other physics-based games with similar mechanics. ","Matthew Stephenson, Jochen Renz, Xiaoyu Ge",,,11,
A Strong XOR Lemma for Randomized Query Complexity,"  We give a strong direct sum theorem for computing $xor \circ g$. Specifically, we show that for every function g and every $k\geq 2$, the randomized query complexity of computing the xor of k instances of g satisfies $\overline{R}_\eps(xor\circ g) = \Theta(k \overline{R}_{\eps/k}(g))$. This matches the naive success amplification upper bound and answers a conjecture of Blais and Brody (CCC19).   As a consequence of our strong direct sum theorem, we give a total function g for which $R(xor \circ g) = \Theta(k \log(k)\cdot R(g))$, answering an open question from Ben-David et al.(arxiv:2006.10957v1). ","Joshua Brody, Jae Tak Kim, Peem Lerdputtipongporn, Hariharan   Srinivasulu",,,11,
On the Mysteries of MAX NAE-SAT,"  MAX NAE-SAT is a natural optimization problem, closely related to its better-known relative MAX SAT. The approximability status of MAX NAE-SAT is almost completely understood if all clauses have the same size $k$, for some $k\ge 2$. We refer to this problem as MAX NAE-$\{k\}$-SAT. For $k=2$, it is essentially the celebrated MAX CUT problem. For $k=3$, it is related to the MAX CUT problem in graphs that can be fractionally covered by triangles. For $k\ge 4$, it is known that an approximation ratio of $1-\frac{1}{2^{k-1}}$, obtained by choosing a random assignment, is optimal, assuming $P\ne NP$. For every $k\ge 2$, an approximation ratio of at least $\frac{7}{8}$ can be obtained for MAX NAE-$\{k\}$-SAT. There was some hope, therefore, that there is also a $\frac{7}{8}$-approximation algorithm for MAX NAE-SAT, where clauses of all sizes are allowed simultaneously.   Our main result is that there is no $\frac{7}{8}$-approximation algorithm for MAX NAE-SAT, assuming the unique games conjecture (UGC). In fact, even for almost satisfiable instances of MAX NAE-$\{3,5\}$-SAT (i.e., MAX NAE-SAT where all clauses have size $3$ or $5$), the best approximation ratio that can be achieved, assuming UGC, is at most $\frac{3(\sqrt{21}-4)}{2}\approx 0.8739$. Using calculus of variations, we extend the analysis of O'Donnell and Wu for MAX CUT to MAX NAE-$\{3\}$-SAT. We obtain an optimal algorithm, assuming UGC, for MAX NAE-$\{3\}$-SAT, slightly improving on previous algorithms. The approximation ratio of the new algorithm is $\approx 0.9089$.   We complement our theoretical results with some experimental results. We describe an approximation algorithm for almost satisfiable instances of MAX NAE-$\{3,5\}$-SAT with a conjectured approximation ratio of 0.8728, and an approximation algorithm for almost satisfiable instances of MAX NAE-SAT with a conjectured approximation ratio of 0.8698. ","Joshua Brakensiek, Neng Huang, Aaron Potechin, Uri Zwick",,,11,
On the computational power and complexity of Spiking Neural Networks,"  The last decade has seen the rise of neuromorphic architectures based on artificial spiking neural networks, such as the SpiNNaker, TrueNorth, and Loihi systems. The massive parallelism and co-locating of computation and memory in these architectures potentially allows for an energy usage that is orders of magnitude lower compared to traditional Von Neumann architectures. However, to date a comparison with more traditional computational architectures (particularly with respect to energy usage) is hampered by the lack of a formal machine model and a computational complexity theory for neuromorphic computation. In this paper we take the first steps towards such a theory. We introduce spiking neural networks as a machine model where---in contrast to the familiar Turing machine---information and the manipulation thereof are co-located in the machine. We introduce canonical problems, define hierarchies of complexity classes and provide some first completeness results. ","Johan Kwisthout, Nils Donselaar",,,11,
"Stability-Preserving, Time-Efficient Mechanisms for School Choice in Two   Rounds","  We address the following dynamic version of the school choice question: a city, named City, admits students in two temporally-separated rounds, denoted $\mathcal{R}_1$ and $\mathcal{R}_2$. In round $\mathcal{R}_1$, the capacity of each school is fixed and mechanism $\mathcal{M}_1$ finds a student optimal stable matching. In round $\mathcal{R}_2$, certain parameters change, e.g., new students move into the City or the City is happy to allocate extra seats to specific schools. We study a number of Settings of this kind and give polynomial time algorithms for obtaining a stable matching for the new situations.   It is well established that switching the school of a student midway, unsynchronized with her classmates, can cause traumatic effects. This fact guides us to two types of results, the first simply disallows any re-allocations in round $\mathcal{R}_2$, and the second asks for a stable matching that minimizes the number of re-allocations. For the latter, we prove that the stable matchings which minimize the number of re-allocations form a sublattice of the lattice of stable matchings. Observations about incentive compatibility are woven into these results. We also give a third type of results, namely proofs of NP-hardness for a mechanism for round $\mathcal{R}_2$ under certain settings. ",Karthik Gajulapalli and James Liu and Tung Mai and Vijay V. Vazirani,,,11,
Power in Liquid Democracy,"  The paper develops a theory of power for delegable proxy voting systems. We define a power index able to measure the influence of both voters and delegators. Using this index, which we characterize axiomatically, we extend an earlier game-theoretic model by incorporating power-seeking behavior by agents. We analytically study the existence of pure strategy Nash equilibria in such a model. Finally, by means of simulations, we study the effect of relevant parameters on the emergence of power inequalities in the model. ",Yuzhe Zhang and Davide Grossi,,,11,
The efficiency of resource allocation mechanisms for budget-constrained   users,"  We study the efficiency of mechanisms for allocating a divisible resource. Given scalar signals submitted by all users, such a mechanism decides the fraction of the resource that each user will receive and a payment that will be collected from her. Users are self-interested and aim to maximize their utility (defined as their value for the resource fraction they receive minus their payment). Starting with the seminal work of Johari and Tsitsiklis [Mathematics of Operations Research, 2004], a long list of papers studied the price of anarchy (in terms of the social welfare --- the total users' value) of resource allocation mechanisms for a variety of allocation and payment rules. Here, we further assume that each user has a budget constraint that invalidates strategies that yield a payment that is higher than the user's budget. This subtle assumption, which is arguably more realistic, constitutes the traditional price of anarchy analysis meaningless as the set of equilibria may change drastically and their social welfare can be arbitrarily far from optimal. Instead, we study the price of anarchy using the liquid welfare benchmark that measures efficiency taking budget constraints into account. We show a tight bound of 2 on the liquid price of anarchy of the well-known Kelly mechanism and prove that this result is essentially best possible among all multi-user resource allocation mechanisms. This comes in sharp contrast to the no-budget setting where there are mechanisms that considerably outperform Kelly in terms of social welfare and even achieve full efficiency. In our proofs, we exploit the particular structure of worst-case games and equilibria, which also allows us to design (nearly) optimal two-player mechanisms by solving simple differential equations. ","Ioannis Caragiannis, Alexandros A. Voudouris",,,11,
Geometrical Regret Matching,"  We argue that the existing regret matchings for Nash equilibrium approximation conduct ""jumpy"" strategy updating when the probabilities of future plays are set to be proportional to positive regret measures. We propose a geometrical regret matching which features ""smooth"" strategy updating. Our approach is simple, intuitive and natural. The analytical and numerical results show that, continuously and ""smoothly"" suppressing ""unprofitable"" pure strategies is sufficient for the game to evolve towards Nash equilibrium, suggesting that in reality the tendency for equilibrium could be pervasive and irresistible. Technically, iterative regret matching gives rise to a sequence of adjusted mixed strategies for our study its approximation to the true equilibrium point. The sequence can be studied in metric space and visualized nicely as a clear path towards an equilibrium point. Our theory has limitations in optimizing the approximation accuracy. ",Sizhong Lan,,,11,
A Permutation-Equivariant Neural Network Architecture For Auction Design,"  Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. Theoretical approaches to the problem have hit some limits in the past decades and analytical solutions are known for only a few simple settings. Computational approaches to the problem through the use of LPs have their own set of limitations. Building on the success of deep learning, a new approach was recently proposed by Duetting et al. (2019) in which the auction is modeled by a feed-forward neural network and the design problem is framed as a learning problem. The neural architectures used in that work are general purpose and do not take advantage of any of the symmetries the problem could present, such as permutation equivariance. In this work, we consider auction design problems that have permutation-equivariant symmetry and construct a neural architecture that is capable of perfectly recovering the permutation-equivariant optimal mechanism, which we show is not possible with the previous architecture. We demonstrate that permutation-equivariant architectures are not only capable of recovering previous results, they also have better generalization properties. ","Jad Rahme, Samy Jelassi, Joan Bruna, S. Matthew Weinberg",,,11,
How Good Is a Two-Party Election Game?,"  In this paper, we propose a simple and intuitive model to investigate the efficiency of the two-party election system, especially regarding the nomination process. Each of the two parties has its own candidates, and each of them brings utilities for the people including the supporters and non-supporters. In an election, each party nominates exactly one of its candidates to compete against the other party's. The candidate wins the election with higher odds if he or she brings more utility for all the people. We model such competition as a ""two-party election game"" such that each party is a player with two or more pure strategies corresponding to its potential candidates, and the payoff of each party is a mixed utility from a selected pair of competing candidates.   By looking into the three models, namely, the linear link, Bradley-Terry, and the softmax models, which differ in how to formulate a candidate's winning odds against the competing candidate, we show that the two-party election game may neither have any pure Nash equilibrium nor a bounded price of anarchy. Nevertheless, by considering the conventional ""egoism"", which states that any candidate benefits his/her party's supporters more than any candidate from the competing party does, we prove that the two-party election game in both the linear link model and the softmax model always has pure Nash equilibria, and furthermore, the price of anarchy is constantly bounded. ","Chuang-Chieh Lin, Chi-Jen Lu, and Po-An Chen",,,11,
Learning Probably Approximately Correct Maximin Strategies in   Simulation-Based Games with Infinite Strategy Spaces,"  We tackle the problem of learning equilibria in simulation-based games. In such games, the players' utility functions cannot be described analytically, as they are given through a black-box simulator that can be queried to obtain noisy estimates of the utilities. This is the case in many real-world games in which a complete description of the elements involved is not available upfront, such as complex military settings and online auctions. In these situations, one usually needs to run costly simulation processes to get an accurate estimate of the game outcome. As a result, solving these games begets the challenge of designing learning algorithms that can find (approximate) equilibria with high confidence, using as few simulator queries as possible. Moreover, since running the simulator during the game is unfeasible, the algorithms must first perform a pure exploration learning phase and, then, use the (approximate) equilibrium learned this way to play the game. In this work, we focus on two-player zero-sum games with infinite strategy spaces. Drawing from the best arm identification literature, we design two algorithms with theoretical guarantees to learn maximin strategies in these games. The first one works in the fixed-confidence setting, guaranteeing the desired confidence level while minimizing the number of queries. Instead, the second algorithm fits the fixed-budget setting, maximizing the confidence without exceeding the given maximum number of queries. First, we formally prove {\delta}-PAC theoretical guarantees for our algorithms under some regularity assumptions, which are encoded by letting the utility functions be drawn from a Gaussian process. Then, we experimentally evaluate our techniques on a testbed made of randomly generated games and instances representing simple real-world security settings. ","Alberto Marchesi, Francesco Trov\`o, Nicola Gatti",,,11,
A Game-Theoretic Analysis of the Social Impact of Connected and   Automated Vehicles,"  In this paper, we address the much-anticipated deployment of connected and automated vehicles (CAVs) in society by modeling and analyzing the social-mobility dilemma in a game-theoretic approach. We formulate this dilemma as a normal-form game of players making a binary decision: whether to travel with a CAV (CAV travel) or not (non-CAV travel) and by constructing an intuitive payoff function inspired by the socially beneficial outcomes of a mobility system consisting of CAVs. We show that the game is equivalent to the Prisoner's dilemma, which implies that the rational collective decision is the opposite of the socially optimum. We present two different solutions to tackle this phenomenon: one with a preference structure and the other with institutional arrangements. In the first approach, we implement a social mechanism that incentivizes players to non-CAV travel and derive a lower bound on the players that ensures an equilibrium of non-CAV travel. In the second approach, we investigate the possibility of players bargaining to create an institution that enforces non-CAV travel and show that as the number of players increases, the incentive ratio of non-CAV travel over CAV travel tends to zero. We conclude by showcasing the last result with a numerical study. ","Ioannis Vasileios Chremos, Logan Beaver and Andreas Malikopoulos",,,11,
Using Multi-Agent Reinforcement Learning in Auction Simulations,"  Game theory has been developed by scientists as a theory of strategic interaction among players who are supposed to be perfectly rational. These strategic interactions might have been presented in an auction, a business negotiation, a chess game, or even in a political conflict aroused between different agents. In this study, the strategic (rational) agents created by reinforcement learning algorithms are supposed to be bidder agents in various types of auction mechanisms such as British Auction, Sealed Bid Auction, and Vickrey Auction designs. Next, the equilibrium points determined by the agents are compared with the outcomes of the Nash equilibrium points for these environments. The bidding strategy of the agents is analyzed in terms of individual rationality, truthfulness (strategy-proof), and computational efficiency. The results show that using a multi-agent reinforcement learning strategy improves the outcomes of the auction simulations. ",Medet Kanmaz and Elif Surer,,,11,
Learning Strategies in Decentralized Matching Markets under Uncertain   Preferences,"  We study two-sided decentralized matching markets in which participants have uncertain preferences. We present a statistical model to learn the preferences. The model incorporates uncertain state and the participants' competition on one side of the market. We derive an optimal strategy that maximizes the agent's expected payoff and calibrate the uncertain state by taking the opportunity costs into account. We discuss the sense in which the matching derived from the proposed strategy has a stability property. We also prove a fairness property that asserts that there exists no justified envy according to the proposed strategy. We provide numerical results to demonstrate the improved payoff, stability and fairness, compared to alternative methods. ",Xiaowu Dai and Michael I. Jordan,,,11,
Approximating Maximin Shares with Mixed Manna,"  We initiate the study of fair allocations of a mixed manna under the popular fairness notion of maximin share (MMS). A mixed manna allows an item to be a good for some agents and chore for others, hence strictly generalizes the well-studied goods (chores) only manna. For the good manna, Procaccia and Wang [PW14] showed non-existence of MMS allocation. This prompted works on finding an $\alpha$-MMS allocation. A series of works obtained efficient algorithms, improving $\alpha$ to $\frac{3}{4}$ for $n\ge 5$ agents. Computing an $\alpha$-MMS allocation for the maximum $\alpha$ for which it exists is known to be NP-hard. But the question of finding $\alpha$-MMS for the near best $\alpha$ remains unresolved.   We make significant progress towards this question for mixed manna by showing a striking dichotomy: We derive two conditions and show that the problem is tractable under these, while dropping either renders the problem intractable. The conditions are: $(i)$ number of agents is constant, and $(ii)$ for every agent, her total value for goods differs significantly from that for chores. For instances satisfying $(i)$ and $(ii)$ we design a PTAS - an efficient algorithm to find $(\alpha-\epsilon)$-MMS allocation given $\epsilon>0$ for the best possible $\alpha$. We also show that if either condition is not satisfied then finding $\alpha$-MMS for any $\alpha\in(0,1]$ is NP-hard, even when solution exists for $\alpha=1$.   As a corollary, our algorithm resolves the open question of designing a PTAS for the goods only setting with constantly many agents (best known $\alpha=\frac{3}{4}$), and similarly also for chores only setting. In terms of techniques, we use market equilibrium as a tool to solve an MMS problem, which may be of independent interest. ","Rucha Kulkarni, Ruta Mehta, Setareh Taki",,,11,
Optimal versus Nash Equilibrium Computation for Networked Resource   Allocation,"  Motivated by emerging resource allocation and data placement problems such as web caches and peer-to-peer systems, we consider and study a class of resource allocation problems over a network of agents (nodes). In this model, nodes can store only a limited number of resources while accessing the remaining ones through their closest neighbors. We consider this problem under both optimization and game-theoretic frameworks. In the case of optimal resource allocation we will first show that when there are only k=2 resources, the optimal allocation can be found efficiently in O(n^2\log n) steps, where n denotes the total number of nodes. However, for k>2 this problem becomes NP-hard with no polynomial time approximation algorithm with a performance guarantee better than 1+1/102k^2, even under metric access costs. We then provide a 3-approximation algorithm for the optimal resource allocation which runs only in linear time O(n). Subsequently, we look at this problem under a selfish setting formulated as a noncooperative game and provide a 3-approximation algorithm for obtaining its pure Nash equilibria under metric access costs. We then establish an equivalence between the set of pure Nash equilibria and flip-optimal solutions of the Max-k-Cut problem over a specific weighted complete graph. Using this reduction, we show that finding the lexicographically smallest Nash equilibrium for k> 2 is NP-hard, and provide an algorithm to find it in O(n^3 2^n) steps. While the reduction to weighted Max-k-Cut suggests that finding a pure Nash equilibrium using best response dynamics might be PLS-hard, it allows us to use tools from quadratic programming to devise more systematic algorithms towards obtaining Nash equilibrium points. ",S. Rasoul Etesami,,,11,
Multiagent trajectory models via game theory and implicit layer-based   learning,"  For prediction of interacting agents' trajectories, we propose an end-to-end trainable architecture that hybridizes neural nets with game-theoretic reasoning, has interpretable intermediate representations, and transfers to robust downstream decision making. It combines (1) a differentiable implicit layer that maps preferences to local Nash equilibria with (2) a learned equilibrium refinement concept and (3) a learned preference revelation net, given initial trajectories as input. This is accompanied by a new class of continuous potential games. We provide theoretical results for explicit gradients and soundness, and several measures to ensure tractability. In experiments, we evaluate our approach on two real-world data sets, where we predict highway driver merging trajectories, and on a simple decision-making transfer task. ","Philipp Geiger, Christoph-Nikolas Straehle",,,11,
Sparsified Linear Programming for Zero-Sum Equilibrium Finding,"  Computational equilibrium finding in large zero-sum extensive-form imperfect-information games has led to significant recent AI breakthroughs. The fastest algorithms for the problem are new forms of counterfactual regret minimization [Brown and Sandholm, 2019]. In this paper we present a totally different approach to the problem, which is competitive and often orders of magnitude better than the prior state of the art. The equilibrium-finding problem can be formulated as a linear program (LP) [Koller et al., 1994], but solving it as an LP has not been scalable due to the memory requirements of LP solvers, which can often be quadratically worse than CFR-based algorithms. We give an efficient practical algorithm that factors a large payoff matrix into a product of two matrices that are typically dramatically sparser. This allows us to express the equilibrium-finding problem as a linear program with size only a logarithmic factor worse than CFR, and thus allows linear program solvers to run on such games. With experiments on poker endgames, we demonstrate in practice, for the first time, that modern linear program solvers are competitive against even game-specific modern variants of CFR in solving large extensive-form games, and can be used to compute exact solutions unlike iterative algorithms like CFR. ","Brian Hu Zhang, Tuomas Sandholm",,,11,
Contracts under Moral Hazard and Adverse Selection,"  In the classical principal-agent problem, a principal must design a contract to incentivize an agent to perform an action on behalf of the principal. We study the classical principal-agent problem in a setting where the agent can be of one of several types (affecting the outcome of actions they might take). This combines the contract theory phenomena of ""moral hazard"" (incomplete information about actions) with that of ""adverse selection"" (incomplete information about types).   We examine this problem through the computational lens. We show that in this setting it is APX-hard to compute either the profit-maximizing single contract or the profit-maximizing menu of contracts (as opposed to in the absence of types, where one can efficiently compute the optimal contract). We then show that the performance of the best linear contract scales especially well in the number of types: if agent has $n$ available actions and $T$ possible types, the best linear contract achieves an $O(n\log T)$ approximation of the best possible profit. Finally, we apply our framework to prove tight worst-case approximation bounds between a variety of benchmarks of mechanisms for the principal. ","Guru Guruganesh, Jon Schneider, Joshua Wang",,,11,
Binary Scoring Rules that Incentivize Precision,"  All proper scoring rules incentivize an expert to predict accurately (report their true estimate), but not all proper scoring rules equally incentivize precision. Rather than consider the expert's belief as exogenously given, we consider a model where a rational expert can endogenously refine their belief by repeatedly paying a fixed cost, and is incentivized to do so by a proper scoring rule.   Specifically, our expert aims to predict the probability that a biased coin flipped tomorrow will land heads or tails, and can flip the coin any number of times today at a cost of $c$ per flip. Our first main result defines an incentivization index for proper scoring rules, and proves that this index measures the expected error of the expert's estimate (where the number of flips today is chosen adaptively to maximize the predictor's expected payoff). Our second main result finds the unique scoring rule which optimizes the incentivization index over all proper scoring rules.   We also consider extensions to minimizing the $\ell^{th}$ moment of error, and again provide an incentivization index and optimal proper scoring rule. In some cases, the resulting scoring rule is differentiable, but not infinitely differentiable. In these cases, we further prove that the optimum can be uniformly approximated by polynomial scoring rules.   Finally, we compare common scoring rules via our measure, and include simulations confirming the relevance of our measure even in domains outside where it provably applies. ","Eric Neyman, Georgy Noarov, S. Matthew Weinberg",,,11,
Optimization of Scoring Rules,"  This paper introduces an objective for optimizing proper scoring rules. The objective is to maximize the increase in payoff of a forecaster who exerts a binary level of effort to refine a posterior belief from a prior belief. In this framework we characterize optimal scoring rules in simple settings, give efficient algorithms for computing optimal scoring rules in complex settings, and identify simple scoring rules that are approximately optimal. In comparison, standard scoring rules in theory and practice -- for example the quadratic rule, scoring rules for the expectation, and scoring rules for multiple tasks that are averages of single-task scoring rules -- can be very far from optimal. ","Jason D. Hartline, Yingkai Li, Liren Shan, Yifan Wu",,,11,
A Predictive Strategy for the Iterated Prisoner's Dilemma,"  The iterated prisoner's dilemma is a game that produces many counter-intuitive and complex behaviors in a social environment, based on very simple basic rules. It illustrates that cooperation can be a good thing even in a competitive world, that individual fitness needs not to be the most important criteria of success, and that some strategies are very strong in a direct confrontation but could still perform poorly on average or are evolutionarily unstable. In this contribution, we present a strategy -- PREDICTOR -- which appears to be ""sentient"" and chooses to cooperate when playing against some strategies, but defects when playing against others, without the need to record ""tags"" for its opponents or an involved decision-making mechanism. To be able to operate in the highly-contextual environment, as modeled by the iterated prisoner's dilemma, PREDICTOR learns from its experience to choose optimal actions by modeling its opponent and predicting a (fictive) future.   It is shown that PREDICTOR is an efficient strategy for playing the iterated prisoner's dilemma and is simple to implement. In a simulated and representative tournament, it achieves high average scores and wins the tournament for various parameter settings. PREDICTOR thereby relies on a brief phase of exploration to improve its model, and it can evolve morality from intrinsically selfish behavior. ",Robert Prentner,,,11,
When showing your hand pays off: Announcing strategic intentions in   Colonel Blotto games,"  In competitive adversarial environments, it is often advantageous to obfuscate one's strategies or capabilities. However, revealing one's strategic intentions may shift the dynamics of the competition in complex ways. Can it ever be advantageous to reveal strategic intentions to an opponent? In this paper, we consider three-stage Colonel Blotto games in which one player can choose whether or not to pre-commit resources to a single battlefield before play begins. This pre-commitment is public knowledge. In response, the opponent can either secure the battlefield by matching the pre-commitment with its own forces, or withdraw. In a two-player setting, we show that a weaker player never has an incentive to pre-commit any amount of resources to a battlefield regardless of how valuable it is. We then consider a three-player setting in which two players fight against a common adversary on separate fronts. Only one of the two players facing the adversary has the option of pre-committing. We find there are instances where this player benefits from pre-committing. The analysis indicates that under non-cooperative team settings and no possibility of forming alliances, there can be incentives to publicly announce one's strategic intentions to an adversary. ",Rahul Chandan and Keith Paarporn and Jason R. Marden,,,11,
Dividing Bads is Harder than Dividing Goods: On the Complexity of Fair   and Efficient Division of Chores,"  We study the chore division problem where a set of agents needs to divide a set of chores (bads) among themselves fairly and efficiently. We assume that agents have linear disutility (cost) functions. Like for the case of goods, competitive division is known to be arguably the best mechanism for the bads as well. However, unlike goods, there are multiple competitive divisions with very different disutility value profiles in bads. Although all competitive divisions satisfy the standard notions of fairness and efficiency, some divisions are significantly fairer and efficient than the others. This raises two important natural questions: Does there exist a competitive division in which no agent is assigned a chore that she hugely dislikes? Are there simple sufficient conditions for the existence and polynomial-time algorithms assuming them?   We investigate both these questions in this paper. We show that the first problem is strongly NP-hard. Further, we derive a simple sufficient condition for the existence, and we show that finding a competitive division is PPAD-hard assuming the condition. These results are in sharp contrast to the case of goods where both problems are strongly polynomial-time solvable. To the best of our knowledge, these are the first hardness results for the chore division problem, and, in fact, for any economic model under linear preferences. ","Bhaskar Ray Chaudhury, Jugal Garg, Peter McGlaughlin, Ruta Mehta",,,11,
New Opportunities for the Formal Proof of Computational Real Geometry?,"  The purpose of this paper is to explore the question ""to what extent could we produce formal, machine-verifiable, proofs in real algebraic geometry?"" The question has been asked before but as yet the leading algorithms for answering such questions have not been formalised. We present a thesis that a new algorithm for ascertaining satisfiability of formulae over the reals via Cylindrical Algebraic Coverings [\'{A}brah\'{a}m, Davenport, England, Kremer, \emph{Deciding the Consistency of Non-Linear Real Arithmetic Constraints with a Conflict Driver Search Using Cylindrical Algebraic Coverings}, 2020] might provide trace and outputs that allow the results to be more susceptible to machine verification than those of competing algorithms. ","Erika {\'A}brah\'am, James Davenport, Matthew England, Gereon Kremer,   and Zak Tonks",,,11,
Formal Power Series on Algebraic Cryptanalysis,"  In cryptography, attacks that utilize a Gr\""{o}bner basis have broken several cryptosystems. The complexity of computing a Gr\""{o}bner basis dominates the overall computing and its estimation is important for such cryptanalysis. The complexity is given by using the solving degree, but it is hard to decide this value of a large scale system arisen from cryptography. Thus the degree of regularity and the first fall degree are used as proxies for the solving degree based on a wealth of experiments. If a given system is semi-regular, the complexity is estimated by using the degree of regularity derived from a certain power series, otherwise, by using the first fall degree derived from a construction of a syzygy. The degree of regularity is also defined on a non-semi-regular system and is experimentally larger than the first fall degree, but those relation is not clear theoretically. Moreover, in contrast to the degree of regularity, the first fall degree has been investigated specifically for each cryptosystem and its discussion on generic systems is not given. In this paper, we show an upper bound for the first fall degree of a polynomial system over a sufficiently large field. In detail, we prove that this upper bound for a non-semi-regular system is the degree of regularity. Moreover, we prove that the upper bound for a multi-graded polynomial system is a certain value only decided by its multi-degree. Furthermore, we show that the condition for the order of a field in our results is satisfied in attacks against actual multivariate cryptosystems. Consequently, under a reasonable condition for the order of a field, we clear a relation between the first fall degree and the degree of regularity and provide a theoretical method using a multivariate power series for cryptanalysis. ",Shuhei Nakamura,,,11,
Sparse Interpolation With Errors in Chebyshev Basis Beyond   Redundant-Block Decoding,"  We present sparse interpolation algorithms for recovering a polynomial with $\le B$ terms from $N$ evaluations at distinct values for the variable when $\le E$ of the evaluations can be erroneous. Our algorithms perform exact arithmetic in the field of scalars $\mathsf{K}$ and the terms can be standard powers of the variable or Chebyshev polynomials, in which case the characteristic of $\mathsf{K}$ is $\ne 2$. Our algorithms return a list of valid sparse interpolants for the $N$ support points and run in polynomial-time. For standard power basis our algorithms sample at $N = \lfloor \frac{4}{3} E + 2 \rfloor B$ points, which are fewer points than $N = 2(E+1)B - 1$ given by Kaltofen and Pernet in 2014. For Chebyshev basis our algorithms sample at $N = \lfloor \frac{3}{2} E + 2 \rfloor B$ points, which are also fewer than the number of points required by the algorithm given by Arnold and Kaltofen in 2015, which has $N = 74 \lfloor \frac{E}{13} + 1 \rfloor$ for $B = 3$ and $E \ge 222$. Our method shows how to correct $2$ errors in a block of $4B$ points for standard basis and how to correct $1$ error in a block of $3B$ points for Chebyshev Basis. ",Erich L. Kaltofen and Zhi-Hong Yang,,,11,
Sparse Polynomial Interpolation Based on Derivative,"  In this paper, we propose two new interpolation algorithms for sparse multivariate polynomials represented by a straight-line program(SLP). Both of our algorithms work over any finite fields $F_q$ with large characteristic. The first one is a Monte Carlo randomized algorithm. Its arithmetic complexity is linear in the number $T$ of non-zero terms of $f$, in the number $n$ of variables. If $q$ is $O((nTD)^{(1)})$, where $D$ is the partial degree bound, then our algorithm has better complexity than other existing algorithms. The second one is a deterministic algorithm. It has better complexity than existing deterministic algorithms over a field with large characteristic. Its arithmetic complexity is quadratic in $n,T,\log D$, i.e., quadratic in the size of the sparse representation. And we also show that the complexity of our deterministic algorithm is the same as the one of deterministic zero-testing of Bl\""{a}ser et al. for the polynomial given by an SLP over finite field (for large characteristic). ",Qiao-Long Huang,,,11,
On the Existence of Telescopers for Rational Functions in Three   Variables,"  Zeilberger's method of creative telescoping is crucial for the computer-generated proofs of combinatorial and special-function identities. Telescopers are linear differential or ($q$-)recurrence operators computed by algorithms for creative telescoping. For a given class of inputs, when telescopers exist and how to construct telescopers efficiently if they exist are two fundamental problems related to creative telescoping. In this paper, we solve the existence problem of telescopers for rational functions in three variables including 18 cases. We reduce the existence problem from the trivariate case to the bivariate case and some related problems. The existence criteria given in this paper enable us to determine the termination of algorithms for creative telescoping with trivariate rational inputs. ",Shaoshi Chen and Lixin Du and Rong-Hua Wang and Chaochao Zhu,,,11,
On Algorithmic Estimation of Analytic Complexity for Polynomial   Solutions to Hypergeometric Systems,  The paper deals with the analytic complexity of solutions to bivariate holonomic hypergeometric systems of the Horn type. We obtain estimates on the analytic complexity of Puiseux polynomial solutions to the hypergeometric systems defined by zonotopes. We also propose algorithms of the analytic complexity estimation for polynomials. ,Vitaly A. Krasikov,,,11,
A machine learning based software pipeline to pick the variable ordering   for algorithms with polynomial inputs,"  We are interested in the application of Machine Learning (ML) technology to improve mathematical software. It may seem that the probabilistic nature of ML tools would invalidate the exact results prized by such software, however, the algorithms which underpin the software often come with a range of choices which are good candidates for ML application. We refer to choices which have no effect on the mathematical correctness of the software, but do impact its performance.   In the past we experimented with one such choice: the variable ordering to use when building a Cylindrical Algebraic Decomposition (CAD). We used the Python library Scikit-Learn (sklearn) to experiment with different ML models, and developed new techniques for feature generation and hyper-parameter selection.   These techniques could easily be adapted for making decisions other than our immediate application of CAD variable ordering. Hence in this paper we present a software pipeline to use sklearn to pick the variable ordering for an algorithm that acts on a polynomial system. The code described is freely available online. ",Dorian Florescu and Matthew England,,,11,
Nearly Optimal Sparse Polynomial Multiplication,"  In the sparse polynomial multiplication problem, one is asked to multiply two sparse polynomials f and g in time that is proportional to the size of the input plus the size of the output. The polynomials are given via lists of their coefficients F and G, respectively. Cole and Hariharan (STOC 02) have given a nearly optimal algorithm when the coefficients are positive, and Arnold and Roche (ISSAC 15) devised an algorithm running in time proportional to the ""structural sparsity"" of the product, i.e. the set supp(F)+supp(G). The latter algorithm is particularly efficient when there not ""too many cancellations"" of coefficients in the product. In this work we give a clean, nearly optimal algorithm for the sparse polynomial multiplication problem. ",Vasileios Nakos,,,11,
Computing solutions of linear Mahler equations,"  Mahler equations relate evaluations of the same function $f$ at iterated $b$th powers of the variable. They arise in particular in the study of automatic sequences and in the complexity analysis of divide-and-conquer algorithms. Recently, the problem of solving Mahler equations in closed form has occurred in connection with number-theoretic questions. A difficulty in the manipulation of Mahler equations is the exponential blow-up of degrees when applying a Mahler operator to a polynomial. In this work, we present algorithms for solving linear Mahler equations for series, polynomials, and rational functions, and get polynomial-time complexity under a mild assumption. Incidentally, we develop an algorithm for computing the gcrd of a family of linear Mahler operators. ","Fr\'ed\'eric Chyzak, Thomas Dreyfus, Philippe Dumas and Marc   Mezzarobba",,,11,
Exact algorithms for semidefinite programs with degenerate feasible set,"  Given symmetric matrices $A_0, A_1, \ldots, A_n$ of size $m$ with rational entries, the set of real vectors $x = (x_1, \ldots, x_n)$ such that the matrix $A_0 + x_1 A_1 + \cdots + x_n A_n$ has non-negative eigenvalues is called a spectrahedron. Minimization of linear functions over spectrahedra is called semidefinite programming. Such problems appear frequently in control theory and real algebra, especially in the context of nonnegativity certificates for multivariate polynomials based on sums of squares. Numerical software for semidefinite programming are mostly based on interior point methods, assuming non-degeneracy properties such as the existence of an interior point in the spectrahedron. In this paper, we design an exact algorithm based on symbolic homotopy for solving semidefinite programs without assumptions on the feasible set, and we analyze its complexity. Because of the exactness of the output, it cannot compete with numerical routines in practice. However, we prove that solving such problems can be done in polynomial time if either $n$ or $m$ is fixed. ","Didier Henrion, Simone Naldi and Mohab Safey El Din",,,11,
Separating Variables in Bivariate Polynomial Ideals,"  We present an algorithm which for any given ideal $I\subseteq\mathbb{K} [x,y]$ finds all elements of $I$ that have the form $f(x) - g(y)$, i.e., all elements in which no monomial is a multiple of $xy$. ","Manfred Buchacher, Manuel Kauers, Gleb Pogudin",,,11,
An Additive Decomposition in S-Primitive Towers,"  We consider the additive decomposition problem in primitive towers and present an algorithm to decompose a function in an S-primitive tower as a sum of a derivative in the tower and a remainder which is minimal in some sense. Special instances of S-primitive towers include differential fields generated by finitely many logarithmic functions and logarithmic integrals. A function in an S-primitive tower is integrable in the tower if and only if the remainder is equal to zero. The additive decomposition is achieved by viewing our towers not as a traditional chain of extension fields, but rather as a direct sum of certain subrings. Furthermore, we can determine whether or not a function in an S-primitive tower has an elementary integral without solving any differential equations. We also show that a kind of S-primitive towers, known as logarithmic towers, can be embedded into a particular extension where we can obtain a finer remainder. ","Hao Du, Jing Guo, Ziming Li and Elaine Wong",,,11,
Deciding the Consistency of Non-Linear Real Arithmetic Constraints with   a Conflict Driven Search Using Cylindrical Algebraic Coverings,"  We present a new algorithm for determining the satisfiability of conjunctions of non-linear polynomial constraints over the reals, which can be used as a theory solver for satisfiability modulo theory (SMT) solving for non-linear real arithmetic. The algorithm is a variant of Cylindrical Algebraic Decomposition (CAD) adapted for satisfiability, where solution candidates (sample points) are constructed incrementally, either until a satisfying sample is found or sufficient samples have been sampled to conclude unsatisfiability. The choice of samples is guided by the input constraints and previous conflicts.   The key idea behind our new approach is to start with a partial sample; demonstrate that it cannot be extended to a full sample; and from the reasons for that rule out a larger space around the partial sample, which build up incrementally into a cylindrical algebraic covering of the space. There are similarities with the incremental variant of CAD, the NLSAT method of Jovanovi\'{c} and de~Moura, and the NuCAD algorithm of Brown; but we present worked examples and experimental results on a preliminary implementation to demonstrate the differences to these, and the benefits of the new approach. ","Erika {\'A}brah\'am, James H. Davenport, Matthew England, and Gereon   Kremer",,,11,
Algorithmic Averaging for Studying Periodic Orbits of Planar   Differential Systems,"  One of the main open problems in the qualitative theory of real planar differential systems is the study of limit cycles. In this article, we present an algorithmic approach for detecting how many limit cycles can bifurcate from the periodic orbits of a given polynomial differential center when it is perturbed inside a class of polynomial differential systems via the averaging method. We propose four symbolic algorithms to implement the averaging method. The first algorithm is based on the change of polar coordinates that allows one to transform a considered differential system to the normal form of averaging. The second algorithm is used to derive the solutions of certain differential systems associated to the unperturbed term of the normal of averaging. The third algorithm exploits the partial Bell polynomials and allows one to compute the integral formula of the averaged functions at any order. The last algorithm is based on the aforementioned algorithms and determines the exact expressions of the averaged functions for the considered differential systems. The implementation of our algorithms is discussed and evaluated using several examples. The experimental results have extended the existing relevant results for certain classes of differential systems. ",Bo Huang,,,11,
On Euler's inequality and automated reasoning with dynamic geometry,"  Euler's inequality $R\geq 2r$ can be investigated in a novel way by using implicit loci in GeoGebra. Some unavoidable side effects of the implicit locus computation introduce unexpected algebraic curves. By using a mixture of symbolic and numerical methods a possible approach is sketched up to investigate the situation. By exploiting fast GPU computations, a web application written in CindyJS helps in understanding the situation even better. ","Zolt\'an Kov\'acs, R\'obert Vajda, Aaron Montag",,,11,
"Guessing Gr{\""o}bner Bases of Structured Ideals of Relations of   Sequences","  Assuming sufficiently many terms of a n-dimensional table defined over a field are given, we aim at guessing the linear recurrence relations with either constant or polynomial coefficients they satisfy. In many applications, the table terms come along with a structure: for instance, they may be zero outside of a cone, they may be built from a Gr{\""o}bner basis of an ideal invariant under the action of a finite group. Thus, we show how to take advantage of this structure to both reduce the number of table queries and the number of operations in the base field to recover the ideal of relations of the table. In applications like in combinatorics, where all these zero terms make us guess many fake relations, this allows us to drastically reduce these wrong guesses. These algorithms have been implemented and, experimentally, they let us handle examples that we could not manage otherwise. Furthermore, we show which kind of cone and lattice structures are preserved by skew polynomial multiplication. This allows us to speed up the guessing of linear recurrence relations with polynomial coefficients by computing sparse Gr{\""o}bner bases or Gr{\""o}bner bases of an ideal invariant under the action of a finite group in a ring of skew polynomials. ","J\'er\'emy Berthomieu (PolSys), Mohab Safey El Din (PolSys)",,,11,
Resultants over principal Artinian rings,"  The resultant of two univariate polynomials is an invariant of great importance in commutative algebra and vastly used in computer algebra systems. Here we present an algorithm to compute it over Artinian principal rings with a modified version of the Euclidean algorithm. Using the same strategy, we show how the reduced resultant and a pair of B\'ezout coefficient can be computed. Particular attention is devoted to the special case of $\mathbf{Z}/n\mathbf{Z}$, where we perform a detailed analysis of the asymptotic cost of the algorithm. Finally, we illustrate how the algorithms can be exploited to improve ideal arithmetic in number fields and polynomial arithmetic over $p$-adic fields. ","Claus Fieker, Tommy Hofmann, Carlo Sircana",,,11,
Cylindrical Algebraic Decomposition with Equational Constraints,"  Cylindrical Algebraic Decomposition (CAD) has long been one of the most important algorithms within Symbolic Computation, as a tool to perform quantifier elimination in first order logic over the reals. More recently it is finding prominence in the Satisfiability Checking community as a tool to identify satisfying solutions of problems in nonlinear real arithmetic.   The original algorithm produces decompositions according to the signs of polynomials, when what is usually required is a decomposition according to the truth of a formula containing those polynomials. One approach to achieve that coarser (but hopefully cheaper) decomposition is to reduce the polynomials identified in the CAD to reflect a logical structure which reduces the solution space dimension: the presence of Equational Constraints (ECs).   This paper may act as a tutorial for the use of CAD with ECs: we describe all necessary background and the current state of the art. In particular, we present recent work on how McCallum's theory of reduced projection may be leveraged to make further savings in the lifting phase: both to the polynomials we lift with and the cells lifted over. We give a new complexity analysis to demonstrate that the double exponent in the worst case complexity bound for CAD reduces in line with the number of ECs. We show that the reduction can apply to both the number of polynomials produced and their degree. ","Matthew England, Russell Bradford and James H. Davenport",,,11,
Computing the multilinear factors of lacunary polynomials without   heights,"  We present a deterministic algorithm which computes the multilinear factors of multivariate lacunary polynomials over number fields. Its complexity is polynomial in $\ell^n$ where $\ell$ is the lacunary size of the input polynomial and $n$ its number of variables, that is in particular polynomial in the logarithm of its degree. We also provide a randomized algorithm for the same problem of complexity polynomial in $\ell$ and $n$.   Over other fields of characteristic zero and finite fields of large characteristic, our algorithms compute the multilinear factors having at least three monomials of multivariate polynomials. Lower bounds are provided to explain the limitations of our algorithm. As a by-product, we also design polynomial-time deterministic polynomial identity tests for families of polynomials which were not known to admit any.   Our results are based on so-called Gap Theorem which reduce high-degree factorization to repeated low-degree factorizations. While previous algorithms used Gap Theorems expressed in terms of the heights of the coefficients, our Gap Theorems only depend on the exponents of the polynomials. This makes our algorithms more elementary and general, and faster in most cases. ","Arkadev Chattopadhyay, Bruno Grenet, Pascal Koiran, Natacha Portier   and Yann Strozecki",,,11,
"Error Correcting Codes, finding polynomials of bounded degree agreeing   on a dense fraction of a set of points",  Here we present some revised arguments to a randomized algorithm proposed by Sudan to find the polynomials of bounded degree agreeing on a dense fraction of a set of points in $\mathbb{F}^{2}$ for some field $\mathbb{F}$. ,Priyank Deshpande,,,11,
SNS: A Solution-based Nonlinear Subspace method for time-dependent model   order reduction,"  Several reduced order models have been developed for nonlinear dynamical systems. To achieve a considerable speed-up, a hyper-reduction step is needed to reduce the computational complexity due to nonlinear terms. Many hyper-reduction techniques require the construction of nonlinear term basis, which introduces a computationally expensive offline phase. A novel way of constructing nonlinear term basis within the hyper-reduction process is introduced. In contrast to the traditional hyper-reduction techniques where the collection of nonlinear term snapshots is required, the SNS method avoids collecting the nonlinear term snapshots. Instead, it uses the solution snapshots that are used for building a solution basis, which enables avoiding an extra data compression of nonlinear term snapshots. As a result, the SNS method provides a more efficient offline strategy than the traditional model order reduction techniques, such as the DEIM, GNAT, and ST-GNAT methods. The SNS method is theoretically justified by the conforming subspace condition and the subspace inclusion relation. It is useful for model order reduction of large-scale nonlinear dynamical problems to reduce the offline cost. It is especially useful for ST-GNAT that has shown promising results, such as a good accuracy with a considerable online speed-up for hyperbolic problems in a recent paper by Choi and Carlberg in SISC 2019, because ST-GNAT involves an expensive offline cost related to collecting nonlinear term snapshots. Numerical results support that the accuracy of the solution from the SNS method is comparable to the traditional methods and a considerable speed-up (i.e., a factor of two to a hundred) is achieved in the offline phase. ",Youngsoo Choi and Deshawn Coombs and Robert Anderson,,,11,
Three-Level Parallel J-Jacobi Algorithms for Hermitian Matrices,"  The paper describes several efficient parallel implementations of the one-sided hyperbolic Jacobi-type algorithm for computing eigenvalues and eigenvectors of Hermitian matrices. By appropriate blocking of the algorithms an almost ideal load balancing between all available processors/cores is obtained. A similar blocking technique can be used to exploit local cache memory of each processor to further speed up the process. Due to diversity of modern computer architectures, each of the algorithms described here may be the method of choice for a particular hardware and a given matrix size. All proposed block algorithms compute the eigenvalues with relative accuracy similar to the original non-blocked Jacobi algorithm. ","Sanja Singer, Sasa Singer, Vedran Novakovic, Davor Davidovic, Kresimir   Bokulic and Aleksandar Uscumlic",,,11,
Finite Volume Simulation Framework for Die Casting with Uncertainty   Quantification,"  The present paper describes the development of a novel and comprehensive computational framework to simulate solidification problems in materials processing, specifically casting processes. Heat transfer, solidification and fluid flow due to natural convection are modeled. Empirical relations are used to estimate the microstructure parameters and mechanical properties. The fractional step algorithm is modified to deal with the numerical aspects of solidification by suitably altering the coefficients in the discretized equation to simulate selectively only in the liquid and mushy zones. This brings significant computational speed up as the simulation proceeds. Complex domains are represented by unstructured hexahedral elements. The algebraic multigrid method, blended with a Krylov subspace solver is used to accelerate convergence. State of the art uncertainty quantification technique is included in the framework to incorporate the effects of stochastic variations in the input parameters. Rigorous validation is presented using published experimental results of a solidification problem. ","Shantanu Shahane, Narayana Aluru, Placid Ferreira, Shiv G Kapoor,   Surya Pratap Vanka",,,11,
Data-driven quasi-interpolant spline surfaces for point cloud   approximation,"  In this paper we investigate a local surface approximation, the Weighted Quasi Interpolant Spline Approximation (wQISA), specifically designed for large and noisy point clouds. We briefly describe the properties of the wQISA representation and introduce a novel data-driven implementation, which combines prediction capability and complexity efficiency. We provide an extended comparative analysis with other continuous approximations on real data, including different types of surfaces and levels of noise, such as 3D models, terrain data and digital environmental data. ",Andrea Raffo and Silvia Biasotti,,,11,
Block-Term Tensor Decomposition: Model Selection and Computation,"  The so-called block-term decomposition (BTD) tensor model has been recently receiving increasing attention due to its enhanced ability of representing systems and signals that are composed of \emph{blocks} of rank higher than one, a scenario encountered in numerous and diverse applications. Its uniqueness and approximation have thus been thoroughly studied. Nevertheless, the challenging problem of estimating the BTD model structure, namely the number of block terms and their individual ranks, has only recently started to attract significant attention. In this paper, a novel method of BTD model selection and computation is proposed, based on the idea of imposing column sparsity \emph{jointly} on the factors and in a \emph{hierarchical} manner and estimating the ranks as the numbers of factor columns of non-negligible magnitude. Following a block successive upper bound minimization (BSUM) approach for the proposed optimization problem is shown to result in an alternating hierarchical iteratively reweighted least squares (HIRLS) algorithm, which is fast converging and enjoys high computational efficiency, as it relies in its iterations on small-sized sub-problems with closed-form solutions. Simulation results for both synthetic examples and a hyper-spectral image de-noising application are reported, which demonstrate the superiority of the proposed scheme over the state-of-the-art in terms of success rate in rank estimation as well as computation time and rate of convergence. ","Athanasios A. Rontogiannis, Eleftherios Kofidis, Paris V. Giampouras",,,11,
Poly-Spline Finite Element Method,"  We introduce an integrated meshing and finite element method pipeline enabling black-box solution of partial differential equations in the volume enclosed by a boundary representation. We construct a hybrid hexahedral-dominant mesh, which contains a small number of star-shaped polyhedra, and build a set of high-order basis on its elements, combining triquadratic B-splines, triquadratic hexahedra (27 degrees of freedom), and harmonic elements. We demonstrate that our approach converges cubically under refinement, while requiring around 50% of the degrees of freedom than a similarly dense hexahedral mesh composed of triquadratic hexahedra. We validate our approach solving Poisson's equation on a large collection of models, which are automatically processed by our algorithm, only requiring the user to provide boundary conditions on their surface. ","Teseo Schneider, Jeremie Dumas, Xifeng Gao, Mario Botsch, Daniele   Panozzo, Denis Zorin",,,11,
A Dual Symmetric Gauss-Seidel Alternating Direction Method of   Multipliers for Hyperspectral Sparse Unmixing,"  Since sparse unmixing has emerged as a promising approach to hyperspectral unmixing, some spatial-contextual information in the hyperspectral images has been exploited to improve the performance of the unmixing recently. The total variation (TV) has been widely used to promote the spatial homogeneity as well as the smoothness between adjacent pixels. However, the computation task for hyperspectral sparse unmixing with a TV regularization term is heavy. Besides, the convergence of the primal alternating direction method of multipliers (ADMM) for the hyperspectral sparse unmixing with a TV regularization term has not been explained in details. In this paper, we design an efficient and convergent dual symmetric Gauss-Seidel ADMM (sGS-ADMM) for hyperspectral sparse unmixing with a TV regularization term. We also present the global convergence and local linear convergence rate analysis for this algorithm. As demonstrated in numerical experiments, our algorithm can obviously improve the efficiency of the unmixing compared with the state-of-the-art algorithm. More importantly, we can obtain images with higher quality. ","Longfei Ren, Chengjing Wang, Peipei Tang, and Zheng Ma",,,11,
Hierarchical Tensor Ring Completion,"  Tensor completion can estimate missing values of a high-order data from its partially observed entries. Recent works show that low rank tensor ring approximation is one of the most powerful tools to solve tensor completion problem. However, existing algorithms need predefined tensor ring rank which may be hard to determine in practice. To address the issue, we propose a hierarchical tensor ring decomposition for more compact representation. We use the standard tensor ring to decompose a tensor into several 3-order sub-tensors in the first layer, and each sub-tensor is further factorized by tensor singular value decomposition (t-SVD) in the second layer. In the low rank tensor completion based on the proposed decomposition, the zero elements in the 3-order core tensor are pruned in the second layer, which helps to automatically determinate the tensor ring rank. To further enhance the recovery performance, we use total variation to exploit the locally piece-wise smoothness data structure. The alternating direction method of multiplier can divide the optimization model into several subproblems, and each one can be solved efficiently. Numerical experiments on color images and hyperspectral images demonstrate that the proposed algorithm outperforms state-of-the-arts ones in terms of recovery accuracy. ","Abdul Ahad, Zhen Long, Ce Zhu, Yipeng Liu",,,11,
Novel Modifications of Parallel Jacobi Algorithms,"  We describe two main classes of one-sided trigonometric and hyperbolic Jacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitian matrices. These types of algorithms exhibit significant advantages over many other eigenvalue algorithms. If the matrices permit, both types of algorithms compute the eigenvalues and eigenvectors with high relative accuracy.   We present novel parallelization techniques for both trigonometric and hyperbolic classes of algorithms, as well as some new ideas on how pivoting in each cycle of the algorithm can improve the speed of the parallel one-sided algorithms. These parallelization approaches are applicable to both distributed-memory and shared-memory machines.   The numerical testing performed indicates that the hyperbolic algorithms may be superior to the trigonometric ones, although, in theory, the latter seem more natural. ","Sanja Singer, Sasa Singer, Vedran Novakovic, Aleksandar Uscumlic and   Vedran Dunjko",,,11,
On the Numerical Solution of Fourth-Order Linear Two-Point Boundary   Value Problems,"  This paper introduces a fast and numerically stable algorithm for the solution of fourth-order linear boundary value problems on an interval. This type of equation arises in a variety of settings in physics and signal processing. Our method reformulates the equation as a collection of second-kind integral equations defined on local subdomains. Each such equation can be stably discretized and solved. The boundary values of these local solutions are matched by solving a banded linear system. The method of deferred corrections is then used to increase the accuracy of the scheme. Deferred corrections requires applying the integral operator to a function on the entire domain, for which we provide an algorithm with linear cost. We illustrate the performance of our method on several numerical examples. ",William Leeb and Vladimir Rokhlin,,,11,
Memoryless scalar quantization for random frames,"  Memoryless scalar quantization (MSQ) is a common technique to quantize frame coefficients of signals (which are used as a model for generalized linear samples), making them compatible with our digital technology. The process of quantization is generally not invertible, and thus one can only recover an approximation to the original signal from its quantized coefficients. The non-linear nature of quantization makes the analysis of the corresponding approximation error challenging, often resulting in the use of a simplifying assumption, called the ""white noise hypothesis"" (WNH) that simplifies this analysis. However, the WNH is known to be not rigorous and, at least in certain cases, not valid.   Given a fixed, deterministic signal, we assume that we use a random frame, whose analysis matrix has independent isotropic sub-Gaussian rows, to collect the measurements, which are consecutively quantized via MSQ. For this setting, the numerically observed decay rate seems to agree with the prediction by the WNH. We rigorously establish sharp non-asymptotic error bounds without using the WNH that explains the observed decay rate. Furthermore, we show that the reconstruction error does not necessarily diminish as redundancy increases. We also extend this approach to the compressed sensing setting, obtaining rigorous error bounds that agree with empirical observations, again, without resorting to the WNH. ",Kateryna Melnykova and Ozgur Yilmaz,,,11,
"Interpolation of scattered data in $\mathbb{R}^3$ using minimum   $L_p$-norm networks, $1<p<\infty$",  We consider the extremal problem of interpolation of scattered data in $\mathbb{R}^3$ by smooth curve networks with minimal $L_p$-norm of the second derivative for $1<p<\infty$. The problem for $p=2$ was set and solved by Nielson (1983). Andersson et al. (1995) gave a new proof of Nielson's result by using a different approach. Partial results for the problem for $1<p<\infty$ were announced without proof in (Vlachkova (1992)). Here we present a complete characterization of the solution for $1<p<\infty$. Numerical experiments are visualized and presented to illustrate and support our results. ,Krassimira Vlachkova,,,11,
Memory footprint reduction for the FFT-based volume integral equation   method via tensor decompositions,"  We present a method of memory footprint reduction for FFT-based, electromagnetic (EM) volume integral equation (VIE) formulations. The arising Green's function tensors have low multilinear rank, which allows Tucker decomposition to be employed for their compression, thereby greatly reducing the required memory storage for numerical simulations. Consequently, the compressed components are able to fit inside a graphical processing unit (GPU) on which highly parallelized computations can vastly accelerate the iterative solution of the arising linear system. In addition, the element-wise products throughout the iterative solver's process require additional flops, thus, we provide a variety of novel and efficient methods that maintain the linear complexity of the classic element-wise product with an additional multiplicative small constant. We demonstrate the utility of our approach via its application to VIE simulations for the Magnetic Resonance Imaging (MRI) of a human head. For these simulations we report an order of magnitude acceleration over standard techniques. ","Ilias I. Giannakopoulos (1), Mikhail S. Litsarev (1), Athanasios G.   Polimeridis (2) ((1) Skoltech Center for Computational Data-Intensive Science   and Engineering, Skolkovo Institute of Science and Technology, Moscow,   Russia, (2) Q Bio, CA, USA)",,,11,
Approximate Newton Methods,"  Many machine learning models involve solving optimization problems. Thus, it is important to deal with a large-scale optimization problem in big data applications. Recently, subsampled Newton methods have emerged to attract much attention due to their efficiency at each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each iteration while commanding a high convergence rate. Other efficient stochastic second order methods are also proposed. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and the performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze both local and global convergence properties of second order methods. Based on this framework, we present our theoretical results which match the performance in real applications well. ","Haishan Ye, Luo Luo, Zhihua Zhang",,,11,
"Foundation of Computer (Algebra) ANALYSIS Systems: Semantics, Logic,   Programming, Verification","  We propose a semantics of operating on real numbers that is sound, Turing-complete, and practical. It modifies the intuitive but super-recursive Blum-Shub-Smale model (formalizing Computer ALGEBRA Systems), to coincide in power with the realistic but inconvenient Type-2 Turing machine underlying Computable Analysis: reconciling both as foundation to a Computer ANALYSIS System.   Several examples illustrate the elegance of rigorous numerical coding in this framework, formalized as a simple imperative programming language ERC with denotational semantics for REALIZING a real function $f$: arguments $x$ are given as exact real numbers, while values $y=f(x)$ suffice to be returned approximately up to absolute error $2^p$ with respect to an additionally given integer parameter $p\to-\infty$. Real comparison (necessarily) becomes partial, possibly 'returning' the lazy Kleenean value UNDEF (subtly different from $\bot$ for classically undefined expressions like 1/0). This asserts closure under composition, and in fact 'Turing-completeness over the reals': All and only functions computable in the sense of Computable Analysis can be realized in ERC. Programs thus operate on a many-sorted structure involving real numbers and integers, connected via the 'error' embedding $Z\ni p\mapsto 2^p\in R$, whose first-order theory is proven decidable and model-complete. This logic serves for formally specifying and formally verifying correctness of ERC programs. We finally expand ERC and its Turing-completeness from real functions to functionALs. ","Sewon Park, Franz Brau{\ss}e, Pieter Collins, SunYoung Kim, Michal   Kone\v{c}n\'y, Gyesik Lee, Norbert M\""uller, Eike Neumann, Norbert Preining,   Martin Ziegler",,,11,
Efficient computation of higher order cumulant tensors,"  In this paper, we introduce a novel algorithm for calculating arbitrary order cumulants of multidimensional data. Since the $d^\text{th}$ order cumulant can be presented in the form of an $d$-dimensional tensor, the algorithm is presented using tensor operations. The algorithm provided in the paper takes advantage of super-symmetry of cumulant and moment tensors. We show that the proposed algorithm considerably reduces the computational complexity and the computational memory requirement of cumulant calculation as compared with existing algorithms. For the sizes of interest, the reduction is of the order of $d!$ compared to the naive algorithm. ","Krzysztof Domino, Piotr Gawron, {\L}ukasz Pawela",,,11,
Data driven approximation of parametrized PDEs by Reduced Basis and   Neural Networks,"  We are interested in the approximation of partial differential equations with a data-driven approach based on the reduced basis method and machine learning. We suppose that the phenomenon of interest can be modeled by a parametrized partial differential equation, but that the value of the physical parameters is unknown or difficult to be directly measured. Our method allows to estimate fields of interest, for instance temperature of a sample of material or velocity of a fluid, given data at a handful of points in the domain. We propose to accomplish this task with a neural network embedding a reduced basis solver as exotic activation function in the last layer. The reduced basis solver accounts for the underlying physical phenomenonon and it is constructed from snapshots obtained from randomly selected values of the physical parameters during an expensive offline phase. The same full order solutions are then employed for the training of the neural network. As a matter of fact, the chosen architecture resembles an asymmetric autoencoder in which the decoder is the reduced basis solver and as such it does not contain trainable parameters. The resulting latent space of our autoencoder includes parameter-dependent quantities feeding the reduced basis solver, which -- depending on the considered partial differential equation -- are the values of the physical parameters themselves or the affine decomposition coefficients of the differential operators. ","Niccol\`o Dal Santo, Simone Deparis and Luca Pegolotti",,,11,
Online adaptive basis refinement and compression for reduced-order   models via vector-space sieving,"  In many applications, projection-based reduced-order models (ROMs) have demonstrated the ability to provide rapid approximate solutions to high-fidelity full-order models (FOMs). However, there is no a priori assurance that these approximate solutions are accurate; their accuracy depends on the ability of the low-dimensional trial basis to represent the FOM solution. As a result, ROMs can generate inaccurate approximate solutions, e.g., when the FOM solution at the online prediction point is not well represented by training data used to construct the trial basis. To address this fundamental deficiency of standard model-reduction approaches, this work proposes a novel online-adaptive mechanism for efficiently enriching the trial basis in a manner that ensures convergence of the ROM to the FOM, yet does not incur any FOM solves. The mechanism is based on the previously proposed adaptive $h$-refinement method for ROMs [12], but improves upon this work in two crucial ways. First, the proposed method enables basis refinement with respect to any orthogonal basis (not just the Kronecker basis), thereby generalizing the refinement mechanism and enabling it to be tailored to the physics characterizing the problem at hand. Second, the proposed method provides a fast online algorithm for periodically compressing the enriched basis via an efficient proper orthogonal decomposition (POD) method, which does not incur any operations that scale with the FOM dimension. These two features allow the proposed method to serve as (1) a failsafe mechanism for ROMs, as the method enables the ROM to satisfy any prescribed error tolerance online (even in the case of inadequate training), and (2) an efficient online basis-adaptation mechanism, as the combination of basis enrichment and compression enables the basis to adapt online while controlling its dimension. ","Philip A. Etter, Kevin T. Carlberg",,,11,
Escaping the Local Minima via Simulated Annealing: Optimization of   Approximately Convex Functions,"  We consider the problem of optimizing an approximately convex function over a bounded convex set in $\mathbb{R}^n$ using only function evaluations. The problem is reduced to sampling from an \emph{approximately} log-concave distribution using the Hit-and-Run method, which is shown to have the same $\mathcal{O}^*$ complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima.   We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an $\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy function evaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concave distribution. We also consider in detail the case when the ""amount of non-convexity"" decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning. ","Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander   Rakhlin",,,11,
Predict-and-recompute conjugate gradient variants,"  The standard implementation of the conjugate gradient algorithm suffers from communication bottlenecks on parallel architectures, due primarily to the two global reductions required every iteration. In this paper, we study conjugate gradient variants which decrease the runtime per iteration by overlapping global synchronizations, and in the case of pipelined variants, matrix-vector products. Through the use of a predict-and-recompute scheme, whereby recursively-updated quantities are first used as a predictor for their true values and then recomputed exactly at a later point in the iteration, these variants are observed to have convergence behavior nearly as good as the standard conjugate gradient implementation on a variety of test problems. We provide a rounding error analysis which provides insight into this observation. It is also verified experimentally that the variants studied do indeed reduce the runtime per iteration in practice and that they scale similarly to previously-studied communication-hiding variants. Finally, because these variants achieve good convergence without the use of any additional input parameters, they have the potential to be used in place of the standard conjugate gradient implementation in a range of applications. ",Tyler Chen and Erin C. Carson,,,11,
A Causal Linear Model to Quantify Edge Unfairness for Unfair Edge   Prioritization and Discrimination Removal,"  The dataset can be generated by an unfair mechanism in numerous settings. For instance, a judicial system is unfair if it rejects the bail plea of an accused based on the race. To mitigate the unfairness in the procedure generating the dataset, we need to identify the sources of unfairness, quantify the unfairness in these sources, quantify how these sources affect the overall unfairness, and prioritize the sources before addressing the real-world issues underlying them. Prior work of (Zhang, et. al, 2017) identifies and removes discrimination after data is generated but does not suggest a methodology to mitigate unfairness in the data generation phase. We use the notion of an unfair edge, same as (Chiappa, et. al, 2018), to be the source of discrimination and quantify unfairness along an unfair edge. We also quantify overall unfairness in a particular decision towards a subset of sensitive attributes in terms of edge unfairness and measure the sensitivity of the former when the latter is varied. Using the formulation of cumulative unfairness in terms of edge unfairness, we alter the discrimination removal methodology discussed in (Zhang, et. al, 2017) by not formulating it as an optimization problem. This helps in getting rid of constraints that grow exponentially in the number of sensitive attributes and values taken by them. Finally, we discuss a priority algorithm for policymakers to address the real-world issues underlying the edges that result in unfairness. The experimental section validates the linear model assumption made to quantify edge unfairness. ","Pavan Ravishankar, Pranshu Malviya, Balaraman Ravindran",,,11,
Unlucky Explorer: A Complete non-Overlapping Map Exploration,"  Nowadays, the field of Artificial Intelligence in Computer Games (AI in Games) is going to be more alluring since computer games challenge many aspects of AI with a wide range of problems, particularly general problems. One of these kinds of problems is Exploration, which states that an unknown environment must be explored by one or several agents. In this work, we have first introduced the Maze Dash puzzle as an exploration problem where the agent must find a Hamiltonian Path visiting all the cells. Then, we have investigated to find suitable methods by a focus on Monte-Carlo Tree Search (MCTS) and SAT to solve this puzzle quickly and accurately. An optimization has been applied to the proposed MCTS algorithm to obtain a promising result. Also, since the prefabricated test cases of this puzzle are not large enough to assay the proposed method, we have proposed and employed a technique to generate solvable test cases to evaluate the approaches. Eventually, the MCTS-based method has been assessed by the auto-generated test cases and compared with our implemented SAT approach that is considered a good rival. Our comparison indicates that the MCTS-based approach is an up-and-coming method that could cope with the test cases with small and medium sizes with faster run-time compared to SAT. However, for certain discussed reasons, including the features of the problem, tree search organization, and also the approach of MCTS in the Simulation step, MCTS takes more time to execute in Large size scenarios. Consequently, we have found the bottleneck for the MCTS-based method in significant test cases that could be improved in two real-world problems. ",Mohammad Sina Kiarostami and Saleh Khalaj Monfared and Mohammadreza   Daneshvaramoli and Ali Oliayi and Negar Yousefian and Dara Rahmati and Saeid   Gorgin,,,11,
Metagame Autobalancing for Competitive Multiplayer Games,"  Automated game balancing has often focused on single-agent scenarios. In this paper we present a tool for balancing multi-player games during game design. Our approach requires a designer to construct an intuitive graphical representation of their meta-game target, representing the relative scores that high-level strategies (or decks, or character types) should experience. This permits more sophisticated balance targets to be defined beyond a simple requirement of equal win chances. We then find a parameterization of the game that meets this target using simulation-based optimization to minimize the distance to the target graph. We show the capabilities of this tool on examples inheriting from Rock-Paper-Scissors, and on a more complex asymmetric fighting game. ","Daniel Hernandez, Charles Takashi Toyin Gbadamosi, James Goodman,   James Alfred Walker",,,11,
Provenance-Based Interpretation of Multi-Agent Information Analysis,"  Analytic software tools and workflows are increasing in capability, complexity, number, and scale, and the integrity of our workflows is as important as ever. Specifically, we must be able to inspect the process of analytic workflows to assess (1) confidence of the conclusions, (2) risks and biases of the operations involved, (3) sensitivity of the conclusions to sources and agents, (4) impact and pertinence of various sources and agents, and (5) diversity of the sources that support the conclusions. We present an approach that tracks agents' provenance with PROV-O in conjunction with agents' appraisals and evidence links (expressed in our novel DIVE ontology). Together, PROV-O and DIVE enable dynamic propagation of confidence and counter-factual refutation to improve human-machine trust and analytic integrity. We demonstrate representative software developed for user interaction with that provenance, and discuss key needs for organizations adopting such approaches. We demonstrate all of these assessments in a multi-agent analysis scenario, using an interactive web-based information validation UI. ","Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen,   Kyle Tunis",,,11,
Improving Confidence in the Estimation of Values and Norms,"  Autonomous agents (AA) will increasingly be interacting with us in our daily lives. While we want the benefits attached to AAs, it is essential that their behavior is aligned with our values and norms. Hence, an AA will need to estimate the values and norms of the humans it interacts with, which is not a straightforward task when solely observing an agent's behavior. This paper analyses to what extent an AA is able to estimate the values and norms of a simulated human agent (SHA) based on its actions in the ultimatum game. We present two methods to reduce ambiguity in profiling the SHAs: one based on search space exploration and another based on counterfactual analysis. We found that both methods are able to increase the confidence in estimating human values and norms, but differ in their applicability, the latter being more efficient when the number of interactions with the agent is to be minimized. These insights are useful to improve the alignment of AAs with human values and norms. ","Luciano Cavalcante Siebert, Rijk Mercuur, Virginia Dignum, Jeroen van   den Hoven, Catholijn Jonker",,,11,
Representing Pure Nash Equilibria in Argumentation,"  In this paper we describe an argumentation-based representation of normal form games, and demonstrate how argumentation can be used to compute pure strategy Nash equilibria. Our approach builds on Modgil's Extended Argumentation Frameworks. We demonstrate its correctness, prove several theoretical properties it satisfies, and outline how it can be used to explain why certain strategies are Nash equilibria to a non-expert human user. ","Bruno Yun, Srdjan Vesic and Nir Oren",,,11,
Driving Tasks Transfer in Deep Reinforcement Learning for   Decision-making of Autonomous Vehicles,"  Knowledge transfer is a promising concept to achieve real-time decision-making for autonomous vehicles. This paper constructs a transfer deep reinforcement learning framework to transform the driving tasks in inter-section environments. The driving missions at the un-signalized intersection are cast into a left turn, right turn, and running straight for automated vehicles. The goal of the autonomous ego vehicle (AEV) is to drive through the intersection situation efficiently and safely. This objective promotes the studied vehicle to increase its speed and avoid crashing other vehicles. The decision-making pol-icy learned from one driving task is transferred and evaluated in another driving mission. Simulation results reveal that the decision-making strategies related to similar tasks are transferable. It indicates that the presented control framework could reduce the time consumption and realize online implementation. ","Hong Shu, Teng Liu, Xingyu Mu, Dongpu Cao",,,11,
QBSUM: a Large-Scale Query-Based Document Summarization Dataset from   Real-world Applications,"  Query-based document summarization aims to extract or generate a summary of a document which directly answers or is relevant to the search query. It is an important technique that can be beneficial to a variety of applications such as search engines, document-level machine reading comprehension, and chatbots. Currently, datasets designed for query-based summarization are short in numbers and existing datasets are also limited in both scale and quality. Moreover, to the best of our knowledge, there is no publicly available dataset for Chinese query-based document summarization. In this paper, we present QBSUM, a high-quality large-scale dataset consisting of 49,000+ data samples for the task of Chinese query-based document summarization. We also propose multiple unsupervised and supervised solutions to the task and demonstrate their high-speed inference and superior performance via both offline experiments and online A/B tests. The QBSUM dataset is released in order to facilitate future advancement of this research field. ","Mingjun Zhao, Shengli Yan, Bang Liu, Xinwang Zhong, Qian Hao, Haolan   Chen, Di Niu, Bowei Long and Weidong Guo",,,11,
From Robotic Process Automation to Intelligent Process Automation:   Emerging Trends,"  In this survey, we study how recent advances in machine intelligence are disrupting the world of business processes. Over the last decade, there has been steady progress towards the automation of business processes under the umbrella of ``robotic process automation'' (RPA). However, we are currently at an inflection point in this evolution, as a new paradigm called ``Intelligent Process Automation'' (IPA) emerges, bringing machine learning (ML) and artificial intelligence (AI) technologies to bear in order to improve business process outcomes. The purpose of this paper is to provide a survey of this emerging theme and identify key open research challenges at the intersection of AI and business processes. We hope that this emerging theme will spark engaging conversations at the RPA Forum. ","Tathagata Chakraborti, Vatche Isahagian, Rania Khalaf, Yasaman   Khazaeni, Vinod Muthusamy, Yara Rizk, Merve Unuvar",,,11,
Using Reinforcement Learning with Partial Vehicle Detection for   Intelligent Traffic Signal Control,"  Intelligent Transportation Systems (ITS) have attracted the attention of researchers and the general public alike as a means to alleviate traffic congestion. Recently, the maturity of wireless technology has enabled a cost-efficient way to achieve ITS by detecting vehicles using Vehicle to Infrastructure (V2I) communications. Traditional ITS algorithms, in most cases, assume that every vehicle is observed, such as by a camera or a loop detector, but a V2I implementation would detect only those vehicles with wireless communications capability. We examine a family of transportation systems, which we will refer to as `Partially Detected Intelligent Transportation Systems'. An algorithm that can act well under a small detection rate is highly desirable due to gradual penetration rates of the underlying wireless technologies such as Dedicated Short Range Communications (DSRC) technology. Artificial Intelligence (AI) techniques for Reinforcement Learning (RL) are suitable tools for finding such an algorithm due to utilizing varied inputs and not requiring explicit analytic understanding or modeling of the underlying system dynamics. In this paper, we report a RL algorithm for partially observable ITS based on DSRC. The performance of this system is studied under different car flows, detection rates, and topologies of the road network. Our system is able to efficiently reduce the average waiting time of vehicles at an intersection, even with a low detection rate. ","Rusheng Zhang, Akihiro Ishikawa, Wenli Wang, Benjamin Striner, Ozan   Tonguz",,,11,
Data Curves Clustering Using Common Patterns Detection,"  For the past decades we have experienced an enormous expansion of the accumulated data that humanity produces. Daily a numerous number of smart devices, usually interconnected over internet, produce vast, real-values datasets. Time series representing datasets from completely irrelevant domains such as finance, weather, medical applications, traffic control etc. become more and more crucial in human day life. Analyzing and clustering these time series, or in general any kind of curves, could be critical for several human activities. In the current paper, the new Curves Clustering Using Common Patterns (3CP) methodology is introduced, which applies a repeated pattern detection algorithm in order to cluster sequences according to their shape and the similarities of common patterns between time series, data curves and eventually any kind of discrete sequences. For this purpose, the Longest Expected Repeated Pattern Reduced Suffix Array (LERP-RSA) data structure has been used in combination with the All Repeated Patterns Detection (ARPaD) algorithm in order to perform highly accurate and efficient detection of similarities among data curves that can be used for clustering purposes and which also provides additional flexibility and features. ",Konstantinos F. Xylogiannopoulos,,,11,
The Tensor Brain: Semantic Decoding for Perception and Memory,"  We analyse perception and memory, using mathematical models for knowledge graphs and tensors, to gain insights into the corresponding functionalities of the human mind. Our discussion is based on the concept of propositional sentences consisting of \textit{subject-predicate-object} (SPO) triples for expressing elementary facts. SPO sentences are the basis for most natural languages but might also be important for explicit perception and declarative memories, as well as intra-brain communication and the ability to argue and reason. A set of SPO sentences can be described as a knowledge graph, which can be transformed into an adjacency tensor. We introduce tensor models, where concepts have dual representations as indices and associated embeddings, two constructs we believe are essential for the understanding of implicit and explicit perception and memory in the brain. We argue that a biological realization of perception and memory imposes constraints on information processing. In particular, we propose that explicit perception and declarative memories require a semantic decoder, which, in a simple realization, is based on four layers: First, a sensory memory layer, as a buffer for sensory input, second, an index layer representing concepts, third, a memoryless representation layer for the broadcasting of information ---the ""blackboard"", or the ""canvas"" of the brain--- and fourth, a working memory layer as a processing center and data buffer. We discuss the operations of the four layers and relate them to the global workspace theory. In a Bayesian brain interpretation, semantic memory defines the prior for observable triple statements. We propose that ---in evolution and during development--- semantic memory, episodic memory, and natural language evolved as emergent properties in agents' process to gain a deeper understanding of sensory information. ",Volker Tresp and Sahand Sharifzadeh and Dario Konopatzki and Yunpu Ma,,,11,
Design and Implementation of TAG: A Tabletop Games Framework,"  This document describes the design and implementation of the Tabletop Games framework (TAG), a Java-based benchmark for developing modern board games for AI research. TAG provides a common skeleton for implementing tabletop games based on a common API for AI agents, a set of components and classes to easily add new games and an import module for defining data in JSON format. At present, this platform includes the implementation of seven different tabletop games that can also be used as an example for further developments. Additionally, TAG also incorporates logging functionality that allows the user to perform a detailed analysis of the game, in terms of action space, branching factor, hidden information, and other measures of interest for Game AI research. The objective of this document is to serve as a central point where the framework can be described at length. TAG can be downloaded at: https://github.com/GAIGResearch/TabletopGames ","Raluca D. Gaina, Martin Balla, Alexander Dockhorn, Raul Montoliu,   Diego Perez-Liebana",,,11,
To Monitor Or Not: Observing Robot's Behavior based on a Game-Theoretic   Model of Trust,"  In scenarios where a robot generates and executes a plan, there may be instances where this generated plan is less costly for the robot to execute but incomprehensible to the human. When the human acts as a supervisor and is held accountable for the robot's plan, the human may be at a higher risk if the incomprehensible behavior is deemed to be infeasible or unsafe. In such cases, the robot, who may be unaware of the human's exact expectations, may choose to execute (1) the most constrained plan (i.e. one preferred by all possible supervisors) incurring the added cost of executing highly sub-optimal behavior when the human is monitoring it and (2) deviate to a more optimal plan when the human looks away. While robots do not have human-like ulterior motives (such as being lazy), such behavior may occur because the robot has to cater to the needs of different human supervisors. In such settings, the robot, being a rational agent, should take any chance it gets to deviate to a lower cost plan. On the other hand, continuous monitoring of the robot's behavior is often difficult for humans because it costs them valuable resources (e.g., time, cognitive overload, etc.). Thus, to optimize the cost for monitoring while ensuring the robots follow the safe behavior, we model this problem in the game-theoretic framework of trust. In settings where the human does not initially trust the robot, pure-strategy Nash Equilibrium provides a useful policy for the human. ","Sailik Sengupta, Zahra Zahedi, Subbarao Kambhampati",,,11,
Verifying Recurrent Neural Networks using Invariant Inference,"  Deep neural networks are revolutionizing the way complex systems are developed. However, these automatically-generated networks are opaque to humans, making it difficult to reason about them and guarantee their correctness. Here, we propose a novel approach for verifying properties of a widespread variant of neural networks, called recurrent neural networks. Recurrent neural networks play a key role in, e.g., natural language processing, and their verification is crucial for guaranteeing the reliability of many critical systems. Our approach is based on the inference of invariants, which allow us to reduce the complex problem of verifying recurrent networks into simpler, non-recurrent problems. Experiments with a proof-of-concept implementation of our approach demonstrate that it performs orders-of-magnitude better than the state of the art. ","Yuval Jacoby, Clark Barrett, Guy Katz",,,11,
Explainable AI for System Failures: Generating Explanations that Improve   Human Assistance in Fault Recovery,"  With the growing capabilities of intelligent systems, the integration of artificial intelligence (AI) and robots in everyday life is increasing. However, when interacting in such complex human environments, the failure of intelligent systems, such as robots, can be inevitable, requiring recovery assistance from users. In this work, we develop automated, natural language explanations for failures encountered during an AI agents' plan execution. These explanations are developed with a focus of helping non-expert users understand different point of failures to better provide recovery assistance. Specifically, we introduce a context-based information type for explanations that can both help non-expert users understand the underlying cause of a system failure, and select proper failure recoveries. Additionally, we extend an existing sequence-to-sequence methodology to automatically generate our context-based explanations. By doing so, we are able develop a model that can generalize context-based explanations over both different failure types and failure scenarios. ","Devleena Das, Siddhartha Banerjee, Sonia Chernova",,,11,
Enforcing Almost-Sure Reachability in POMDPs,"  Partially-Observable Markov Decision Processes (POMDPs) are a well-known formal model for planning scenarios where agents operate under limited information about their environment. In safety-critical domains, the agent must adhere to a policy satisfying certain behavioral constraints. We study the problem of computing policies that almost-surely reach some goal state while a set of bad states is never visited. In particular, we present an iterative symbolic approach that computes a so-called winning region, that is, a set of system configurations such that all policies that stay within this set are guaranteed to satisfy the constraints. The empirical evaluation demonstrates the scalability and efficacy of our approach. In addition, we show the applicability to the safe exploration of POMDPs by restricting the agent behavior to these winning regions. ","Sebastian Junges, Nils Jansen, Sanjit A. Seshia",,,11,
A Novel CNet-assisted Evolutionary Level Repairer and Its Applications   to Super Mario Bros,"  Applying latent variable evolution to game level design has become more and more popular as little human expert knowledge is required. However, defective levels with illegal patterns may be generated due to the violation of constraints for level design. A traditional way of repairing the defective levels is programming specific rule-based repairers to patch the flaw. However, programming these constraints is sometimes complex and not straightforward. An autonomous level repairer which is capable of learning the constraints is needed. In this paper, we propose a novel approach, CNet, to learn the probability distribution of tiles giving its surrounding tiles on a set of real levels, and then detect the illegal tiles in generated new levels. Then, an evolutionary repairer is designed to search for optimal replacement schemes equipped with a novel search space being constructed with the help of CNet and a novel heuristic function. The proposed approaches are proved to be effective in our case study of repairing GAN-generated and artificially destroyed levels of Super Mario Bros. game. Our CNet-assisted evolutionary repairer can also be easily applied to other games of which the levels can be represented by a matrix of objects or tiles. ","Tianye Shu, Ziqi Wang, Jialin Liu, Xin Yao",,,11,
Griddly: A platform for AI research in games,"  In recent years, there have been immense breakthroughs in Game AI research, particularly with Reinforcement Learning (RL). Despite their success, the underlying games are usually implemented with their own preset environments and game mechanics, thus making it difficult for researchers to prototype different game environments. However, testing the RL agents against a variety of game environments is critical for recent effort to study generalization in RL and avoid the problem of overfitting that may otherwise occur. In this paper, we present Griddly as a new platform for Game AI research that provides a unique combination of highly configurable games, different observer types and an efficient C++ core engine. Additionally, we present a series of baseline experiments to study the effect of different observation configurations and generalization ability of RL agents. ","Chris Bamford, Shengyi Huang, Simon Lucas",,,11,
Exploring Exploration: Comparing Children with RL Agents in Unified   Environments,"  Research in developmental psychology consistently shows that children explore the world thoroughly and efficiently and that this exploration allows them to learn. In turn, this early learning supports more robust generalization and intelligent behavior later in life. While much work has gone into developing methods for exploration in machine learning, artificial agents have not yet reached the high standard set by their human counterparts. In this work we propose using DeepMind Lab (Beattie et al., 2016) as a platform to directly compare child and agent behaviors and to develop new exploration techniques. We outline two ongoing experiments to demonstrate the effectiveness of a direct comparison, and outline a number of open research questions that we believe can be tested using this methodology. ","Eliza Kosoy, Jasmine Collins, David M. Chan, Sandy Huang, Deepak   Pathak, Pulkit Agrawal, John Canny, Alison Gopnik, Jessica B. Hamrick",,,11,
Multidimensional Enrichment of Spatial RDF Data for SOLAP -- Full   Version,"  Large volumes of spatial data and multidimensional data are being published on the Semantic Web, which has led to new opportunities for advanced analysis, such as Spatial Online Analytical Processing (SOLAP). The RDF Data Cube (QB) and QB4OLAP vocabularies have been widely used for annotating and publishing statistical and multidimensional RDF data. Although such statistical data sets might have spatial information, such as coordinates, the lack of spatial semantics and spatial multidimensional concepts in QB4OLAP and QB prevents users from employing SOLAP queries over spatial data using SPARQL. The QB4SOLAP vocabulary, on the other hand, fully supports annotating spatial and multidimensional data on the Semantic Web and enables users to query endpoints with SOLAP operators in SPARQL. To bridge the gap between QB/QB4OLAP and QB4SOLAP, we propose an RDF2SOLAP enrichment model that automatically annotates spatial multidimensional concepts with QB4SOLAP and in doing so enables SOLAP on existing QB and QB4OLAP data on the Semantic Web. Furthermore, we present and evaluate a wide range of enrichment algorithms and apply them on a non-trivial real-world use case involving governmental open data with complex geometry types. ","Nurefsan G\""ur, Torben Bach Pedersen, Katja Hose, and Mikael Midtgaard",,,11,
A Survey of Blocking and Filtering Techniques for Entity Resolution,"  Efficiency techniques are an integral part of Entity Resolution, since its infancy. In this survey, we organized the bulk of works in the field into Blocking, Filtering and hybrid techniques, facilitating their understanding and use. We also provided an in-dept coverage of each category, further classifying the corresponding works into novel sub-categories. Lately, the efficiency techniques have received more attention, due to the rise of Big Data. This includes large volumes of semi-structured data, which pose challenges not only to the scalability of efficiency techniques, but also to their core assumptions: the requirement of Blocking for schema knowledge and of Filtering for high similarity thresholds. The former led to the introduction of schema-agnostic Blocking in conjunction with Block Processing techniques, while the latter led to more relaxed criteria of similarity. Our survey covers these new fields in detail, putting in context all relevant works. ","George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, Themis Palpanas",,,11,
"On the Efficiency of K-Means Clustering: Evaluation, Optimization, and   Algorithm Selection","  This paper presents a thorough evaluation of the existing methods that accelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze the pruning mechanisms of existing methods, and summarize their common pipeline into a unified evaluation framework UniK. UniK embraces a class of well-known methods and enables a fine-grained performance breakdown. Within UniK, we thoroughly evaluate the pros and cons of existing methods using multiple performance metrics on a number of datasets. Furthermore, we derive an optimized algorithm over UniK, which effectively hybridizes multiple existing methods for more aggressive pruning. To take this further, we investigate whether the most efficient method for a given clustering task can be automatically selected by machine learning, to benefit practitioners and researchers. ","Sheng Wang, Yuan Sun, Zhifeng Bao",,,11,
GraphQL Live Querying with DynamoDB,"  We present a method of implementing GraphQL live queries at the database level. Our DynamoDB simulation in Go mimics a distributed key-value store and implements live queries to expose possible pitfalls. Two key components for implementing live queries are storing fields selected in a live query and determining which object fields have been updated in each database write. A stream(key, fields) request to the system contains fields to include in the live query stream and on subsequent put(key, object) operations, the database asynchronously determines which fields were updated and pushes a new query view to the stream if those fields overlap with the stream() request. Following a discussion of our implementation, we explore motivations for using live queries such as simplifying software communication, minimizing data transfer, and enabling real-time data and describe an architecture for building software with GraphQL and live queries. ",Austin Silveria,,,11,
MorphStore: Analytical Query Engine with a Holistic Compression-Enabled   Processing Model,"  In this paper, we present MorphStore, an open-source in-memory columnar analytical query engine with a novel holistic compression-enabled processing model. Basically, compression using lightweight integer compression algorithms already plays an important role in existing in-memory column-store database systems, but mainly for base data. In particular, during query processing, these systems only keep the data compressed until an operator cannot process the compressed data directly, whereupon the data is decompressed, but not recompressed. Thus, the full potential of compression during query processing is not exploited. To overcome that, we developed a novel compression-enabled processing model as presented in this paper. As we are going to show, the continuous usage of compression for all base data and all intermediates is very beneficial to reduce the overall memory footprint as well as to improve the query performance. ","Patrick Damme, Annett Ungeth\""um, Johannes Pietrzyk, Alexander Krause,   Dirk Habich, Wolfgang Lehner",,,11,
"SoK: Chasing Accuracy and Privacy, and Catching Both in Differentially   Private Histogram Publication","  Histograms and synthetic data are of key importance in data analysis. However, researchers have shown that even aggregated data such as histograms, containing no obvious sensitive attributes, can result in privacy leakage. To enable data analysis, a strong notion of privacy is required to avoid risking unintended privacy violations.   Such a strong notion of privacy is differential privacy, a statistical notion of privacy that makes privacy leakage quantifiable. The caveat regarding differential privacy is that while it has strong guarantees for privacy, privacy comes at a cost of accuracy. Despite this trade off being a central and important issue in the adoption of differential privacy, there exists a gap in the literature regarding providing an understanding of the trade off and how to address it appropriately.   Through a systematic literature review (SLR), we investigate the state-of-the-art within accuracy improving differentially private algorithms for histogram and synthetic data publishing. Our contribution is two-fold: 1) we identify trends and connections in the contributions to the field of differential privacy for histograms and synthetic data and 2) we provide an understanding of the privacy/accuracy trade off challenge by crystallizing different dimensions to accuracy improvement. Accordingly, we position and visualize the ideas in relation to each other and external work, and deconstruct each algorithm to examine the building blocks separately with the aim of pinpointing which dimension of accuracy improvement each technique/approach is targeting. Hence, this systematization of knowledge (SoK) provides an understanding of in which dimensions and how accuracy improvement can be pursued without sacrificing privacy. ",Boel Nelson and Jenni Reuben,,,11,
"Cost Models for Big Data Query Processing: Learning, Retrofitting, and   Our Findings","  Query processing over big data is ubiquitous in modern clouds, where the system takes care of picking both the physical query execution plans and the resources needed to run those plans, using a cost-based query optimizer. A good cost model, therefore, is akin to better resource efficiency and lower operational costs. Unfortunately, the production workloads at Microsoft show that costs are very complex to model for big data systems. In this work, we investigate two key questions: (i) can we learn accurate cost models for big data systems, and (ii) can we integrate the learned models within the query optimizer. To answer these, we make three core contributions. First, we exploit workload patterns to learn a large number of individual cost models and combine them to achieve high accuracy and coverage over a long period. Second, we propose extensions to Cascades framework to pick optimal resources, i.e, number of containers, during query planning. And third, we integrate the learned cost models within the Cascade-style query optimizer of SCOPE at Microsoft. We evaluate the resulting system, Cleo, in a production environment using both production and TPC-H workloads. Our results show that the learned cost models are 2 to 3 orders of magnitude more accurate, and 20X more correlated with the actual runtimes, with a large majority (70%) of the plan changes leading to substantial improvements in latency as well as resource usage. ","Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, Wangchao le",,,11,
Leveraging Soft Functional Dependencies for Indexing Multi-dimensional   Data,"  A new proposal in database indexing has been for index structures to automatically learn and use the distribution of the underlying data to improve their performance. Initial work on \textit{learned indexes} has repeatedly shown that by learning the distribution of the data, index structures such as the B-Tree, can boost their performance by an order of magnitude while using a smaller memory footprint. In this work we propose a new class of learned indexes for multidimensional data that instead of learning only from distribution of keys, learns from correlations between columns of the dataset. Our approach is motivated by the observation that in real datasets, correlation between two or more attributes of the data is a common occurrence. This idea of learning from functional dependencies has been previously explored and implemented in many state of the art query optimisers to predict selectivity of queries and come up with better query plans. In this project we aim to take the use of learned functional dependencies a step further in databases. Consequently, we focus on using learned functional dependencies to reduce the dimensionality of datasets. With this we attempt to work around the curse of dimensionality - which in the context of spatial data stipulates that with every additional dimension, the performance of an index deteriorates further - to accelerate query execution. In more precise terms, we learn how to infer one (or multiple) attributes from the remaining attributes and hence no longer need to index predicted columns. This method reduces the dimensionality of the index and thus makes it more efficient. We show experimentally that by predicting correlated attributes in the data, rather than indexing them, we can improve the query execution time and reduce the memory overhead of the index at the same time. ","Behzad Ghaffari, Ali Hadian, Thomas Heinis",,,11,
Database Repairing with Soft Functional Dependencies,"  A common interpretation of soft constraints penalizes the database for every violation of every constraint, where the penalty is the cost (weight) of the constraint. A computational challenge is that of finding an optimal subset: a collection of database tuples that minimizes the total penalty when each tuple has a cost of being excluded. When the constraints are strict (i.e., have an infinite cost), this subset is a ""cardinality repair"" of an inconsistent database; in soft interpretations, this subset corresponds to a ""most probable world"" of a probabilistic database, a ""most likely intention"" of a probabilistic unclean database, and so on. Within the class of functional dependencies, the complexity of finding a cardinality repair is thoroughly understood. Yet, very little is known about the complexity of this problem in the more general soft semantics. This paper makes a significant progress in this direction. In addition to general insights about the hardness and approximability of the problem, we present algorithms for two special cases: a single functional dependency, and a bipartite matching. The latter is the problem of finding an optimal ""almost matching"" of a bipartite graph where a penalty is paid for every lost edge and every violation of monogamy. ","Nofar Carmeli, Martin Grohe, Benny Kimelfeld, Ester Livshits, and   Muhammad Tibi",,,11,
On the complexity of query containment and computing certain answers in   the presence of ACs,"  We often add arithmetic to extend the expressiveness of query languages and study the complexity of problems such as testing query containment and finding certain answers in the framework of answering queries using views. When adding arithmetic comparisons, the complexity of such problems is higher than the complexity of their counterparts without them. It has been observed that we can achieve lower complexity if we restrict some of the comparisons in the containing query to be closed or open semi-interval comparisons. Here, focusing a) on the problem of containment for conjunctive queries with arithmetic comparisons (CQAC queries, for short), we prove upper bounds on its computational complexity and b) on the problem of computing certain answers, we find large classes of CQAC queries and views where this problem is polynomial. ",Foto N. Afrati and Matthew Damigos,,,11,
Privacy Preserving Distributed Machine Learning with Federated Learning,"  Edge computing and distributed machine learning have advanced to a level that can revolutionize a particular organization. Distributed devices such as the Internet of Things (IoT) often produce a large amount of data, eventually resulting in big data that can be vital in uncovering hidden patterns, and other insights in numerous fields such as healthcare, banking, and policing. Data related to areas such as healthcare and banking can contain potentially sensitive data that can become public if they are not appropriately sanitized. Federated learning (FedML) is a recently developed distributed machine learning (DML) approach that tries to preserve privacy by bringing the learning of an ML model to data owners'. However, literature shows different attack methods such as membership inference that exploit the vulnerabilities of ML models as well as the coordinating servers to retrieve private data. Hence, FedML needs additional measures to guarantee data privacy. Furthermore, big data often requires more resources than available in a standard computer. This paper addresses these issues by proposing a distributed perturbation algorithm named as DISTPAB, for privacy preservation of horizontally partitioned data. DISTPAB alleviates computational bottlenecks by distributing the task of privacy preservation utilizing the asymmetry of resources of a distributed environment, which can have resource-constrained devices as well as high-performance computers. Experiments show that DISTPAB provides high accuracy, high efficiency, high scalability, and high attack resistance. Further experiments on privacy-preserving FedML show that DISTPAB is an excellent solution to stop privacy leaks in DML while preserving high data utility. ","M.A.P. Chamikara, P.Bertok, I.Khalil, D.Liu, S.Camtepe",,,11,
Adaptive Low-level Storage of Very Large Knowledge Graphs,"  The increasing availability and usage of Knowledge Graphs (KGs) on the Web calls for scalable and general-purpose solutions to store this type of data structures. We propose Trident, a novel storage architecture for very large KGs on centralized systems. Trident uses several interlinked data structures to provide fast access to nodes and edges, with the physical storage changing depending on the topology of the graph to reduce the memory footprint. In contrast to single architectures designed for single tasks, our approach offers an interface with few low-level and general-purpose primitives that can be used to implement tasks like SPARQL query answering, reasoning, or graph analytics. Our experiments show that Trident can handle graphs with 10^11 edges using inexpensive hardware, delivering competitive performance on multiple workloads. ",Jacopo Urbani and Ceriel Jacobs,,,11,
TASM: A Tile-Based Storage Manager for Video Analytics,"  The amount of video data being produced is rapidly growing. At the same time, advances in machine learning and computer vision have enabled applications to query over the contents of videos. For example, an ornithology application may retrieve birds of various species from a nature video. However, modern video data management systems store videos as a single encoded file, which does not provide opportunities to optimize queries for spatial subsets of videos. We propose utilizing a feature in modern video codecs called ""tiles"" to enable spatial random access into encoded videos. We present the design of TASM, a tile-based storage manager, and describe techniques it uses to optimize the physical layout of videos for various query workloads. We demonstrate how TASM can significantly improve the performance of queries over videos when the workload is known, as well as how it can incrementally adapt the physical layout of videos to improve performance even when the workload is not known. Layouts picked by TASM speed up individual queries by an average of 51% and up to 94% while maintaining good quality. ","Maureen Daum, Brandon Haynes, Dong He, Amrita Mazumdar, Magdalena   Balazinska, Alvin Cheung",,,11,
A Dichotomy for Homomorphism-Closed Queries on Probabilistic Graphs,"  We study the problem of probabilistic query evaluation (PQE) over probabilistic graphs, namely, tuple-independent probabilistic databases (TIDs) on signatures of arity two. Our focus is the class of queries that is closed under homomorphisms, or equivalently, the infinite unions of conjunctive queries, denoted UCQ^\infty. Our main result states that all unbounded queries in UCQ^\infty are #P-hard for PQE. As bounded queries in UCQ^\infty are already classified by the dichotomy of Dalvi and Suciu [17], our results and theirs imply a complete dichotomy on PQE for UCQ^\infty queries over probabilistic graphs. This dichotomy covers in particular all fragments in UCQ^\infty such as negation-free (disjunctive) Datalog, regular path queries, and a large class of ontology-mediated queries on arity-two signatures. Our result is shown by reducing from counting the valuations of positive partitioned 2-DNF formulae (#PP2DNF) for some queries, or from the source-to-target reliability problem in an undirected graph (#U-ST-CON) for other queries, depending on properties of minimal models. ","Antoine Amarilli, \.Ismail \.Ilkan Ceylan",,,11,
An Algorithm for Context-Free Path Queries over Graph Databases,"  RDF (Resource Description Framework) is a standard language to represent graph databases. Query languages for RDF databases usually include primitives to support path queries, linking pairs of vertices of the graph that are connected by a path of labels belonging to a given language. Languages such as SPARQL include support for paths defined by regular languages (by means of Regular Expressions). A context-free path query is a path query whose language can be defined by a context-free grammar. Context-free path queries can be used to implement queries such as the ""same generation queries"", that are not expressible by Regular Expressions. In this paper, we present a novel algorithm for context-free path query processing. We prove the correctness of our approach and show its run-time and memory complexity. We show the viability of our approach by means of a prototype implemented in Go. We run our prototype using the same cases of study as proposed in recent works, comparing our results with another, recently published algorithm. The experiments include both synthetic and real RDF databases. Our algorithm can be seen as a step forward, towards the implementation of more expressive query languages. ","Ciro M. Medeiros, Martin A. Musicante, Umberto S. Costa",,,11,
PushdownDB: Accelerating a DBMS using S3 Computation,"  This paper studies the effectiveness of pushing parts of DBMS analytics queries into the Simple Storage Service (S3) engine of Amazon Web Services (AWS), using a recently released capability called S3 Select. We show that some DBMS primitives (filter, projection, aggregation) can always be cost-effectively moved into S3. Other more complex operations (join, top-K, group-by) require reimplementation to take advantage of S3 Select and are often candidates for pushdown. We demonstrate these capabilities through experimentation using a new DBMS that we developed, PushdownDB. Experimentation with a collection of queries including TPC-H queries shows that PushdownDB is on average 30% cheaper and 6.7X faster than a baseline that does not use S3 Select. ","Xiangyao Yu, Matt Youill, Matthew Woicik, Abdurrahman Ghanem, Marco   Serafini, Ashraf Aboulnaga, Michael Stonebraker",,,11,
Hardware-Conscious Stream Processing: A Survey,"  Data stream processing systems (DSPSs) enable users to express and run stream applications to continuously process data streams. To achieve real-time data analytics, recent researches keep focusing on optimizing the system latency and throughput. Witnessing the recent great achievements in the computer architecture community, researchers and practitioners have investigated the potential of adoption hardware-conscious stream processing by better utilizing modern hardware capacity in DSPSs. In this paper, we conduct a systematic survey of recent work in the field, particularly along with the following three directions: 1) computation optimization, 2) stream I/O optimization, and 3) query deployment. Finally, we advise on potential future research directions. ","Shuhao Zhang, Feng Zhang, Yingjun Wu, Bingsheng He, Paul Johns",,,11,
Knowledge Scientists: Unlocking the data-driven organization,"  Organizations across all sectors are increasingly undergoing deep transformation and restructuring towards data-driven operations. The central role of data highlights the need for reliable and clean data. Unreliable, erroneous, and incomplete data lead to critical bottlenecks in processing pipelines and, ultimately, service failures, which are disastrous for the competitive performance of the organization. Given its central importance, those organizations which recognize and react to the need for reliable data will have the advantage in the coming decade. We argue that the technologies for reliable data are driven by distinct concerns and expertise which complement those of the data scientist and the data engineer. Those organizations which identify the central importance of meaningful, explainable, reproducible, and maintainable data will be at the forefront of the democratization of reliable data. We call the new role which must be developed to fill this critical need the Knowledge Scientist. The organizational structures, tools, methodologies and techniques to support and make possible the work of knowledge scientists are still in their infancy. As organizations not only use data but increasingly rely on data, it is time to empower the people who are central to this transformation. ","George Fletcher, Paul Groth, Juan Sequeda",,,11,
Optimizing Federated Queries Based on the Physical Design of a Data Lake,"  The optimization of query execution plans is known to be crucial for reducing the query execution time. In particular, query optimization has been studied thoroughly for relational databases over the past decades. Recently, the Resource Description Framework (RDF) became popular for publishing data on the Web. As a consequence, federations composed of different data models like RDF and relational databases evolved. One type of these federations are Semantic Data Lakes where every data source is kept in its original data model and semantically annotated with ontologies or controlled vocabularies. However, state-of-the-art query engines for federated query processing over Semantic Data Lakes often rely on optimization techniques tailored for RDF. In this paper, we present query optimization techniques guided by heuristics that take the physical design of a Data Lake into account. The heuristics are implemented on top of Ontario, a SPARQL query engine for Semantic Data Lakes. Using source-specific heuristics, the query engine is able to generate more efficient query execution plans by exploiting the knowledge about indexes and normalization in relational databases. We show that heuristics which take the physical design of the Data Lake into account are able to speed up query processing. ",Philipp D. Rohde and Maria-Esther Vidal,,,11,
NoSQL Databases: Yearning for Disambiguation,"  The demanding requirements of the new Big Data intensive era raised the need for flexible storage systems capable of handling huge volumes of unstructured data and of tackling the challenges that traditional databases were facing. NoSQL Databases, in their heterogeneity, are a powerful and diverse set of databases tailored to specific industrial and business needs. However, the lack of theoretical background creates a lack of consensus even among experts about many NoSQL concepts, leading to ambiguity and confusion. In this paper, we present a survey of NoSQL databases and their classification by data model type. We also conduct a benchmark in order to compare different NoSQL databases and distinguish their characteristics. Additionally, we present the major areas of ambiguity and confusion around NoSQL databases and their related concepts, and attempt to disambiguate them. ","Chaimae Asaad, Karim Ba\""ina, Mounir Ghogho",,,11,
Undecidability of $D_{<:}$ and Its Decidable Fragments,"  Dependent Object Types (DOT) is a calculus with path dependent types, intersection types, and object self-references, which serves as the core calculus of Scala 3. Although the calculus has been proven sound, it remains open whether type checking in DOT is decidable. In this paper, we establish undecidability proofs of type checking and subtyping of $D_{<:}$, a syntactic subset of DOT. It turns out that even for $D_{<:}$, undecidability is surprisingly difficult to show, as evidenced by counterexamples for past attempts. To prove undecidability, we discover an equivalent definition of the $D_{<:}$ subtyping rules in normal form. Besides being easier to reason about, this definition makes the phenomenon of bad bounds explicit as a single inference rule. After removing this rule, we discover two decidable fragments of $D_{<:}$ subtyping and identify algorithms to decide them. We prove soundness and completeness of the algorithms with respect to the fragments, and we prove that the algorithms terminate. Our proofs are mechanized in a combination of Coq and Agda. ","Jason Hu, Ond\v{r}ej Lhot\'ak",,,11,
A Deductive Verification Framework for Circuit-building Quantum Programs,"  While recent progress in quantum hardware open the door for significant speedup in certain key areas, quantum algorithms are still hard to implement right, and the validation of such quantum programs is a challenge. Early attempts either suffer from the lack of automation or parametrized reasoning, or target high-level abstract algorithm description languages far from the current de facto consensus of circuit-building quantum programming languages. As a consequence, no significant quantum algorithm implementation has been currently verified in a scale-invariant manner. We propose Qbricks, the first formal verification environment for circuit-building quantum programs, featuring clear separation between code and proof, parametric specifications and proofs, high degree of proof automation and allowing to encode quantum programs in a natural way, i.e. close to textbook style. Qbricks builds on best practice of formal verification for the classical case and tailor them to the quantum case: we bring a new domain-specific circuit-building language for quantum programs, namely Qbricks-DSL, together with a new logical specification language Qbricks-Spec and a dedicated Hoare-style deductive verification rule named Hybrid Quantum Hoare Logic. Especially, we introduce and intensively build upon HOPS, a higher-order extension of the recent path-sum symbolic representation, used for both specification and automation. To illustrate the opportunity of Qbricks, we implement the first verified parametric implementations of several famous and non-trivial quantum algorithms, including the quantum part of Shor integer factoring (Order Finding - Shor-OF), quantum phase estimation (QPE) - a basic building block of many quantum algorithms, and Grover search. These breakthroughs were amply facilitated by the specification and automated deduction principles introduced within Qbricks. ","Christophe Chareton, S\'ebastien Bardin, Fran\c{c}ois Bobot, Valentin   Perrelle, Benoit Valiron",,,11,
The Decidability of Verification under Promising 2.0,"  In PLDI'20, Lee et al. introduced the \emph{promising } semantics PS 2.0 of the C++ concurrency that captures most of the common program transformations while satisfying the DRF guarantee. The reachability problem for finite-state programs under PS 2.0 with only release-acquire accesses is already known to be undecidable. Therefore, we address, in this paper, the reachability problem for programs running under PS 2.0 with relaxed accesses together with promises. We show that this problem is undecidable even in the case where the input program has finite state. Given this undecidability result, we consider the fragment of PS 2.0 with only relaxed accesses allowing bounded number of promises. We show that under this restriction, the reachability is decidable, albeit very expensive: it is non-primitive recursive. Given this high complexity with bounded number of promises and the undecidability result for the RA fragment of PS 2.0, we consider a bounded version of the reachability problem. To this end, we bound both the number of promises and the ""view-switches"", i.e, the number of times the processes may switch their local views of the global memory. We provide a code-to-code translation from an input program under PS 2.0, with relaxed and release-acquire memory accesses along with promises, to a program under SC. This leads to a reduction of the bounded reachability problem under PS 2.0 to the bounded context-switching problem under SC. We have implemented a prototype tool and tested it on a set of benchmarks, demonstrating that many bugs in programs can be found using a small bound. ","Parosh Aziz Abdulla, Mohamed Faouzi Atig, Adwait Godbole,   Shankaranarayanan Krishna, Viktor Vafeiadis",,,11,
Evaluation of Logic Programs with Built-Ins and Aggregation: A Calculus   for Bag Relations,"  We present a scheme for translating logic programs, which may use aggregation and arithmetic, into algebraic expressions that denote bag relations over ground terms of the Herbrand universe. To evaluate queries against these relations, we develop an operational semantics based on term rewriting of the algebraic expressions. This approach can exploit arithmetic identities and recovers a range of useful strategies, including lazy strategies that defer work until it becomes possible or necessary. ",Matthew Francis-Landau and Tim Vieira and Jason Eisner,,,11,
First Infrastructure and Experimentation in Echo-debugging,"  As applications get developed, bugs inevitably get introduced. Often, it is unclear why a given code change introduced a given bug. To find this causal relation and more effectively debug, developers can leverage the existence of a previous version of the code, without the bug. But traditional debug-ging tools are not designed for this type of work, making this operation tedious. In this article, we propose as exploratory work the echo-debugger, a tool to debug two different executions in parallel, and the Convergence Divergence Mapping (CDM) algorithm to locate all the control-flow divergences and convergences of these executions. In this exploratory work, we present the architecture of the tool and a scenario to solve a non trivial bug. ","Thomas Dupriez (CNRS, CRIStAL, RMOD), Steven Costiou (CNRS, CRIStAL,   RMOD), St\'ephane Ducasse (CNRS, CRIStAL, RMOD)",,,11,
The Prolog debugger and declarative programming,"  Logic programming is a declarative programming paradigm. Programming language Prolog makes logic programming possible, at least to a substantial extent. However the Prolog debugger works solely in terms of the operational semantics. So it is incompatible with declarative programming. This report discusses this issue and tries to find how the debugger may be used from the declarative point of view. The results are rather not encouraging.   Also, the box model of Byrd, used by the debugger, is explained in terms of SLD-resolution. ",W{\l}odzimierz Drabent,,,11,
Correctly Implementing Synchronous Message Passing in the Pi-Calculus By   Concurrent Haskell's MVars,"  Comparison of concurrent programming languages and correctness of program transformations in concurrency are the focus of this research. As criterion we use contextual semantics adapted to concurrency, where may -- as well as should -- convergence are observed. We investigate the relation between the synchronous pi-calculus and a core language of Concurrent Haskell (CH). The contextual semantics is on the one hand forgiving with respect to the details of the operational semantics, and on the other hand implies strong requirements for the interplay between the processes after translation. Our result is that CH embraces the synchronous pi-calculus. Our main task is to find and prove correctness of encodings of pi-calculus channels by CH's concurrency primitives, which are MVars. They behave like (blocking) 1-place buffers modelling the shared-memory. The first developed translation uses an extra private MVar for every communication.We also automatically generate and check potentially correct translations that reuse the MVars where one MVar contains the message and two additional MVars for synchronization are used to model the synchronized communication of a single channel in the pi-calculus.Our automated experimental results lead to the conjecture that one additional MVar is insufficient. ","Manfred Schmidt-Schau{\ss} (Goethe-University, Frankfurt, Germany),   David Sabel (LMU, Munich, Germany)",,,11,
Extensible Datasort Refinements,"  Refinement types turn typechecking into lightweight verification. The classic form of refinement type is the datasort refinement, in which datasorts identify subclasses of inductive datatypes.   Existing type systems for datasort refinements require that all the refinements of a type be specified when the type is declared; multiple refinements of the same type can be obtained only by duplicating type definitions, and consequently, duplicating code.   We enrich the traditional notion of a signature, which describes the inhabitants of datasorts, to allow re-refinement via signature extension, without duplicating definitions. Since arbitrary updates to a signature can invalidate the inversion principles used to check case expressions, we develop a definition of signature well-formedness that ensures that extensions maintain existing inversion principles. This definition allows different parts of a program to extend the same signature in different ways, without conflicting with each other. Each part can be type-checked independently, allowing separate compilation. ",Jana Dunfield,,,11,
Duality of Session Types: The Final Cut,"  Duality is a central concept in the theory of session types. Since a flaw was found in the original definition of duality for recursive types, several other definitions have been published. As their connection is not obvious, we compare the competing definitions, discuss tradeoffs, and prove some equivalences. Some of the results are mechanized in Agda. ","Simon J. Gay (School of Computing Science, University of Glasgow, UK),   Peter Thiemann (Institut f\""ur Informatik, University of Freiburg, Germany),   Vasco T. Vasconcelos (Faculdade de Ci\^encias, University of Lisbon,   Portugal)",,,11,
Diamonds are not forever: Liveness in reactive programming with guarded   recursion,"  When designing languages for functional reactive programming (FRP) the main challenge is to provide the user with a simple, flexible interface for writing programs on a high level of abstraction while ensuring that all programs can be implemented efficiently in a low-level language. To meet this challenge, a new family of modal FRP languages has been proposed, in which variants of Nakano's guarded fixed point operator are used for writing recursive programs guaranteeing properties such as causality and productivity. As an apparent extension to this it has also been suggested to use Linear Temporal Logic (LTL) as a language for reactive programming through the Curry-Howard isomorphism, allowing properties such as termination, liveness and fairness to be encoded in types. However, these two ideas are in conflict with each other, since the fixed point operator introduces non-termination into the inductive types that are supposed to provide termination guarantees.   In this paper we show that by regarding the modal time step operator of LTL a submodality of the one used for guarded recursion (rather than equating them), one can obtain a modal type system capable of expressing liveness properties while retaining the power of the guarded fixed point operator. We introduce the language Lively RaTT, a modal FRP language with a guarded fixed point operator and an `until' type constructor as in LTL, and show how to program with events and fair streams. Using a step-indexed Kripke logical relation we prove operational properties of Lively RaTT including productivity and causality as well as the termination and liveness properties expected of types from LTL. Finally, we prove that the type system of Lively RaTT guarantees the absence of implicit space leaks. ","Patrick Bahr, Christian Uldal Graulund, Rasmus M{\o}gelberg",,,11,
A General Framework for Automatic Termination Analysis of Logic Programs,"  This paper describes a general framework for automatic termination analysis of logic programs, where we understand by ``termination'' the finitenes s of the LD-tree constructed for the program and a given query. A general property of mappings from a certain subset of the branches of an infinite LD-tree into a finite set is proved. From this result several termination theorems are derived, by using different finite sets. The first two are formulated for the predicate dependency and atom dependency graphs. Then a general result for the case of the query-mapping pairs relevant to a program is proved (cf. \cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\em TermiLog} system described in \cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this system it is not possible to prove termination for programs involving arithmetic predicates, since the usual order for the integers is not well-founded. A new method, which can be easily incorporated in {\em TermiLog} or similar systems, is presented, which makes it possible to prove termination for programs involving arithmetic predicates. It is based on combining a finite abstraction of the integers with the technique of the query-mapping pairs, and is essentially capable of dividing a termination proof into several cases, such that a simple termination function suffices for each case. Finally several possible extensions are outlined. ","Nachum Dershowitz, Naomi Lindenstrauss, Yehoshua Sagiv, Alexander   Serebrenik",,,11,
ARGG-HDL: A High Level Python Based Object-Oriented HDL Framework,"  We present a High-Level Python-based Hardware Description Language (ARGG-HDL), It uses Python as its source language and converts it to standard VHDL. Compared to other approaches of building converters from a high-level programming language into a hardware description language, this new approach aims to maintain an object-oriented paradigm throughout the entire process. Instead of removing all the high-level features from Python to make it into an HDL, this approach goes the opposite way. It tries to show how certain features from a high-level language can be implemented in an HDL, providing the corresponding benefits of high-level programming for the user. ","R. Peschke, K. Nishimura, G. Varner",,,11,
A Proposal for a Revision of ISO Modula-2,"  The Modula-2 language was first specified in [Wir78] by N. Wirth at ETH Zurich in 1978 and then revised several times. The last revision [Wir88] was published in 1988. The resulting language reports included ambiguities and lacked a comprehensive standard library. To resolve the ambiguities and specify a comprehensive standard library an ISO/IEC working group was formed and commenced work in 1987. A base standard was then ratified and published as IS 10514-1 in 1996 [JTC96]. Several conforming compilers have since been developed. At least five remain available of which at least three are actively maintained and one has been open sourced. Meanwhile, various deficiencies of the standard have become apparent but since its publication, no revision and no maintenance has been carried out. This paper discusses some of the deficiencies of IS 10514-1 and proposes a limited revision that could be carried out with moderate effort. The scope of the paper has been deliberately limited to the core language of the base standard and therefore excludes the standard library. ",Benjamin Kowarsch,,,11,
Proceedings of the 12th International Workshop on Programming Language   Approaches to Concurrency- and Communication-cEntric Software,"  Modern hardware platforms, from the very small to the very large, increasingly provide parallel and distributed computing resources for applications to maximise performance. Many applications therefore need to make effective use of tens, hundreds, and even thousands of compute nodes. Computation in such systems is thus inherently concurrent and communication centric. Effectively programming such applications is challenging; performance, correctness, and scalability are difficult to achieve. The development of effective programming methodologies for this increasingly parallel landscape therefore demands exploration and understanding of a wide variety of foundational and practical ideas. The International Workshop on Programming Language Approaches to Concurrency- and Communication-cEntric Software (PLACES) is dedicated to work in this area. The workshop offers a forum for researchers from different fields to exchange new ideas about these challenges to modern and future programming, where concurrency and distribution are the norm rather than a marginal concern. This volume contains the proceedings of the 12th edition of PLACES, which was co-located with ETAPS 2020 in Dublin, Ireland. ","Stephanie Balzer (Carnegie Mellon University), Luca Padovani   (Universit\`a di Torino)",,,11,
Data Flow Refinement Type Inference,"  Refinement types enable lightweight verification of functional programs. Algorithms for statically inferring refinement types typically work by reduction to solving systems of constrained Horn clauses extracted from typing derivations. An example is Liquid type inference, which solves the extracted constraints using predicate abstraction. However, the reduction to constraint solving in itself already signifies an abstraction of the program semantics that affects the precision of the overall static analysis. To better understand this issue, we study the type inference problem in its entirety through the lens of abstract interpretation. We propose a new refinement type system that is parametric with the choice of the abstract domain of type refinements as well as the degree to which it tracks context-sensitive control flow information. We then derive an accompanying parametric inference algorithm as an abstract interpretation of a novel data flow semantics of functional programs. We further show that the type system is sound and complete with respect to the constructed abstract semantics. Our theoretical development reveals the key abstraction steps inherent in refinement type inference algorithms. The trade-off between precision and efficiency of these abstraction steps is controlled by the parameters of the type system. Existing refinement type systems and their respective inference algorithms, such as Liquid types, are captured by concrete parameter instantiations. We have implemented our framework in a prototype tool and evaluated it for a range of new parameter instantiations (e.g., using octagons and polyhedra for expressing type refinements). The tool compares favorably against other existing tools. Our evaluation indicates that our approach can be used to systematically construct new refinement type inference algorithms that are both robust and precise. ","Zvonimir Pavlinovic, Yusen Su, Thomas Wies",,,11,
A Categorical Programming Language,"  A theory of data types based on category theory is presented. We organize data types under a new categorical notion of F,G-dialgebras which is an extension of the notion of adjunctions as well as that of T-algebras. T-algebras are also used in domain theory, but while domain theory needs some primitive data types, like products, to start with, we do not need any. Products, coproducts and exponentiations (i.e. function spaces) are defined exactly like in category theory using adjunctions. F,G-dialgebras also enable us to define the natural number object, the object for finite lists and other familiar data types in programming. Furthermore, their symmetry allows us to have the dual of the natural number object and the object for infinite lists (or lazy lists). We also introduce a programming language in a categorical style using F,G-dialgebras as its data type declaration mechanism. We define the meaning of the language operationally and prove that any program terminates using Tait's computability method. ",Tatsuya Hagino,,,11,
Using SWISH to realise interactive web based tutorials for logic based   languages,"  Programming environments have evolved from purely text based to using graphical user interfaces, and now we see a move towards web based interfaces, such as Jupyter. Web based interfaces allow for the creation of interactive documents that consist of text and programs, as well as their output. The output can be rendered using web technology as, e.g., text, tables, charts or graphs. This approach is particularly suitable for capturing data analysis workflows and creating interactive educational material. This article describes SWISH, a web front-end for Prolog that consists of a web server implemented in SWI-Prolog and a client web application written in JavaScript. SWISH provides a web server where multiple users can manipulate and run the same material, and it can be adapted to support Prolog extensions. In this paper we describe the architecture of SWISH, and describe two case studies of extensions of Prolog, namely Probabilistic Logic Programming (PLP) and Logic Production System (LPS), which have used SWISH to provide tutorial sites. ","Jan Wielemaker, Fabrizio Riguzzi, Bob Kowalski, Torbj\""orn Lager,   Fariba Sadri, Miguel Calejo",,,11,
Efficient global register allocation,"  In a compiler, an essential component is the register allocator. Two main algorithms have dominated implementations, graph coloring and linear scan, differing in how live values are modeled. Graph coloring uses an edge in an `interference graph' to show that two values cannot reside in the same register. Linear scan numbers all values, creates intervals between definition and uses, and then intervals that do not overlap may be allocated to the same register. For both algorithms the liveness models are computed at considerable runtime and memory cost. Furthermore, these algorithms do little to improve code quality, where the target architecture and register coalescing are important concerns.   We describe a new register allocation algorithm with lightweight implementation characteristics. The algorithm introduces a `future-active' set for values that will reside in a register later in the allocation. Registers are allocated and freed in the manner of linear scan, although other ordering heuristics could improve code quality or lower runtime cost. An advantageous property of the approach is an ability to make these trade-offs. A key result is the `future-active' set can remove any liveness model for over 90% of instructions and 80% of methods. The major contribution is the allocation algorithm that, for example, solves an inability of the similarly motivated Treescan register allocator to look ahead of the instruction being allocated - allowing an unconstrained allocation order, and an ability to better handle fixed registers and loop carried values. The approach also is not reliant on properties of SSA form, similar to the original linear scan work. An analysis is presented in a production compiler for Java code compiled through SSA form to Android dex files. ",Ian Rogers,,,11,
Synthesizing Queries via Interactive Sketching,"  We propose a novel approach to program synthesis, focusing on synthesizing database queries. At a high level, our proposed algorithm takes as input a sketch with soft constraints encoding user intent, and then iteratively interacts with the user to refine the sketch. At each step, our algorithm proposes a candidate refinement of the sketch, which the user can either accept or reject. By leveraging this rich form of user feedback, our algorithm is able to both resolve ambiguity in user intent and improve scalability. In particular, assuming the user provides accurate inputs and responses, then our algorithm is guaranteed to converge to the true program (i.e., one that the user approves) in polynomial time. We perform a qualitative evaluation of our algorithm, showing how it can be used to synthesize a variety of queries on a database of academic publications. ",Osbert Bastani and Xin Zhang and Armando Solar-Lezama,,,11,
Reconciling Event Structures with Modern Multiprocessors,"  Weakestmo is a recently proposed memory consistency model that uses event structures to resolve the infamous ""out-of-thin-air"" problem. Although it has been shown to have important benefits over other memory models, its established compilation schemes are suboptimal in that they add more fences than necessary. In this paper, we prove the correctness in Coq of the intended compilation schemes for Weakestmo to a range of hardware memory models (x86, POWER, ARMv7, ARMv8, RISC-V). Our proof is the first that establishes correctness of compilation of an event-structure-based model that forbids ""thin-air"" behaviors, as well as the first mechanized compilation proof of a weak memory model supporting sequentially consistent accesses to such a range of hardware platforms. Our compilation proof goes via the recent Intermediate Memory Model (IMM), which we suitably extend with sequentially consistent accesses. ","Evgenii Moiseenko, Anton Podkopaev, Ori Lahav, Orestis Melkonian and   Viktor Vafeiadis",,,11,
Inner Attention Supported Adaptive Cooperation for Heterogeneous Multi   Robots Teaming based on Multi-agent Reinforcement Learning,"  Humans can selectively focus on different information based on different tasks requirements, other people's abilities and availability. Therefore, they can adapt quickly to a completely different and complex environments. If, like people, robot could obtain the same abilities, then it would greatly increase their adaptability to new and unexpected situations. Recent efforts in Heterogeneous Multi Robots Teaming have try to achieve this ability, such as the methods based on communication and multi-modal information fusion strategies. However, these methods will not only suffer from the exponential explosion problem with the increase of robots number but also need huge computational resources. To that end, we introduce an inner attention actor-critic method that replicates aspects of human flexibly cooperation. By bringing attention mechanism on computer vision, natural language process into the realm of multi-robot cooperation, our attention method is able to dynamically select which robots to attend to. In order to test the effectiveness of our proposed method, several simulation experiments have been designed. And the results show that inner attention mechanism can enable flexible cooperation and lower resources consuming in rescuing tasks. ","Chao Huang, Rui Liu",,,11,
"RUR53: an Unmanned Ground Vehicle for Navigation, Recognition and   Manipulation","  This paper proposes RUR53: an Unmanned Ground Vehicle able to autonomously navigate through, identify, and reach areas of interest; and there recognize, localize, and manipulate work tools to perform complex manipulation tasks. The proposed contribution includes a modular software architecture where each module solves specific sub-tasks and that can be easily enlarged to satisfy new requirements. Included indoor and outdoor tests demonstrate the capability of the proposed system to autonomously detect a target object (a panel) and precisely dock in front of it while avoiding obstacles. They show it can autonomously recognize and manipulate target work tools (i.e., wrenches and valve stems) to accomplish complex tasks (i.e., use a wrench to rotate a valve stem). A specific case study is described where the proposed modular architecture lets easy switch to a semi-teleoperated mode. The paper exhaustively describes description of both the hardware and software setup of RUR53, its performance when tests at the 2017 Mohamed Bin Zayed International Robotics Challenge, and the lessons we learned when participating at this competition, where we ranked third in the Gran Challenge in collaboration with the Czech Technical University in Prague, the University of Pennsylvania, and the University of Lincoln (UK). ","Nicola Castaman, Elisa Tosello, Morris Antonello, Nicola Bagarello,   Silvia Gandin, Marco Carraro, Matteo Munaro, Roberto Bortoletto, Stefano   Ghidoni, Emanuele Menegatti, Enrico Pagello",,,11,
Alpha-N: Shortest Path Finder Automated Delivery Robot with Obstacle   Detection and Avoiding System,"  Alpha N A self-powered, wheel driven Automated Delivery Robot is presented in this paper. The ADR is capable of navigating autonomously by detecting and avoiding objects or obstacles in its path. It uses a vector map of the path and calculates the shortest path by Grid Count Method of Dijkstra Algorithm. Landmark determination with Radio Frequency Identification tags are placed in the path for identification and verification of source and destination, and also for the recalibration of the current position. On the other hand, an Object Detection Module is built by Faster RCNN with VGGNet16 architecture for supporting path planning by detecting and recognizing obstacles. The Path Planning System is combined with the output of the GCM, the RFID Reading System and also by the binary results of ODM. This PPS requires a minimum speed of 200 RPM and 75 seconds duration for the robot to successfully relocate its position by reading an RFID tag. In the result analysis phase, the ODM exhibits an accuracy of 83.75 percent, RRS shows 92.3 percent accuracy and the PPS maintains an accuracy of 85.3 percent. Stacking all these 3 modules, the ADR is built, tested and validated which shows significant improvement in terms of performance and usability comparing with other service robots. ","A. A. Neloy, R. A. Bindu, S. Alam, R. Haque, M. Saif, A. Khan, N. M.   Mishu, and S. Siddique",,,11,
Dynamically Feasible Deep Reinforcement Learning Policy for Robot   Navigation in Dense Mobile Crowds,"  We present a novel Deep Reinforcement Learning (DRL) based policy to compute dynamically feasible and spatially aware velocities for a robot navigating among mobile obstacles. Our approach combines the benefits of the Dynamic Window Approach (DWA) in terms of satisfying the robot's dynamics constraints with state-of-the-art DRL-based navigation methods that can handle moving obstacles and pedestrians well. Our formulation achieves these goals by embedding the environmental obstacles' motions in a novel low-dimensional observation space. It also uses a novel reward function to positively reinforce velocities that move the robot away from the obstacle's heading direction leading to significantly lower number of collisions. We evaluate our method in realistic 3-D simulated environments and on a real differential drive robot in challenging dense indoor scenarios with several walking pedestrians. We compare our method with state-of-the-art collision avoidance methods and observe significant improvements in terms of success rate (up to 33\% increase), number of dynamics constraint violations (up to 61\% decrease), and smoothness. We also conduct ablation studies to highlight the advantages of our observation space formulation, and reward structure. ","Utsav Patel, Nithish Kumar, Adarsh Jagan Sathyamoorthy and Dinesh   Manocha",,,11,
Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot   Experience,"  Manipulating deformable objects, such as fabric, is a long standing problem in robotics, with state estimation and control posing a significant challenge for traditional methods. In this paper, we show that it is possible to learn fabric folding skills in only an hour of self-supervised real robot experience, without human supervision or simulation. Our approach relies on fully convolutional networks and the manipulation of visual inputs to exploit learned features, allowing us to create an expressive goal-conditioned pick and place policy that can be trained efficiently with real world robot data only. Folding skills are learned with only a sparse reward function and thus do not require reward function engineering, merely an image of the goal configuration. We demonstrate our method on a set of towel-folding tasks, and show that our approach is able to discover sequential folding strategies, purely from trial-and-error. We achieve state-of-the-art results without the need for demonstrations or simulation, used in prior approaches. Videos available at: https://sites.google.com/view/learningtofold ","Robert Lee, Daniel Ward, Akansel Cosgun, Vibhavari Dasagi, Peter   Corke, Jurgen Leitner",,,11,
Efficient reinforcement learning control for continuum robots based on   Inexplicit Prior Knowledge,"  Compared to rigid robots that are generally studied in reinforcement learning, the physical characteristics of some sophisticated robots such as soft or continuum robots are higher complicated. Moreover, recent reinforcement learning methods are data-inefficient and can not be directly deployed to the robot without simulation. In this paper, we propose an efficient reinforcement learning method based on inexplicit prior knowledge in response to such problems. We first corroborate the method by simulation and employed directly in the real world. By using our method, we can achieve active visual tracking and distance maintenance of a tendon-driven robot which will be critical in minimally invasive procedures. Codes are available at https://github.com/Skylark0924/TendonTrack. ","Junjia Liu, Jiaying Shou, Zhuang Fu, Hangfei Zhou, Rongli Xie, Jun   Zhang, Jian Fei and Yanna Zhao",,,11,
Alternating Minimization Based Trajectory Generation for Quadrotor   Aggressive Flight,"  With much research has been conducted into trajectory planning for quadrotors, planning with spatial and temporal optimal trajectories in real-time is still challenging. In this paper, we propose a framework for generating large-scale piecewise polynomial trajectories for aggressive autonomous flights, with highlights on its superior computational efficiency and simultaneous spatial-temporal optimality. Exploiting the implicitly decoupled structure of the planning problem, we conduct alternating minimization between boundary conditions and time durations of trajectory pieces. In each minimization phase, we leverage the algebraic convenience of the sub-problem to escape poor local minima and achieve the lowest time consumption. Theoretical analysis for the global/local convergence rate of our proposed method is provided. Moreover, based on polynomial theory, an extremely fast feasibility check method is designed for various kinds of constraints. By incorporating the method into our alternating structure, a constrained minimization algorithm is constructed to optimize trajectories on the premise of feasibility. Benchmark evaluation shows that our algorithm outperforms state-of-the-art methods regarding efficiency, optimality, and scalability. Aggressive flight experiments in a limited space with dense obstacles are presented to demonstrate the performance of the proposed algorithm. We release our implementation as an open-source ros-package. ","Zhepei Wang, Xin Zhou, Chao Xu, Jian Chu, and Fei Gao",,,11,
EU Long-term Dataset with Multiple Sensors for Autonomous Driving,"  The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community. ","Zhi Yan, Li Sun, Tomas Krajnik, and Yassine Ruichek",,,11,
AIR-Act2Act: Human-human interaction dataset for teaching non-verbal   social behaviors to robots,"  To better interact with users, a social robot should understand the users' behavior, infer the intention, and respond appropriately. Machine learning is one way of implementing robot intelligence. It provides the ability to automatically learn and improve from experience instead of explicitly telling the robot what to do. Social skills can also be learned through watching human-human interaction videos. However, human-human interaction datasets are relatively scarce to learn interactions that occur in various situations. Moreover, we aim to use service robots in the elderly-care domain; however, there has been no interaction dataset collected for this domain. For this reason, we introduce a human-human interaction dataset for teaching non-verbal social behaviors to robots. It is the only interaction dataset that elderly people have participated in as performers. We recruited 100 elderly people and two college students to perform 10 interactions in an indoor environment. The entire dataset has 5,000 interaction samples, each of which contains depth maps, body indexes and 3D skeletal data that are captured with three Microsoft Kinect v2 cameras. In addition, we provide the joint angles of a humanoid NAO robot which are converted from the human behavior that robots need to learn. The dataset and useful python scripts are available for download at https://github.com/ai4r/AIR-Act2Act. It can be used to not only teach social skills to robots but also benchmark action recognition algorithms. ","Woo-Ri Ko, Minsu Jang, Jaeyeon Lee and Jaehong Kim",,,11,
Hybrid Visual Servoing Tracking Control of Uncalibrated Robotic Systems   for Dynamic Dwarf Culture Orchards Harvest,"  The paper is concerned with the dynamic tracking problem of SNAP orchards harvesting robots in the presence of multiple uncalibrated model parameters in the application of dwarf culture orchards harvest. A new hybrid visual servoing adaptive tracking controller and three adaptive laws are proposed to guarantee harvesting robots to finish the dynamic harvesting task and the adaption to unknown parameters including camera intrinsic and extrinsic model and robot dynamics. By the Lyapunov theory, asymptotic convergence of the closed-loop system with the proposed control scheme is rigorously proven. Experimental and simulation results have been conducted to verify the performance of the proposed control scheme. The results demonstrate its effectiveness and superiority. ",Tao Li and Quan Qiu and Chunjiang Zhao,,,11,
XTDrone: A Customizable Multi-Rotor UAVs Simulation Platform,"  A customizable multi-rotor UAVs simulation platform based on ROS, Gazebo and PX4 is presented. The platform, which is called XTDrone, integrates dynamic models, sensor models, control algorithm, state estimation algorithm, and 3D scenes. The platform supports multi UAVs and other robots. The platform is modular and each module can be modified, which means that users can test its own algorithm, such as SLAM, object detection, motion planning, attitude control, multi-UAV cooperation, and cooperation with other robots on the platform. The platform runs in lockstep, so the simulation speed can be adjusted according to the computer performance. In this paper, two cases, evaluating different visual SLAM algorithm and realizing UAV formation, are shown to demonstrate how the platform works. ","Kun Xiao, Shaochang Tan, Guohui Wang, Xueyan An, Xiang Wang, Xiangke   Wang",,,11,
Guided Uncertainty-Aware Policy Optimization: Combining Learning and   Model-Based Strategies for Sample-Efficient Policy Learning,"  Traditional robotic approaches rely on an accurate model of the environment, a detailed description of how to perform the task, and a robust perception system to keep track of the current state. On the other hand, reinforcement learning approaches can operate directly from raw sensory inputs with only a reward signal to describe the task, but are extremely sample-inefficient and brittle. In this work, we combine the strengths of model-based methods with the flexibility of learning-based methods to obtain a general method that is able to overcome inaccuracies in the robotics perception/actuation pipeline, while requiring minimal interactions with the environment. This is achieved by leveraging uncertainty estimates to divide the space in regions where the given model-based policy is reliable, and regions where it may have flaws or not be well defined. In these uncertain regions, we show that a locally learned-policy can be used directly with raw sensory inputs. We test our algorithm, Guided Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing peg insertion. Videos are available at https://sites.google.com/view/guapo-rl ","Michelle A. Lee, Carlos Florensa, Jonathan Tremblay, Nathan Ratliff,   Animesh Garg, Fabio Ramos, Dieter Fox",,,11,
A Lobster-inspired Hybrid Actuator With Rigid and Soft Components,"  Soft actuators have drawn significant attention from researchers with an inherently compliant design to address the safety issues in physical human-robot interactions. However, they are also vulnerable and pose new challenges in the design, fabrication, and analysis due to their inherent material softness. In this paper, a novel hybrid actuator design is presented with bio-inspirations from the lobster, or crustaceans in a broader perspective. We enclose a soft chamber with rectangular cross-section using a series of articulated rigid shells to produce bending under pneumatic input. By mimicking the shell pattern of lobsters' abdomen, foldable rigid shells are designed to provide the soft actuator with full protection throughout the motion range. The articulation of the rigid shells predefines the actuator's bending motions. As a result, the proposed design enables one to analyze this hybrid actuator with simplified quasi-static models and rigid-body kinematics, which are further validated by mechanical tests. This paper demonstrates that the proposed hybrid actuator design is capable of bridging the major design drawbacks of the entirely rigid and soft robots while preserving their engineering merits in performance. ","Yaohui Chen, Sing Le, Qiao Chu Tan, Oscar Lau, Chaoyang Song",,,11,
Refined Analysis of Asymptotically-Optimal Kinodynamic Planning in the   State-Cost Space,"  We present a novel analysis of AO-RRT: a tree-based planner for motion planning with kinodynamic constraints, originally described by Hauser and Zhou (AO-X, 2016). AO-RRT explores the state-cost space and has been shown to efficiently obtain high-quality solutions in practice without relying on the availability of a computationally-intensive two-point boundary-value solver. Our main contribution is an optimality proof for the single-tree version of the algorithm---a variant that was not analyzed before. Our proof only requires a mild and easily-verifiable set of assumptions on the problem and system: Lipschitz-continuity of the cost function and the dynamics. In particular, we prove that for any system satisfying these assumptions, any trajectory having a piecewise-constant control function and positive clearance from the obstacles can be approximated arbitrarily well by a trajectory found by AO-RRT. We also discuss practical aspects of AO-RRT and present experimental comparisons of variants of the algorithm. ","Michal Kleinbort, Edgar Granados, Kiril Solovey, Riccardo Bonalli,   Kostas E. Bekris, Dan Halperin",,,11,
Rolling in the Deep -- Hybrid Locomotion for Wheeled-Legged Robots using   Online Trajectory Optimization,"  Wheeled-legged robots have the potential for highly agile and versatile locomotion. The combination of legs and wheels might be a solution for any real-world application requiring rapid, and long-distance mobility skills on challenging terrain. In this paper, we present an online trajectory optimization framework for wheeled quadrupedal robots capable of executing hybrid walking-driving locomotion strategies. By breaking down the optimization problem into a wheel and base trajectory planning, locomotion planning for high dimensional wheeled-legged robots becomes more tractable, can be solved in real-time on-board in a model predictive control fashion, and becomes robust against unpredicted disturbances. The reference motions are tracked by a hierarchical whole-body controller that sends torque commands to the robot. Our approach is verified on a quadrupedal robot with non-steerable wheels attached to its legs. The robot performs hybrid locomotion with a great variety of gait sequences on rough terrain. Besides, we validated the robotic platform at the Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge, where the robot rapidly mapped, navigated and explored dynamic underground environments. ","Marko Bjelonic, Prajish K. Sankar, C. Dario Bellicoso, Heike Vallery,   Marco Hutter",,,11,
"What is the Best Grid-Map for Self-Driving Cars Localization? An   Evaluation under Diverse Types of Illumination, Traffic, and Environment","  The localization of self-driving cars is needed for several tasks such as keeping maps updated, tracking objects, and planning. Localization algorithms often take advantage of maps for estimating the car pose. Since maintaining and using several maps is computationally expensive, it is important to analyze which type of map is more adequate for each application. In this work, we provide data for such analysis by comparing the accuracy of a particle filter localization when using occupancy, reflectivity, color, or semantic grid maps. To the best of our knowledge, such evaluation is missing in the literature. For building semantic and colour grid maps, point clouds from a Light Detection and Ranging (LiDAR) sensor are fused with images captured by a front-facing camera. Semantic information is extracted from images with a deep neural network. Experiments are performed in varied environments, under diverse conditions of illumination and traffic. Results show that occupancy grid maps lead to more accurate localization, followed by reflectivity grid maps. In most scenarios, the localization with semantic grid maps kept the position tracking without catastrophic losses, but with errors from 2 to 3 times bigger than the previous. Colour grid maps led to inaccurate and unstable localization even using a robust metric, the entropy correlation coefficient, for comparing online data and the map. ","Filipe Mutz, Thiago Oliveira-Santos, Avelino Forechi, Karin S. Komati,   Claudine Badue, Felipe M. G. Fran\c{c}a, Alberto F. De Souza",,,11,
A Novel Variable Stiffness Soft Robotic Gripper,"  We propose a novel tri-fingered soft robotic gripper with decoupled stiffness and shape control capability for performing adaptive grasping with minimum system complexity. The proposed soft fingers adaptively conform to object shapes facilitating the handling of objects of different types, shapes, and sizes. Each soft gripper finger has an inextensible articulable backbone and is actuated by pneumatic muscles. We derive a kinematic model of the gripper and use an empirical approach to map input pressures to stiffness and bending deformation of fingers. We use these mappings to achieve decoupled stiffness and shape control. We conduct tests to quantify the ability to hold objects as the gripper changes orientation, the ability to maintain the grasping status as the gripper moves, and the amount of force required to release the object from the gripped fingers, respectively. The results validate the proposed gripper's performance and show how stiffness control can improve the grasping quality. ",Dimuthu D. Arachchige and Yue Chen and Ian D. Walker and Isuru S.   Godage,,,11,
An autonomous swarm of micro flying robots with range-based relative   localization,"  Accurate relative localization is an important requirement for a swarm of robots, especially when performing a cooperative task. This paper presents an autonomous multi-robot system equipped with a fully onboard range-based relative positioning system. It uses onboard sensing of velocity, yaw rate, and height as inputs, and then estimates the relative position of other robots in its own body frame by fusing these quantities with ranging measurements obtained from onboard ultra wide-band (UWB) antennas. Simulations concisely show the high precision, efficiency, and stability of the proposed localization method. Experiments are conducted on a team of 5 Crazyflie 2.0 quadrotors, demonstrating autonomous formation flight and pattern formation, and coordinated flight through a window. All results indicate the effectiveness of the proposed relative positioning method for multi-robot systems. ","Shushuai Li, Mario Coppola, Christophe De Wagter and Guido C. H. E. de   Croon",,,11,
Amodal 3D Reconstruction for Robotic Manipulation via Stability and   Connectivity,"  Learning-based 3D object reconstruction enables single- or few-shot estimation of 3D object models. For robotics, this holds the potential to allow model-based methods to rapidly adapt to novel objects and scenes. Existing 3D reconstruction techniques optimize for visual reconstruction fidelity, typically measured by chamfer distance or voxel IOU. We find that when applied to realistic, cluttered robotics environments, these systems produce reconstructions with low physical realism, resulting in poor task performance when used for model-based control. We propose ARM, an amodal 3D reconstruction system that introduces (1) a stability prior over object shapes, (2) a connectivity prior, and (3) a multi-channel input representation that allows for reasoning over relationships between groups of objects. By using these priors over the physical properties of objects, our system improves reconstruction quality not just by standard visual metrics, but also performance of model-based control on a variety of robotics manipulation tasks in challenging, cluttered environments. Code is available at github.com/wagnew3/ARM. ","William Agnew, Christopher Xie, Aaron Walsman, Octavian Murad, Caelen   Wang, Pedro Domingos, Siddhartha Srinivasa",,,11,
Linear Features Observation Model for Autonomous Vehicle Localization,"  Precise localization is a core ability of an autonomous vehicle. It is a prerequisite for motion planning and execution. The well-established localization approaches such as Kalman and particle filters require a probabilistic observation model allowing to compute a likelihood of measurement given a system state vector, usually vehicle pose, and a map. The higher precision of the localization system may be achieved through the development of a more sophisticated observation model considering various measurement error sources. Meanwhile model needs to be simple to be computable in real-time. This paper proposes an observation model for visually detected linear features. Examples of such features include, but not limited to, road markings and road boundaries. The proposed observation model depicts two core detection error sources: shift error and angular error. It also considers the probability of false-positive detection. The structure of the proposed model allows precomputing and incorporating the measurement error directly into the map represented by a multichannel digital image. Measurement error precomputation and storing the map as an image speeds up observation likelihood computation and in turn localization system. The experimental evaluation on real autonomous vehicle demonstrates that the proposed model allows for precise and reliable localization in a variety of scenarios. ","Oleg Shipitko, Vladislav Kibalov and Maxim Abramov",,,11,
Image Stylization for Robust Features,"  Local features that are robust to both viewpoint and appearance changes are crucial for many computer vision tasks. In this work we investigate if photorealistic image stylization improves robustness of local features to not only day-night, but also weather and season variations. We show that image stylization in addition to color augmentation is a powerful method of learning robust features. We evaluate learned features on visual localization benchmarks, outperforming state of the art baseline models despite training without ground-truth 3D correspondences using synthetic homographies only.   We use trained feature networks to compete in Long-Term Visual Localization and Map-based Localization for Autonomous Driving challenges achieving competitive scores. ","Iaroslav Melekhov, Gabriel J. Brostow, Juho Kannala, Daniyar   Turmukhambetov",,,11,
Two-stream Encoder-Decoder Network for Localizing Image Forgeries,"  This paper proposes a novel two-stream encoder-decoder network, which utilizes both the high-level and the low-level image features for precisely localizing forged regions in a manipulated image. This is motivated from the fact that the forgery creation process generally introduces both the high-level artefacts (e.g. unnatural contrast) and the low-level artefacts (e.g. noise inconsistency) to the forged images. In the proposed two-stream network, one stream learns the low-level manipulation-related features in the encoder side by extracting noise residuals through a set of high-pass filters in the first layer of the encoder network. In the second stream, the encoder learns the high-level image manipulation features from the input image RGB values. The coarse feature maps of both the encoders are upsampled by their corresponding decoder network to produce dense feature maps. The dense feature maps of the two streams are concatenated and fed to a final convolutional layer with sigmoidal activation to produce pixel-wise prediction. We have carried out experimental analysis on multiple standard forensics datasets to evaluate the performance of the proposed method. The experimental results show the efficacy of the proposed method with respect to the state-of-the-art. ",Aniruddha Mazumdar and Prabin Kumar Bora,,,11,
PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time,"  Marker-less 3D human motion capture from a single colour camera has seen significant progress. However, it is a very challenging and severely ill-posed problem. In consequence, even the most accurate state-of-the-art approaches have significant limitations. Purely kinematic formulations on the basis of individual joints or skeletons, and the frequent frame-wise reconstruction in state-of-the-art methods greatly limit 3D accuracy and temporal stability compared to multi-view or marker-based motion capture. Further, captured 3D poses are often physically incorrect and biomechanically implausible, or exhibit implausible environment interactions (floor penetration, foot skating, unnatural body leaning and strong shifting in depth), which is problematic for any use case in computer graphics. We, therefore, present PhysCap, the first algorithm for physically plausible, real-time and marker-less human 3D motion capture with a single colour camera at 25 fps. Our algorithm first captures 3D human poses purely kinematically. To this end, a CNN infers 2D and 3D joint positions, and subsequently, an inverse kinematics step finds space-time coherent joint angles and global 3D pose. Next, these kinematic reconstructions are used as constraints in a real-time physics-based pose optimiser that accounts for environment constraints (e.g., collision handling and floor placement), gravity, and biophysical plausibility of human postures. Our approach employs a combination of ground reaction force and residual force for plausible root control, and uses a trained neural network to detect foot contact events in images. Our method captures physically plausible and temporally stable global 3D human motion, without physically implausible postures, floor penetrations or foot skating, from video in real time and in general scenes. The video is available at http://gvv.mpi-inf.mpg.de/projects/PhysCap ","Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt",,,11,
ULSAM: Ultra-Lightweight Subspace Attention Module for Compact   Convolutional Neural Networks,"  The capability of the self-attention mechanism to model the long-range dependencies has catapulted its deployment in vision models. Unlike convolution operators, self-attention offers infinite receptive field and enables compute-efficient modeling of global dependencies. However, the existing state-of-the-art attention mechanisms incur high compute and/or parameter overheads, and hence unfit for compact convolutional neural networks (CNNs). In this work, we propose a simple yet effective ""Ultra-Lightweight Subspace Attention Mechanism"" (ULSAM), which infers different attention maps for each feature map subspace. We argue that leaning separate attention maps for each feature subspace enables multi-scale and multi-frequency feature representation, which is more desirable for fine-grained image classification. Our method of subspace attention is orthogonal and complementary to the existing state-of-the-arts attention mechanisms used in vision models. ULSAM is end-to-end trainable and can be deployed as a plug-and-play module in the pre-existing compact CNNs. Notably, our work is the first attempt that uses a subspace attention mechanism to increase the efficiency of compact CNNs. To show the efficacy of ULSAM, we perform experiments with MobileNet-V1 and MobileNet-V2 as backbone architectures on ImageNet-1K and three fine-grained image classification datasets. We achieve $\approx$13% and $\approx$25% reduction in both the FLOPs and parameter counts of MobileNet-V2 with a 0.27% and more than 1% improvement in top-1 accuracy on the ImageNet-1K and fine-grained image classification datasets (respectively). Code and trained models are available at https://github.com/Nandan91/ULSAM. ","Rajat Saini, Nandan Kumar Jha, Bedanta Das, Sparsh Mittal, C. Krishna   Mohan",,,11,
Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep   Visual Speech Recognition,"  Recent advances in deep learning have heightened interest among researchers in the field of visual speech recognition (VSR). Currently, most existing methods equate VSR with automatic lip reading, which attempts to recognise speech by analysing lip motion. However, human experience and psychological studies suggest that we do not always fix our gaze at each other's lips during a face-to-face conversation, but rather scan the whole face repetitively. This inspires us to revisit a fundamental yet somehow overlooked problem: can VSR models benefit from reading extraoral facial regions, i.e. beyond the lips? In this paper, we perform a comprehensive study to evaluate the effects of different facial regions with state-of-the-art VSR models, including the mouth, the whole face, the upper face, and even the cheeks. Experiments are conducted on both word-level and sentence-level benchmarks with different characteristics. We find that despite the complex variations of the data, incorporating information from extraoral facial regions, even the upper face, consistently benefits VSR performance. Furthermore, we introduce a simple yet effective method based on Cutout to learn more discriminative features for face-based VSR, hoping to maximise the utility of information encoded in different facial regions. Our experiments show obvious improvements over existing state-of-the-art methods that use only the lip region as inputs, a result we believe would probably provide the VSR community with some new and exciting insights. ","Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen",,,11,
Rethinking Object Detection in Retail Stores,"  The convention standard for object detection uses a bounding box to represent each individual object instance. However, it is not practical in the industry-relevant applications in the context of warehouses due to severe occlusions among groups of instances of the same categories. In this paper, we propose a new task, ie, simultaneously object localization and counting, abbreviated as Locount, which requires algorithms to localize groups of objects of interest with the number of instances. However, there does not exist a dataset or benchmark designed for such a task. To this end, we collect a large-scale object localization and counting dataset with rich annotations in retail stores, which consists of 50,394 images with more than 1.9 million object instances in 140 categories. Together with this dataset, we provide a new evaluation protocol and divide the training and testing subsets to fairly evaluate the performance of algorithms for Locount, developing a new benchmark for the Locount task. Moreover, we present a cascaded localization and counting network as a strong baseline, which gradually classifies and regresses the bounding boxes of objects with the predicted numbers of instances enclosed in the bounding boxes, trained in an end-to-end manner. Extensive experiments are conducted on the proposed dataset to demonstrate its significance and the analysis discussions on failure cases are provided to indicate future directions. Dataset is available at https://isrc.iscas.ac.cn/gitlab/research/locount-dataset. ","Yuanqiang Cai, Longyin Wen, Libo Zhang, Dawei Du, Weiqiang Wang,   Pengfei Zhu",,,11,
Classifying All Interacting Pairs in a Single Shot,"  In this paper, we introduce a novel human interaction detection approach, based on CALIPSO (Classifying ALl Interacting Pairs in a Single shOt), a classifier of human-object interactions. This new single-shot interaction classifier estimates interactions simultaneously for all human-object pairs, regardless of their number and class. State-of-the-art approaches adopt a multi-shot strategy based on a pairwise estimate of interactions for a set of human-object candidate pairs, which leads to a complexity depending, at least, on the number of interactions or, at most, on the number of candidate pairs. In contrast, the proposed method estimates the interactions on the whole image. Indeed, it simultaneously estimates all interactions between all human subjects and object targets by performing a single forward pass throughout the image. Consequently, it leads to a constant complexity and computation time independent of the number of subjects, objects or interactions in the image. In detail, interaction classification is achieved on a dense grid of anchors thanks to a joint multi-task network that learns three complementary tasks simultaneously: (i) prediction of the types of interaction, (ii) estimation of the presence of a target and (iii) learning of an embedding which maps interacting subject and target to a same representation, by using a metric learning strategy. In addition, we introduce an object-centric passive-voice verb estimation which significantly improves results. Evaluations on the two well-known Human-Object Interaction image datasets, V-COCO and HICO-DET, demonstrate the competitiveness of the proposed method (2nd place) compared to the state-of-the-art while having constant computation time regardless of the number of objects and interactions in the image. ",Sanaa Chafik and Astrid Orcesi and Romaric Audigier and Bertrand   Luvison,,,11,
Procrustean Regression Networks: Learning 3D Structure of Non-Rigid   Objects from 2D Annotations,"  We propose a novel framework for training neural networks which is capable of learning 3D information of non-rigid objects when only 2D annotations are available as ground truths. Recently, there have been some approaches that incorporate the problem setting of non-rigid structure-from-motion (NRSfM) into deep learning to learn 3D structure reconstruction. The most important difficulty of NRSfM is to estimate both the rotation and deformation at the same time, and previous works handle this by regressing both of them. In this paper, we resolve this difficulty by proposing a loss function wherein the suitable rotation is automatically determined. Trained with the cost function consisting of the reprojection error and the low-rank term of aligned shapes, the network learns the 3D structures of such objects as human skeletons and faces during the training, whereas the testing is done in a single-frame basis. The proposed method can handle inputs with missing entries and experimental results validate that the proposed framework shows superior reconstruction performance to the state-of-the-art method on the Human 3.6M, 300-VW, and SURREAL datasets, even though the underlying network structure is very simple. ","Sungheon Park, Minsik Lee, Nojun Kwak",,,11,
Effortless Deep Training for Traffic Sign Detection Using Templates and   Arbitrary Natural Images,"  Deep learning has been successfully applied to several problems related to autonomous driving. Often, these solutions rely on large networks that require databases of real image samples of the problem (i.e., real world) for proper training. The acquisition of such real-world data sets is not always possible in the autonomous driving context, and sometimes their annotation is not feasible (e.g., takes too long or is too expensive). Moreover, in many tasks, there is an intrinsic data imbalance that most learning-based methods struggle to cope with. It turns out that traffic sign detection is a problem in which these three issues are seen altogether. In this work, we propose a novel database generation method that requires only (i) arbitrary natural images, i.e., requires no real image from the domain of interest, and (ii) templates of the traffic signs, i.e., templates synthetically created to illustrate the appearance of the category of a traffic sign. The effortlessly generated training database is shown to be effective for the training of a deep detector (such as Faster R-CNN) on German traffic signs, achieving 95.66% of mAP on average. In addition, the proposed method is able to detect traffic signs with an average precision, recall and F1-score of about 94%, 91% and 93%, respectively. The experiments surprisingly show that detectors can be trained with simple data generation methods and without problem domain data for the background, which is in the opposite direction of the common sense for deep learning. ","Lucas Tabelini Torres, Thiago M. Paix\~ao, Rodrigo F. Berriel, Alberto   F. De Souza, Claudine Badue, Nicu Sebe and Thiago Oliveira-Santos",,,11,
URIE: Universal Image Enhancement for Visual Recognition in the Wild,"  Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded. ","Taeyoung Son, Juwon Kang, Namyup Kim, Sunghyun Cho and Suha Kwak",,,11,
Minimal Solvers for Rectifying from Radially-Distorted Scales and Change   of Scales,"  This paper introduces the first minimal solvers that jointly estimate lens distortion and affine rectification from the image of rigidly-transformed coplanar features. The solvers work on scenes without straight lines and, in general, relax strong assumptions about scene content made by the state of the art. The proposed solvers use the affine invariant that coplanar repeats have the same scale in rectified space. The solvers are separated into two groups that differ by how the equal scale invariant of rectified space is used to place constraints on the lens undistortion and rectification parameters. We demonstrate a principled approach for generating stable minimal solvers by the Gr\""obner basis method, which is accomplished by sampling feasible monomial bases to maximize numerical stability. Synthetic and real-image experiments confirm that the proposed solvers demonstrate superior robustness to noise compared to the state of the art. Accurate rectifications on imagery taken with narrow to fisheye field-of-view lenses demonstrate the wide applicability of the proposed method. The method is fully automatic. ","James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman,   Ond\v{r}ej Chum",,,11,
Cross-Identity Motion Transfer for Arbitrary Objects through   Pose-Attentive Video Reassembling,"  We propose an attention-based networks for transferring motions between arbitrary objects. Given a source image(s) and a driving video, our networks animate the subject in the source images according to the motion in the driving video. In our attention mechanism, dense similarities between the learned keypoints in the source and the driving images are computed in order to retrieve the appearance information from the source images. Taking a different approach from the well-studied warping based models, our attention-based model has several advantages. By reassembling non-locally searched pieces from the source contents, our approach can produce more realistic outputs. Furthermore, our system can make use of multiple observations of the source appearance (e.g. front and sides of faces) to make the results more accurate. To reduce the training-testing discrepancy of the self-supervised learning, a novel cross-identity training scheme is additionally introduced. With the training scheme, our networks is trained to transfer motions between different subjects, as in the real testing scenario. Experimental results validate that our method produces visually pleasing results in various object domains, showing better performances compared to previous works. ","Subin Jeon, Seonghyeon Nam, Seoung Wug Oh, Seon Joo Kim",,,11,
Co-training for On-board Deep Object Detection,"  Providing ground truth supervision to train visual models has been a bottleneck over the years, exacerbated by domain shifts which degenerate the performance of such models. This was the case when visual tasks relied on handcrafted features and shallow machine learning and, despite its unprecedented performance gains, the problem remains open within the deep learning paradigm due to its data-hungry nature. Best performing deep vision-based object detectors are trained in a supervised manner by relying on human-labeled bounding boxes which localize class instances (i.e.objects) within the training images.Thus, object detection is one of such tasks for which human labeling is a major bottleneck. In this paper, we assess co-training as a semi-supervised learning method for self-labeling objects in unlabeled images, so reducing the human-labeling effort for developing deep object detectors. Our study pays special attention to a scenario involving domain shift; in particular, when we have automatically generated virtual-world images with object bounding boxes and we have real-world images which are unlabeled. Moreover, we are particularly interested in using co-training for deep object detection in the context of driver assistance systems and/or self-driving vehicles. Thus, using well-established datasets and protocols for object detection in these application contexts, we will show how co-training is a paradigm worth to pursue for alleviating object labeling, working both alone and together with task-agnostic domain adaptation. ",Gabriel Villalonga and Antonio M. Lopez,,,11,
Unsupervised Multiple Person Tracking using AutoEncoder-Based Lifted   Multicuts,"  Multiple Object Tracking (MOT) is a long-standing task in computer vision. Current approaches based on the tracking by detection paradigm either require some sort of domain knowledge or supervision to associate data correctly into tracks. In this work, we present an unsupervised multiple object tracking approach based on visual features and minimum cost lifted multicuts. Our method is based on straight-forward spatio-temporal cues that can be extracted from neighboring frames in an image sequences without superivison. Clustering based on these cues enables us to learn the required appearance invariances for the tracking task at hand and train an autoencoder to generate suitable latent representation. Thus, the resulting latent representations can serve as robust appearance cues for tracking even over large temporal distances where no reliable spatio-temporal features could be extracted. We show that, despite being trained without using the provided annotations, our model provides competitive results on the challenging MOT Benchmark for pedestrian tracking. ","Kalun Ho, Janis Keuper, Margret Keuper",,,11,
On the Realization and Analysis of Circular Harmonic Transforms for   Feature Detection,"  Circular-harmonic spectra are a compact representation of local image features in two dimensions. It is well known that the computational complexity of such transforms is greatly reduced when polar separability is exploited in steerable filter-banks. Further simplifications are possible when Cartesian separability is incorporated using the radial apodization (i.e. weight, window, or taper) described here, as a consequence of the Laguerre/Hermite correspondence over polar/Cartesian coordinates. The chosen form also mitigates undesirable discretization artefacts due to angular aliasing. The possible utility of circular-harmonic spectra for the description of simple features is illustrated using real data from an airborne electro-optic sensor. The spectrum is deployed in a test-statistic to detect and characterize corners of arbitrary angle and orientation (i.e. wedges). The test-statistic considers uncertainty due to finite sampling and clutter/noise. ",Hugh L Kennedy,,,11,
A Single Video Super-Resolution GAN for Multiple Downsampling Operators   based on Pseudo-Inverse Image Formation Models,"  The popularity of high and ultra-high definition displays has led to the need for methods to improve the quality of videos already obtained at much lower resolutions. Current Video Super-Resolution methods are not robust to mismatch between training and testing degradation models since they are trained against a single degradation model (usually bicubic downsampling). This causes their performance to deteriorate in real-life applications. At the same time, the use of only the Mean Squared Error during learning causes the resulting images to be too smooth. In this work we propose a new Convolutional Neural Network for video super resolution which is robust to multiple degradation models. During training, which is performed on a large dataset of scenes with slow and fast motions, it uses the pseudo-inverse image formation model as part of the network architecture in conjunction with perceptual losses, in addition to a smoothness constraint that eliminates the artifacts originating from these perceptual losses. The experimental validation shows that our approach outperforms current state-of-the-art methods and is robust to multiple degradations. ",Santiago L\'opez-Tapia and Alice Lucas and Rafael Molina and Aggelos   K. Katsaggelos,,,11,
Extreme Value Preserving Networks,"  Recent evidence shows that convolutional neural networks (CNNs) are biased towards textures so that CNNs are non-robust to adversarial perturbations over textures, while traditional robust visual features like SIFT (scale-invariant feature transforms) are designed to be robust across a substantial range of affine distortion, addition of noise, etc with the mimic of human perception nature. This paper aims to leverage good properties of SIFT to renovate CNN architectures towards better accuracy and robustness. We borrow the scale-space extreme value idea from SIFT, and propose extreme value preserving networks (EVPNets). Experiments demonstrate that EVPNets can achieve similar or better accuracy than conventional CNNs, while achieving much better robustness on a set of adversarial attacks (FGSM,PGD,etc) even without adversarial training. ","Mingjie Sun, Jianguo Li, Changshui Zhang",,,11,
Scale Invariant Fully Convolutional Network: Detecting Hands Efficiently,"  Existing hand detection methods usually follow the pipeline of multiple stages with high computation cost, i.e., feature extraction, region proposal, bounding box regression, and additional layers for rotated region detection. In this paper, we propose a new Scale Invariant Fully Convolutional Network (SIFCN) trained in an end-to-end fashion to detect hands efficiently. Specifically, we merge the feature maps from high to low layers in an iterative way, which handles different scales of hands better with less time overhead comparing to concatenating them simply. Moreover, we develop the Complementary Weighted Fusion (CWF) block to make full use of the distinctive features among multiple layers to achieve scale invariance. To deal with rotated hand detection, we present the rotation map to get rid of complex rotation and derotation layers. Besides, we design the multi-scale loss scheme to accelerate the training process significantly by adding supervision to the intermediate layers of the network. Compared with the state-of-the-art methods, our algorithm shows comparable accuracy and runs a 4.23 times faster speed on the VIVA dataset and achieves better average precision on Oxford hand detection dataset at a speed of 62.5 fps. ","Dan Liu, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Feiyue Huang,   Siwei Lyu",,,11,
Finding Action Tubes with a Sparse-to-Dense Framework,"  The task of spatial-temporal action detection has attracted increasing attention among researchers. Existing dominant methods solve this problem by relying on short-term information and dense serial-wise detection on each individual frames or clips. Despite their effectiveness, these methods showed inadequate use of long-term information and are prone to inefficiency. In this paper, we propose for the first time, an efficient framework that generates action tube proposals from video streams with a single forward pass in a sparse-to-dense manner. There are two key characteristics in this framework: (1) Both long-term and short-term sampled information are explicitly utilized in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS) is designed to effectively approximate the tube output while keeping the system tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and UCFSports benchmark datasets, achieving promising results that are competitive to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our framework about 7.6 times more efficient than the nearest competitor. ","Yuxi Li, Weiyao Lin, Tao Wang, John See, Rui Qian, Ning Xu, Limin   Wang, Shugong Xu",,,11,
NMS by Representative Region: Towards Crowded Pedestrian Detection by   Proposal Pairing,"  Although significant progress has been made in pedestrian detection recently, pedestrian detection in crowded scenes is still challenging. The heavy occlusion between pedestrians imposes great challenges to the standard Non-Maximum Suppression (NMS). A relative low threshold of intersection over union (IoU) leads to missing highly overlapped pedestrians, while a higher one brings in plenty of false positives. To avoid such a dilemma, this paper proposes a novel Representative Region NMS approach leveraging the less occluded visible parts, effectively removing the redundant boxes without bringing in many false positives. To acquire the visible parts, a novel Paired-Box Model (PBM) is proposed to simultaneously predict the full and visible boxes of a pedestrian. The full and visible boxes constitute a pair serving as the sample unit of the model, thus guaranteeing a strong correspondence between the two boxes throughout the detection pipeline. Moreover, convenient feature integration of the two boxes is allowed for the better performance on both full and visible pedestrian detection tasks. Experiments on the challenging CrowdHuman and CityPersons benchmarks sufficiently validate the effectiveness of the proposed approach on pedestrian detection in the crowded situation. ","Xin Huang, Zheng Ge, Zequn Jie and Osamu Yoshie",,,11,
Set It and Forget It! Turnkey ECC for Instant Integration,"  Historically, Elliptic Curve Cryptography (ECC) is an active field of applied cryptography where recent focus is on high speed, constant time, and formally verified implementations. While there are a handful of outliers where all these concepts join and land in real-world deployments, these are generally on a case-by-case basis: e.g.\ a library may feature such X25519 or P-256 code, but not for all curves. In this work, we propose and implement a methodology that fully automates the implementation, testing, and integration of ECC stacks with the above properties. We demonstrate the flexibility and applicability of our methodology by seamlessly integrating into three real-world projects: OpenSSL, Mozilla's NSS, and the GOST OpenSSL Engine, achieving roughly 9.5x, 4.5x, 13.3x, and 3.7x speedup on any given curve for key generation, key agreement, signing, and verifying, respectively. Furthermore, we showcase the efficacy of our testing methodology by uncovering flaws and vulnerabilities in OpenSSL, and a specification-level vulnerability in a Russian standard. Our work bridges the gap between significant applied cryptography research results and deployed software, fully automating the process. ","Dmitry Belyavsky, Billy Bob Brumley, Jes\'us-Javier Chi-Dom\'inguez,   Luis Rivera-Zamarripa, Igor Ustinov",,,11,
TurboCC: A Practical Frequency-Based Covert Channel With Intel Turbo   Boost,"  Covert channels are communication channels used by attackers to transmit information from a compromised system when the access control policy of the system does not allow doing so. Previous work has shown that CPU frequency scaling can be used as a covert channel to transmit information between otherwise isolated processes. Modern systems either try to save power or try to operate near their power limits in order to maximize performance, so they implement mechanisms to vary the frequency based on load. Existing covert channels based on this approach are either easily thwarted by software countermeasures or only work on completely idle systems. In this paper, we show how the automatic frequency scaling provided by Intel Turbo Boost can be used to construct a covert channel that is both hard to prevent without significant performance impact and can tolerate significant background system load. As Intel Turbo Boost selects the maximum CPU frequency based on the number of active cores, our covert channel modulates information onto the maximum CPU frequency by placing load on multiple additional CPU cores. Our prototype of the covert channel achieves a throughput of up to 61 bit/s on an idle system and up to 43 bit/s on a system with 25% utilization. ","Manuel Kalmbach, Mathias Gottschlag, Tim Schmidt, Frank Bellosa",,,11,
"Comments on the ""Generalized"" KLJN Key Exchanger with Arbitrary   Resistors: Power, Impedance, Security","  In (Nature) Science Report 5 (2015) 13653, Vadai, Mingesz and Gingl (VMG) introduce a new Kirchhoff-law-Johnson-noise (KLJN) secure key exchanger that operates with 4 arbitrary resistors (instead of 2 arbitrary resistance values forming 2 identical resistor pairs in the original system). They state that in this new, VMG-KLJN, non-equilibrium system with nonzero power flow, the security during the exchange of the two (HL and LH) bit values is as strong as in the original KLJN scheme. Moreover, they claim that, at practical conditions, their VMG-KLJN protocol ""supports more robust protection against attacks"". First, we investigate the power flow and thermal equilibrium issues of the VMG-KLJN system with 4 arbitrary resistors. Then we introduce a new KLJN protocol that allows the arbitrary choice of 3 resistors from the 4, while it still operates with zero power flow during the exchange of single bits by utilizing a specific value of the 4th resistor and a binary temperature set for the exchanged (HL and LH) bit values. Then we show that, in general, the KLJN schemes with more than 2 arbitrary resistors (including our new protocol mentioned above) are prone to 4 new passive attacks utilizing the parasitic capacitance and inductance in the cable, while the original KLJN scheme is naturally immune against these new attacks. The core of the security vulnerability exploited by these attacks is the different line resistances in the HL and LH cases. Therefore, on the contrary of the statement and claim cited above, the practical VMG-KLJN system is less secure than the original KLJN scheme. We introduce another 2, modified, non-equilibrium KLJN systems to eliminate the vulnerability against some - but not all - of these attacks. However the price for that is the loss of arbitrariness of the selection of the 4th resistor and the information leak still remains greater than zero. ","Shahriar Ferdous, Christiana Chamon, Laszlo B. Kish",,,11,
Addressing the Privacy Implications of Mixed Reality: A Regulatory   Approach,"  Mixed reality (MR) technologies are emerging into the mainstream with affordable devices like the Oculus Quest. These devices blend the physical and virtual in novel ways that blur the lines that exist in legal precedent, like those between speech and conduct. In this paper, we discuss the challenges of regulating immersive technologies, focusing on the potential for extensive data collection, and examine the trade-offs of three potential approaches to protecting data privacy in the context of mixed reality environments. ",Nicole Shadowen and Diane Hosfelt,,,11,
EvilCoder: Automated Bug Insertion,"  The art of finding software vulnerabilities has been covered extensively in the literature and there is a huge body of work on this topic. In contrast, the intentional insertion of exploitable, security-critical bugs has received little (public) attention yet. Wanting more bugs seems to be counterproductive at first sight, but the comprehensive evaluation of bug-finding techniques suffers from a lack of ground truth and the scarcity of bugs.   In this paper, we propose EvilCoder, a system to automatically find potentially vulnerable source code locations and modify the source code to be actually vulnerable. More specifically, we leverage automated program analysis techniques to find sensitive sinks which match typical bug patterns (e.g., a sensitive API function with a preceding sanity check), and try to find data-flow connections to user-controlled sources. We then transform the source code such that exploitation becomes possible, for example by removing or modifying input sanitization or other types of security checks. Our tool is designed to randomly pick vulnerable locations and possible modifications, such that it can generate numerous different vulnerabilities on the same software corpus. We evaluated our tool on several open-source projects such as for example libpng and vsftpd, where we found between 22 and 158 unique connected source-sink pairs per project. This translates to hundreds of potentially vulnerable data-flow paths and hundreds of bugs we can insert. We hope to support future bug-finding techniques by supplying freshly generated, bug-ridden test corpora so that such techniques can (finally) be evaluated and compared in a comprehensive and statistically meaningful way. ",Jannik Pewny and Thorsten Holz,,,11,
The Dark (and Bright) Side of IoT: Attacks and Countermeasures for   Identifying Smart Home Devices and Services,"  We present a new machine learning-based attack that exploits network patterns to detect the presence of smart IoT devices and running services in the WiFi radio spectrum. We perform an extensive measurement campaign of data collection, and we build up a model describing the traffic patterns characterizing three popular IoT smart home devices, i.e., Google Nest Mini, Amazon Echo, and Amazon Echo Dot. We prove that it is possible to detect and identify with overwhelming probability their presence and the services running by the aforementioned devices in a crowded WiFi scenario. This work proves that standard encryption techniques alone are not sufficient to protect the privacy of the end-user, since the network traffic itself exposes the presence of both the device and the associated service. While more work is required to prevent non-trusted third parties to detect and identify the user's devices, we introduce Eclipse, a technique to mitigate these types of attacks, which reshapes the traffic making the identification of the devices and the associated services similar to the random classification baseline. ","Ahmed Mohamed Hussain, Gabriele Oligeri, and Thiemo Voigt",,,11,
Blockchain-Enabled Internet-of-Things Platform for End-to-End Industrial   Hemp Supply Chain,"  After being legalized as an agricultural commodity by the 2018 U.S. Farm Bill, the Industrial Hemp production is moved from limited pilot programs to a regulated agriculture production system, and the market keeps increasing since then. However, Industrial Hemp Supply Chain (IHSC) faces several critical challenges, including high complexity and variability, data tampering, and lack of immutable information tracking system. In this paper, we develop a blockchain enabled internet-of-things (IoT) platform for IHSC to support process tracking, scalability, interoperability, and risk management. Basically, we create a two-layer blockchain with proof-of-authority based smart contract, which can leverage local authorities with state/federal regulators to ensure and accelerate quality control verification and regulatory compliance. Then, we develop a user-friendly mobile app so that each participant can use smart phone to real-time collect and upload their data to the cloud, and further share the process verification and tracking information through the blockchain network. Our study indicates the proposed platform can support interoperability, improve the efficiency of quality control verification, and ensure the safety of regulated IHSC. ","Keqi Wang, Wencen Wu, Wei Xie, Jinxiang Pei, Qi Zhou",,,11,
On the Role of Hash-based Signatures in Quantum-Safe Internet of Things:   Current Solutions and Future Directions,"  The Internet of Things (IoT) is gaining ground as a pervasive presence around us by enabling miniaturized things with computation and communication capabilities to collect, process, analyze, and interpret information. Consequently, trustworthy data act as fuel for applications that rely on the data generated by these things, for critical decision-making processes, data debugging, risk assessment, forensic analysis, and performance tuning. Currently, secure and reliable data communication in IoT is based on public-key cryptosystems such as Elliptic Curve Cryptosystem (ECC). Nevertheless, reliance on the security of de-facto cryptographic primitives is at risk of being broken by the impending quantum computers. Therefore, the transition from classical primitives to quantum-safe primitives is indispensable to ensure the overall security of data en route. In this paper, we investigate applications of one of the post-quantum signatures called Hash-Based Signature (HBS) schemes for the security of IoT devices in the quantum era. We give a succinct overview of the evolution of HBS schemes with emphasis on their construction parameters and associated strengths and weaknesses. Then, we outline the striking features of HBS schemes and their significance for the IoT security in the quantum era. We investigate the optimal selection of HBS in the IoT networks with respect to their performance-constrained requirements, resource-constrained nature, and design optimization objectives. In addition to ongoing standardization efforts, we also highlight current and future research and deployment challenges along with possible solutions. Finally, we outline the essential measures and recommendations that must be adopted by the IoT ecosystem while preparing for the quantum world. ","Sabah Suhail, Rasheed Hussain, Abid Khan, and Choong Seon Hong",,,11,
DarKnight: A Data Privacy Scheme for Training and Inference of Deep   Neural Networks,"  Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains. In this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy. We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance. ","Hanieh Hashemi, Yongqin Wang, Murali Annavaram",,,11,
"Privacy metrics for trajectory data based on k-anonymity, l-diversity   and t-closeness","  Mobility patterns of vehicles and people provide powerful data sources for location-based services such as fleet optimization and traffic flow analysis. These data, in particular pick-up/origin and drop-off/destination of vehicles, carry high privacy risk due to the semantic context spatial-temporal data encompass. Therefore, location-based service providers must balance the value they extract from trajectory data (utility), with protecting the privacy of the individuals behind those trajectories. In order to optimize this trade-off, privacy risks must be measured. Existing privacy measures for non-sequential data are not suitable for trajectory data and this paper provides an answer to this issue. We introduce a model of an adversary with imperfect knowledge that is based on the concept of equivalence classes. We then adapt standard privacy measures, i.e. k-anonymity, l-diversity and t-closeness to the peculiarities of trajectory data. Our approach to measuring trajectory privacy provides a general measure, independent of whether and what anonymization has been applied, which can be used to intuitively compare privacy of different datasets. This work is of high relevance to all service providers acting as processors of trajectory data who want to manage privacy risks and optimize the privacy vs. utility trade-off of their services. ",Stefano Bennati and Aleksandra Kovacevic,,,11,
TimingCamouflage+: Netlist Security Enhancement with Unconventional   Timing (with Appendix),"  With recent advances in reverse engineering, attackers can reconstruct a netlist to counterfeit chips by opening the die and scanning all layers of authentic chips. This relatively easy counterfeiting is made possible by the use of the standard simple clocking scheme, where all combinational blocks function within one clock period, so that a netlist of combinational logic gates and flip-flops is sufficient to duplicate a design. In this paper, we propose to invalidate the assumption that a netlist completely represents the function of a circuit with unconventional timing. With the introduced wave-pipelining paths, attackers have to capture gate and interconnect delays during reverse engineering, or to test a huge number of combinational paths to identify the wave-pipelining paths. To hinder the test-based attack, we construct false paths with wave-pipelining to increase the counterfeiting challenge. Experimental results confirm that wave-pipelining true paths and false paths can be constructed in benchmark circuits successfully with only a negligible cost, thus thwarting the potential attack techniques. ","Grace Li Zhang, Bing Li, Meng Li, Bei Yu, David Z. Pan, Michaela   Brunner, Georg Sigl and Ulf Schlichtmann",,,11,
Location-based Behavioral Authentication Using GPS Distance Coherence,"  Most of the current user authentication systems are based on PIN code, password, or biometrics traits which can have some limitations in usage and security. Lifestyle authentication has become a new research approach. A promising idea for it is to use the location history since it is relatively unique. Even when people are living in the same area or have occasional travel, it does not vary from day to day. For Global Positioning System (GPS) data, the previous work used the longitude, the latitude, and the timestamp as the features for the classification. In this paper, we investigate a new approach utilizing the distance coherence which can be extracted from the GPS itself without the need to require other information. We applied three ensemble classification RandomForest, ExtraTrees, and Bagging algorithms; and the experimental result showed that the approach can achieve 99.42%, 99.12%, and 99.25% of accuracy, respectively. ",Tran Phuong Thao,,,11,
Crucial and Redundant Shares and Compartments in Secret Sharing,"  Secret sharing is the well-known problem of splitting a secret into multiple shares, which are distributed to shareholders. When enough or the correct combination of shareholders work together the secret can be restored. We introduce two new types of shares to the secret sharing scheme of Shamir. Crucial shares are always needed for the reconstruction of the secret, whereas mutual redundant shares only help once in reconstructing the secret. Further, we extend the idea of crucial and redundant shares to a compartmented secret sharing scheme. The scheme, which is based on Shamir's, allows distributing the secret to different compartments, that hold shareholders themselves. In each compartment, another secret sharing scheme can be applied. Using the modifications the overall complexity of general access structures realized through compartmented secret sharing schemes can be reduced. This improves the computational complexity. Also, the number of shares can be reduced and some complex access structures can be realized with ideal amount and size of shares. ","Fabian Schillinger, Christian Schindelhauer",,,11,
Building secure distributed applications the DECENT way,"  Remote attestation (RA) authenticates code running in trusted execution environments (TEEs), allow trusted code to be deployed even on untrusted hosts. However, trust relationships established by one component in a distributed application may impact the security of other components, making it difficult to reason about the security of the application as a whole. Furthermore, traditional RA approaches interact badly with modern web service design, which tends to employ small interacting microservices, short session lifetimes, and little or no state.   This paper presents the Decent Application Platform, a framework for building secure decentralized applications. Decent applications authenticate and authorize distributed enclave components using a protocol based on self-attestation certificates, a reusable credential based on RA and verifiable by a third party. Components mutually authenticate each other not only based on their code, but also based on the other components they trust, ensuring that no transitively-connected components receive unauthorized information. While some other TEE frameworks support mutual authentication in some form, Decent is the only system that supports mutual authentication without requiring an additional trusted third party besides the trusted hardware's manufacturer. We have verified the secrecy and authenticity of Decent application data in ProVerif, and implemented two applications to evaluate Decent's expressiveness and performance: DecentRide, a ride-sharing service, and DecentHT, a distributed hash table. On the YCSB benchmark, we show that DecentHT achieves 7.5x higher throughput and 3.67x lower latency compared to a non-Decent implementation. ","Haofan Zheng, Owen Arden",,,11,
Dissecting Android Cryptocurrency Miners,"  Cryptojacking applications pose a serious threat to mobile devices. Due to the extensive computations, they deplete the battery fast and can even damage the device. In this work we make a step towards combating this threat. We collected and manually verified a large dataset of Android mining apps. In this paper, we analyze the gathered miners and identify how they work, what are the most popular libraries and APIs used to facilitate their development, and what static features are typical for this class of applications. Further, we analyzed our dataset using VirusTotal. The majority of our samples is considered malicious by at least one VirusTotal scanner, but 16 apps are not detected by any engine; and at least 5 apks were not seen previously by the service.   Mining code could be obfuscated or fetched at runtime, and there are many confusing miner-related apps that actually do not mine. Thus, static features alone are not sufficient for miner detection. We have collected a feature set of dynamic metrics both for miners and unrelated benign apps, and built a machine learning-based tool for dynamic detection. Our BrenntDroid tool is able to detect miners with 95% of accuracy on our dataset.   This preprint is a technical report accompanying the paper ""Dissecting Android Cryptocurrency Miners"" published in ACM CODASPY 2020. ","Stanislav Dashevskyi, Yury Zhauniarovich, Olga Gadyatskaya, Aleksandr   Pilgun, Hamza Ouhssain",,,11,
Public-Key Based Authentication Architecture for IoT Devices Using PUF,"  Nowadays, Internet of Things (IoT) is a trending topic in the computing world. Notably, IoT devices have strict design requirements and are often referred to as constrained devices. Therefore, security techniques and primitives that are lightweight are more suitable for such devices, e.g., Static Random-Access Memory (SRAM) Physical Unclonable Functions (PUFs) and Elliptic Curve Cryptography (ECC). SRAM PUF is an intrinsic security primitive that is seeing widespread adoption in the IoT segment. ECC is a public-key algorithm technique that has been gaining popularity among constrained IoT devices. The popularity is due to using significantly smaller operands when compared to other public-key techniques such as RSA (Rivest Shamir Adleman). This paper shows the design, development, and evaluation of an application-specific secure communication architecture based on SRAM PUF technology and ECC for constrained IoT devices. More specifically, it introduces an Elliptic Curve Diffie-Hellman (ECDH) public-key based cryptographic protocol that utilizes PUF-derived keys as the root-of-trust for silicon authentication. Also, it proposes a design of a modular hardware architecture that supports the protocol. Finally, to analyze the practicality as well as the feasibility of the proposed protocol, we demonstrate the solution by prototyping and verifying a protocol variant on the commercial Xilinx Zynq-7000 APSoC device. ","Haji Akhundov, Erik van der Sluis, Said Hamdioui and Mottaqiallah   Taouil",,,11,
JEDI: Many-to-Many End-to-End Encryption and Key Delegation for IoT,"  As the Internet of Things (IoT) emerges over the next decade, developing secure communication for IoT devices is of paramount importance. Achieving end-to-end encryption for large-scale IoT systems, like smart buildings or smart cities, is challenging because multiple principals typically interact indirectly via intermediaries, meaning that the recipient of a message is not known in advance. This paper proposes JEDI (Joining Encryption and Delegation for IoT), a many-to-many end-to-end encryption protocol for IoT. JEDI encrypts and signs messages end-to-end, while conforming to the decoupled communication model typical of IoT systems. JEDI's keys support expiry and fine-grained access to data, common in IoT. Furthermore, JEDI allows principals to delegate their keys, restricted in expiry or scope, to other principals, thereby granting access to data and managing access control in a scalable, distributed way. Through careful protocol design and implementation, JEDI can run across the spectrum of IoT devices, including ultra low-power deeply embedded sensors severely constrained in CPU, memory, and energy consumption. We apply JEDI to an existing IoT messaging system and demonstrate that its overhead is modest. ","Sam Kumar, Yuncong Hu, Michael P Andersen, Raluca Ada Popa, David E.   Culler",,,11,
Stabilizing Congestion in Decentralized Record-Keepers,"  We argue that recent developments in proof-of-work consensus mechanisms can be used in accordance with advancements in formal verification techniques to build a distributed payment protocol that addresses important economic drawbacks from cost efficiency, scalability and adaptablity common to current decentralized record-keeping systems. We enable the protocol to autonomously adjust system throughput according to a feasibly computable statistic - system difficulty. We then provide a formal economic analysis of a decentralized market place for record-keeping that is consistent with our protocol design and show that, when block rewards are zero, the system admits stable, self-regulating levels of transaction fees and wait-times across varying levels of demand. We also provide an analysis of the various technological requirements needed to instantiate such a system in a commercially viable setting, and identify relevant research directions. ",Assimakis Kattis and Fabian Trottner,,,11,
"One Sparse Perturbation to Fool them All, almost Always!","  Constructing adversarial perturbations for deep neural networks is an important direction of research. Crafting image-dependent adversarial perturbations using white-box feedback has hitherto been the norm for such adversarial attacks. However, black-box attacks are much more practical for real-world applications. Universal perturbations applicable across multiple images are gaining popularity due to their innate generalizability. There have also been efforts to restrict the perturbations to a few pixels in the image. This helps to retain visual similarity with the original images making such attacks hard to detect. This paper marks an important step which combines all these directions of research. We propose the DEceit algorithm for constructing effective universal pixel-restricted perturbations using only black-box feedback from the target network. We conduct empirical investigations using the ImageNet validation set on the state-of-the-art deep neural classifiers by varying the number of pixels to be perturbed from a meagre 10 pixels to as high as all pixels in the image. We find that perturbing only about 10% of the pixels in an image using DEceit achieves a commendable and highly transferable Fooling Rate while retaining the visual quality. We further demonstrate that DEceit can be successfully applied to image dependent attacks as well. In both sets of experiments, we outperformed several state-of-the-art methods. ","Arka Ghosh, Sankha Subhra Mullick, Shounak Datta, Swagatam Das,   Rammohan Mallipeddi, Asit Kr. Das",,,11,
Cyber Deception for Computer and Network Security: Survey and Challenges,"  Cyber deception has recently received increasing attentions as a promising mechanism for proactive cyber defense. Cyber deception strategies aim at injecting intentionally falsified information to sabotage the early stage of attack reconnaissance and planning in order to render the final attack action harmless or ineffective. Motivated by recent advances in cyber deception research, we in this paper provide a formal view of cyber deception, and review high-level deception schemes and actions. We also summarize and classify recent research results of cyber defense techniques built upon the concept of cyber deception, including game-theoretic modeling at the strategic level, network-level deception, in-host-system deception and cryptography based deception. Finally, we lay out and discuss in detail the research challenges towards developing full-fledged cyber deception frameworks and mechanisms. ","Zhuo Lu, Cliff Wang, Shangqing Zhao",,,11,
Filterbank design for end-to-end speech separation,"  Single-channel speech separation has recently made great progress thanks to learned filterbanks as used in ConvTasNet. In parallel, parameterized filterbanks have been proposed for speaker recognition where only center frequencies and bandwidths are learned. In this work, we extend real-valued learned and parameterized filterbanks into complex-valued analytic filterbanks and define a set of corresponding representations and masking strategies. We evaluate these filterbanks on a newly released noisy speech separation dataset (WHAM). The results show that the proposed analytic learned filterbank consistently outperforms the real-valued filterbank of ConvTasNet. Also, we validate the use of parameterized filterbanks and show that complex-valued representations and masks are beneficial in all conditions. Finally, we show that the STFT achieves its best performance for 2ms windows. ","Manuel Pariente, Samuele Cornell, Antoine Deleforge and Emmanuel   Vincent",,,11,
wav2shape: Hearing the Shape of a Drum Machine,"  Disentangling and recovering physical attributes, such as shape and material, from a few waveform examples is a challenging inverse problem in audio signal processing, with numerous applications in musical acoustics as well as structural engineering. We propose to address this problem via a combination of time--frequency analysis and supervised machine learning. We start by synthesizing a dataset of sounds using the functional transformation method. Then, we represent each percussive sound in terms of its time-invariant scattering transform coefficients and formulate the parametric estimation of the resonator as multidimensional regression with a deep convolutional neural network. We interpolate scattering coefficients over the surface of the drum as a surrogate for potentially missing data, and study the response of the neural network to interpolated samples. Lastly, we resynthesize drum sounds from scattering coefficients, therefore paving the way towards a deep generative model of drum sounds whose latent variables are physically interpretable. ",Han Han and Vincent Lostanlen,,,11,
Adversarial Training for Multi-domain Speaker Recognition,"  In real-life applications, the performance of speaker recognition systems always degrades when there is a mismatch between training and evaluation data. Many domain adaptation methods have been successfully used for eliminating the domain mismatches in speaker recognition. However, usually both training and evaluation data themselves can be composed of several subsets. These inner variances of each dataset can also be considered as different domains. Different distributed subsets in source or target domain dataset can also cause multi-domain mismatches, which are influential to speaker recognition performance. In this study, we propose to use adversarial training for multi-domain speaker recognition to solve the domain mismatch and the dataset variance problems. By adopting the proposed method, we are able to obtain both multi-domain-invariant and speaker-discriminative speech representations for speaker recognition. Experimental results on DAC13 dataset indicate that the proposed method is not only effective to solve the multi-domain mismatch problem, but also outperforms the compared unsupervised domain adaptation methods. ",Qing Wang and Wei Rao and Pengcheng Guo and Lei Xie,,,11,
Influence of Event Duration on Automatic Wheeze Classification,"  Patients with respiratory conditions typically exhibit adventitious respiratory sounds, such as wheezes. Wheeze events have variable duration. In this work we studied the influence of event duration on wheeze classification, namely how the creation of the non-wheeze class affected the classifiers' performance. First, we evaluated several classifiers on an open access respiratory sound database, with the best one reaching sensitivity and specificity values of 98% and 95%, respectively. Then, by changing one parameter in the design of the non-wheeze class, i.e., event duration, the best classifier only reached sensitivity and specificity values of 55% and 76%, respectively. These results demonstrate the importance of experimental design on the assessment of wheeze classification algorithms' performance. ","Bruno M. Rocha, Diogo Pessoa, Alda Marques, Paulo Carvalho, Rui Pedro   Paiva",,,11,
GACELA -- A generative adversarial context encoder for long audio   inpainting,"  We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features. ","Andres Marafioti, Piotr Majdak, Nicki Holighaus, Nathana\""el Perraudin",,,11,
"Sound Synthesis, Propagation, and Rendering: A Survey","  Sound, as a crucial sensory channel, plays a vital role in improving the reality and immersiveness of a virtual environment, following only vision in importance. Sound can provide important clues such as sound directionality and spatial size. This paper gives a broad overview of research works on sound simulation in virtual reality, games, multimedia, computer-aided design. We first survey various sound synthesis methods, including harmonic synthesis, texture synthesis, spectral analysis, and physics-based synthesis. Then, we summarize popular sound propagation techniques, namely wave-based methods, geometric-based methods, and hybrid methods. Next, the sound rendering methods are reviewed. We further demonstrate the latest deep learning based sound simulation approaches. Finally, we point to some future directions of this field. To the best of our knowledge, this is the first attempt to provide a comprehensive summary of sound research in the field of computer graphics. ","Shiguang Liu, Dinesh Manocha",,,11,
A Neural Network Based Framework for Archetypical Sound Synthesis,"  This paper describes a preliminary approach to algorithmically reproduce the archetypical structure adopted by humans to classify sounds. In particular, we propose an approach to predict the human perceived chaos/order level in a sound and synthesize new timbres that present the desired amount of this feature. We adopted a Neural Network based method, in order to exploit its inner predisposition to model perceptive and abstract features. We finally discuss the obtained accuracy and possible implications in creative contexts. ","Eric Guizzo, Alberto Novello",,,11,
On Loss Functions for Supervised Monaural Time-Domain Speech Enhancement,"  Many deep learning-based speech enhancement algorithms are designed to minimize the mean-square error (MSE) in some transform domain between a predicted and a target speech signal. However, optimizing for MSE does not necessarily guarantee high speech quality or intelligibility, which is the ultimate goal of many speech enhancement algorithms. Additionally, only little is known about the impact of the loss function on the emerging class of time-domain deep learning-based speech enhancement systems. We study how popular loss functions influence the performance of deep learning-based speech enhancement systems. First, we demonstrate that perceptually inspired loss functions might be advantageous if the receiver is the human auditory system. Furthermore, we show that the learning rate is a crucial design parameter even for adaptive gradient-based optimizers, which has been generally overlooked in the literature. Also, we found that waveform matching performance metrics must be used with caution as they in certain situations can fail completely. Finally, we show that a loss function based on scale-invariant signal-to-distortion ratio (SI-SDR) achieves good general performance across a range of popular speech enhancement evaluation metrics, which suggests that SI-SDR is a good candidate as a general-purpose loss function for speech enhancement systems. ","Morten Kolb{\ae}k, Zheng-Hua Tan, S{\o}ren Holdt Jensen, Jesper Jensen",,,11,
Learning with Learned Loss Function: Speech Enhancement with Quality-Net   to Improve Perceptual Evaluation of Speech Quality,"  Utilizing a human-perception-related objective function to train a speech enhancement model has become a popular topic recently. The main reason is that the conventional mean squared error (MSE) loss cannot represent auditory perception well. One of the typical hu-man-perception-related metrics, which is the perceptual evaluation of speech quality (PESQ), has been proven to provide a high correlation to the quality scores rated by humans. Owing to its complex and non-differentiable properties, however, the PESQ function may not be used to optimize speech enhancement models directly. In this study, we propose optimizing the enhancement model with an approximated PESQ function, which is differentiable and learned from the training data. The experimental results show that the learned surrogate function can guide the enhancement model to further boost the PESQ score (in-crease of 0.18 points compared to the results trained with MSE loss) and maintain the speech intelligibility. ","Szu-Wei Fu, Chien-Feng Liao, Yu Tsao",,,11,
Addressing Missing Labels in Large-Scale Sound Event Recognition Using a   Teacher-Student Framework With Loss Masking,"  The study of label noise in sound event recognition has recently gained attention with the advent of larger and noisier datasets. This work addresses the problem of missing labels, one of the big weaknesses of large audio datasets, and one of the most conspicuous issues for AudioSet. We propose a simple and model-agnostic method based on a teacher-student framework with loss masking to first identify the most critical missing label candidates, and then ignore their contribution during the learning process. We find that a simple optimisation of the training label set improves recognition performance without additional computation. We discover that most of the improvement comes from ignoring a critical tiny portion of the missing labels. We also show that the damage done by missing labels is larger as the training set gets smaller, yet it can still be observed even when training with massive amounts of audio. We believe these insights can generalize to other large-scale datasets. ","Eduardo Fonseca, Shawn Hershey, Manoj Plakal, Daniel P. W. Ellis, Aren   Jansen, R. Channing Moore, Xavier Serra",,,11,
The IQIYI System for Voice Conversion Challenge 2020,"  This paper presents the IQIYI voice conversion system (T24) for Voice Conversion 2020. In the competition, each target speaker has 70 sentences. We have built an end-to-end voice conversion system based on PPG. First, the ASR acoustic model calculates the BN feature, which represents the content-related information in the speech. Then the Mel feature is calculated through an improved prosody tacotron model. Finally, the Mel spectrum is converted to wav through an improved LPCNet. The evaluation results show that this system can achieve better voice conversion effects. In the case of using 16k rather than 24k sampling rate audio, the conversion result is relatively good in naturalness and similarity. Among them, our best results are in the similarity evaluation of the Task 2, the 2nd in the ASV-based objective evaluation and the 5th in the subjective evaluation. ","Wendong Gan, Haitao Chen, Yin Yan, Jianwei Li, Bolong Wen, Xueping Xu,   Hai Li",,,11,
Regularized Fast Multichannel Nonnegative Matrix Factorization with   ILRMA-based Prior Distribution of Joint-Diagonalization Process,"  In this paper, we address a convolutive blind source separation (BSS) problem and propose a new extended framework of FastMNMF by introducing prior information for joint diagonalization of the spatial covariance matrix model. Recently, FastMNMF has been proposed as a fast version of multichannel nonnegative matrix factorization under the assumption that the spatial covariance matrices of multiple sources can be jointly diagonalized. However, its source-separation performance was not improved and the physical meaning of the joint-diagonalization process was unclear. To resolve these problems, we first reveal a close relationship between the joint-diagonalization process and the demixing system used in independent low-rank matrix analysis (ILRMA). Next, motivated by this fact, we propose a new regularized FastMNMF supported by ILRMA and derive convergence-guaranteed parameter update rules. From BSS experiments, we show that the proposed method outperforms the conventional FastMNMF in source-separation accuracy with almost the same computation time. ","Keigo Kamo, Yuki Kubo, Norihiro Takamune, Daichi Kitamura, Hiroshi   Saruwatari, Yu Takahashi, Kazunobu Kondo",,,11,
DropClass and DropAdapt: Dropping classes for deep speaker   representation learning,"  Many recent works on deep speaker embeddings train their feature extraction networks on large classification tasks, distinguishing between all speakers in a training set. Empirically, this has been shown to produce speaker-discriminative embeddings, even for unseen speakers. However, it is not clear that this is the optimal means of training embeddings that generalize well. This work proposes two approaches to learning embeddings, based on the notion of dropping classes during training. We demonstrate that both approaches can yield performance gains in speaker verification tasks. The first proposed method, DropClass, works via periodically dropping a random subset of classes from the training data and the output layer throughout training, resulting in a feature extractor trained on many different classification tasks. Combined with an additive angular margin loss, this method can yield a 7.9% relative improvement in equal error rate (EER) over a strong baseline on VoxCeleb. The second proposed method, DropAdapt, is a means of adapting a trained model to a set of enrolment speakers in an unsupervised manner. This is performed by fine-tuning a model on only those classes which produce high probability predictions when the enrolment speakers are used as input, again also dropping the relevant rows from the output layer. This method yields a large 13.2% relative improvement in EER on VoxCeleb. The code for this paper has been made publicly available. ","Chau Luu, Peter Bell, Steve Renals",,,11,
Hodge and Podge: Hybrid Supervised Sound Event Detection with Multi-Hot   MixMatch and Composition Consistence Training,"  In this paper, we propose a method called Hodge and Podge for sound event detection. We demonstrate Hodge and Podge on the dataset of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge Task 4. This task aims to predict the presence or absence and the onset and offset times of sound events in home environments. Sound event detection is challenging due to the lack of large scale real strongly labeled data. Recently deep semi-supervised learning (SSL) has proven to be effective in modeling with weakly labeled and unlabeled data. This work explores how to extend deep SSL to result in a new, state-of-the-art sound event detection method called Hodge and Podge. With convolutional recurrent neural networks (CRNN) as the backbone network, first, a multi-scale squeeze-excitation mechanism is introduced and added to generate a pyramid squeeze-excitation CRNN. The pyramid squeeze-excitation layer can pay attention to the issue that different sound events have different durations, and to adaptively recalibrate channel-wise spectrogram responses. Further, in order to remedy the lack of real strongly labeled data problem, we propose multi-hot MixMatch and composition consistency training with temporal-frequency augmentation. Our experiments with the public DCASE2019 challenge task 4 validation data resulted in an event-based F-score of 43.4\%, and is about absolutely 1.6\% better than state-of-the-art methods in the challenge. While the F-score of the official baseline is 25.8\%. ",Ziqiang Shi and Liu Liu and Huibin Lin and Rujie Liu,,,11,
A Study of Transfer Learning in Music Source Separation,"  Supervised deep learning methods for performing audio source separation can be very effective in domains where there is a large amount of training data. While some music domains have enough data suitable for training a separation system, such as rock and pop genres, many musical domains do not, such as classical music, choral music, and non-Western music traditions. It is well known that transferring learning from related domains can result in a performance boost for deep learning systems, but it is not always clear how best to do pretraining. In this work we investigate the effectiveness of data augmentation during pretraining, the impact on performance as a result of pretraining and downstream datasets having similar content domains, and also explore how much of a model must be retrained on the final target task, once pretrained. ","Andreas Bugler, Bryan Pardo, Prem Seetharaman",,,11,
"Automatic and perceptual discrimination between dysarthria, apraxia of   speech, and neurotypical speech","  Automatic techniques in the context of motor speech disorders~(MSDs) are typically two-class techniques aiming to discriminate between dysarthria and neurotypical speech or between dysarthria and apraxia of speech (AoS). Further, although such techniques are proposed to support the perceptual assessment of clinicians, the automatic and perceptual classification accuracy have never been compared. In this paper, we propose a three-class automatic technique and a set of handcrafted features for the discrimination of dysarthria, AoS and neurotypical speech. Instead of following the commonly used One-versus-One or One-versus-Rest approaches for multi-class classification, a hierarchical approach is proposed. Further, a perceptual study is conducted where speech and language pathologists are asked to listen to recordings of dysarthria, AoS, and neurotypical speech and decide which class the recordings belong to. The proposed automatic technique is evaluated on the same recordings and the automatic and perceptual classification performance are compared. The presented results show that the proposed hierarchical classification approach yields a higher classification accuracy than baseline One-versus-One and One-versus-Rest approaches. Further, the presented results show that the proposed approach yields a higher classification accuracy than the perceptual assessment of speech and language pathologists, demonstrating the potential advantages of integrating automatic tools in clinical practice. ",I. Kodrasi and M. Pernon and M. Laganaro and H. Bourlard,,,11,
Short-segment heart sound classification using an ensemble of deep   convolutional neural networks,"  This paper proposes a framework based on deep convolutional neural networks (CNNs) for automatic heart sound classification using short-segments of individual heart beats. We design a 1D-CNN that directly learns features from raw heart-sound signals, and a 2D-CNN that takes inputs of two- dimensional time-frequency feature maps based on Mel-frequency cepstral coefficients (MFCC). We further develop a time-frequency CNN ensemble (TF-ECNN) combining the 1D-CNN and 2D-CNN based on score-level fusion of the class probabilities. On the large PhysioNet CinC challenge 2016 database, the proposed CNN models outperformed traditional classifiers based on support vector machine and hidden Markov models with various hand-crafted time- and frequency-domain features. Best classification scores with 89.22% accuracy and 89.94% sensitivity were achieved by the ECNN, and 91.55% specificity and 88.82% modified accuracy by the 2D-CNN alone on the test set. ","Fuad Noman, Chee-Ming Ting, Sh-Hussain Salleh, and Hernando Ombao",,,11,
Improving Sequence-to-Sequence Acoustic Modeling by Adding   Text-Supervision,"  This paper presents methods of making using of text supervision to improve the performance of sequence-to-sequence (seq2seq) voice conversion. Compared with conventional frame-to-frame voice conversion approaches, the seq2seq acoustic modeling method proposed in our previous work achieved higher naturalness and similarity. In this paper, we further improve its performance by utilizing the text transcriptions of parallel training data. First, a multi-task learning structure is designed which adds auxiliary classifiers to the middle layers of the seq2seq model and predicts linguistic labels as a secondary task. Second, a data-augmentation method is proposed which utilizes text alignment to produce extra parallel sequences for model training. Experiments are conducted to evaluate our proposed method with training sets at different sizes. Experimental results show that the multi-task learning with linguistic labels is effective at reducing the errors of seq2seq voice conversion. The data-augmentation method can further improve the performance of seq2seq voice conversion when only 50 or 100 training utterances are available. ","Jing-Xuan Zhang, Zhen-Hua Ling, Yuan Jiang, Li-Juan Liu, Chen Liang,   Li-Rong Dai",,,11,
Cross-cultural data shows musical scales evolved to maximise imperfect   fifths,"  Musical scales are used throughout the world, but the question of how they evolved remains open. Some suggest that scales based on the harmonic series are inherently pleasant, while others propose that scales are chosen that are easy to communicate. However, testing these theories has been hindered by the sparseness of empirical evidence. Here, we assimilate data from diverse ethnomusicological sources into a cross-cultural database of scales. We generate populations of scales based on multiple theories and assess their similarity to empirical distributions from the database. Most scales tend to include intervals which are close in size to perfect fifths (``imperfect fifths''), and packing arguments explain the salient features of the distributions. Scales are also preferred if their intervals are compressible, which may facilitate efficient communication and memory of melodies. While scales appear to evolve according to various selection pressures, the simplest h imperfect-fifths packing model best fits the empirical data. ",John M. McBride and Tsvi Tlusty,,,11,
Introducing SPAIN (SParse Audio INpainter),"  A novel sparsity-based algorithm for audio inpainting is proposed. It is an adaptation of the SPADE algorithm by Kiti\'c et al., originally developed for audio declipping, to the task of audio inpainting. The new SPAIN (SParse Audio INpainter) comes in synthesis and analysis variants. Experiments show that both A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms. Moreover, A-SPAIN performs on a par with the state-of-the-art method based on linear prediction in terms of the SNR, and, for larger gaps, SPAIN is even slightly better in terms of the PEMO-Q psychoacoustic criterion. ","Ond\v{r}ej Mokr\'y, Pavel Z\'avi\v{s}ka, Pavel Rajmic, V\'it\v{e}zslav   Vesel\'y",,,11,
Joint Estimation of Topology and Injection Statistics in Distribution   Grids with Missing Nodes,"  Optimal operation of distribution grid resources relies on accurate estimation of its state and topology. Practical estimation of such quantities is complicated by the limited presence of real-time meters. This paper discusses a theoretical framework to jointly estimate the operational topology and statistics of injections in radial distribution grids under limited availability of nodal voltage measurements. In particular we show that our proposed algorithms are able to provably learn the exact grid topology and injection statistics at all unobserved nodes as long as they are not adjacent. The algorithm design is based on novel ordered trends in voltage magnitude fluctuations at node groups, that are independently of interest for radial physical flow networks. The complexity of the designed algorithms is theoretically analyzed and their performance validated using both linearized and non-linear AC power flow samples in test distribution grids. ","Deepjyoti Deka, Michael Chertkov, and Scott Backhaus",,,11,
Affine Dependence of Network Controllability/Observability on Its   Subsystem Parameters and Connections,"  This paper investigates observability/controllability of a networked dynamic system (NDS) in which system matrices of its subsystems are expressed through linear fractional transformations (LFT). Some relations have been obtained between this NDS and descriptor systems about their observability/controllability. A necessary and sufficient condition is established with the associated matrices depending affinely on subsystem parameters/connections. An attractive property of this condition is that all the required calculations are performed independently on each individual subsystem. Except well-posedness, not any other conditions are asked for subsystem parameters/connections. This is in sharp contrast to recent results on structural observability/controllability which is proven to be NP hard. Some characteristics are established for a subsystem which are helpful in constructing an observable/controllable NDS. It has been made clear that subsystems with an input matrix of full column rank are helpful in constructing an observable NDS, while subsystems with an output matrix of full row rank are helpful in constructing a controllable NDS. These results are extended to an NDS with descriptor form subsystems. As a byproduct, the full normal rank condition of previous works on network observability/controllability has been completely removed. On the other hand, satisfaction of this condition is shown to be appreciative in building an observable/controllability NDS. ",Tong Zhou and Yuyu Zhou,,,11,
Distributed Optimal Power Flow Algorithms Over Time-Varying   Communication Networks,"  In this paper, we consider the problem of optimally coordinating the response of a group of distributed energy resources (DERs) in distribution systems by solving the so-called optimal power flow (OPF) problem. The OPF problem is concerned with determining an optimal operating point, at which some cost function, e.g., generation cost or power losses, is minimized, and operational constraints are satisfied. To solve the OPF problem, we propose distributed algorithms that are able to operate over time-varying communication networks and have geometric convergence rate. We solve the second-order cone program (SOCP) relaxation of the OPF problem for radial distribution systems, which is formulated using the so-called DistFlow model. Theoretical results are further supported by the numerical simulations. ",Madi Zholbaryssov and Alejandro D. Dominguez-Garcia,,,11,
Critical Clearing Time Sensitivity for Inequality Constrained Systems,"  From a stability perspective, a renewable generation (RG)-rich power system is a constrained system. As the quasistability boundary of a constrained system is structurally very different from that of an unconstrained system, finding the sensitivity of critical clearing time (CCT) to change in system parameters is very beneficial for a constrained power system, especially for planning/revising constraints arising from system protection settings. In this paper, we derive the first order sensitivity of a constrained power system using trajectory sensitivities of fault-on and post-fault trajectories. The results for the test system demonstrate the dependence between ability to meet angle and frequency constraints, and change in power system parameters such as operating conditions and inertia. ","Chetan Mishra, Anamitra Pal, Virgilio A. Centeno",,,11,
On reduction of differential inclusions and Lyapunov stability,"  In this paper, locally Lipschitz, regular functions are utilized to identify and remove infeasible directions from set-valued maps that define differential inclusions. The resulting reduced set-valued map is point-wise smaller (in the sense of set containment) than the original set-valued map. The corresponding reduced differential inclusion, defined by the reduced set-valued map, is utilized to develop a generalized notion of a derivative for locally Lipschitz candidate Lyapunov functions in the direction(s) of a set-valued map. The developed generalized derivative yields less conservative statements of Lyapunov stability theorems, invariance theorems, invariance-like results, and Matrosov theorems for differential inclusions. Included illustrative examples demonstrate the utility of the developed theory. ",Rushikesh Kamalapurkar and Warren E. Dixon and Andrew R. Teel,,,11,
Vulnerability Assessment of Large-scale Power Systems to False Data   Injection Attacks,"  This paper studies the vulnerability of large-scale power systems to false data injection (FDI) attacks through their physical consequences. Prior work has shown that an attacker-defender bi-level linear program (ADBLP) can be used to determine the worst-case consequences of FDI attacks aiming to maximize the physical power flow on a target line. This ADBLP can be transformed into a single-level mixed-integer linear program, but it is hard to solve on large power systems due to numerical difficulties. In this paper, four computationally efficient algorithms are presented to solve the attack optimization problem on large power systems. These algorithms are applied on the IEEE 118-bus system and the Polish system with 2383 buses to conduct vulnerability assessments, and they provide feasible attacks that cause line overflows, as well as upper bounds on the maximal power flow resulting from any attack. ","Zhigang Chu, Jiazi Zhang, Oliver Kosut, and Lalitha Sankar",,,11,
On the design of stabilizing cycles for switched linear systems,"  Given a family of systems, identifying stabilizing switching signals in terms of infinite walks constructed by concatenating cycles on the underlying directed graph of a switched system that satisfy certain conditions, is a well-known technique in the literature. This paper deals with a new {method to design} these cycles for stability of switched linear systems. We employ properties of the subsystem matrices and mild assumption on the admissible switches between the subsystems {for this purpose}. In contrast to prior works, {our construction of} stabilizing cycles does not involve design of Lyapunov-like functions and storage of sets of scalars in memory prior to the application of a cycle detection algorithm. As a result, {the} techniques {proposed in this paper} offer improved numerical tractability. ",Atreyee Kundu,,,11,
Rapid Information Transfer in Networks with Delayed Self Reinforcement,"  The cohesiveness of response to external stimuli depends on rapid distortion-free information transfer across the network. Aligning with the information from the network has been used to model such information transfer. Nevertheless, the rate of such diffusion-type, neighbor-based information transfer is limited by the update rate at which each individual can sense and process information. Moreover, models of the diffusion-type information transfer do not predict the superfluid-like information transfer observed in nature. The contribution of this article is to show that self reinforcement, where each individual augments its neighbor-averaged information update using its previous update, can (i) increase the information-transfer rate without requiring an increased, individual update-rate; and (ii) capture the observed superfluid-like information transfer. This improvement in the information-transfer rate without modification of the network structure or increase of the bandwidth of each agent can lead to better understanding and design of networks with fast response. ",Santosh Devasia,,,11,
Small Satellite Constellation Separation using Linear Programming based   Differential Drag Commands,"  We study the optimal control of an arbitrarily large constellation of small satellites operating in low Earth orbit. Simulating the lack of on-board propulsion, we limit our actuation to the use of differential drag maneuvers to make in-plane changes to the satellite orbits. We propose an efficient method to separate a cluster of satellites into a desired constellation shape while respecting actuation constraints and maximizing the operational lifetime of the constellation. By posing the problem as a linear program, we solve for the optimal drag commands for each of the satellites on a daily basis with a shrinking-horizon model predictive control approach. We then apply this control strategy in a nonlinear orbital dynamics simulation with a simple, varying atmospheric density model. We demonstrate the ability to control a cluster of 100+ satellites starting at the same initial conditions in a circular low Earth orbit to form an equally spaced constellation (with a relative angular separation error tolerance of one-tenth a degree). The constellation separation task can be executed in 71 days, a time frame that is competitive for the state-of-the-practice. This method allows us to trade the time required to converge to the desired constellation with a sacrifice in the overall constellation lifetime, measured as the maximum altitude loss experienced by one of the satellites in the group after the separation maneuvers. ","Emmanuel Sin, Murat Arcak, Andrew Packard",,,11,
On the Trade-off Between Controllability and Robustness in Networks of   Diffusively Coupled Agents,"  In this paper, we demonstrate a conflicting relationship between two crucial properties---controllability and robustness---in linear dynamical networks of diffusively coupled agents. In particular, for any given number of nodes $N$ and diameter $D$, we identify networks that are maximally robust using the notion of Kirchhoff index and then analyze their strong structural controllability. For this, we compute the minimum number of leaders, which are the nodes directly receiving external control inputs, needed to make such networks controllable under all feasible coupling weights between agents. Then, for any $N$ and $D$, we obtain a sharp upper bound on the minimum number of leaders needed to design strong structurally controllable networks with $N$ nodes and diameter $D$. We also discuss that the bound is best possible for arbitrary $N$ and $D$. Moreover, we construct a family of graphs for any $N$ and $D$ such that the graphs have maximal edge sets (maximal robustness) while being strong structurally controllable with the number of leaders in the proposed sharp bound. We then analyze the robustness of this graph family. The results suggest that optimizing robustness increases the number of leaders needed for strong structural controllability. Our analysis is based on graph-theoretic methods and can be applied to exploit network structure to co-optimize robustness and controllability in networks. ",Waseem Abbas and Mudassir Shabbir and A. Yasin Yazicioglu and Aqsa   Akber,,,11,
"Faster Response in Bounded-Update-Rate, Discrete-time Networks using   Delayed Self-Reinforcement","  The response speed of a network impacts the efficacy of its actions to external stimuli. However, for a given bound on the update rate, the network-response speed is limited by the need to maintain stability. This work increases the network-response speed without having to increase the update rate by using delayed self-reinforcement (DSR), where each agent uses its already available information from the network to strengthen its individual update law. Example simulation results are presented that show more than an order of magnitude improvement in the response speed (quantified using the settling time) with the proposed DSR approach. ",Santosh Devasia,,,11,
On the smoothness of nonlinear system identification,"  We shed new light on the \textit{smoothness} of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and $\beta$-smoothness of the objective function might blow up exponentially with the simulation length, making it hard to numerically find minima within those regions or, even, to escape from them. In addition to providing theoretical understanding of this problem, this paper also proposes the use of multiple shooting as a viable solution. The proposed method minimizes the error between a prediction model and the observed values. Rather than running the prediction model over the entire dataset, multiple shooting splits the data into smaller subsets and runs the prediction model over each subset, making the simulation length a design parameter and making it possible to solve problems that would be infeasible using a standard approach. The equivalence to the original problem is obtained by including constraints in the optimization. The new method is illustrated by estimating the parameters of nonlinear systems with chaotic or unstable behavior, as well as neural networks. We also present a comparative analysis of the proposed method with multi-step-ahead prediction error minimization. ","Ant\^onio H. Ribeiro, Koen Tiels, Jack Umenberger, Thomas B. Sch\""on,   Luis A. Aguirre",,,11,
Privacy preservation in continuous-time average consensus algorithm via   deterministic additive perturbation signals,"  This paper considers the problem of privacy preservation against passive internal and external malicious agents in the continuous-time Laplacian average consensus algorithm over strongly connected and weight-balanced digraphs. For this problem, we evaluate the effectiveness of use of additive perturbation signals as a privacy preservation measure against malicious agents that know the graph topology. Our results include (a) identifying the necessary and sufficient conditions on admissible additive perturbation signals that do not perturb the convergence point of the algorithm from the average of initial values of the agents; (b) obtaining the necessary and sufficient condition on the knowledge set of a malicious agent that enables it to identify the initial value of another agent; (c) designing observers that internal and external malicious agents can use to identify the initial conditions of another agent when their knowledge set on that agent enables them to do so. We demonstrate our results through a numerical example. ","Navid Rezazadeh, Solmaz S. Kia",,,11,
Event Detection and Localization in Distribution Grids with Phasor   Measurement Units,"  The recent introduction of synchrophasor technology into power distribution systems has given impetus to various monitoring, diagnostic, and control applications, such as system identification and event detection, which are crucial for restoring service, preventing outages, and managing equipment health. Drawing on the existing framework for inferring topology and admittances of a power network from voltage and current phasor measurements, this paper proposes an online algorithm for event detection and localization in unbalanced three-phase distribution systems. Using a convex relaxation and a matrix partitioning technique, the proposed algorithm is capable of identifying topology changes and attributing them to specific categories of events. The performance of this algorithm is evaluated on a standard test distribution feeder with synthesized loads, and it is shown that a tripped line can be detected and localized in an accurate and timely fashion, highlighting its potential for real-world applications. ","Omid Ardakanian, Ye Yuan, Roel Dobbe, Alexandra von Meier, Steven Low,   Claire Tomlin",,,11,
Commutativity of First-order Discrete-Time Linear Time-varying Systems,"  After introducing the concept of commutativity for continuous-time linear time-varying systems, the related literature and the results obtained so far are presented. For a simple introduction of the commutativity of discrete-time linear time-varying systems, the problem is formulated for first-order systems. Finally, explicit necessary and sufficient conditions for the commutativity of first-order discrete-time linear time-varying systems are derived, and their advantageous use in digital system design is illustrated; which are the main objectives of the paper. The results are verified by examples which include an application in amplitude modulation for digital telecommunication. ",Mehmet Emir Koksal,,,11,
Stochastic Battery Model for Aggregation of Thermostatically Controlled   Loads,"  The potential of demand side as a frequency reserve proposes interesting opportunity in handling imbalances due to intermittent renewable energy sources. This paper proposes a novel approach for computing the parameters of a stochastic battery model representing the aggregation of Thermostatically Controlled Loads (TCLs). A hysteresis based non-disruptive control is used using priority stack algorithm to track the reference regulation signal. The parameters of admissible ramp-rate and the charge limits of the battery are dynamically calculated using the information from TCLs that is the status (on/off), availability and relative temperature distance till the switching boundary. The approach builds on and improves on the existing research work by providing a straight-forward mechanism for calculation of stochastic parameters of equivalent battery model. The effectiveness of proposed approach is demonstrated by a test case having a large number of residential TCLs tracking a scaled down real frequency regulation signal. ","Sohail Khan, Mohsin Shahzad, Usman Habib, Wolfgang Gawlik, Peter   Palensky",,,11,
On Adaptive Linear-Quadratic Regulators,"  Performance of adaptive control policies is assessed through the regret with respect to the optimal regulator, which reflects the increase in the operating cost due to uncertainty about the dynamics parameters. However, available results in the literature do not provide a quantitative characterization of the effect of the unknown parameters on the regret. Further, there are problems regarding the efficient implementation of some of the existing adaptive policies. Finally, results regarding the accuracy with which the system's parameters are identified are scarce and rather incomplete.   This study aims to comprehensively address these three issues. First, by introducing a novel decomposition of adaptive policies, we establish a sharp expression for the regret of an arbitrary policy in terms of the deviations from the optimal regulator. Second, we show that adaptive policies based on slight modifications of the Certainty Equivalence scheme are efficient. Specifically, we establish a regret of (nearly) square-root rate for two families of randomized adaptive policies. The presented regret bounds are obtained by using anti-concentration results on the random matrices employed for randomizing the estimates of the unknown parameters. Moreover, we study the minimal additional information on dynamics matrices that using them the regret will become of logarithmic order. Finally, the rates at which the unknown parameters of the system are being identified are presented. ","Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis",,,11,
Cautious Model Predictive Control using Gaussian Process Regression,"  Gaussian process (GP) regression has been widely used in supervised machine learning due to its flexibility and inherent ability to describe uncertainty in function estimation. In the context of control, it is seeing increasing use for modeling of nonlinear dynamical systems from data, as it allows the direct assessment of residual model uncertainty. We present a model predictive control (MPC) approach that integrates a nominal system with an additive nonlinear part of the dynamics modeled as a GP. Approximation techniques for propagating the state distribution are reviewed and we describe a principled way of formulating the chance constrained MPC problem, which takes into account residual uncertainties provided by the GP model to enable cautious control. Using additional approximations for efficient computation, we finally demonstrate the approach in a simulation example, as well as in a hardware implementation for autonomous racing of remote controlled race cars, highlighting improvements with regard to both performance and safety over a nominal controller. ",Lukas Hewing and Juraj Kabzan and Melanie N. Zeilinger,,,11,
Reachability analysis of linear hybrid systems via block decomposition,"  Reachability analysis aims at identifying states reachable by a system within a given time horizon. This task is known to be computationally expensive for linear hybrid systems. Reachability analysis works by iteratively applying continuous and discrete post operators to compute states reachable according to continuous and discrete dynamics, respectively. In this paper, we enhance both of these operators and make sure that most of the involved computations are performed in low-dimensional state space. In particular, we improve the continuous-post operator by performing computations in high-dimensional state space only for time intervals relevant for the subsequent application of the discrete-post operator. Furthermore, the new discrete-post operator performs low-dimensional computations by leveraging the structure of the guard and assignment of a considered transition. We illustrate the potential of our approach on a number of challenging benchmarks. ","Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Kostiantyn Potomkin,   Christian Schilling",,,11,
Decomposition of Third Order Linear Time-Varying System into its Second   and First Order Commutative Pairs,"  Decomposition is a common tool for synthesis of many physical systems. It is also used for analyzing large scale systems which then known as tearing and reconstruction. On the other hand, commutativity of cascade connected systems have gained a grade deal of interest and its possible benefits have been pointed out on the literature. In this paper, the necessary and sufficient conditions for decomposition of any third order linear time-varying system as a commutative pair of first and second order systems of which parameters are also explicitly expressed, are investigated. Further, additional requirements in case of non-zero initial conditions are derived. This paper highlights the direct formulas for realization of any third order linear time-varying system as a series (cascade) connection of first and second order subsystems. This series connection is commutative so that it is independent from the sequence of subsystems in the connection. Hence, the convenient sequence can be decided by considering the overall performance of the system when the sensitivity, disturbance and robustness effects are considered. Realization covers transient responses as well as steady state responses. ",Mehmet Emir Koksal and Ali Yakar,,,11,
From 2-sequents and Linear Nested Sequents to Natural Deduction for   Normal Modal Logics,"  We extend to natural deduction the approach of Linear Nested Sequents and 2-sequents. Formulas are decorated with a spatial coordinate, which allows a formulation of formal systems in the original spirit of natural deduction---only one introduction and one elimination rule per connective, no additional (structural) rule, no explicit reference to the accessibility relation of the intended Kripke models. We give systems for the normal modal logics from K to S4. For the intuitionistic versions of the systems, we define proof reduction, and prove proof normalisation, thus obtaining a syntactical proof of consistency. For logics K and K4 we use existence predicates (following Scott) for formulating sound deduction rules. ","Simone Martini, Andrea Masini, Margherita Zorzi",,,11,
Abstract Interpretation of Temporal Concurrent Constraint Programs,"  Timed Concurrent Constraint Programming (tcc) is a declarative model for concurrency offering a logic for specifying reactive systems, i.e. systems that continuously interact with the environment. The universal tcc formalism (utcc) is an extension of tcc with the ability to express mobility. Here mobility is understood as communication of private names as typically done for mobile systems and security protocols. In this paper we consider the denotational semantics for tcc, and we extend it to a ""collecting"" semantics for utcc based on closure operators over sequences of constraints. Relying on this semantics, we formalize a general framework for data flow analyses of tcc and utcc programs by abstract interpretation techniques. The concrete and abstract semantics we propose are compositional, thus allowing us to reduce the complexity of data flow analyses. We show that our method is sound and parametric with respect to the abstract domain. Thus, different analyses can be performed by instantiating the framework. We illustrate how it is possible to reuse abstract domains previously defined for logic programming to perform, for instance, a groundness analysis for tcc programs. We show the applicability of this analysis in the context of reactive systems. Furthermore, we make also use of the abstract semantics to exhibit a secrecy flaw in a security protocol. We also show how it is possible to make an analysis which may show that tcc programs are suspension free. This can be useful for several purposes, such as for optimizing compilation or for debugging. ","Moreno Falaschi, Carlos Olarte, Catuscia Palamidessi",,,11,
It's Time to Play Safe: Shield Synthesis for Timed Systems,"  Erroneous behaviour in safety critical real-time systems may inflict serious consequences. In this paper, we show how to synthesize timed shields from timed safety properties given as timed automata. A timed shield enforces the safety of a running system while interfering with the system as little as possible. We present timed post-shields and timed pre-shields. A timed pre-shield is placed before the system and provides a set of safe outputs. This set restricts the choices of the system. A timed post-shield is implemented after the system. It monitors the system and corrects the system's output only if necessary. We further extend the timed post-shield construction to provide a guarantee on the recovery phase, i.e., the time between a specification violation and the point at which full control can be handed back to the system. In our experimental results, we use timed post-shields to ensure the safety in a reinforcement learning setting for controlling a platoon of cars, during the learning and execution phase, and study the effect. ","Roderick Bloem, Peter Gj{\o}l Jensen, Bettina K\""onighofer, Kim   Guldstrand Larsen, Florian Lorber and Alexander Palmisano",,,11,
Logic of fusion,"  The starting point of this work is the observation that the Curry-Howard isomorphism, relating types and propositions, programs and proofs, composition and cut, extends to the correspondence of program fusion and cut elimination. This simple idea suggests logical interpretations of some of the basic methods of generic and transformational programming. In the present paper, we provide a logical analysis of the general form of build fusion, also known as deforestation, over the inductive and the coinductive datatypes, regular or nested. The analysis is based on a novel logical interpretation of parametricity in terms of the paranatural transformations, introduced in the paper. ",Dusko Pavlovic,,,11,
Deductive Stability Proofs for Ordinary Differential Equations,"  Stability is required for real world controlled systems as it ensures that they can tolerate small, real world perturbations around their desired operating states. This paper shows how stability for continuous systems modeled by ordinary differential equations (ODEs) can be formally verified in differential dynamic logic (dL). The key insight is to specify ODE stability by suitably nesting the dynamic modalities of dL with first-order logic quantifiers. Elucidating the logical structure of stability properties in this way has three key benefits: i) it provides a flexible means of formally specifying various stability properties of interest, ii) it yields rigorous proofs of those stability properties from dL's axioms with dL's ODE safety and liveness proof principles, and iii) it enables formal analysis of the relationships between various stability properties which, in turn, inform proofs of those properties. These benefits are put into practice through an implementation of stability proofs for several examples in KeYmaera X, a hybrid systems theorem prover based on dL. ",Yong Kiam Tan and Andr\'e Platzer,,,11,
Tracelets and Tracelet Analysis Of Compositional Rewriting Systems,"  Taking advantage of a recently discovered associativity property of rule compositions, we extend the classical concurrency theory for rewriting systems over adhesive categories. We introduce the notion of tracelets, which are defined as minimal derivation traces that universally encode sequential compositions of rewriting rules. Tracelets are compositional, capture the causality of equivalence classes of traditional derivation traces, and intrinsically suggest a clean mathematical framework for the definition of various notions of abstractions of traces. We illustrate these features by introducing a first prototype for a framework of tracelet analysis, which as a key application permits to formulate a first-of-its-kind algorithm for the static generation of minimal derivation traces with prescribed terminal events. ","Nicolas Behr (Universit\'e de Paris, IRIF, CNRS)",,,11,
Typal Heterogeneous Equality Types,"  The usual homogeneous form of equality type in Martin-L\""of Type Theory contains identifications between elements of the same type. By contrast, the heterogeneous form of equality contains identifications between elements of possibly different types. This paper introduces a simple set of axioms for such types. The axioms are equivalent to the combination of systematic elimination rules for both forms of equality, albeit with typal (also known as ""propositional"") computation properties, together with Streicher's Axiom K, or equivalently, the principle of uniqueness of identity proofs. ",Andrew M. Pitts,,,11,
"Probabilistic Rewriting: On Normalization, Termination, and Unique   Normal Forms","  While a mature body of work supports the study of rewriting systems, even infinitary ones, abstract tools for Probabilistic Rewriting are still limited. Here, we investigate questions such as uniqueness of the result (unique limit distribution) and we develop a set of proof techniques to analyze and compare reduction strategies. The goal is to have tools to support the operational analysis of probabilistic calculi (such as probabilistic lambda-calculi) whose evaluation is also non-deterministic, in the sense that different reductions are possible.   In particular, we investigate how the behavior of different rewrite sequences starting from the same term compare w.r.t. normal forms, and propose a robust analogue of the notion of ""unique normal form"". Our approach is that of Abstract Rewrite Systems, i.e. we search for general properties of probabilistic rewriting, which hold independently of the specific structure of the objects. ",Claudia Faggian,,,11,
On Termination of Transactions over Semantic Document Models,"  We consider the framework of Document Modeling, which lays the formal basis for representing the document lifecycle in Business Process Management systems. We formulate document models in the scope of the logic-based Semantic Modeling language and study the question whether transactions given by a document model terminate on any input. We show that in general this problem is undecidable and formulate sufficient conditions, which guarantee decidability and polynomial boundedness of effects of transactions. ",Andrei Mantsivoda and Denis Ponomaryov,,,11,
Computing Tropical Prevarieties with Satisfiability Modulo Theories   (SMT) Solvers,"  A novel way to use SMT (Satisfiability Modulo Theories) solvers to compute the tropical prevariety (resp. equilibrium) of a polynomial system is presented. The new method is benchmarked against a naive approach that uses purely polyhedral methods. It turns out that the SMT approach is faster than the polyhedral approach for models that would otherwise take more than one minute to compute, in many cases by a factor of 60 or more, and in the worst case is only slower by a factor of two. Furthermore, the new approach is an anytime algorithm, thus offering a way to compute parts of the solution when the polyhedral approach is infeasible. ","Christoph L\""uders",,,11,
Formal Verification of Arithmetic RTL: Translating Verilog to C++ to   ACL2,"  We present a methodology for formal verification of arithmetic RTL designs that combines sequential logic equivalence checking with interactive theorem proving. An intermediate model of a Verilog module is hand-coded in Restricted Algorithmic C (RAC), a primitive subset of C augmented by the integer and fixed-point register class templates of Algorithmic C. The model is designed to be as abstract and compact as possible, but sufficiently faithful to the RTL to allow efficient equivalence checking with a commercial tool. It is then automatically translated to the logic of ACL2, enabling a mechanically checked proof of correctness with respect to a formal architectural specification. In this paper, we describe the RAC language, the translation process, and some techniques that facilitate formal analysis of the resulting ACL2 code. ",David M. Russinoff (Arm),,,11,
Anti-unification in Constraint Logic Programming,"  Anti-unification refers to the process of generalizing two (or more) goals into a single, more general, goal that captures some of the structure that is common to all initial goals. In general one is typically interested in computing what is often called a most specific generalization, that is a generalization that captures a maximal amount of shared structure. In this work we address the problem of anti-unification in CLP, where goals can be seen as unordered sets of atoms and/or constraints. We show that while the concept of a most specific generalization can easily be defined in this context, computing it becomes an NP-complete problem. We subsequently introduce a generalization algorithm that computes a well-defined abstraction whose computation can be bound to a polynomial execution time. Initial experiments show that even a naive implementation of our algorithm produces acceptable generalizations in an efficient way. Under consideration for acceptance in TPLP. ",Gonzague Yernaux and Wim Vanhoof,,,11,
"A Cook's tour of duality in logic: from quantifiers, through Vietoris,   to measures","  We identify and highlight certain landmark results in Samson Abramsky's work which we believe are fundamental to current developments and future trends. In particular, we focus on the use of (i) topological duality methods to solve problems in logic and computer science; (ii) category theory and, more particularly, free (and co-free) constructions; (iii) these tools to unify the `power' and `structure' strands in computer science. ","Mai Gehrke, Tomas Jakl, Luca Reggio",,,11,
Operationally-based Program Equivalence Proofs using LCTRSs,"  We propose an operationally-based deductive proof method for program equivalence. It is based on encoding the language semantics as logically constrained term rewriting systems (LCTRSs) and the two programs as terms. The main feature of our method is its flexibility. We illustrate this flexibility in two applications, which are novel. For the first application, we show how to encode low-level details such as stack size in the language semantics and how to prove equivalence between two programs operating at different levels of abstraction. For our running example, we show how our method can prove equivalence between a recursive function operating with an unbounded stack and its tail-recursive optimized version operating with a bounded stack. This type of equivalence checking can be used to ensure that new, undesirable behavior is not introduced by a more concrete level of abstraction. For the second application, we show how to formalize read-sets and write-sets of symbolic expressions and statements by extending the operational semantics in a conservative way. This enables the relational verification of program schemas, which we exploit to prove correctness of compiler optimizations, some of which cannot be proven by existing tools. Our method requires an extension of standard LCTRSs with axiomatized symbols. We also present a prototype implementation that proves the feasibility of both applications that we propose. ","\c{S}tefan Ciob\^ac\u{a}, Dorel Lucanu and Andrei Sebastian   Buruian\u{a}",,,11,
Can determinism and compositionality coexist in RML? (extended version),"  Runtime verification (RV) consists in dynamically verifying that the event traces generated by single runs of a system under scrutiny (SUS) are compliant with the formal specification of its expected properties. RML (Runtime Monitoring Language) is a simple but expressive Domain Specific Language for RV; its semantics is based on a trace calculus formalized by a deterministic rewriting system which drives the implementation of the interpreter of the monitors generated by the RML compiler from the specifications. While determinism of the trace calculus ensures better performances of the generated monitors, it makes the semantics of its operators less intuitive. In this paper we move a first step towards a compositional semantics of the RML trace calculus, by interpreting its basic operators as operations on sets of instantiated event traces and by proving that such an interpretation is equivalent to the operational semantics of the calculus. ",Davide Ancona and Angelo Ferrando and Viviana Mascardi,,,11,
Actris 2.0: Asynchronous Session-Type Based Reasoning in Separation   Logic,"  Message passing is a useful abstraction for implementing concurrent programs. For real-world systems, however, it is often combined with other programming and concurrency paradigms, such as higher-order functions, mutable state, shared-memory concurrency, and locks. We present Actris: a logic for proving functional correctness of programs that use a combination of the aforementioned features. Actris combines the power of modern concurrent separation logics with a first-class protocol mechanism---based on session types---for reasoning about message passing in the presence of other concurrency paradigms. We show that Actris provides a suitable level of abstraction by proving functional correctness of a variety of examples, including a distributed merge sort, a distributed load-balancing mapper, and a variant of the map-reduce model, using concise specifications. While Actris was already presented in a conference paper (POPL'20), this paper expands the prior presentation significantly. Moreover, it extends Actris to Actris 2.0 with a notion of subprotocols---based on session-type subtyping---that permits additional flexibility when composing channel endpoints, and that takes full advantage of the asynchronous semantics of message passing in Actris. Soundness of Actris 2.0 is proved using a model of its protocol mechanism in the Iris framework. We have mechanised the theory of Actris, together with custom tactics, as well as all examples in the paper, in the Coq proof assistant. ","Jonas Kastberg Hinrichsen, Jesper Bengtson and Robbert Krebbers",,,11,
Featured Games,"  Feature-based SPL analysis and family-based model checking have seen rapid development. Many model checking problems can be reduced to two-player games on finite graphs. A prominent example is mu-calculus model checking, which is generally done by translating to parity games, but also many quantitative model-checking problems can be reduced to (quantitative) games.   In their FASE'20 paper, ter Beek et al.\ introduce parity games with variability in order to develop family-based mu-calculus model checking of featured transition systems. We generalize their model to general featured games and show how these may be analysed in a family-based manner.   We introduce featured reachability games, featured minimum reachability games, featured discounted games, featured energy games, and featured parity games. We show how to compute winners and values of such games in a family-based manner. We also show that all these featured games admit optimal featured strategies, which project to optimal strategies for any product. Further, we develop family-based algorithms, using late splitting, to compute winners, values, and optimal strategies for all the featured games we have introduced. ","Uli Fahrenberg, Axel Legay",,,11,
On Higher-Order Probabilistic Subrecursion (Long Version),"  We study the expressive power of subrecursive probabilistic higher-order calculi. More specifically, we show that endowing a very expressive deterministic calculus like G\""odel's $\mathbb{T}$ with various forms of probabilistic choice operators may result in calculi which are not equivalent as for the class of distributions they give rise to, although they all guarantee almost-sure termination. Along the way, we introduce a probabilistic variation of the classic reducibility technique, and we prove that the simplest form of probabilistic choice leaves the expressive power of $\mathbb{T}$ essentially unaltered. The paper ends with some observations about functional expressivity: expectedly, all the considered calculi capture precisely the functions which $\mathbb{T}$ itself represents, at least when standard notions of observations are considered. ","Flavien Breuvart, Ugo Dal Lago, Agathe Herrou",,,11,
Modular Action Language ALM,"  The paper introduces a new modular action language, ALM, and illustrates the methodology of its use. It is based on the approach of Gelfond and Lifschitz (1993; 1998) in which a high-level action language is used as a front end for a logic programming system description. The resulting logic programming representation is used to perform various computational tasks. The methodology based on existing action languages works well for small and even medium size systems, but is not meant to deal with larger systems that require structuring of knowledge. ALM is meant to remedy this problem. Structuring of knowledge in ALM is supported by the concepts of module (a formal description of a specific piece of knowledge packaged as a unit), module hierarchy, and library, and by the division of a system description of ALM into two parts: theory and structure. A theory consists of one or more modules with a common theme, possibly organized into a module hierarchy based on a dependency relation. It contains declarations of sorts, attributes, and properties of the domain together with axioms describing them. Structures are used to describe the domain's objects. These features, together with the means for defining classes of a domain as special cases of previously defined ones, facilitate the stepwise development, testing, and readability of a knowledge base, as well as the creation of knowledge representation libraries. To appear in Theory and Practice of Logic Programming (TPLP). ",Daniela Inclezan and Michael Gelfond,,,11,
Ghost Signals: Verifying Termination of Busy-Waiting,"  Programs for multiprocessor machines commonly perform busy-waiting for synchronization. We propose a separation logic using so-called ghost signals to modularly verify termination of such programs under fair scheduling. Intuitively spoken, ghost signals lift the runtime concept of wait-notify synchronization to the verification level and allow a thread to busy-wait for an event $X$ while another thread promises to trigger $X$. ","Tobias Reinhard, Bart Jacobs",,,11,
The Vector Heat Method,"  This paper describes a method for efficiently computing parallel transport of tangent vectors on curved surfaces, or more generally, any vector-valued data on a curved manifold. More precisely, it extends a vector field defined over any region to the rest of the domain via parallel transport along shortest geodesics. This basic operation enables fast, robust algorithms for extrapolating level set velocities, inverting the exponential map, computing geometric medians and Karcher/Fr\'{e}chet means of arbitrary distributions, constructing centroidal Voronoi diagrams, and finding consistently ordered landmarks. Rather than evaluate parallel transport by explicitly tracing geodesics, we show that it can be computed via a short-time heat flow involving the connection Laplacian. As a result, transport can be achieved by solving three prefactored linear systems, each akin to a standard Poisson problem. To implement the method we need only a discrete connection Laplacian, which we describe for a variety of geometric data structures (point clouds, polygon meshes, etc). We also study the numerical behavior of our method, showing empirically that it converges under refinement, and augment the construction of intrinsic Delaunay triangulations (iDT) so that they can be used in the context of tangent vector field processing. ","Nicholas Sharp, Yousuf Soliman, Keenan Crane",,,11,
ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills,"  Humans are highly adept at walking in environments with foot placement constraints, including stepping-stone scenarios where the footstep locations are fully constrained. Finding good solutions to stepping-stone locomotion is a longstanding and fundamental challenge for animation and robotics. We present fully learned solutions to this difficult problem using reinforcement learning. We demonstrate the importance of a curriculum for efficient learning and evaluate four possible curriculum choices compared to a non-curriculum baseline. Results are presented for a simulated human character, a realistic bipedal robot simulation and a monster character, in each case producing robust, plausible motions for challenging stepping stone sequences and terrains. ","Zhaoming Xie, Hung Yu Ling, Nam Hee Kim, Michiel van de Panne",,,11,
Geometry-guided Dense Perspective Network for Speech-Driven Facial   Animation,"  Realistic speech-driven 3D facial animation is a challenging problem due to the complex relationship between speech and face. In this paper, we propose a deep architecture, called Geometry-guided Dense Perspective Network (GDPnet), to achieve speaker-independent realistic 3D facial animation. The encoder is designed with dense connections to strengthen feature propagation and encourage the re-use of audio features, and the decoder is integrated with an attention mechanism to adaptively recalibrate point-wise feature responses by explicitly modeling interdependencies between different neuron units. We also introduce a non-linear face reconstruction representation as a guidance of latent space to obtain more accurate deformation, which helps solve the geometry-related deformation and is good for generalization across subjects. Huber and HSIC (Hilbert-Schmidt Independence Criterion) constraints are adopted to promote the robustness of our model and to better exploit the non-linear and high-order correlations. Experimental results on the public dataset and real scanned dataset validate the superiority of our proposed GDPnet compared with state-of-the-art model. ","Jingying Liu, Binyuan Hui, Kun Li, Yunke Liu, Yu-Kun Lai, Yuxiang   Zhang, Yebin Liu and Jingyu Yang",,,11,
ShapeAssembly: Learning to Generate Programs for 3D Shape Structure   Synthesis,"  Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly, a domain-specific ""assembly-language"" for 3D shape structures. ShapeAssembly programs construct shapes by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. Its functions are parameterized with free variables, so that one program structure is able to capture a family of related shapes. We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. The program captures the subset of variability that is interpretable and editable. The deep model captures correlations across shape collections that are hard to express procedurally. We evaluate our approach by comparing shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds. ","R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang,   Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie",,,11,
Adding Custom Intersectors to the C++ Ray Tracing Template Library   Visionaray,"  Most ray tracing libraries allow the user to provide custom functionality that is executed when a potential ray surface interaction was encountered to determine if the interaction was valid or traversal should be continued. This is e.g. useful for alpha mask validation and allows the user to reuse existing ray object intersection routines rather than reimplementing them. Augmenting ray traversal with custom intersection logic requires some kind of callback mechanism that injects user code into existing library routines. With template libraries, this injection can happen statically since the user compiles the binary code herself. We present an implementation of this ""custom intersector"" approach and its integration into the C++ ray tracing template library Visionaray. ",Stefan Zellmann,,,11,
Point2Mesh: A Self-Prior for Deformable Meshes,"  In this paper, we introduce Point2Mesh, a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e., unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity. ","Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or",,,11,
FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers   in Dynamic Projection Mapping,"  This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range. ","Daiki Tone, Daisuke Iwai, Shinsaku Hiura, Kosuke Sato",,,11,
pylustrator: Code generation for reproducible figures for publication,"  One major challenge in science is to make all results potentially reproducible. Thus, along with the raw data, every step from basic processing of the data, evaluation, to the generation of the figures, has to be documented as clearly as possible. While there are many programming libraries that cover the basic processing and plotting steps (e.g. Matplotlib in Python), no library yet addresses the reproducible composing of single plots into meaningful figures for publication. Thus, up to now it is still state-of-the-art to generate publishable figures using image-processing or vector-drawing software leading to unwanted alterations of the presented data in the worst case and to figure quality reduction in the best case. Pylustrator a open source library based on the Matplotlib aims to fill this gap and provides a tool to easily generate the code necessary to compose publication figures from single plots. It provides a graphical user interface where the user can interactively compose the figures. All changes are tracked and converted to code that is automatically integrated into the calling script file. Thus, this software provides the missing link from raw data to the complete plot published in scientific journals and thus contributes to the transparency of the complete evaluation procedure. ",Richard Gerum,,,11,
Fast 3D Indoor Scene Synthesis with Discrete and Exact Layout Pattern   Extraction,"  We present a fast framework for indoor scene synthesis, given a room geometry and a list of objects with learnt priors. Unlike existing data-driven solutions, which often extract priors by co-occurrence analysis and statistical model fitting, our method measures the strengths of spatial relations by tests for complete spatial randomness (CSR), and extracts complex priors based on samples with the ability to accurately represent discrete layout patterns. With the extracted priors, our method achieves both acceleration and plausibility by partitioning input objects into disjoint groups, followed by layout optimization based on the Hausdorff metric. Extensive experiments show that our framework is capable of measuring more reasonable relations among objects and simultaneously generating varied arrangements in seconds. ","Song-Hai Zhang, Shao-Kui Zhang, Wei-Yu Xie, Cheng-Yang Luo, Hong-Bo Fu",,,11,
Accurate Face Rig Approximation with Deep Differential Subspace   Reconstruction,"  To be suitable for film-quality animation, rigs for character deformation must fulfill a broad set of requirements. They must be able to create highly stylized deformation, allow a wide variety of controls to permit artistic freedom, and accurately reflect the design intent. Facial deformation is especially challenging due to its nonlinearity with respect to the animation controls and its additional precision requirements, which often leads to highly complex face rigs that are not generalizable to other characters. This lack of generality creates a need for approximation methods that encode the deformation in simpler structures. We propose a rig approximation method that addresses these issues by learning localized shape information in differential coordinates and, separately, a subspace for mesh reconstruction. The use of differential coordinates produces a smooth distribution of errors in the resulting deformed surface, while the learned subspace provides constraints that reduce the low frequency error in the reconstruction. Our method can reconstruct both face and body deformations with high fidelity and does not require a set of well-posed animation examples, as we demonstrate with a variety of production characters. ","Steven L. Song, Weiqi Shi, Michael Reed",,,11,
Least-Squares Affine Reflection Using Eigen Decomposition,  This note summarizes the steps to computing the best-fitting affine reflection that aligns two sets of corresponding points. ,Alec Jacobson,,,11,
Developable B-spline surface generation from control rulings,"  An intuitive design method is proposed for generating developable ruled B-spline surfaces from a sequence of straight line segments indicating the surface shape. The first and last line segments are enforced to be the head and tail ruling lines of the resulting surface while the interior lines are required to approximate rulings on the resulting surface as much as possible. This manner of developable surface design is conceptually similar to the popular way of the freeform curve and surface design in the CAD community, observing that a developable ruled surface is a single parameter family of straight lines. This new design mode of the developable surface also provides more flexibility than the widely employed way of developable surface design from two boundary curves of the surface. The problem is treated by numerical optimization methods with which a particular level of distance error is allowed. We thus provide an effective tool for creating surfaces with a high degree of developability when the input control rulings do not lie in exact developable surfaces. We consider this ability as the superiority over analytical methods in that it can deal with arbitrary design inputs and find practically useful results. ",Zixuan Hu and Pengbo Bo,,,11,
LCollision: Fast Generation of Collision-Free Human Poses using Learned   Non-Penetration Constraints,"  We present a learning-based method (LCollision) that synthesizes collision-free 3D human poses. At the crux of our approach is a novel deep architecture that simultaneously decodes new human poses from the latent space and classifies the collision status. These two components of our architecture are used as the objective function and surrogate hard-constraints in a constrained-optimization algorithm for collision-free human pose generation. A novel aspect of our approach is the use of a bilevel autoencoder that decomposes whole-body collisions into groups of collisions between localized body parts. We show that solving our constrained optimization formulation can resolve significantly more collision artifacts than prior learning algorithms. Furthermore, in a large test set of $2.5\times 10^6$ randomized poses from three major datasets, our architecture achieves a collision-prediction accuracy of $94.1\%$ with $80\times$ speedup over exact collision detection algorithms. To the best of our knowledge, LCollision is the first approach that can obtain high accuracy in terms of handling non-penetration and collision constraints in a learning framework. ","Qingyang Tan, Zherong Pan, Dinesh Manocha",,,11,
Real-Time Visualization in Non-Isotropic Geometries,"  Non-isotropic geometries are of interest to low-dimensional topologists, physicists and cosmologists. However, they are challenging to comprehend and visualize. We present novel methods of computing real-time native geodesic rendering of non-isotropic geometries. Our methods can be applied not only to visualization, but also are essential for potential applications in machine learning and video games. ",Eryk Kopczy\'nski and Dorota Celi\'nska-Kopczy\'nska,,,11,
Lightform: Procedural Effects for Projected AR,"  Projected augmented reality, also called projection mapping or video mapping, is a form of augmented reality that uses projected light to directly augment 3D surfaces, as opposed to using pass-through screens or headsets. The value of projected AR is its ability to add a layer of digital content directly onto physical objects or environments in a way that can be instantaneously viewed by multiple people, unencumbered by a screen or additional setup.   Because projected AR typically involves projecting onto non-flat, textured objects (especially those that are conventionally not used as projection surfaces), the digital content needs to be mapped and aligned to precisely fit the physical scene to ensure a compelling experience. Current projected AR techniques require extensive calibration at the time of installation, which is not conducive to iteration or change, whether intentional (the scene is reconfigured) or not (the projector is bumped or settles). The workflows are undefined and fragmented, thus making it confusing and difficult for many to approach projected AR. For example, a digital artist may have the software expertise to create AR content, but could not complete an installation without experience in mounting, blending, and realigning projector(s); the converse is true for many A/V installation teams/professionals. Projection mapping has therefore been limited to high-end event productions, concerts, and films, because it requires expensive, complex tools, and skilled teams ($100K+ budgets).   Lightform provides a technology that makes projected AR approachable, practical, intelligent, and robust through integrated hardware and computer-vision software. Lightform brings together and unites a currently fragmented workflow into a single cohesive process that provides users with an approachable and robust method to create and control projected AR experiences. ","Brittany Factura, Laura LaPerche, Phil Reyneri, Brett Jones, Kevin   Karsch",,,11,
Inspection of histological 3D reconstructions in virtual reality,"  3D reconstruction is a challenging current topic in medical research. We perform 3D reconstructions from serial sections stained by immunohistological methods. This paper presents an immersive visualisation solution to quality control (QC), inspect, and analyse such reconstructions. QC is essential to establish correct digital processing methodologies. Visual analytics, such as annotation placement, mesh painting, and classification utility, facilitates medical research insights. We propose a visualisation in virtual reality (VR) for these purposes. In this manner, we advance the microanatomical research of human bone marrow and spleen. Both 3D reconstructions and original data are available in VR. Data inspection is streamlined by subtle implementation details and general immersion in VR. ","Oleg Lobachev, Moritz Berthold, Henriette Pfeffer, Michael Guthe,   Birte S. Steiniger",,,11,
Unwind: Interactive Fish Straightening,"  The ScanAllFish project is a large-scale effort to scan all the world's 33,100 known species of fishes. It has already generated thousands of volumetric CT scans of fish species which are available on open access platforms such as the Open Science Framework. To achieve a scanning rate required for a project of this magnitude, many specimens are grouped together into a single tube and scanned all at once. The resulting data contain many fish which are often bent and twisted to fit into the scanner. Our system, Unwind, is a novel interactive visualization and processing tool which extracts, unbends, and untwists volumetric images of fish with minimal user interaction. Our approach enables scientists to interactively unwarp these volumes to remove the undesired torque and bending using a piecewise-linear skeleton extracted by averaging isosurfaces of a harmonic function connecting the head and tail of each fish. The result is a volumetric dataset of a individual, straight fish in a canonical pose defined by the marine biologist expert user. We have developed Unwind in collaboration with a team of marine biologists: Our system has been deployed in their labs, and is presently being used for dataset construction, biomechanical analysis, and the generation of figures for scientific publication. ","Francis Williams, Alexander Bock, Harish Doraiswamy, Cassandra   Donatelli, Kayla Hall, Adam Summers, Daniele Panozzo, Cl\'audio T. Silva",,,11,
Octahedral Frames for Feature-Aligned Cross-Fields,"  We present a method for designing smooth cross fields on surfaces that automatically align to sharp features of an underlying geometry. Our approach introduces a novel class of energies based on a representation of cross fields in the spherical harmonic basis. We provide theoretical analysis of these energies in the smooth setting, showing that they penalize deviations from surface creases while otherwise promoting intrinsically smooth fields. We demonstrate the applicability of our method to quad-meshing and include an extensive benchmark comparing our fields to other automatic approaches for generating feature-aligned cross fields on triangle meshes. ","Paul Zhang, Josh Vekhter, Edward Chien, David Bommes, Etienne Vouga,   Justin Solomon",,,11,
Virtual reality for 3D histology: multi-scale visualization of organs   with interactive feature exploration,"  Virtual reality (VR) enables data visualization in an immersive and engaging manner, and it can be used for creating ways to explore scientific data. Here, we use VR for visualization of 3D histology data, creating a novel interface for digital pathology. Our contribution includes 3D modeling of a whole organ and embedded objects of interest, fusing the models with associated quantitative features and full resolution serial section patches, and implementing the virtual reality application. Our VR application is multi-scale in nature, covering two object levels representing different ranges of detail, namely organ level and sub-organ level. In addition, the application includes several data layers, including the measured histology image layer and multiple representations of quantitative features computed from the histology. In this interactive VR application, the user can set visualization properties, select different samples and features, and interact with various objects. In this work, we used whole mouse prostates (organ level) with prostate cancer tumors (sub-organ objects of interest) as example cases, and included quantitative histological features relevant for tumor biology in the VR model. Due to automated processing of the histology data, our application can be easily adopted to visualize other organs and pathologies from various origins. Our application enables a novel way for exploration of high-resolution, multidimensional data for biomedical research purposes, and can also be used in teaching and researcher training. ","Kaisa Liimatainen, Leena Latonen, Masi Valkonen, Kimmo Kartasalo,   Pekka Ruusuvuori",,,11,
3D Pseudo Stereo Visualization with Gpgpu Support,"  This article discusses the study of a computer system for creating 3D pseudo-stereo images and videos using hardware and software support for accelerating a synthesis process based on General Purpose Graphics Processing Unit (GPGPU) technology. Based on the general strategy of 3D pseudo-stereo synthesis previously proposed by the authors, Compute Unified Device Architect (CUDA) method considers the main implementation stages of 3D pseudo-stereo synthesis: (i) the practical implementation study; (ii) the synthesis characteristics for obtaining images; (iii) the video in Ultra-High Definition (UHD) 4K resolution using the Graphics Processing Unit (GPU). Respectively with these results of 4K content test on evaluation systems with a GPU the acceleration average of 60.6 and 6.9 times is obtained for images and videos. The research results show consistency with previously identified forecasts for processing 4K image frames. They are confirming the possibility of synthesizing 3D pseudo-stereo algorithms in real time using powerful support for modern Graphics Processing Unit/Graphics Processing Clusters (GPU/GPC). ",Anas M. Al-Oraiqat and Sergii A. Zori,,,11,
Supervised Discriminative Sparse PCA with Adaptive Neighbors for   Dimensionality Reduction,"  Dimensionality reduction is an important operation in information visualization, feature extraction, clustering, regression, and classification, especially for processing noisy high dimensional data. However, most existing approaches preserve either the global or the local structure of the data, but not both. Approaches that preserve only the global data structure, such as principal component analysis (PCA), are usually sensitive to outliers. Approaches that preserve only the local data structure, such as locality preserving projections, are usually unsupervised (and hence cannot use label information) and uses a fixed similarity graph. We propose a novel linear dimensionality reduction approach, supervised discriminative sparse PCA with adaptive neighbors (SDSPCAAN), to integrate neighborhood-free supervised discriminative sparse PCA and projected clustering with adaptive neighbors. As a result, both global and local data structures, as well as the label information, are used for better dimensionality reduction. Classification experiments on nine high-dimensional datasets validated the effectiveness and robustness of our proposed SDSPCAAN. ","Zhenhua Shi, Dongrui Wu, Jian Huang, Yu-Kai Wang, Chin-Teng Lin",,,11,
MIM: Mutual Information Machine,"  We introduce the Mutual Information Machine (MIM), a probabilistic auto-encoder for learning joint distributions over observations and latent variables. MIM reflects three design principles: 1) low divergence, to encourage the encoder and decoder to learn consistent factorizations of the same underlying distribution; 2) high mutual information, to encourage an informative relation between data and latent variables; and 3) low marginal entropy, or compression, which tends to encourage clustered latent representations. We show that a combination of the Jensen-Shannon divergence and the joint entropy of the encoding and decoding distributions satisfies these criteria, and admits a tractable cross-entropy bound that can be optimized directly with Monte Carlo and stochastic gradient descent. We contrast MIM learning with maximum likelihood and VAEs. Experiments show that MIM learns representations with high mutual information, consistent encoding and decoding distributions, effective latent clustering, and data log likelihood comparable to VAE, while avoiding posterior collapse. ","Micha Livne, Kevin Swersky, David J. Fleet",,,11,
Neighborhood Structure Assisted Non-negative Matrix Factorization and   its Application in Unsupervised Point Anomaly Detection,"  Dimensionality reduction is considered as an important step for ensuring competitive performance in unsupervised learning such as anomaly detection. Non-negative matrix factorization (NMF) is a popular and widely used method to accomplish this goal. But NMF, together with its recent, enhanced version, like graph regularized NMF or symmetric NMF, do not have the provision to include the neighborhood structure information and, as a result, may fail to provide satisfactory performance in presence of nonlinear manifold structure. To address that shortcoming, we propose to consider and incorporate the neighborhood structural similarity information within the NMF framework by modeling the data through a minimum spanning tree. What motivates our choice is the understanding that in the presence of complicated data structure, a minimum spanning tree can approximate the intrinsic distance between two data points better than a simple Euclidean distance does, and consequently, it constitutes a more reasonable basis for differentiating anomalies from the normal class data. We label the resulting method as the neighborhood structure assisted NMF. By comparing the formulation and properties of the neighborhood structure assisted NMF with other versions of NMF including graph regularized NMF and symmetric NMF, it is apparent that the inclusion of the neighborhood structure information using minimum spanning tree makes a key difference. We further devise both offline and online algorithmic versions of the proposed method. Empirical comparisons using twenty benchmark datasets as well as an industrial dataset extracted from a hydropower plant demonstrate the superiority of the neighborhood structure assisted NMF and support our claim of merit. ","Imtiaz Ahmed, Xia Ben Hu, Mithun P. Acharya and Yu Ding",,,11,
Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill   Discovery,"  Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games. ","Jiachen Yang, Igor Borovikov, Hongyuan Zha",,,11,
"Model Evaluation, Model Selection, and Algorithm Selection in Machine   Learning","  The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small. ",Sebastian Raschka,,,11,
A New Open-Access Platform for Measuring and Sharing mTBI Data,"  Despite numerous research efforts, the precise mechanisms of concussion have yet to be fully uncovered. Clinical studies on high-risk populations, such as contact sports athletes, have become more common and give insight on the link between impact severity and brain injury risk through the use of wearable sensors and neurological testing. However, as the number of institutions operating these studies grows, there is a growing need for a platform to share these data to facilitate our understanding of concussion mechanisms and aid in the development of suitable diagnostic tools. To that end, this paper puts forth two contributions: 1) a centralized, open-source platform for storing and sharing head impact data, in collaboration with the Federal Interagency Traumatic Brain Injury Research informatics system (FITBIR), and 2) a deep learning impact detection algorithm (MiGNet) to differentiate between true head impacts and false positives for the previously biomechanically validated instrumented mouthguard sensor (MiG2.0), all of which easily interfaces with FITBIR. We report 96% accuracy using MiGNet, based on a neural network model, improving on previous work based on Support Vector Machines achieving 91% accuracy, on an out of sample dataset of high school and collegiate football head impacts. The integrated MiG2.0 and FITBIR system serve as a collaborative research tool to be disseminated across multiple institutions towards creating a standardized dataset for furthering the knowledge of concussion biomechanics. ","August G. Domel, Samuel J. Raymond, Chiara Giordano, Yuzhe Liu, Seyed   Abdolmajid Yousefsani, Michael Fanton, Ileana Pirozzi, Ali Kight, Brett   Avery, Athanasia Boumis, Tyler Fetters, Simran Jandu, William M Mehring, Sam   Monga, Nicole Mouchawar, India Rangel, Eli Rice, Pritha Roy, Sohrab Sami,   Heer Singh, Lyndia Wu, Calvin Kuo, Michael Zeineh, Gerald Grant, David B.   Camarillo",,,11,
Collaborative Machine Learning with Incentive-Aware Model Rewards,"  Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets. ","Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, Bryan Kian Hsiang   Low",,,11,
Rethinking Generalisation,"  In this paper, a new approach to computing the generalisation performance is presented that assumes the distribution of risks, $\rho(r)$, for a learning scenario is known. From this, the expected error of a learning machine using empirical risk minimisation is computed for both classification and regression problems. A critical quantity in determining the generalisation performance is the power-law behaviour of $\rho(r)$ around its minimum value---a quantity we call attunement. The distribution $\rho(r)$ is computed for the case of all Boolean functions and for the perceptron used in two different problem settings. Initially a simplified analysis is presented where an independence assumption about the losses is made. A more accurate analysis is carried out taking into account chance correlations in the training set. This leads to corrections in the typical behaviour that is observed. ","Antonia Marcu and Adam Pr\""ugel-Bennett",,,11,
Towards Frequency-Based Explanation for Robust CNN,"  Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions. ","Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal and Zihao Ding",,,11,
Fair Algorithms for Hierarchical Agglomerative Clustering,"  Hierarchical Agglomerative Clustering (HAC) algorithms are extensively utilized in modern data science and machine learning, and seek to partition the dataset into clusters while generating a hierarchical relationship between the data samples themselves. HAC algorithms are employed in a number of applications, such as biology, natural language processing, and recommender systems. Thus, it is imperative to ensure that these algorithms are fair-- even if the dataset contains biases against certain protected groups, the cluster outputs generated should not be discriminatory against samples from any of these groups. However, recent work in clustering fairness has mostly focused on center-based clustering algorithms, such as k-median and k-means clustering. Therefore, in this paper, we propose fair algorithms for performing HAC that enforce fairness constraints 1) irrespective of the distance linkage criteria used, 2) generalize to any natural measures of clustering fairness for HAC, 3) work for multiple protected groups, and 4) have competitive running times to vanilla HAC. To the best of our knowledge, this is the first work that studies fairness for HAC algorithms. We also propose an algorithm with lower asymptotic time complexity than HAC algorithms that can rectify existing HAC outputs and make them subsequently fair as a result. Moreover, we carry out extensive experiments on multiple real-world UCI datasets to demonstrate the working of our algorithms. ","Anshuman Chhabra, Prasant Mohapatra",,,11,
Backward Feature Correction: How Deep Learning Performs Deep Learning,"  How does a 110-layer ResNet learn a high-complexity classifier using relatively few training examples and short training time? We present a theory towards explaining this in terms of hierarchical learning. We refer hierarchical learning as the learner learns to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning efficiently and automatically by applying SGD.   On the conceptual side, we present, to the best of our knowledge, the FIRST theory result indicating how deep neural networks can be sample and time efficient on certain hierarchical learning tasks, when NO KNOWN non-hierarchical algorithms (such as kernel method, linear regression over feature mappings, tensor decomposition, sparse coding, and their simple combinations) are efficient. We establish a principle called ""backward feature correction"", where training higher layers in the network can improve the features of lower level ones. We believe this is the key to understand the deep learning process in multi-layer neural networks.   On the technical side, we show for every input dimension $d > 0$, there is a concept class consisting of degree $\omega(1)$ multi-variate polynomials so that, using $\omega(1)$-layer neural networks as learners, SGD can learn any target function from this class in $\mathsf{poly}(d)$ time using $\mathsf{poly}(d)$ samples to any $\frac{1}{\mathsf{poly}(d)}$ error, through learning to represent it as a composition of $\omega(1)$ layers of quadratic functions. In contrast, we present lower bounds stating that several non-hierarchical learners, including any kernel methods, neural tangent kernels, must suffer from $d^{\omega(1)}$ sample or time complexity to learn this concept class even to $d^{-0.01}$ error. ",Zeyuan Allen-Zhu and Yuanzhi Li,,,11,
Complex data labeling with deep learning methods: Lessons from fisheries   acoustics,"  Quantitative and qualitative analysis of acoustic backscattered signals from the seabed bottom to the sea surface is used worldwide for fish stocks assessment and marine ecosystem monitoring. Huge amounts of raw data are collected yet require tedious expert labeling. This paper focuses on a case study where the ground truth labels are non-obvious: echograms labeling, which is time-consuming and critical for the quality of fisheries and ecological analysis. We investigate how these tasks can benefit from supervised learning algorithms and demonstrate that convolutional neural networks trained with non-stationary datasets can be used to stress parts of a new dataset needing human expert correction. Further development of this approach paves the way toward a standardization of the labeling process in fisheries acoustics and is a good case study for non-obvious data labeling processes. ","J.M.A.Sarr, T. Brochier, P.Brehmer, Y.Perrot, A.Bah, A.Sarr\'e,   M.A.Jeyid, M.Sidibeh, S.El Ayoub",,,11,
Probabilistic Feature Selection and Classification Vector Machine,"  Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency by failing to eliminate irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection method that adopts truncated Gaussian distributions as both sample and feature priors. The proposed method, called probabilistic feature selection and classification vector machine (PFCVMLP ), is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP . By tightening the bound, the importance of feature selection is demonstrated. ","Bingbing Jiang, Chang Li, Maarten de Rijke, Xin Yao and Huanhuan Chen",,,11,
K-TanH: Efficient TanH For Deep Learning,"  We propose K-TanH, a novel, highly accurate, hardware efficient approximation of popular activation function TanH for Deep Learning. K-TanH consists of parameterized low-precision integer operations, such as, shift and add/subtract (no floating point operation needed) where parameters are stored in very small look-up tables that can fit in CPU registers. K-TanH can work on various numerical formats, such as, Float32 and BFloat16. High quality approximations to other activation functions, e.g., Sigmoid, Swish and GELU, can be derived from K-TanH. Our AVX512 implementation of K-TanH demonstrates $>5\times$ speed up over Intel SVML, and it is consistently superior in efficiency over other approximations that use floating point arithmetic. Finally, we achieve state-of-the-art Bleu score and convergence results for training language translation model GNMT on WMT16 data sets with approximate TanH obtained via K-TanH on BFloat16 inputs. ","Abhisek Kundu, Alex Heinecke, Dhiraj Kalamkar, Sudarshan Srinivasan,   Eric C. Qin, Naveen K. Mellempudi, Dipankar Das, Kunal Banerjee, Bharat Kaul,   Pradeep Dubey",,,11,
Multi-future Merchant Transaction Prediction,"  The multivariate time series generated from merchant transaction history can provide critical insights for payment processing companies. The capability of predicting merchants' future is crucial for fraud detection and recommendation systems. Conventionally, this problem is formulated to predict one multivariate time series under the multi-horizon setting. However, real-world applications often require more than one future trend prediction considering the uncertainties, where more than one multivariate time series needs to be predicted. This problem is called multi-future prediction. In this work, we combine the two research directions and propose to study this new problem: multi-future, multi-horizon and multivariate time series prediction. This problem is crucial as it has broad use cases in the financial industry to reduce the risk while improving user experience by providing alternative futures. This problem is also challenging as now we not only need to capture the patterns and insights from the past but also train a model that has a strong inference capability to project multiple possible outcomes. To solve this problem, we propose a new model using convolutional neural networks and a simple yet effective encoder-decoder structure to learn the time series pattern from multiple perspectives. We use experiments on real-world merchant transaction data to demonstrate the effectiveness of our proposed model. We also provide extensive discussions on different model design choices in our experimental section. ","Chin-Chia Michael Yeh, Zhongfang Zhuang, Wei Zhang, Liang Wang",,,11,
Information Aware Max-Norm Dirichlet Networks for Predictive Uncertainty   Estimation,"  Precise estimation of uncertainty in predictions for AI systems is a critical factor in ensuring trust and safety. Deep neural networks trained with a conventional method are prone to over-confident predictions. In contrast to Bayesian neural networks that learn approximate distributions on weights to infer prediction confidence, we propose a novel method, Information Aware Dirichlet networks, that learn an explicit Dirichlet prior distribution on predictive distributions by minimizing a bound on the expected $L_\infty$ norm of the prediction error and penalizing information associated with incorrect outcomes. Properties of the new cost function are derived to indicate how improved uncertainty estimation is achieved. Experiments using real datasets show that our technique outperforms by a large margin state-of-the-art neural networks for estimating within-distribution and out-of-distribution uncertainty, and detecting adversarial examples. ",Theodoros Tsiligkaridis,,,11,
TrajGAIL: Generating Urban Vehicle Trajectories using Generative   Adversarial Imitation Learning,"  Recently, an abundant amount of urban vehicle trajectory data has been collected in road networks. Many studies have used machine learning algorithms to analyze patterns in vehicle trajectories to predict location sequences of individual travelers. Unlike the previous studies that used a discriminative modeling approach, this research suggests a generative modeling approach to learn the underlying distributions of urban vehicle trajectory data. A generative model for urban vehicle trajectories can better generalize from training data by learning the underlying distribution of the training data and, thus, produce synthetic vehicle trajectories similar to real vehicle trajectories with limited observations. Synthetic trajectories can provide solutions to data sparsity or data privacy issues in using location data. This research proposesTrajGAIL, a generative adversarial imitation learning framework for the urban vehicle trajectory generation. In TrajGAIL, learning location sequences in observed trajectories is formulated as an imitation learning problem in a partially observable Markov decision process. The model is trained by the generative adversarial framework, which uses the reward function from the adversarial discriminator. The model is tested with both simulation and real-world datasets, and the results show that the proposed model obtained significant performance gains compared to existing models in sequence modeling. ","Seongjin Choi, Jiwon Kim, Hwasoo Yeo",,,11,
Norm-based generalisation bounds for multi-class convolutional neural   networks,"  We show generalisation error bounds for deep learning with two main improvements over the state of the art. (1) Our bounds have no explicit dependence on the number of classes except for logarithmic factors. This holds even when formulating the bounds in terms of the $L^2$-norm of the weight matrices, where previous bounds exhibit at least a square-root dependence on the number of classes. (2) We adapt the classic Rademacher analysis of DNNs to incorporate weight sharing---a task of fundamental theoretical importance which was previously attempted only under very restrictive assumptions. In our results, each convolutional filter contributes only once to the bound, regardless of how many times it is applied. Further improvements exploiting pooling and sparse connections are provided. The presented bounds scale as the norms of the parameter matrices, rather than the number of parameters. In particular, contrary to bounds based on parameter counting, they are asymptotically tight (up to log factors) when the weights approach initialisation, making them suitable as a basic ingredient in bounds sensitive to the optimisation procedure. We also show how to adapt the recent technique of loss function augmentation to our situation to replace spectral norms by empirical analogues whilst maintaining the advantages of our approach. ",Antoine Ledent and Waleed Mustafa and Yunwen Lei and Marius Kloft,,,11,
Predicting Sim-to-Real Transfer with Probabilistic Dynamics Models,"  We propose a method to predict the sim-to-real transfer performance of RL policies. Our transfer metric simplifies the selection of training setups (such as algorithm, hyperparameters, randomizations) and policies in simulation, without the need for extensive and time-consuming real-world rollouts. A probabilistic dynamics model is trained alongside the policy and evaluated on a fixed set of real-world trajectories to obtain the transfer metric. Experiments show that the transfer metric is highly correlated with policy performance in both simulated and real-world robotic environments for complex manipulation tasks. We further show that the transfer metric can predict the effect of training setups on policy transfer performance. ","Lei M. Zhang, Matthias Plappert, Wojciech Zaremba",,,11,
An Overview of Neural Network Compression,"  Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.   Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof.   We assume a basic familiarity with deep learning architectures\footnote{For an introduction to deep learning, see ~\citet{goodfellow2016deep}}, namely, Recurrent Neural Networks~\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long}, Convolutional Neural Networks~\citep{fukushima1980neocognitron}~\footnote{For an up to date overview see~\citet{khan2019survey}} and Self-Attention based networks~\citep{vaswani2017attention}\footnote{For a general overview of self-attention networks, see ~\citet{chaudhari2019attentive}.},\footnote{For more detail and their use in natural language processing, see~\citet{hu2019introductory}}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures. ",James O' Neill,,,11,
GTOPX Space Mission Benchmarks,"  This contribution introduces the GTOPX space mission benchmark collection, which is an extension of GTOP database published by the European Space Agency (ESA). GTOPX consists of ten individual benchmark instances representing real-world interplanetary space trajectory design problems. In regard to the original GTOP collection, GTOPX includes three new problem instances featuring mixed-integer and multi-objective properties. GTOPX enables a simplified user handling, unified benchmark function call and some minor bug corrections to the original GTOP implementation. Furthermore, GTOPX is linked from it's original C++ source code to Python and Matlab based on dynamic link libraries, assuring computationally fast and accurate reproduction of the benchmark results in all three programming languages. Space mission trajectory design problems as those represented in GTOPX are known to be highly non-linear and difficult to solve. The GTOPX collection, therefore, aims particularly at researchers wishing to put advanced (meta)heuristic and hybrid optimization algorithms to the test. The goal of this paper is to provide researchers with a manual and reference to the newly available GTOPX benchmark software. ","Martin Schlueter, Mehdi Neshat, Mohamed Wahib, Masaharu Munetomo,   Markus Wagner",,,11,
Training robust anomaly detection using ML-Enhanced simulations,"  This paper describes the use of neural networks to enhance simulations for subsequent training of anomaly-detection systems. Simulations can provide edge conditions for anomaly detection which may be sparse or non-existent in real-world data. Simulations suffer, however, by producing data that is ""too clean"" resulting in anomaly detection systems that cannot transition from simulated data to actual conditions. Our approach enhances simulations using neural networks trained on real-world data to create outputs that are more realistic and variable than traditional simulations. ",Philip Feldman,,,11,
The MOEADr Package - A Component-Based Framework for Multiobjective   Evolutionary Algorithms Based on Decomposition,"  Multiobjective Evolutionary Algorithms based on Decomposition (MOEA/D) represent a widely used class of population-based metaheuristics for the solution of multicriteria optimization problems. We introduce the MOEADr package, which offers many of these variants as instantiations of a component-oriented framework. This approach contributes for easier reproducibility of existing MOEA/D variants from the literature, as well as for faster development and testing of new composite algorithms. The package offers an standardized, modular implementation of MOEA/D based on this framework, which was designed aiming at providing researchers and practitioners with a standard way to discuss and express MOEA/D variants. In this paper we introduce the design principles behind the MOEADr package, as well as its current components. Three case studies are provided to illustrate the main aspects of the package. ","Felipe Campelo, Lucas S. Batista, Claus Aranha",,,11,
A Novel Update Mechanism for Q-Networks Based On Extreme Learning   Machines,"  Reinforcement learning is a popular machine learning paradigm which can find near optimal solutions to complex problems. Most often, these procedures involve function approximation using neural networks with gradient based updates to optimise weights for the problem being considered. While this common approach generally works well, there are other update mechanisms which are largely unexplored in reinforcement learning. One such mechanism is Extreme Learning Machines. These were initially proposed to drastically improve the training speed of neural networks and have since seen many applications. Here we attempt to apply extreme learning machines to a reinforcement learning problem in the same manner as gradient based updates. This new algorithm is called Extreme Q-Learning Machine (EQLM). We compare its performance to a typical Q-Network on the cart-pole task - a benchmark reinforcement learning problem - and show EQLM has similar long-term learning performance to a Q-Network. ","Callum Wilson, Annalisa Riccardi, Edmondo Minisci",,,11,
Lower Bounds for Non-Elitist Evolutionary Algorithms via Negative   Multiplicative Drift,"  A decent number of lower bounds for non-elitist population-based evolutionary algorithms has been shown by now. Most of them are technically demanding due to the (hard to avoid) use of negative drift theorems -- general results which translate an expected progress away from the target into a high hitting time.   We propose a simple negative drift theorem for multiplicative drift scenarios and show that it can simplify existing analyses. We discuss in more detail Lehre's (PPSN 2010) \emph{negative drift in populations} method, one of the most general tools to prove lower bounds on the runtime of non-elitist mutation-based evolutionary algorithms for discrete search spaces. Together with other arguments, we obtain an alternative and simpler proof, which also strengthens and simplifies this method. In particular, now only three of the five technical conditions of the previous result have to be verified. The lower bounds we obtain are explicit instead of only asymptotic. This allows to compute concrete lower bounds for concrete algorithms, but also enables us to show that super-polynomial runtimes appear already when the reproduction rate is only a $(1 - \omega(n^{-1/2}))$ factor below the threshold. For the special case of algorithms using standard bit mutation with a random mutation rate (called uniform mixing in the language of hyper-heuristics), we prove the result stated by Dang and Lehre (PPSN 2016) and extend it to mutation rates other than $\Theta(1/n)$, which includes the heavy-tailed mutation operator proposed by Doerr, Le, Makhmara, and Nguyen (GECCO 2017). We finally apply our method and a novel domination argument to show an exponential lower bound for the runtime of the mutation-only simple genetic algorithm on \onemax for arbitrary population size. ",Benjamin Doerr,,,11,
Vehicle Routing and Scheduling for Regular Mobile Healthcare Services,"  We propose our solution to a particular practical problem in the domain of vehicle routing and scheduling. The generic task is finding the best allocation of the minimum number of \emph{mobile resources} that can provide periodical services in remote locations. These \emph{mobile resources} are based at a single central location. Specifications have been defined initially for a real-life application that is the starting point of an ongoing project. Particularly, the goal is to mitigate health problems in rural areas around a city in Romania. Medically equipped vans are programmed to start daily routes from county capital, provide a given number of examinations in townships within the county and return to the capital city in the same day. From the health care perspective, each van is equipped with an ultrasound scanner, and they are scheduled to investigate pregnant woman each trimester aiming to diagnose potential problems. The project is motivated by reports currently ranking Romania as the country with the highest infant mortality rate in the European Union.   We developed our solution in two phases: modeling of the most relevant parameters and data available for our goal and then design and implement an algorithm that provides an optimized solution. The most important metric of an output scheduling is the number of vans that are necessary to provide a given amount of examination time per township, followed by total travel time or fuel consumption, number of different routes, and others. Our solution implements two probabilistic algorithms out of which we chose the one that performs the best. ","Cosmin Pascaru, Paul Diac",,,11,
Forecasting Solar Activity with Two Computational Intelligence Models (A   Comparative Study),"  Solar activity It is vital to accurately predict solar activity, in order to decrease the plausible damage of electronic equipment in the event of a large high-intensity solar eruption. Recently, we have proposed BELFIS (Brain Emotional Learning-based Fuzzy Inference System) as a tool for the forecasting of chaotic systems. The structure of BELFIS is designed based on the neural structure of fear conditioning. The function of BELFIS is implemented by assigning adaptive networks to the components of the BELFIS structure. This paper especially focuses on performance evaluation of BELFIS as a predictor by forecasting solar cycles 16 to 24. The performance of BELFIS is compared with other computational models used for this purpose, and in particular with adaptive neuro-fuzzy inference system (ANFIS). ","M.Parsapoor, U.Bilstrup, B.Svensson",,,11,
Fitness Landscape Analysis of Dimensionally-Aware Genetic Programming   Featuring Feynman Equations,"  Genetic programming is an often-used technique for symbolic regression: finding symbolic expressions that match data from an unknown function. To make the symbolic regression more efficient, one can also use dimensionally-aware genetic programming that constrains the physical units of the equation. Nevertheless, there is no formal analysis of how much dimensionality awareness helps in the regression process. In this paper, we conduct a fitness landscape analysis of dimensionallyaware genetic programming search spaces on a subset of equations from Richard Feynmans well-known lectures. We define an initialisation procedure and an accompanying set of neighbourhood operators for conducting the local search within the physical unit constraints. Our experiments show that the added information about the variable dimensionality can efficiently guide the search algorithm. Still, further analysis of the differences between the dimensionally-aware and standard genetic programming landscapes is needed to help in the design of efficient evolutionary operators to be used in a dimensionally-aware regression. ","Marko Durasevic, Domagoj Jakobovic, Marcella Scoczynski Ribeiro   Martins, Stjepan Picek, and Markus Wagner",,,11,
Under the Hood of Neural Networks: Characterizing Learned   Representations by Functional Neuron Populations and Network Ablations,"  The need for more transparency of the decision-making processes in artificial neural networks steadily increases driven by their applications in safety critical and ethically challenging domains such as autonomous driving or medical diagnostics. We address today's lack of transparency of neural networks and shed light on the roles of single neurons and groups of neurons within the network fulfilling a learned task. Inspired by research in the field of neuroscience, we characterize the learned representations by activation patterns and network ablations, revealing functional neuron populations that a) act jointly in response to specific stimuli or b) have similar impact on the network's performance after being ablated. We find that neither a neuron's magnitude or selectivity of activation, nor its impact on network performance are sufficient stand-alone indicators for its importance for the overall task. We argue that such indicators are essential for future advances in transfer learning and modern neuroscience. ","Richard Meyes, Constantin Waubert de Puiseau, Andres Posada-Moreno,   Tobias Meisen",,,11,
Global Convergence of the (1+1) Evolution Strategy,"  We establish global convergence of the (1+1) evolution strategy, i.e., convergence to a critical point independent of the initial state. More precisely, we show the existence of a critical limit point, using a suitable extension of the notion of a critical point to measurable functions. At its core, the analysis is based on a novel progress guarantee for elitist, rank-based evolutionary algorithms. By applying it to the (1+1) evolution strategy we are able to provide an accurate characterization of whether global convergence is guaranteed with full probability, or whether premature convergence is possible. We illustrate our results on a number of example applications ranging from smooth (non-convex) cases over different types of saddle points and ridge functions to discontinuous and extremely rugged problems. ",Tobias Glasmachers,,,11,
On the Resilience of an Ant-based System in Fuzzy Environments. An   Empirical Study,"  The current work describes an empirical study conducted in order to investigate the behavior of an optimization method in a fuzzy environment. MAX-MIN Ant System, an efficient implementation of a heuristic method is used for solving an optimization problem derived from the Traveling Salesman Problem (TSP). Several publicly-available symmetric TSP instances and their fuzzy variants are tested in order to extract some general features. The entry data was adapted by introducing a two-dimensional systematic degree of fuzziness, proportional with the number of nodes, the dimension of the instance and also with the distances between nodes, the scale of the instance. The results show that our proposed method can handle the data uncertainty, showing good resilience and adaptability. ","Gloria Cerasela Crisan, Camelia-M. Pintea, Petrica C. Pop",,,11,
Deep Reservoir Networks with Learned Hidden Reservoir Weights using   Direct Feedback Alignment,"  Deep Reservoir Computing has emerged as a new paradigm for deep learning, which is based around the reservoir computing principle of maintaining random pools of neurons combined with hierarchical deep learning. The reservoir paradigm reflects and respects the high degree of recurrence in biological brains, and the role that neuronal dynamics play in learning. However, one issue hampering deep reservoir network development is that one cannot backpropagate through the reservoir layers. Recent deep reservoir architectures do not learn hidden or hierarchical representations in the same manner as deep artificial neural networks, but rather concatenate all hidden reservoirs together to perform traditional regression. Here we present a novel Deep Reservoir Network for time series prediction and classification that learns through the non-differentiable hidden reservoir layers using a biologically-inspired backpropagation alternative called Direct Feedback Alignment, which resembles global dopamine signal broadcasting in the brain. We demonstrate its efficacy on two real world multidimensional time series datasets. ","Matthew Evanusa and Cornelia Ferm\""uller and Yiannis Aloimonos",,,11,
A Decomposition-based Large-scale Multi-modal Multi-objective   Optimization Algorithm,"  A multi-modal multi-objective optimization problem is a special kind of multi-objective optimization problem with multiple Pareto subsets. In this paper, we propose an efficient multi-modal multi-objective optimization algorithm based on the widely used MOEA/D algorithm. In our proposed algorithm, each weight vector has its own sub-population. With a clearing mechanism and a greedy removal strategy, our proposed algorithm can effectively preserve equivalent Pareto optimal solutions (i.e., different Pareto optimal solutions with same objective values). Experimental results show that our proposed algorithm can effectively preserve the diversity of solutions in the decision space when handling large-scale multi-modal multi-objective optimization problems. ","Yiming Peng, Hisao Ishibuchi",,,11,
Deep Learning in Target Space,"  Deep learning uses neural networks which are parameterised by their weights. The neural networks are usually trained by tuning the weights to directly minimise a given loss function. In this paper we propose to reparameterise the weights into targets for the firing strengths of the individual nodes in the network. Given a set of targets, it is possible to calculate the weights which make the firing strengths best meet those targets. It is argued that using targets for training addresses the problem of exploding gradients, by a process which we call cascade untangling, and makes the loss-function surface smoother to traverse, and so leads to easier, faster training, and also potentially better generalisation, of the neural network. It also allows for easier learning of deeper and recurrent network structures. The necessary conversion of targets to weights comes at an extra computational expense, which is in many cases manageable. Learning in target space can be combined with existing neural-network optimisers, for extra gain. Experimental results show the speed of using target space, and examples of improved generalisation, for fully-connected networks and convolutional networks, and the ability to recall and process long time sequences and perform natural-language processing with recurrent networks. ","Michael Fairbank, Spyridon Samothrakis and Luca Citi",,,11,
Multi-Level Evolution Strategies for High-Resolution Black-Box Control,"  This paper introduces a multi-level (m-lev) mechanism into Evolution Strategies (ESs) in order to address a class of global optimization problems that could benefit from fine discretization of their decision variables. Such problems arise in engineering and scientific applications, which possess a multi-resolution control nature, and thus may be formulated either by means of low-resolution variants (providing coarser approximations with presumably lower accuracy for the general problem) or by high-resolution controls. A particular scientific application concerns practical Quantum Control (QC) problems, whose targeted optimal controls may be discretized to increasingly higher resolution, which in turn carries the potential to obtain better control yields. However, state-of-the-art derivative-free optimization heuristics for high-resolution formulations nominally call for an impractically large number of objective function calls. Therefore, an effective algorithmic treatment for such problems is needed. We introduce a framework with an automated scheme to facilitate guided-search over increasingly finer levels of control resolution for the optimization problem, whose on-the-fly learned parameters require careful adaptation. We instantiate the proposed m-lev self-adaptive ES framework by two specific strategies, namely the classical elitist single-child (1+1)-ES and the non-elitist multi-child derandomized $(\mu_W,\lambda)$-sep-CMA-ES. We first show that the approach is suitable by simulation-based optimization of QC systems which were heretofore viewed as too complex to address. We also present a laboratory proof-of-concept for the proposed approach on a basic experimental QC system objective. ",Ofer M. Shir and Xi Xing and Herschel Rabitz,,,11,
"On higher order computations, rewiring the connectome, and non-von   Neumann computer architecture","  Structural plasticity in the brain (i.e. rewiring the connectome) may be viewed as mechanisms for dynamic reconfiguration of neural circuits. First order computations in the brain are done by static neural circuits, whereas higher order computations are done by dynamic reconfigurations of the links (synapses) between the neural circuits. Static neural circuits correspond to first order computable functions. Synapse creation (activation) between them correspond to the mathematical notion of function composition. Functionals are higher order functions that take functions as their arguments. The construction of functionals is based on dynamic reconfigurations of function compositions. Perhaps the functionals correspond to rewiring mechanisms of the connectome. The architecture of human mind is different than the von Neumann computer architecture. Higher order computations in the human brain (based on functionals) may suggest a non-von Neumann computer architecture, a challenge posed by John Backus in 1977 \cite{Backus}. The presented work is a substantial extension and revision of the paper published in Proc. ICANN2016. ",Stanislaw Ambroszkiewicz,,,11,
More Effective Randomized Search Heuristics for Graph Coloring Through   Dynamic Optimization,"  Dynamic optimization problems have gained significant attention in evolutionary computation as evolutionary algorithms (EAs) can easily adapt to changing environments. We show that EAs can solve the graph coloring problem for bipartite graphs more efficiently by using dynamic optimization. In our approach the graph instance is given incrementally such that the EA can reoptimize its coloring when a new edge introduces a conflict. We show that, when edges are inserted in a way that preserves graph connectivity, Randomized Local Search (RLS) efficiently finds a proper 2-coloring for all bipartite graphs. This includes graphs for which RLS and other EAs need exponential expected time in a static optimization scenario. We investigate different ways of building up the graph by popular graph traversals such as breadth-first-search and depth-first-search and analyse the resulting runtime behavior. We further show that offspring populations (e. g. a (1+$\lambda$) RLS) lead to an exponential speedup in $\lambda$. Finally, an island model using 3 islands succeeds in an optimal time of $\Theta(m)$ on every $m$-edge bipartite graph, outperforming offspring populations. This is the first example where an island model guarantees a speedup that is not bounded in the number of islands. ","Jakob Bossek, Frank Neumann, Pan Peng, Dirk Sudholt",,,11,
Exploring Adversarial Attack in Spiking Neural Networks with   Spike-Compatible Gradient,"  Recently, backpropagation through time inspired learning algorithms are widely introduced into SNNs to improve the performance, which brings the possibility to attack the models accurately given Spatio-temporal gradient maps. We propose two approaches to address the challenges of gradient input incompatibility and gradient vanishing. Specifically, we design a gradient to spike converter to convert continuous gradients to ternary ones compatible with spike inputs. Then, we design a gradient trigger to construct ternary gradients that can randomly flip the spike inputs with a controllable turnover rate, when meeting all zero gradients. Putting these methods together, we build an adversarial attack methodology for SNNs trained by supervised algorithms. Moreover, we analyze the influence of the training loss function and the firing threshold of the penultimate layer, which indicates a ""trap"" region under the cross-entropy loss that can be escaped by threshold tuning. Extensive experiments are conducted to validate the effectiveness of our solution. Besides the quantitative analysis of the influence factors, we evidence that SNNs are more robust against adversarial attack than ANNs. This work can help reveal what happens in SNN attack and might stimulate more research on the security of SNN models and neuromorphic devices. ","Ling Liang, Xing Hu, Lei Deng, Yujie Wu, Guoqi Li, Yufei Ding, Peng   Li, Yuan Xie",,,11,
Artificial Intelligence in Surgery: Neural Networks and Deep Learning,"  Deep neural networks power most recent successes of artificial intelligence, spanning from self-driving cars to computer aided diagnosis in radiology and pathology. The high-stake data intensive process of surgery could highly benefit from such computational methods. However, surgeons and computer scientists should partner to develop and assess deep learning applications of value to patients and healthcare systems. This chapter and the accompanying hands-on material were designed for surgeons willing to understand the intuitions behind neural networks, become familiar with deep learning concepts and tasks, grasp what implementing a deep learning model in surgery means, and finally appreciate the specific challenges and limitations of deep neural networks in surgery. For the associated hands-on material, please see https://github.com/CAMMA-public/ai4surgery. ","Deepak Alapatt and Pietro Mascagni, Vinkle Srivastav, Nicolas Padoy",,,11,
Multi-layer local optima networks for the analysis of advanced local   search-based algorithms,"  A Local Optima Network (LON) is a graph model that compresses the fitness landscape of a particular combinatorial optimization problem based on a specific neighborhood operator and a local search algorithm. Determining which and how landscape features affect the effectiveness of search algorithms is relevant for both predicting their performance and improving the design process. This paper proposes the concept of multi-layer LONs as well as a methodology to explore these models aiming at extracting metrics for fitness landscape analysis. Constructing such models, extracting and analyzing their metrics are the preliminary steps into the direction of extending the study on single neighborhood operator heuristics to more sophisticated ones that use multiple operators. Therefore, in the present paper we investigate a twolayer LON obtained from instances of a combinatorial problem using bitflip and swap operators. First, we enumerate instances of NK-landscape model and use the hill climbing heuristic to build the corresponding LONs. Then, using LON metrics, we analyze how efficiently the search might be when combining both strategies. The experiments show promising results and demonstrate the ability of multi-layer LONs to provide useful information that could be used for in metaheuristics based on multiple operators such as Variable Neighborhood Search. ","Marcella Scoczynski Ribeiro Martins, Mohamed El Yafrani, Myriam R. B.   S. Delgado, and Ricardo Luders",,,11,
A Faster-Than Relation for Semi-Markov Decision Processes,"  When modeling concurrent or cyber-physical systems, non-functional requirements such as time are important to consider. In order to improve the timing aspects of a model, it is necessary to have some notion of what it means for a process to be faster than another, which can guide the stepwise refinement of the model. To this end we study a faster-than relation for semi-Markov decision processes and compare it to standard notions for relating systems. We consider the compositional aspects of this relation, and show that the faster-than relation is not a precongruence with respect to parallel composition, hence giving rise to so-called parallel timing anomalies. We take the first steps toward understanding this problem by identifying decidable conditions sufficient to avoid parallel timing anomalies in the absence of non-determinism. ","Mathias Ruggaard Pedersen (Aalborg University), Giorgio Bacci (Aalborg   University), Kim Guldstrand Larsen (Aalborg University)",,,11,
Formal assessment of some properties of Context-Aware Systems,"  Context-Aware systems are becoming useful components in autonomic and monitoring applications and the assessment of their properties is an important step towards reliable implementation, especially in safety-critical applications. In this paper, using an avalanche/landslide alert system as a running example, we propose a technique, based on Boolean Control Networks, to verify that the system dynamics has stable equilibrium states, corresponding to constant inputs, and hence it does not exhibit oscillatory behaviors, and to establish other useful properties in order to implement a precise and timely alarm system. ",Fabio A. Schreiber and Maria Elena Valcher,,,11,
Taking-and-merging games as rewrite games,"  This work is a contribution to the study of rewrite games. Positions are finite words, and the possible moves are defined by a finite number of local rewriting rules. We introduce and investigate taking-and-merging games, that is, where each rule is of the form a^k->epsilon.   We give sufficient conditions for a game to be such that the losing positions (resp. the positions with a given Grundy value) form a regular language or a context-free language. We formulate several related open questions in parallel with the famous conjecture of Guy about the periodicity of the Grundy function of octal games.   Finally we show that more general rewrite games quickly lead to undecidable problems. Namely, it is undecidable whether there exists a winning position in a given regular language, even if we restrict to games where each move strictly reduces the length of the current position. We formulate several related open questions in parallel with the famous conjecture of Guy about the periodicity of the Grundy function of octal games. ",Eric Duch\^ene and Victor Marsault and Aline Parreau and Michel Rigo,,,11,
Binary intersection formalized,"  We provide a reformulation and a formalization of the classical result by Juhani Karhum\""aki characterizing intersections of two languages of the form $\{x,y\}^*\cap \{u,v\}^*$. We use the terminology of morphisms which allows to formulate the result in a shorter and more transparent way, and we formalize the result in the proof assistant Isabelle/HOL. ",\v{S}t\v{e}p\'an Holub and \v{S}t\v{e}p\'an Starosta,,,11,
"Words With Few Palindromes, Revisited","  In 2013, Fici and Zamboni proved a number of theorems about finite and infinite words having only a small number of factors that are palindromes. In this paper we rederive some of their results, and obtain some new ones, by a different method based on finite automata. ",Lukas Fleischer and Jeffrey Shallit,,,11,
Palindromic Length of Words with Many Periodic Palindromes,"  The palindromic length $\text{PL}(v)$ of a finite word $v$ is the minimal number of palindromes whose concatenation is equal to $v$. In 2013, Frid, Puzynina, and Zamboni conjectured that: If $w$ is an infinite word and $k$ is an integer such that $\text{PL}(u)\leq k$ for every factor $u$ of $w$ then $w$ is ultimately periodic.   Suppose that $w$ is an infinite word and $k$ is an integer such $\text{PL}(u)\leq k$ for every factor $u$ of $w$. Let $\Omega(w,k)$ be the set of all factors $u$ of $w$ that have more than $\sqrt[k]{k^{-1}\vert u\vert}$ palindromic prefixes. We show that $\Omega(w,k)$ is an infinite set and we show that for each positive integer $j$ there are palindromes $a,b$ and a word $u\in \Omega(w,k)$ such that $(ab)^j$ is a factor of $u$ and $b$ is nonempty. Note that $(ab)^j$ is a periodic word and $(ab)^ia$ is a palindrome for each $i\leq j$. These results justify the following question: What is the palindromic length of a concatenation of a suffix of $b$ and a periodic word $(ab)^j$ with ""many"" periodic palindromes?   It is known that $\lvert\text{PL}(uv)-\text{PL}(u)\rvert\leq \text{PL}(v)$, where $u$ and $v$ are nonempty words. The main result of our article shows that if $a,b$ are palindromes, $b$ is nonempty, $u$ is a nonempty suffix of $b$, $\vert ab\vert$ is the minimal period of $aba$, and $j$ is a positive integer with $j\geq3\text{PL}(u)$ then $\text{PL}(u(ab)^j)-\text{PL}(u)\geq 0$. ",Josef Rukavicka,,,11,
Pebble Minimization of Polyregular Functions,"  We show that a polyregular word-to-word function is regular if and only if its output size is at most linear in its input size. Moreover a polyregular function can be realized by: a transducer with two pebbles if and only if its output has quadratic size in its input, a transducer with three pebbles if and only if its output has cubic size in its input, etc. Moreover the characterization is decidable and, given a polyregular function, one can compute a transducer realizing it with the minimal number of pebbles. We apply the result to mso interpretations from words to words. We show that mso interpretations of dimension k exactly coincide with k-pebble transductions. ",Nathan Lhote,,,11,
Abelian periods of factors of Sturmian words,"  We study the abelian period sets of Sturmian words, which are codings of irrational rotations on a one-dimensional torus. The main result states that the minimum abelian period of a factor of a Sturmian word of angle $\alpha$ with continued fraction expansion $[0; a_1, a_2, \ldots]$ is either $tq_k$ with $1 \leq t \leq a_{k+1}$ (a multiple of a denominator $q_k$ of a convergent of $\alpha$) or $q_{k,\ell}$ (a denominator $q_{k,\ell}$ of a semiconvergent of $\alpha$). This result generalizes a result of Fici et. al stating that the abelian period set of the Fibonacci word is the set of Fibonacci numbers. A characterization of the Fibonacci word in terms of its abelian period set is obtained as a corollary. ","Jarkko Peltom\""aki",,,11,
Decidability and k-Regular Sequences,"  In this paper we consider a number of natural decision problems involving k-regular sequences. Specifically, they arise from - lower and upper bounds on growth rate; in particular boundedness, - images, - regularity (recognizability by a deterministic finite automaton) of preimages, and - factors, such as squares and palindromes of such sequences. We show that the decision problems are undecidable. ",Daniel Krenn and Jeffrey Shallit,,,11,
A note on the class of languages generated by F-systems over regular   languages,"  An F-system is a computational model that performs a folding operation on strings of a given language, following directions coded on strings of another given language. This note considers the case in which both given languages are regular, and it shows that such F-system generates linear context-free languages. The demonstration is based on constructing a one-turn pushdown automaton for the generated language. ",Jorge C. Lucero,,,11,
The Big-O Problem for Labelled Markov Chains and Weighted Automata,"  Given two weighted automata, we consider the problem of whether one is big-O of the other, i.e., if the weight of every finite word in the first is not greater than some constant multiple of the weight in the second.   We show that the problem is undecidable, even for the instantiation of weighted automata as labelled Markov chains. Moreover, even when it is known that one weighted automaton is big-O of another, the problem of finding or approximating the associated constant is also undecidable.   Our positive results show that the big-O problem is polynomial-time solvable for unambiguous automata, coNP-complete for unlabelled weighted automata (i.e., when the alphabet is a single character) and decidable, subject to Schanuel's conjecture, when the language is bounded (i.e., a subset of $w_1^*\dots w_m^*$ for some finite words $w_1,\dots,w_m$).   On labelled Markov chains, the problem can be restated as a ratio total variation distance, which, instead of finding the maximum difference between the probabilities of any two events, finds the maximum ratio between the probabilities of any two events. The problem is related to $\epsilon$-differential privacy, for which the optimal constant of the big-O notation is exactly $\exp(\epsilon)$. ","Dmitry Chistikov, Stefan Kiefer, Andrzej S. Murawski and David Purser",,,11,
A non-regular language of infinite trees that is recognizable by a   sort-wise finite algebra,"  $\omega$-clones are multi-sorted structures that naturally emerge as algebras for infinite trees, just as $\omega$-semigroups are convenient algebras for infinite words. In the algebraic theory of languages, one hopes that a language is regular if and only if it is recognized by an algebra that is finite in some simple sense. We show that, for infinite trees, the situation is not so simple: there exists an $\omega$-clone that is finite on every sort and finitely generated, but recognizes a non-regular language. ","Miko{\l}aj Boja\'nczyk, Bartek Klin",,,11,
Congruences for Stochastic Automata,"  Congruences for stochastic automata are defined, the correspondin factor automata are constructed and investigated for automata ove analytic spaces. We study the behavior under finite and infinite streams. Congruences consist of multiple parts, it is shown that factoring can be done in multiple steps, guided by these parts. ",Ernst-Erich Doberkat,,,11,
Grey-Box Learning of Register Automata,"  Model learning (a.k.a. active automata learning) is a highly effective technique for obtaining black-box finite state models of software components. Thus far, generalisation to infinite state systems with inputs/outputs that carry data parameters has been challenging. Existing model learning tools for infinite state systems face scalability problems and can only be applied to restricted classes of systems (register automata with equality/inequality). In this article, we show how we can boost the performance of model learning techniques by extracting the constraints on input and output parameters from a run, and making this grey-box information available to the learner. More specifically, we provide new implementations of the tree oracle and equivalence oracle from RALib, which use the derived constraints. We extract the constraints from runs of Python programs using an existing tainting library for Python, and compare our grey-box version of RALib with the existing black-box version on several benchmarks, including some data structures from Python's standard library. Our proof-of-principle implementation results in almost two orders of magnitude improvement in terms of numbers of inputs sent to the software system. Our approach, which can be generalised to richer model classes, also enables RALib to learn models that are out of reach of black-box techniques, such as combination locks. ","Bharat Garhewal, Frits Vaandrager, Falk Howar, Timo Schrijvers, Toon   Lenaerts, Rob Smits",,,11,
Kontrol Edilebilir ptSTL Formulu Sentezi -- Synthesis of Controllable   ptSTL Formulas,"  In this work, we develop an approach to anomaly detection and prevention problem using Signal Temporal Logic (STL). This approach consists of two steps: detection of the causes of the anomalities as STL formulas and prevention of the satisfaction of the formula via controller synthesis. This work focuses on the first step and proposes a formula template such that any controllable cause can be represented in this template. An efficient algorithm to synthesize formulas in this template is presented. Finally, the results are shown on an example.   -----   Bu bildiride anomali tespiti ve onlenmesi problemine, Sinyal Zamansal Mantigi (Signal Temporal Logic) tabanli iki asamali bir cozum sunulmaktadir. Ilk asama nedenlerin tespiti, ikinci asama ise bir kontrol stratejisi ile nedenlerin sistem uzerinde engellenmesidir. Iki asama birbirine bagimlidir. Bu bildiride, ilk asama olan istenmeyen olaylarin nedenlerinin tespitinde kullanilan neden formulu sablonu gelistirilmektedir. Bildiride kullanilan sablon ile butun kontrol edilebilir formuller tanimlanabilmektedir. Bu sablon icin verimli bir formul sentezleme algoritmasi sunulmus, ve sonuclar ornek bir sistem uzerinde gosterilmistir. ","Irmak Saglam, Ebru Aydin Gol",,,11,
On the Succinctness of Alternating Parity Good-for-Games Automata,"  We study alternating parity good-for-games (GFG) automata, i.e., alternating parity automata where both conjunctive and disjunctive choices can be resolved in an online manner, without knowledge of the suffix of the input word still to be read.   We show that they can be exponentially more succinct than both their nondeterministic and universal counterparts. Furthermore, we present a single exponential determinisation procedure and an Exptime upper bound to the problem of recognising whether an alternating automaton is GFG. We also study the complexity of deciding ""half-GFGness"", a property specific to alternating automata that only requires nondeterministic choices to be resolved in an online manner. We show that this problem is PSpace-hard already for alternating automata on finite words. ","Udi Boker, Denis Kuperberg, Karoliina Lehtinen, Micha{\l} Skrzypczak",,,11,
Descriptional Complexity of Winning Sets of Regular Languages,"  We investigate certain word-construction games with variable turn orders. In these games, Alice and Bob take turns on choosing consecutive letters of a word of fixed length, with Alice winning if the result lies in a predetermined target language. The turn orders that result in a win for Alice form a binary language that is regular whenever the target language is, and we prove some upper and lower bounds for its state complexity based on that of the target language. ","Pierre Marcus and Ilkka T\""orm\""a",,,11,
Non-maximal sensitivity to synchronism in periodic elementary cellular   automata: exact asymptotic measures,"  In [11] and [13] the authors showed that elementary cellular automata rules 0, 3, 8, 12, 15, 28, 32, 34, 44, 51, 60, 128, 136, 140, 160, 162, 170, 200 and 204 (and their conjugation, reflection, reflected-conjugation) are not maximum sensitive to synchronism, i.e. they do not have a different dynamics for each (non-equivalent) block-sequential update schedule (defined as ordered partitions of cell positions). In this work we present exact measurements of the sensitivity to synchronism for these rules, as functions of the size. These exhibit a surprising variety of values and associated proof methods, such as the special pairs of rule 128, and the connection to the bissection of Lucas numbers of rule 8. ","Pedro P. B. de Oliveira, Enrico Formenti, K\'evin Perrot, Sara Riva,   and Eurico L. P. Ruivo",,,11,
Opportunistic Synthesis in Reactive Games under Information Asymmetry,"  Reactive synthesis is a class of methods to construct a provably-correct control system, referred to as a robot, with respect to a temporal logic specification in the presence of a dynamic and uncontrollable environment. This is achieved by modeling the interaction between the robot and its environment as a two-player zero-sum game. However, existing reactive synthesis methods assume both players to have complete information, which is not the case in many strategic interactions. In this paper, we use a variant of hypergames to model the interaction between the robot and its environment; which has incomplete information about the specification of the robot. This model allows us to identify a subset of game states from where the robot can leverage the asymmetrical information to achieve a better outcome, which is not possible if both players have symmetrical and complete information. We then introduce a novel method of opportunistic synthesis by defining a Markov Decision Process (MDP) using the hypergame under temporal logic specifications. When the environment plays some stochastic strategy in its perceived sure-winning and sure-losing regions of the game, we show that by following the opportunistic strategy, the robot is ensured to only improve the outcome of the game - measured by satisfaction of sub-specifications - whenever an opportunity becomes available. We demonstrate the correctness and optimality of this method using a robot motion planning example in the presence of an adversary. ","Abhishek N. Kulkarni, Jie Fu",,,11,
Transfinite Lyndon words,"  In this paper, we extend the notion of Lyndon word to transfinite words. We prove two main results. We first show that, given a transfinite word, there exists a unique factorization in Lyndon words that are densely non-increasing, a relaxation of the condition used in the case of finite words.   In the annex, we prove that the factorization of a rational word has a special form and that it can be computed from a rational expression describing the word. ",Olivier Carton and Luc Boasson,,,11,
Compiling Neural Networks for a Computational Memory Accelerator,"  Computational memory (CM) is a promising approach for accelerating inference on neural networks (NN) by using enhanced memories that, in addition to storing data, allow computations on them. One of the main challenges of this approach is defining a hardware/software interface that allows a compiler to map NN models for efficient execution on the underlying CM accelerator. This is a non-trivial task because efficiency dictates that the CM accelerator is explicitly programmed as a dataflow engine where the execution of the different NN layers form a pipeline. In this paper, we present our work towards a software stack for executing ML models on such a multi-core CM accelerator. We describe an architecture for the hardware and software, and focus on the problem of implementing the appropriate control logic so that data dependencies are respected. We propose a solution to the latter that is based on polyhedral compilation. ",Kornilios Kourtis and Martino Dazzi and Nikolas Ioannou and Tobias   Grosser and Abu Sebastian and Evangelos Eleftheriou,,,11,
BSF: a parallel computation model for scalability estimation of   iterative numerical algorithms on cluster computing systems,"  This paper examines a new parallel computation model called bulk synchronous farm (BSF) that focuses on estimating the scalability of compute-intensive iterative algorithms aimed at cluster computing systems. In the BSF model, a computer is a set of processor nodes connected by a network and organized according to the mas-ter/slave paradigm. A cost metric of the BSF model is presented. This cost metric requires the algorithm to be represented in the form of operations on lists. This allows us to derive an equation that predicts the scalability boundary of a parallel program: the maximum number of processor nodes after which the speedup begins to de-crease. The paper includes several examples of applying the BSF model to designing and analyzing parallel nu-merical algorithms. The large-scale computational experiments conducted on a cluster computing system confirm the adequacy of the analytical estimations obtained using the BSF model. ",Leonid B. Sokolinsky,,,11,
Byzantine Fault-Tolerance in Decentralized Optimization under Minimal   Redundancy,"  This paper considers the problem of Byzantine fault-tolerance in multi-agent decentralized optimization. In this problem, each agent has a local cost function. The goal of a decentralized optimization algorithm is to allow the agents to cooperatively compute a common minimum point of their aggregate cost function. We consider the case when a certain number of agents may be Byzantine faulty. Such faulty agents may not follow a prescribed algorithm, and they may share arbitrary or incorrect information with other non-faulty agents. Presence of such Byzantine agents renders a typical decentralized optimization algorithm ineffective. We propose a decentralized optimization algorithm with provable exact fault-tolerance against a bounded number of Byzantine agents, provided the non-faulty agents have a minimal redundancy. ","Nirupam Gupta, Thinh T. Doan and Nitin H. Vaidya",,,11,
Assessing Impact of Data Partitioning for Approximate Memory in C/C++   Code,"  Approximate memory is a technique to mitigate the performance gap between memory subsystems and CPUs with its reduced access latency at a cost of data integrity. To gain benefit from approximate memory for realistic applications, it is crucial to partition applications' data to approximate data and critical data and apply different error rates. However, error rates cannot be controlled in a fine-grained manner (e.g., per byte) due to fundamental limitations of how approximate memory can be realized. Due to this, if approximate data and critical data are interleaved in a data structure (e.g., a C struct that has a pointer and an approximatable number as its members), data partitioning may degrade the application's performance because the data structure must be split to separate memory regions that have different error rates. This paper is the first to conduct an analysis of realistic C/C++ code to assess the impact of this problem. First, we find the type of data (e.g., ""int"", ""struct point"") that is assessed by the instruction that incurs the largest number of cache misses in a benchmark, which we refer to as the target data type. Second, we qualitatively estimate if the target data type of an application has approximate data and critical data interleaved. To this end, we set up three criteria to analyze it because definitively distinguishing a piece of data as approximate data or critical data is infeasible since it depends on each use-case. We analyze 11 memory intensive benchmarks from SPEC CPU 2006 and 2 graph analytics frameworks, and show that the target data types of 9 benchmarks are either a C struct or a C++ class (criterion 1). Among them, two have a pointer and a non-pointer member together (criterion 2) and three have a floating point number and other members together (criterion 3). ",Soramichi Akiyama,,,11,
Daydream: Accurately Estimating the Efficacy of Optimizations for DNN   Training,"  Modern deep neural network (DNN) training jobs use complex and heterogeneous software/hardware stacks. The efficacy of software-level optimizations can vary significantly when used in different deployment configurations. It is onerous and error-prone for ML practitioners and system developers to implement each optimization separately, and determine which ones will improve performance in their own configurations. Unfortunately, existing profiling tools do not aim to answer predictive questions such as ""How will optimization X affect the performance of my model?"". We address this critical limitation, and proposes a new profiling tool, Daydream, to help programmers efficiently explore the efficacy of DNN optimizations. Daydream models DNN execution with a fine-grained dependency graph based on low-level traces collected by CUPTI, and predicts runtime by simulating execution based on the dependency graph. Daydream maps the low-level traces using DNN domain-specific knowledge, and introduces a set of graph-transformation primitives that can easily model a wide variety of optimizations. We show that Daydream is able to model most mainstream DNN optimization techniques, and accurately predict the efficacy of optimizations that will result in significant performance improvements. ","Hongyu Zhu, Amar Phanishayee, Gennady Pekhimenko",,,11,
The Architectural Implications of Facebook's DNN-based Personalized   Recommendation,"  The widespread application of deep learning has changed the landscape of computation in the data center. In particular, personalized recommendation for content ranking is now largely accomplished leveraging deep neural networks. However, despite the importance of these models and the amount of compute cycles they consume, relatively little research attention has been devoted to systems for recommendation. To facilitate research and to advance the understanding of these workloads, this paper presents a set of real-world, production-scale DNNs for personalized recommendation coupled with relevant performance metrics for evaluation. In addition to releasing a set of open-source workloads, we conduct in-depth analysis that underpins future system design and optimization for at-scale recommendation: Inference latency varies by 60% across three Intel server generations, batching and co-location of inferences can drastically improve latency-bounded throughput, and the diverse composition of recommendation models leads to different optimization strategies. ","Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon   Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Bill Jia, Hsien-Hsin S.   Lee, Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong,   Xuan Zhang",,,11,
Lazy State Determination: More concurrency for contending linearizable   transactions,"  The concurrency control algorithms in transactional systems limits concurrency to provide strong semantics, which leads to poor performance under high contention. As a consequence, many transactional systems eschew strong semantics to achieve acceptable performance. We show that by leveraging semantic information associated with the transactional programs to increase concurrency, it is possible to significantly improve performance while maintaining linearizability. To this end, we introduce the lazy state determination API to easily expose the semantics of application transactions to the database, and propose new optimistic and pessimistic concurrency control algorithms that leverage this information to safely increase concurrency in the presence of contention. Our evaluation shows that our approach can achieve up to 5x more throughput with 1.5c less latency than standard techniques in the popular TPC-C benchmark. ","Tiago M. Vale, Jo\~ao Leit\~ao, Nuno Pregui\c{c}a, Rodrigo Rodrigues,   Ricardo J. Dias, Jo\~ao M. Louren\c{c}o",,,11,
Benchmarking 50-Photon Gaussian Boson Sampling on the Sunway TaihuLight,"  Boson sampling is expected to be one of an important milestones that will demonstrate quantum supremacy. The present work establishes the benchmarking of Gaussian boson sampling (GBS) with threshold detection based on the Sunway TaihuLight supercomputer. To achieve the best performance and provide a competitive scenario for future quantum computing studies, the selected simulation algorithm is fully optimized based on a set of innovative approaches, including a parallel scheme and instruction-level optimizing method. Furthermore, data precision and instruction scheduling are handled in a sophisticated manner by an adaptive precision optimization scheme and a DAG-based heuristic search algorithm, respectively. Based on these methods, a highly efficient and parallel quantum sampling algorithm is designed. The largest run enables us to obtain one Torontonian function of a 100 x 100 submatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in 256-bit precision. ","Yuxuan Li, Mingcheng Chen, Yaojian Chen, Haitian Lu, Lin Gan, Chaoyang   Lu, Jianwei Pan, Haohuan Fu, and Guangwen Yang",,,11,
"Prisoners, Rooms, and Lightswitches","  We examine a new variant of the classic prisoners and lightswitches puzzle: A warden leads his $n$ prisoners in and out of $r$ rooms, one at a time, in some order, with each prisoner eventually visiting every room an arbitrarily large number of times. The rooms are indistinguishable, except that each one has $s$ lightswitches; the prisoners win their freedom if at some point a prisoner can correctly declare that each prisoner has been in every room at least once. What is the minimum number of switches per room, $s$, such that the prisoners can manage this? We show that if the prisoners do not know the switches' starting configuration, then they have no chance of escape -- but if the prisoners do know the starting configuration, then the minimum sufficient $s$ is surprisingly small. The analysis gives rise to a number of puzzling open questions, as well. ","Daniel M. Kane, Scott Duke Kominers",,,11,
The Ultimate DataFlow for Ultimate SuperComputers-on-a-Chips,"  This article starts from the assumption that near future 100BTransistor SuperComputers-on-a-Chip will include N big multi-core processors, 1000N small many-core processors, a TPU-like fixed-structure systolic array accelerator for the most frequently used Machine Learning algorithms needed in bandwidth-bound applications and a flexible-structure reprogrammable accelerator for less frequently used Machine Learning algorithms needed in latency-critical applications. ","Veljko Milutinovic, Milos Kotlar, Ivan Ratkovic, Nenad Korolija,   Miljan Djordjevic, Kristy Yoshimot and Mateo Valero",,,11,
Overview of the IBM Neural Computer Architecture,"  The IBM Neural Computer (INC) is a highly flexible, re-configurable parallel processing system that is intended as a research and development platform for emerging machine intelligence algorithms and computational neuroscience. It consists of hundreds of programmable nodes, primarily based on Xilinx's Field Programmable Gate Array (FPGA) technology. The nodes are interconnected in a scalable 3d mesh topology. We overview INC, emphasizing unique features such as flexibility and scalability both in the types of computations performed and in the available modes of communication, enabling new machine intelligence approaches and learning strategies not well suited to the matrix manipulation/SIMD libraries that GPUs are optimized for. This paper describes the architecture of the machine and applications are to be described in detail elsewhere. ","Pritish Narayanan, Charles E. Cox, Alexis Asseman, Nicolas Antoine,   Harald Huels, Winfried W. Wilcke and Ahmet S. Ozcan",,,11,
A CAD-Based tool for fault tolerant distributed embedded systems,"  Reliability and availability analysis are essential in dependable critical embedded systems. The classical implementation of dependability for an embedded system relies on merging both fundamental structures with the required dependability techniques to form one composite structure. The separation of the basic system components from the dependability components, reduces complexity and improves the design. The goal of this work is to assist implementing reconfiguration-based fault tolerance in safety-critical embedded systems applications. The primary intention is to reduce the repair time in order to enhance fault tolerance and produce dependable embedded systems. The proposed solution is a dedicated CAD-tool designed to generate a reference strategy for the system manager of a distributed embedded system to control and automatically reconfigure the processing elements of the system. The proposed tool auto-generates program codes to be executed by a system manager to govern the DES. It also computes different reliability solutions with necessary supporting calculated parameters and graphs sorted to support the fault tolerance design of the system. The proposed tool can be used to simulate possible configurations based on the desired degrees of faults and system reliability. The graphical interface of the tool is unique and hides the complexity of the systems underneath. A comparison with a similar tool is presented. ","Mahmoud I. Banat, Belal H. Sababha, Sami Al-Hamdan",,,11,
Using Nesting to Push the Limits of Transactional Data Structure   Libraries,"  Transactional data structure libraries (TDSL) combine the ease-of-programming of transactions with the high performance and scalability of custom-tailored concurrent data structures. They can be very efficient thanks to their ability to exploit data structure semantics in order to reduce overhead, aborts, and wasted work compared to general-purpose software transactional memory. However, TDSLs were not previously used for complex use-cases involving long transactions and a variety of data structures.   In this paper, we boost the performance and usability of a TDSL, allowing it to support complex applications. A key idea is nesting. Nested transactions create checkpoints within a longer transaction, so as to limit the scope of abort, without changing the semantics of the original transaction. We build a Java TDSL with built-in support for nesting in a number of data structures. We conduct a case study of a complex network intrusion detection system that invests a significant amount of work to process each packet. Our study shows that our library outperforms TL2 twofold without nesting, and by up to 16x when nesting is used. Finally, we discuss cross-library nesting, namely dynamic composition of transactions from multiple libraries. ","Gal Assa, Hagar Meir, Guy Golan-Gueta, Idit Keidar, Alexander   Spiegelman",,,11,
BAASH: Enabling Blockchain-as-a-Service on High-Performance Computing   Systems,"  The state-of-the-art approach to manage blockchains is to process blocks of transactions in a shared-nothing environment. Although blockchains have the potential to provide various services for high-performance computing (HPC) systems, HPC will not be able to embrace blockchains before the following two missing pieces become available: (i) new consensus protocols being aware of the shared-storage architecture in HPC, and (ii) new fault-tolerant mechanisms compensating for HPC's programming model---the message passing interface (MPI)---that is vulnerable for blockchain-like workloads. To this end, we design a new set of consensus protocols crafted for the HPC platforms and a new fault-tolerance subsystem compensating for the failures caused by faulty MPI processes. Built on top of the new protocols and fault-tolerance mechanism, a prototype system is implemented and evaluated with two million transactions on a 500-core HPC cluster, showing $6\times$, $12\times$, and $75\times$ higher throughput than Hyperldeger, Ethereum, and Parity, respectively. ",Abdullah Al-Mamun and Dongfang Zhao,,,11,
Money Transfer Made Simple,"  It has recently been shown (PODC 2019) that, contrarily to a common belief, money transfer in the presence of faulty (Byzantine) processes does not require strong agreement such as consensus. This article goes one step further: namely, it shows that money transfers do not need to explicitly capture the causality relation that links individual transfers. A simple FIFO order between each pair of processes is sufficient. To this end, the article presents a generic money transfer algorithm that can be instantiated in both the crash failure model and the Byzantine failure model. The genericity dimension lies in the underlying reliable broadcast abstraction which must be suited to the appropriate failure model. Interestingly, whatever the failure model, the money transfer algorithm only requires adding a single sequence number to its messages as control information. Moreover, as a side effect of the proposed algorithm, it follows that money transfer is a weaker problem that the construction of a read/write register in the asynchronous message-passing crash-prone model. ","Alex Auvolat (WIDE, Univ-Rennes, IRISA, DI-ENS, CNRS), Davide Frey   (WIDE, Univ-Rennes, IRISA, CNRS), Michel Raynal (WIDE, Univ-Rennes, IRISA,   POLYU, CNRS), Fran\c{c}ois Ta\""iani (WIDE, Univ-Rennes, IRISA, CNRS)",,,11,
Communication-Optimal Parallel Standard and Karatsuba Integer   Multiplication in the Distributed Memory Model,"  We present COPSIM a parallel implementation of standard integer multiplication for the distributed memory setting, and COPK a parallel implementation of Karatsuba's fast integer multiplication algorithm for a distributed memory setting. When using $\mathcal{P}$ processors, each equipped with a local non-shared memory, to compute the product of tho $n$-digits integer numbers, under mild conditions, our algorithms achieve optimal speedup of the computational time. That is, $\mathcal{O}\left(n^2/\mathcal{P}\right)$ for COPSIM, and $\mathcal{O}\left(n^{\log_2 3}/\mathcal{P}\right)$ for COPK. The total amount of memory required across the processors is $\mathcal{O}\left(n\right)$, that is, within a constant factor of the minimum space required to store the input values. We rigorously analyze the Input/Output (I/O) cost of the proposed algorithms. We show that their bandwidth cost (i.e., the number of memory words sent or received by at least one processors) matches asymptotically corresponding known I/O lower bounds, and their latency (i.e., the number of messages sent or received in the algorithm's critical execution path) is asymptotically within a multiplicative factor $\mathcal{O}\left(\log^2_2 \mathcal{P}\right)$ of the corresponding known I/O lower bounds. Hence, our algorithms are asymptotically optimal with respect to the bandwidth cost and almost asymptotically optimal with respect to the latency cost. ",Lorenzo De Stefani,,,11,
Power-Aware Run-Time Scheduler for Mixed-Criticality Systems on   Multi-Core Platform,"  In modern multi-core Mixed-Criticality (MC) systems, a rise in peak power consumption due to parallel execution of tasks with maximum frequency, specially in the overload situation, may lead to thermal issues, which may affect the reliability and timeliness of MC systems. Therefore, managing peak power consumption has become imperative in multi-core MC systems. In this regard, we propose an online peak power and thermal management heuristic for multi-core MC systems. This heuristic reduces the peak power consumption of the system as much as possible during runtime by exploiting dynamic slack and per-cluster Dynamic Voltage and Frequency Scaling (DVFS). Specifically, our approach examines multiple tasks ahead to determine the most appropriate one for slack assignment, that has the most impact on the system peak power and temperature. However, changing the frequency and selecting a proper task for slack assignment and a proper core for task re-mapping at runtime can be time-consuming and may cause deadline violation which is not admissible for high-criticality tasks. Therefore, we analyze and then optimize our run-time scheduler and evaluate it for various platforms. The proposed approach is experimentally validated on the ODROID-XU3 (DVFS-enabled heterogeneous multi-core platform) with various embedded real-time benchmarks. Results show that our heuristic achieves up to 5.25% reduction in system peak power and 20.33\% reduction in maximum temperature compared to an existing method while meeting deadline constraints in different criticality modes. ","Behnaz Ranjbar, Tuan D.A.Nguyen, Alireza Ejlali, and Akash Kumar",,,11,
Dynamic Graph Operations: A Consistent Non-blocking Approach,"  Graph algorithms enormously contribute to the domains such as blockchains, social networks, biological networks, telecommunication networks, and several others. The ever-increasing demand of data-volume, as well as speed of such applications, have essentially transported these applications from their comfort zone: static setting, to a challenging territory of dynamic updates. At the same time, mainstreaming of multi-core processors have entailed that the dynamic applications should be able to exploit concurrency as soon as parallelization gets inhibited. Thus, the design and implementation of efficient concurrent dynamic graph algorithms have become significant.   This paper reports a novel library of concurrent shared-memory algorithms for breadth-first search (BFS), single-source shortest-path (SSSP), and betweenness centrality (BC) in a dynamic graph. The presented algorithms are provably non-blocking and linearizable. We extensively evaluate C++ implementations of the algorithms through several micro-benchmarks. The experimental results demonstrate the scalability with the number of threads. Our experiments also highlight the limitations of static graph analytics methods in a dynamic setting. ",Bapi Chatterjee and Sathya Peri and Muktikanta Sa,,,11,
"$k$-ported vs. $k$-lane Broadcast, Scatter, and Alltoall Algorithms","  In $k$-ported message-passing systems, a processor can simultaneously receive $k$ different messages from $k$ other processors, and send $k$ different messages to $k$ other processors that may or may not be different from the processors from which messages are received. Modern clustered systems may not have such capabilities. Instead, compute nodes consisting of $n$ processors can simultaneously send and receive $k$ messages from other nodes, by letting $k$ processors on the nodes concurrently send and receive at most one message. We pose the question of how to design good algorithms for this $k$-lane model, possibly by adapting algorithms devised for the traditional $k$-ported model. We discuss and compare a number of (non-optimal) $k$-lane algorithms for the broadcast, scatter and alltoall collective operations (as found in, e.g., MPI), and experimentally evaluate these on a small $36\times 32$-node cluster with a dual OmniPath network (corresponding to $k=2$). Results are preliminary. ","Jesper Larsson Tr\""aff",,,11,
Evaluation of Automatic GPU and FPGA Offloading for Function Blocks of   Applications,"  In the recent years, systems using FPGAs, GPUs have increased due to their advantages such as power efficiency compared to CPUs. However, use in systems such as FPGAs and GPUs requires understanding hardware-specific technical specifications such as HDL and CUDA, which is a high hurdle. Based on this background, I previously proposed environment adaptive software that enables automatic conversion, configuration, and high-performance operation of once written code according to the hardware to be placed. As an element of the concept, I proposed a method to automatically offload loop statements of application source code for CPU to FPGA and GPU. In this paper, I propose and evaluate a method for offloading a function block, which is a larger unit, instead of individual loop statements in an application, to achieve higher speed by automatic offloading to GPU and FPGA. I implement the proposed method and evaluate with existing applications offloading to GPU. ",Yoji Yamato,,,11,
EPiT : A Software Testing Tool for Generation of Test Cases   Automatically,"  Software test cases can be defined as a set of condition where a tester needs to test and determine that the System Under Test (SUT) satisfied with the expected result correctly. This paper discusses the optimization technique in generating cases automatically by using EpiT (Eclipse Plug-in Tool). EpiT is developed to optimize the generation of test cases from source code in order to reduce time used for conventional manually creating test cases. By using code smell functionality, EpiT helps to generate test cases automatically from Java programs by checking its line of code (LOC). The implementation of EpiT will also be presented based on several case studies conducted to show the optimization of the test cases generated. Based on the results presented, EpiT is proven to solve the problem for software tester to generate test case manually and check the optimization from the source code using code smell technique. ","Rosziati Ibrahim, Ammar Aminuddin Bani Amin, Sapiee Jamel, Jahari   Abdul Wahab",,,11,
Theodolite: Scalability Benchmarking of Distributed Stream Processing   Engines,"  Distributed stream processing engines are designed with a focus on scalability to process big data volumes in a continuous manner. We present the Theodolite method for benchmarking the scalability of distributed stream processing engines. Core of this method is the definition of use cases that microservices implementing stream processing have to fulfill. For each use case, our method identifies relevant workload dimensions that might affect the scalability of a use case. We propose to design one benchmark per use case and relevant workload dimension. We present a general benchmarking framework, which can be applied to execute the individual benchmarks for a given use case and workload dimension. Our framework executes an implementation of the use case's dataflow architecture for different workloads of the given dimension and various numbers of processing instances. This way, it identifies how resources demand evolves with increasing workloads. Within the scope of this paper, we present 4 identified use cases, derived from processing Industrial Internet of Things data, and 7 corresponding workload dimensions. We provide implementations of 4 benchmarks with Kafka Streams as well as an implementation of our benchmarking framework to execute scalability benchmarks in cloud environments. We use both for evaluating the Theodolite method and for benchmarking Kafka Streams' scalability for different deployment options. ","S\""oren Henning, Wilhelm Hasselbring",,,11,
AppAngio: Revealing Contextual Information of Android App Behaviors by   API-Level Audit Logs,"  Android users are now suffering severe threats from unwanted behaviors of various apps. The analysis of apps' audit logs is one of the essential methods for some device manufacturers to unveil the underlying malice within apps. We propose and implement AppAngio, a novel system that reveals contextual information in Android app behaviors by API-level audit logs. Our goal is to help analysts of device manufactures understand what has happened on users' devices and facilitate the identification of the malice within apps. The key module of AppAngio is identifying the path matched with the logs on the app's control-flow graph (CFG). The challenge, however, is that the limited-quantity logs may incur high computational complexity in the log matching, where there are a large number of candidates caused by the coupling relation of successive logs. To address the challenge, we propose a divide and conquer strategy that precisely positions the nodes matched with log records on the corresponding CFGs and connects the nodes with as few backtracks as possible. Our experiments show that AppAngio reveals the contextual information of behaviors in real-world apps. Moreover, the revealed results assist the analysts in identifying malice of app behaviors and complement existing analysis schemes. Meanwhile, AppAngio incurs negligible performance overhead on the Android device. ","Zhaoyi Meng, Yan Xiong, Wenchao Huang, Fuyou Miao, Jianmeng Huang",,,11,
The List is the Process: Reliable Pre-Integration Tracking of Commits on   Mailing Lists,"  A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history.   We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth.   Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements. ","Ralf Ramsauer, Daniel Lohmann, Wolfgang Mauerer",,,11,
Prioritizing documentation effort: Can we do better?,"  Code documentations are essential for software quality assurance, but due to time or economic pressures, code developers are often unable to write documents for all modules in a project. Recently, a supervised artificial neural network (ANN) approach is proposed to prioritize important modules for documentation effort. However, as a supervised approach, there is a need to use labeled training data to train the prediction model, which may not be easy to obtain in practice. Furthermore, it is unclear whether the ANN approach is generalizable, as it is only evaluated on several small data sets. In this paper, we propose an unsupervised approach based on PageRank to prioritize documentation effort. This approach identifies ""important"" modules only based on the dependence relationships between modules in a project. As a result, the PageRank approach does not need any training data to build the prediction model. In order to evaluate the effectiveness of the PageRank approach, we use six additional large data sets to conduct the experiments in addition to the same data sets collected from open-source projects as used in prior studies. The experimental results show that the PageRank approach is superior to the state-of-the-art ANN approach in prioritizing important modules for documentation effort. In particular, due to the simplicity and effectiveness, we advocate that the PageRank approach should be used as an easy-to-implement baseline in future research on documentation effort prioritization, and any new approach should be compared with it to demonstrate its effectiveness. ","Shiran Liu, Zhaoqiang Guo, Yanhui Li, Hongmin Lu, Lin Chen, Lei Xu,   Yuming Zhou, Baowen Xu",,,11,
Protocol and Tools for Conducting Agile Software Engineering Research in   an Industrial-Academic Setting: A Preliminary Study,"  Conducting empirical research in software engineering industry is a process, and as such, it should be generalizable. The aim of this paper is to discuss how academic researchers may address some of the challenges they encounter during conducting empirical research in the software industry by means of a systematic and structured approach. The protocol developed in this paper should serve as a practical guide for researchers and help them with conducting empirical research in this complex environment. ","Katarzyna Biesialska, Xavier Franch, Victor Munt\'es-Mulero",,,11,
Quality Management of Machine Learning Systems,"  In the past decade, Artificial Intelligence (AI) has become a part of our daily lives due to major advances in Machine Learning (ML) techniques. In spite of an explosive growth in the raw AI technology and in consumer facing applications on the internet, its adoption in business applications has conspicuously lagged behind. For business/mission-critical systems, serious concerns about reliability and maintainability of AI applications remain. Due to the statistical nature of the output, software 'defects' are not well defined. Consequently, many traditional quality management techniques such as program debugging, static code analysis, functional testing, etc. have to be reevaluated. Beyond the correctness of an AI model, many other new quality attributes, such as fairness, robustness, explainability, transparency, etc. become important in delivering an AI system. The purpose of this paper is to present a view of a holistic quality management framework for ML applications based on the current advances and identify new areas of software engineering research to achieve a more trustworthy AI. ",P. Santhanam,,,11,
Githru: Visual Analytics for Understanding Software Development History   Through Git Metadata Analysis,"  Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time. ","Youngtaek Kim, Jaeyoung Kim, Hyeon Jeon, Young-Ho Kim, Hyunjoo Song,   Bohyoung Kim, Jinwook Seo",,,11,
A framework to evaluate the viability of robotic process automation for   business process activities,"  Robotic process automation (RPA) is a technology for centralized automation of business processes. RPA automates user interaction with graphical user interfaces, whereby it promises efficiency gains and a reduction of human negligence during process execution. To harness these benefits, organizations face the challenge of classifying process activities as viable automation candidates for RPA. Therefore, this work aims to support practitioners in evaluating RPA automation candidates. We design a framework that consists of thirteen criteria grouped into five perspectives which offer different evaluation aspects. These criteria leverage a profound understanding of the process step. We demonstrate and evaluate the framework by applying it to a real-life data set. ","Christian Wellmann, Matthias Stierle, Sebastian Dunzer, Martin Matzner",,,11,
Automated Discovery of Business Process Simulation Models from Event   Logs,"  Business process simulation is a versatile technique to estimate the performance of a process under multiple scenarios. This, in turn, allows analysts to compare alternative options to improve a business process. A common roadblock for business process simulation is that constructing accurate simulation models is cumbersome and error-prone. Modern information systems store detailed execution logs of the business processes they support. Previous work has shown that these logs can be used to discover simulation models. However, existing methods for log-based discovery of simulation models do not seek to optimize the accuracy of the resulting models. Instead they leave it to the user to manually tune the simulation model to achieve the desired level of accuracy. This article presents an accuracy-optimized method to discover business process simulation models from execution logs. The method decomposes the problem into a series of steps with associated configuration parameters. A hyper-parameter optimization method is used to search through the space of possible configurations so as to maximize the similarity between the behavior of the simulation model and the behavior observed in the log. The method has been implemented as a tool and evaluated using logs from different domains. ","Manuel Camargo, Marlon Dumas, Oscar Gonz\'alez-Rojas",,,11,
Monitoring Data Distribution and Exploitation in a Global-Scale   Microservice Artefact Observatory,"  Reusable microservice artefacts are often deployed as black or grey boxes, with little concern for their properties and quality, beyond a syntactical interface description. This leads application developers to chaotic and opportunistic assumptions about how a composite application will behave in the real world. Systematically analyzing and tracking these publicly available artefacts will grant much needed predictability to microservice-based deployments. By establishing a distributed observatory and knowledge base, it is possible to track microservice repositories and analyze the artefacts reliably, and provide insights on their properties and quality to developers and researchers alike. This position paper argues for a federated research infrastructure with consensus voting among participants to establish and preserve ground truth about the insights. ","Panagiotis Gkikopoulos, Josef Spillner and Valerio Schiavoni",,,11,
Assessing Practitioner Beliefs about Software Engineering,"  Software engineering is a highly dynamic discipline. Hence, as times change, so too might our beliefs about core processes in this field. This paper checks some five beliefs that originated in the past decades that comment on the relationships between (i) developer productivity; (ii) software quality and (iii) years of developer experience. Using data collected from 1,356 developers in the period 1995 to 2006, we found support for only one of the five beliefs titled ""Quality entails productivity"". We found no clear support for four other beliefs based on programming languages and software developers. However, from the sporadic evidence of the four other beliefs we learned that a narrow scope could delude practitioners in misinterpreting certain effects to hold in their day to day work. Lastly, through an aggregated view of assessing the five beliefs, we find programming languages act as a confounding factor for developer productivity and software quality. Thus the overall message of this work is that it is both important and possible to revisit old beliefs in SE. Researchers and practitioners should routinely retest old beliefs. ","Shrikanth NC, William Nichols, Fahmid Morshed Fahid, Tim Menzies",,,11,
Retrieve and Refine: Exemplar-based Neural Comment Generation,"  Code comment generation which aims to automatically generate natural language descriptions for source code, is a crucial task in the field of automatic software development. Traditional comment generation methods use manually-crafted templates or information retrieval (IR) techniques to generate summaries for source code. In recent years, neural network-based methods which leveraged acclaimed encoder-decoder deep learning framework to learn comment generation patterns from a large-scale parallel code corpus, have achieved impressive results. However, these emerging methods only take code-related information as input. Software reuse is common in the process of software development, meaning that comments of similar code snippets are helpful for comment generation. Inspired by the IR-based and template-based approaches, in this paper, we propose a neural comment generation approach where we use the existing comments of similar code snippets as exemplars to guide comment generation. Specifically, given a piece of code, we first use an IR technique to retrieve a similar code snippet and treat its comment as an exemplar. Then we design a novel seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to assist in the target comment generation based on the semantic similarity between the source code and the similar code. We evaluate our approach on a large-scale Java corpus, which contains about 2M samples, and experimental results demonstrate that our model outperforms the state-of-the-art methods by a substantial margin. ","Bolin Wei, Yongmin Li, Ge Li, Xin Xia, Zhi Jin",,,11,
Immutable Log Storage as a Service on Private and Public Blockchains,"  During the normal operation of a Cloud solution, no one pays attention to the logs except the system reliability engineers, who may periodically check them to ensure that the Cloud platform's performance conforms to the Service Level Agreements (SLA). However, the moment a component fails, or a customer complains about a breach of SLA, the importance of logs increases significantly. All departments, including management, customer support, and even the actual customer, may turn to logs to determine the cause and timeline of the issue and to find the party responsible for the issue. The party at fault may be motivated to tamper with the logs to hide their role. Given the number and volume of logs generated by the Cloud platforms, many tampering opportunities exist. We argue that the critical nature of logs calls for immutability and verification mechanism without the presence of a single trusted party.   This paper proposes such a mechanism by describing a blockchain-based log system, called Logchain, which can be integrated with existing private and public blockchain solutions. Logchain uses the immutability feature of blockchain to provide a tamper-resistance storage platform for log storage. Additionally, we propose a hierarchical structure to combine the hash-binding of two blockchains to address blockchains' scalability issues. To validate the mechanism, we integrate Logchain into two different types of blockchains. We choose Ethereum as a public, permission-less blockchain and IBM Blockchain as a private, permission-based one. We show that the solution is scalable on both the private and public blockchains. Additionally, we perform the analysis of the cost of ownership for private and public blockchains implementations to help a reader selecting an implementation that would be applicable to their needs. ",William Pourmajidi and Lei Zhang and John Steinbacher and Tony Erwin   and Andriy Miranskyy,,,11,
Teddy: Automatic Recommendation of Pythonic Idiom Usage For Pull-Based   Software Projects,"  Pythonic code is idiomatic code that follows guiding principles and practices within the Python community. Offering performance and readability benefits, Pythonic code is claimed to be widely adopted by experienced Python developers, but can be a learning curve to novice programmers. To aid with Pythonic learning, we create an automated tool, called Teddy, that can help checking the Pythonic idiom usage. The tool offers a prevention mode with Just-In-Time analysis to recommend the use of Pythonic idiom during code review and a detection mode with historical analysis to run a thorough scan of idiomatic and non-idiomatic code. In this paper, we first describe our tool and an evaluation of its performance. Furthermore, we present a case study that demonstrates how to use Teddy in a real-life scenario on an Open Source project. An evaluation shows that Teddy has high precision for detecting Pythonic idiom and non-Pythonic code. Using interactive visualizations, we demonstrate how novice programmers can navigate and identify Pythonic idiom and non-Pythonic code in their projects. Our video demo with the full interactive visualizations is available at https://youtu.be/vOCQReSvBxA. ","Purit Phan-udom, Naruedon Wattanakul, Tattiya Sakulniwat, Chaiyong   Ragkhitwetsagul, Thanwadee Sunetnanta, Morakot Choetkiertikul, Raula   Gaikovina Kula",,,11,
An Exploratory Study of Code Smells in Web Games,"  With the continuous growth of the internet market, games are becoming more and more popular worldwide. However, increased market competition for game demands developers to write more efficient games in terms of performance, security, and maintenance. The continuous evolution of software systems and its increasing complexity may result in bad design decisions. Researchers analyzed the cognitive, behavioral and social effects of games. Also, gameplay and game mechanics have been a research area to enhance game playing, but to the extent of our knowledge, there hardly exists any research work that studies the bad coding practices in game development. Hence, through our study, we try to analyze and identify the presence of bad coding practices called code smells that may cause quality issues in games. To accomplish this, we created a dataset of 361 web games written in JavaScript. On this dataset, we run a JavaScript code smell detection tool JSNose to find the occurrence and distribution of code smell in web games. Further, we did a manual study on 9 web games to find violation of existing game programming patterns. Our results show that existing tools are mostly language-specific and are not enough in the context of games as they were not able to detect the anti-patterns or bad coding practices that are game-specific, motivating the need of game-specific code smell detection tools. ","Vartika Agrahari, Sridhar Chimalakonda",,,11,
The Unit-B Method -- Refinement Guided by Progress Concerns,"  We present Unit-B, a formal method inspired by Event-B and UNITY. Unit-B aims at the stepwise design of software systems satisfying safety and liveness properties. The method features the novel notion of coarse and fine schedules, a generalisation of weak and strong fairness for specifying events' scheduling assumptions. Based on events schedules, we propose proof rules to reason about progress properties and a refinement order preserving both liveness and safety properties. We illustrate our approach by an example to show that systems development can be driven by not only safety but also liveness requirements. ",Simon Hudon and Thai Son Hoang and Jonathan S. Ostroff,,,11,
SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps,"  Mobile apps provide various critical services, such as banking, communication, and healthcare. To this end, they have access to our personal information and have the ability to perform actions on our behalf. Hence, securing mobile apps is crucial to ensuring the privacy and safety of its users.   Recent research efforts have focused on developing solutions to secure mobile ecosystems (i.e., app platforms, apps, and app stores), specifically in the context of detecting vulnerabilities in Android apps. Despite this attention, known vulnerabilities are often found in mobile apps, which can be exploited by malicious apps to harm the user. Further, fixing vulnerabilities after developing an app has downsides in terms of time, resources, user inconvenience, and information loss.   In an attempt to address this concern, we have developed SeMA, a mobile app development methodology that builds on existing mobile app design artifacts such as storyboards. With SeMA, security is a first-class citizen in an app's design -- app designers and developers can collaborate to specify and reason about the security properties of an app at an abstract level without being distracted by implementation level details. Our realization of SeMA using Android Studio tooling demonstrates the methodology is complementary to existing design and development practices. An evaluation of the effectiveness of SeMA shows the methodology can detect and help prevent 49 vulnerabilities known to occur in Android apps. Further, a usability study of the methodology involving ten real-world developers shows the methodology is likely to reduce the development time and help developers uncover and prevent known vulnerabilities while designing apps. ","Joydeep Mitra, Venkatesh-Prasad Ranganath, Torben Amtoft, Mike Higgins",,,11,
Does Migrate a Monolithic System to Microservices Decrease the Technical   Debt?,"  Background. The migration from monolithic systems to microservices involves deep refactoring of the systems. Therefore, the migration usually has a big economic impact and companies tend to postpone several activities during this process, mainly to speed-up the migration itself, but also because of the need to release new features. Objective. We monitored the Technical Debt of a small and medium enterprise while migrating a legacy monolithic system to an ecosystem of microservices to analyze changes in the code technical debt before and after the migration to microservices. Method. We conducted a case study analyzing more than four years of the history of a big project (280K Lines of Code) where two teams extracted five business processes from the monolithic system as microservices, by first analyzing the Technical Debt with SonarQube and then performing a qualitative study with the developers to understand the perceived quality of the system and the motivation for eventually postponed activities. Result. The development of microservices helps to reduce the Technical Debt in the long run. Despite an initial spike in the Technical Debt, due to the development of the new microservice, after a relatively short period, the Technical Debt tends to grow slower than in the monolithic system. ","Valentina Lenarduzzi, Francesco Lomio, Nyyti Saarim\""aki, Davide Taibi",,,11,
Are Game Engines Software Frameworks? A Three-perspective Study,"  Game engines help developers create video games and avoid duplication of code and effort, like frameworks for traditional software systems. In this paper, we explore open-source game engines along three perspectives: literature, code, and human. First, we explore and summarise the academic literature on game engines. Second, we compare the characteristics of the 282 most popular engines and the 282 most popular frameworks in GitHub. Finally, we survey 124 engine developers about their experience with the development of their engines. We report that: (1) Game engines are not well-studied in software-engineering research with few studies having engines as object of research. (2) Open-source game engines are slightly larger in terms of size and complexity and less popular and engaging than traditional frameworks. Their programming languages differ greatly from frameworks. Engine projects have shorter histories with less releases. (3) Developers perceive game engines as different from traditional frameworks. Generally, they build game engines to (a) better control the environment and source code, (b) learn about game engines, and (c) develop specific games. We conclude that open-source game engines have differences compared to traditional open-source frameworks although this differences do not demand special treatments. ","Cristiano Politowski, Fabio Petrillo, Jo\~ao Eduardo Montandon, Marco   Tulio Valente, Yann-Ga\""el Gu\'eh\'eneuc",,,11,
Vectorization and Minimization of Memory Footprint for Linear High-Order   Discontinuous Galerkin Schemes,"  We present a sequence of optimizations to the performance-critical compute kernels of the high-order discontinuous Galerkin solver of the hyperbolic PDE engine ExaHyPE -- successively tackling bottlenecks due to SIMD operations, cache hierarchies and restrictions in the software design.   Starting from a generic scalar implementation of the numerical scheme, our first optimized variant applies state-of-the-art optimization techniques by vectorizing loops, improving the data layout and using Loop-over-GEMM to perform tensor contractions via highly optimized matrix multiplication functions provided by the LIBXSMM library. We show that memory stalls due to a memory footprint exceeding our L2 cache size hindered the vectorization gains. We therefore introduce a new kernel that applies a sum factorization approach to reduce the kernel's memory footprint and improve its cache locality. With the L2 cache bottleneck removed, we were able to exploit additional vectorization opportunities, by introducing a hybrid Array-of-Structure-of-Array data layout that solves the data layout conflict between matrix multiplications kernels and the point-wise functions to implement PDE-specific terms.   With this last kernel, evaluated in a benchmark simulation at high polynomial order, only 2\% of the floating point operations are still performed using scalar instructions and 22.5\% of the available performance is achieved. ","Jean-Matthieu Gallard, Leonhard Rannabauer, Anne Reinarz, Michael   Bader",,,11,
Blends in Maple,"  A blend of two Taylor series for the same smooth real- or complex-valued function of a single variable can be useful for approximation. We use an explicit formula for a two-point Hermite interpolational polynomial to construct such blends. We show a robust Maple implementation that can stably and efficiently evaluate blends using linear-cost Horner form, evaluate their derivatives to arbitrary order at the same time, or integrate a blend exactly. The implementation is suited for use with evalhf. We provide a top-level user interface and efficient module exports for programmatic use.   This work intended for presentation at the Maple Conference 2020. See www.maplesoft.com/mapleconference ",Robert M. Corless and Erik Postma,,,11,
Array Programming with NumPy,"  Array programming provides a powerful, compact, expressive syntax for accessing, manipulating, and operating on data in vectors, matrices, and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It plays an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, material science, engineering, finance, and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves and the first imaging of a black hole. Here we show how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring, and analyzing scientific data. NumPy is the foundation upon which the entire scientific Python universe is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Because of its central position in the ecosystem, NumPy increasingly plays the role of an interoperability layer between these new array computation libraries. ","Charles R. Harris, K. Jarrod Millman, St\'efan J. van der Walt, Ralf   Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,   Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,   Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\'andez del   R\'io, Mark Wiebe, Pearu Peterson, Pierre G\'erard-Marchant, Kevin Sheppard,   Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, Travis E.   Oliphant",,,11,
DSLib: An open source library for the dominant set clustering method,"  DSLib is an open-source implementation of the Dominant Set (DS) clustering algorithm written entirely in Matlab. The DS method is a graph-based clustering technique rooted in the evolutionary game theory that starts gaining lots of interest in the computer science community. Thanks to its duality with game theory and its strict relation to the notion of maximal clique, has been explored in several directions not only related to clustering problems. Applications in graph matching, segmentation, classification and medical imaging are common in literature. This package provides an implementation of the original DS clustering algorithm since no code has been officially released yet, together with a still growing collection of methods and variants related to it. Our library is integrable into a Matlab pipeline without dependencies, it is simple to use and easily extendable for upcoming works. The latest source code, the documentation and some examples can be downloaded from https://xwasco.github.io/DominantSetLibrary. ","Sebastiano Vascon, Samuel Rota Bul\`o, Vittorio Murino, Marcello   Pelillo",,,11,
Slate: extending Firedrake's domain-specific abstraction to hybridized   solvers for geoscience and beyond,"  Within the finite element community, discontinuous Galerkin (DG) and mixed finite element methods have become increasingly popular in simulating geophysical flows. However, robust and efficient solvers for the resulting saddle-point and elliptic systems arising from these discretizations continue to be an on-going challenge. One possible approach for addressing this issue is to employ a method known as hybridization, where the discrete equations are transformed such that classic static condensation and local post-processing methods can be employed. However, it is challenging to implement hybridization as performant parallel code within complex models, whilst maintaining separation of concerns between applications scientists and software experts. In this paper, we introduce a domain-specific abstraction within the Firedrake finite element library that permits the rapid execution of these hybridization techniques within a code-generating framework. The resulting framework composes naturally with Firedrake's solver environment, allowing for the implementation of hybridization and static condensation as runtime-configurable preconditioners via the Python interface to PETSc, petsc4py. We provide examples derived from second order elliptic problems and geophysical fluid dynamics. In addition, we demonstrate that hybridization shows great promise for improving the performance of solvers for mixed finite element discretizations of equations related to large-scale geophysical flows. ","Thomas H. Gibson, Lawrence Mitchell, David A. Ham, Colin J. Cotter",,,11,
Dune-CurvedGrid -- A Dune module for surface parametrization,"  In this paper we introduce and describe an implementation of curved (surface) geometries within the Dune framework for grid-based discretizations. Therefore, we employ the abstraction of geometries as local-functions bound to a grid element, and the abstraction of a grid as connectivity of elements together with a grid-function that can be localized to the elements to provide element local parametrizations of the curved surface. ",Simon Praetorius and Florian Stenger,,,11,
Parallel Robust Computation of Generalized Eigenvectors of Matrix   Pencils,"  In this paper we consider the problem of computing generalized eigenvectors of a matrix pencil in real Schur form. In exact arithmetic, this problem can be solved using substitution. In practice, substitution is vulnerable to floating-point overflow. The robust solvers xTGEVC in LAPACK prevent overflow by dynamically scaling the eigenvectors. These subroutines are sequential scalar codes which compute the eigenvectors one by one. In this paper we discuss how to derive robust blocked algorithms. The new StarNEig library contains a robust task-parallel solver Zazamoukh which runs on top of StarPU. Our numerical experiments show that Zazamoukh achieves a super-linear speedup compared with DTGEVC for sufficiently large matrices. ",Carl Christian Kjelgaard Mikkelsen and Mirko Myllykoski,,,11,
Interpolation of Dense and Sparse Rational Functions and other   Improvements in $\texttt{FireFly}$,"  We present the main improvements and new features in version $\texttt{2.0}$ of the open-source $\texttt{C++}$ library $\texttt{FireFly}$ for the interpolation of rational functions. This includes algorithmic improvements, e.g. a hybrid algorithm for dense and sparse rational functions and an algorithm to identify and remove univariate factors. The new version is applied to a Feynman-integral reduction to showcase the runtime improvements achieved. Moreover, $\texttt{FireFly}$ now supports parallelization with $\texttt{MPI}$ and offers new tools like a parser for expressions or an executable for the insertion of replacement tables. ","Jonas Klappert, Sven Yannick Klein, and Fabian Lange",,,11,
FlexRiLoG -- A SageMath Package for Motions of Graphs,  In this paper we present the SageMath package FlexRiLoG (short for flexible and rigid labelings of graphs). Based on recent results the software generates motions of graphs using special edge colorings. The package computes and illustrates the colorings and the motions. We present the structure and usage of the package. ,Georg Grasegger and Jan Legersk\'y,,,11,
"Awkward Arrays in Python, C++, and Numba","  The Awkward Array library has been an important tool for physics analysis in Python since September 2018. However, some interface and implementation issues have been raised in Awkward Array's first year that argue for a reimplementation in C++ and Numba. We describe those issues, the new architecture, and present some examples of how the new interface will look to users. Of particular importance is the separation of kernel functions from data structure management, which allows a C++ implementation and a Numba implementation to share kernel functions, and the algorithm that transforms record-oriented data into columnar Awkward Arrays. ","Jim Pivarski (1), Peter Elmer (1), David Lange (1) ((1) Princeton   University)",,,11,
Role-Oriented Code Generation in an Engine for Solving Hyperbolic PDE   Systems,"  The development of a high performance PDE solver requires the combined expertise of interdisciplinary teams with respect to application domain, numerical scheme and low-level optimization. In this paper, we present how the ExaHyPE engine facilitates the collaboration of such teams by isolating three roles: application, algorithms, and optimization expert. We thus support team members in letting them focus on their own area of expertise while integrating their contributions into an HPC production code. Inspired by web application development practices, ExaHyPE relies on two custom code generation modules, the Toolkit and the Kernel Generator, which follow a Model-View-Controller architectural pattern on top of the Jinja2 template engine library. Using Jinja2's templates to abstract the critical components of the engine and generated glue code, we isolate the application development from the engine. The template language also allows us to define and use custom template macros that isolate low-level optimizations from the numerical scheme described in the templates. We present three use cases, each focusing on one of our user roles, showcasing how the design of the code generation modules allows to easily expand the solver schemes to support novel demands from applications, to add optimized algorithmic schemes (with reduced memory footprint, e.g.), or provide improved low-level SIMD vectorization support. ","Jean-Matthieu Gallard, Lukas Krenz, Leonhard Rannabauer, Anne Reinarz,   Michael Bader",,,11,
Parallelizing multiple precision Taylor series method for integrating   the Lorenz system,"  A hybrid MPI+OpenMP strategy for parallelizing multiple precision Taylor series method is proposed, realized and tested. To parallelize the algorithm we combine MPI and OpenMP parallel technologies together with GMP library (GNU miltiple precision libary) and the tiny MPIGMP library. The details of the parallelization are explained on the paradigmatic model of the Lorenz system. We succeed to obtain a correct reference solution in the rather long time interval - [0,7000]. The solution is verified by comparing the results for 2700-th order Taylor series method and precision of ~ 3374 decimal digits, and those with 2800-th order and precision of ~ 3510 decimal digits. With 192 CPU cores in Nestum cluster, Sofia, Bulgaria, the 2800-th order computation was ~ 145 hours with speedup ~ 105. ","I. Hristov, R. Hristova, S. Dimova, P. Armyanov, N. Shegunov, I.   Puzynin, T. Puzynina, Z. Sharipov, Z. Tukhliev",,,11,
Ginkgo: A Modern Linear Operator Algebra Framework for High Performance   Computing,"  In this paper, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo's design principle abstracts all functionality as ""linear operators"", motivating the notation of a ""linear operator algebra library"". Ginkgo's current focus is oriented towards providing sparse linear algebra functionality for high performance GPU architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific back ends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo's usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo's high performance on state-of-the-art GPU architectures. ","Hartwig Anzt, Terry Cojean, Goran Flegar, Fritz G\""obel, Thomas   Gr\""utzmacher, Pratik Nayak, Tobias Ribizel, Yuhsiang Mike Tsai, Enrique S.   Quintana-Ort\'i",,,11,
A user-guide to Gridap -- grid-based approximation of partial   differential equations in Julia,"  We present Gridap, a new scientific software library for the numerical approximation of partial differential equations (PDEs) using grid-based approximations. Gridap is an open-source software project exclusively written in the Julia programming language. The main motivation behind the development of this library is to provide an easy-to-use framework for the development of complex PDE solvers in a dynamically typed style without sacrificing the performance of statically typed languages. This work is a tutorial-driven user guide to the library. It covers some popular linear and nonlinear PDE systems for scalar and vector fields, single and multi-field problems, conforming and nonconforming finite element discretizations, on structured and unstructured meshes of simplices and hexahedra. ",Francesc Verdugo and Santiago Badia,,,11,
A practical approach to testing random number generators in computer   algebra systems,"  This paper has a practical aim. For a long time, implementations of pseudorandom number generators in standard libraries of programming languages had poor quality. The situation started to improve only recently. Up to now, a large number of libraries and weakly supported mathematical packages use outdated algorithms for random number generation. Four modern sets of statistical tests that can be used for verifying random number generators are described. It is proposed to use command line utilities, which makes it possible to avoid low-level programming in such languages as C or C++. Only free open source systems are considered. ",Migran N. Gevorkyan and Dmitry S. Kulyabov and Anastasia V. Demidova   and Anna V. Korolkova,,,11,
Batched computation of the singular value decompositions of order two by   the AVX-512 vectorization,"  In this paper a vectorized algorithm for simultaneously computing up to eight singular value decompositions (SVDs, each of the form $A=U\Sigma V^{\ast}$) of real or complex matrices of order two is proposed. The algorithm extends to a batch of matrices of an arbitrary length $n$, that arises, for example, in the annihilation part of the parallel Kogbetliantz algorithm for the SVD of a square matrix of order $2n$. The SVD algorithm for a single matrix of order two is derived first. It scales, in most instances error-free, the input matrix $A$ such that its singular values $\Sigma_{ii}$ cannot overflow whenever its elements are finite, and then computes the URV factorization of the scaled matrix, followed by the SVD of a non-negative upper-triangular middle factor. A vector-friendly data layout for the batch is then introduced, where the same-indexed elements of each of the input and the output matrices form vectors, and the algorithm's steps over such vectors are described. The vectorized approach is then shown to be about three times faster than processing each matrix in isolation, while slightly improving accuracy over the straightforward method for the $2\times 2$ SVD. ",Vedran Novakovi\'c,,,11,
SLEEF: A Portable Vectorized Library of C Standard Mathematical   Functions,"  In this paper, we present techniques used to implement our portable vectorized library of C standard mathematical functions written entirely in C language. In order to make the library portable while maintaining good performance, intrinsic functions of vector extensions are abstracted by inline functions or preprocessor macros. We implemented the functions so that they can use sub-features of vector extensions such as fused multiply-add, mask registers and extraction of mantissa. In order to make computation with SIMD instructions efficient, the library only uses a small number of conditional branches, and all the computation paths are vectorized. We devised a variation of the Payne-Hanek argument reduction for trigonometric functions and a floating point remainder, both of which are suitable for vector computation. We compare the performance of our library to Intel SVML. ",Naoki Shibata and Francesco Petrogalli,,,11,
Characteristics-based Simulink implementation of first-order quasilinear   partial differential equations,"  The paper deals with solving first-order quasilinear partial differential equations in an online simulation environment, such as Simulink, utilizing the well-known and well-recommended method of characteristics. Compared to the commonly applied space discretization methods on static grids, the characteristics-based approach provides better numerical stability. Simulink subsystem implementing the method of characteristics is developed. It employs Simulink's built-in solver and its zero-crossing detection algorithm to perform simultaneous integration of a pool of characteristics as well as to create new characteristics dynamically and discard the old ones. Numerical accuracy of the solution thus obtained is established. The subsystem has been tested on a full-state feedback example and produced better results than the space discretization-based ""method of lines"". The implementation is available for download and can be used in a wide range of models. ","Anton Ponomarev, Julian Hofmann, Lutz Gr\""oll",,,11,
SODECL: An Open Source Library for Calculating Multiple Orbits of a   System of Stochastic Differential Equations in Parallel,"  Stochastic differential equations (SDEs) are widely used to model systems affected by random processes. In general, the analysis of an SDE model requires numerical solutions to be generated many times over multiple parameter combinations. However, this process often requires considerable computational resources to be practicable. Due to the embarrassingly parallel nature of the task, devices such as multi-core processors and graphics processing units (GPUs) can be employed for acceleration.   Here, we present {\bf SODECL} (\url{https://github.com/avramidis/sodecl}), a software library that utilises such devices to calculate multiple orbits of an SDE model. To evaluate the acceleration provided by SODECL, we compared the time required to calculate multiple orbits of an exemplar stochastic model when one CPU core is used, to the time required when using all CPU cores or a GPU. In addition, to assess scalability, we investigated how the model size affected execution time on different parallel compute devices.   Our results show that when using all 32 CPU cores of a high-end high-performance computing node, the task is accelerated by a factor of up to $\simeq$6.7, compared to when using a single CPU core. Executing the task on a high-end GPU yielded accelerations of up to $\simeq$4.5, compared to a single CPU core. ","Eleftherios Avramidis, Marta Lalik, Ozgur E. Akman",,,11,
A highly scalable approach to solving linear systems using two-stage   multisplitting,"  Iterative methods for solving large sparse systems of linear equations are widely used in many HPC applications. Extreme scaling of these methods can be difficult, however, since global communication to form dot products is typically required at every iteration.   To try to overcome this limitation we propose a hybrid approach, where the matrix is partitioned into blocks. Within each block, we use a highly optimised (parallel) conventional solver, but we then couple the blocks together using block Jacobi or some other multisplitting technique that can be implemented in either a synchronous or an asynchronous fashion. This allows us to limit the block size to the point where the conventional iterative methods no longer scale, and to avoid global communication (and possibly synchronisation) across all processes.   Our block framework has been built to use PETSc, a popular scientific suite for solving sparse linear systems, as the synchronous intra-block solver, and we demonstrate results on up to 32768 cores of a Cray XE6 system. At this scale, the conventional solvers are still more efficient, though trends suggest that the hybrid approach may be beneficial at higher core counts. ","Nick Brown, J. Mark Bull, Iain Bethune",,,11,
ILS-MPM: an implicit level-set-based material point method for   frictional particulate contact mechanics of deformable particles,"  Finite element simulations of frictional multi-body contact problems via conformal meshes can be challenging and computationally demanding. To render geometrical features, unstructured meshes must be used and this unavoidably increases the degrees of freedom and therefore makes the construction of slave/master pairs more demanding. In this work, we introduce an implicit material point method designed to bypass the meshing of bodies by employing level set functions to represent boundaries at structured grids. This implicit function representation provides an elegant mean to link an unbiased intermediate reference surface with the true boundaries by closest point projection as shown in leichner et al. (2019). We then enforce the contact constraints by a penalty method where the Coulomb friction law is implemented as an elastoplastic constitutive model such that a return mapping algorithm can be used to provide constitutive updates for both the stick and slip states. To evolve the geometry of the contacts properly, the Hamilton-Jacobi equation is solved incrementally such that the level set and material points are both updated accord to the deformation field. To improve the accuracy and regularity of the numerical integration of the material point method, a moving least square method is used to project numerical values of the material points back to the standard locations for Gaussian-Legendre quadrature. Several benchmarks are used to verify the proposed model. Comparisons with discrete element simulations are made to analyze the importance of stress fields on predicting the macroscopic responses of granular assemblies. ",Chuanqi Liu and Waiching Sun,,,11,
Modular-topology optimization with Wang tilings: An application to truss   structures,"  Modularity is appealing for solving many problems in optimization. It brings the benefits of manufacturability and reconfigurability to structural optimization, and enables a trade-off between the computational performance of a Periodic Unit Cell (PUC) and the efficacy of non-uniform designs in multi-scale material optimization. Here, we introduce a novel strategy for concurrent minimum-compliance design of truss modules topologies and their macroscopic assembly encoded using Wang tiling, a formalism providing independent control over the number of modules and their interfaces. We tackle the emerging bilevel optimization problem with a combination of meta-heuristics and mathematical programming. At the upper level, we employ a genetic algorithm to optimize module assemblies. For each assembly, we obtain optimal module topologies as a solution to a convex second-order conic program that exploits the underlying modularity, incorporating stress constraints, multiple load cases, and reuse of module(s) for various structures. Merits of the proposed strategy are illustrated with three representative examples, clearly demonstrating that the best designs obtained by our method exhibited decreased compliance: from 56% to 69% compared to the PUC designs. ","Marek Tyburec, Jan Zeman, Martin Do\v{s}k\'a\v{r}, Martin   Kru\v{z}\'ik, and Mat\v{e}j Lep\v{s}",,,11,
Swarm robotics and complex behaviour of continuum material,"  In swarm robotics, just as for an animal swarm in Nature, one of the aims is to reach and maintain a desired configuration. One of the possibilities for the team, to reach this aim, is to see what its neighbours are doing. This approach generates a rules system governing the movement of the single robot just by reference to neighbour's motion. The same approach is used in position based dynamics to simulate behaviour of complex continuum materials under deformation. Therefore, in some previous works, we have considered a two-dimensional lattice of particles and calculated its time evolution by using a rules system derived from our experience in swarm robotics. The new position of a particle, like the element of a swarm, is determined by the spatial position of the other particles. No dynamic is considered, but it can be thought as being hidden in the behaviour rules. This method has given good results in some simple situations reproducing the behaviour of deformable bodies under imposed strain. In this paper we try to stress our model to highlight its limits and how they can be improved. Some other, more complex, examples are computed and discussed. Shear test, different lattice, different fracture mechanism and ASTM shape sample behaviour have been investigated by the software tool we have developed. ",R. dell'Erba,,,11,
AutoMat -- Automatic Differentiation for Generalized Standard Materials   on GPUs,"  We propose a universal method for the evaluation of generalized standard materials that greatly simplifies the material law implementation process. By means of automatic differentiation and a numerical integration scheme, AutoMat reduces the implementation effort to two potential functions. By moving AutoMat to the GPU, we close the performance gap to conventional evaluation routines and demonstrate in detail that the expression level reverse mode of automatic differentiation as well as its extension to second order derivatives can be applied inside CUDA kernels. We underline the effectiveness and the applicability of AutoMat by integrating it into the FFT-based homogenization scheme of Moulinec and Suquet and discuss the benefits of using AutoMat with respect to runtime and solution accuracy for an elasto-viscoplastic example. ","Johannes Bl\""uhdorn, Nicolas R. Gauger, Matthias Kabel",,,11,
A hybridizable discontinuous Galerkin method for electromagnetics with a   view on subsurface applications,"  Two Hybridizable Discontinuous Galerkin (HDG) schemes for the solution of Maxwell's equations in the time domain are presented. The first method is based on an electromagnetic diffusion equation, while the second is based on Faraday's and Maxwell--Amp\`ere's laws. Both formulations include the diffusive term depending on the conductivity of the medium. The three-dimensional formulation of the electromagnetic diffusion equation in the framework of HDG methods, the introduction of the conduction current term and the choice of the electric field as hybrid variable in a mixed formulation are the key points of the current study. Numerical results are provided for validation purposes and convergence studies of spatial and temporal discretizations are carried out. The test cases include both simulation in dielectric and conductive media. ","Luca Berardocco, Martin Kronbichler and Volker Gravemeier",,,11,
Molecular dynamics simulation of crack growth in mono-crystal nickel   with voids and inclusions,"  In this study, the crack propagation of the pre-cracked mono-crystal nickel with the voids and inclusions has been investigated by molecular dynamics simulations. Different sizes of voids, inclusions and materials of inclusions are used to fully study the effect of the voids and inclusions during the crack propagation process. The dislocations evolution, stress distribution and crack length are analyzed as the associated mechanical properties. The results indicate that the voids and inclusions can change the path of crack propagation of the pre-cracked mono-crystal nickel. Moreover, the results show that the voids and inclusions can lead a better resistance to plastic deformation of the mono-crystal and the inclusions can make the system more difficult to fracture. ","Zhenxing Cheng, Hu Wang, Gui-Rong Liu, Guangyao Li",,,11,
The Radial Point Interpolation Mixed Collocation (RPIMC) Method for the   Solution of Transient Diffusion Problems,"  The Radial Point Interpolation Mixed Collocation (RPIMC) method is proposed in this paper for transient analysis of diffusion problems. RPIMC is an efficient purely meshless method where the solution of the field variable is obtained through collocation. The field function and its gradient are both interpolated (mixed collocation approach) leading to reduced $C$-continuity requirement compared to strong-form collocation schemes. The method's accuracy is evaluated in heat conduction benchmark problems. The RPIMC convergence is compared against the Meshless Local Petrov-Galerkin Mixed Collocation (MLPG-MC) method and the Finite Element Method (FEM). Due to the delta Kronecker property of RPIMC, improved accuracy can be achieved as compared to MLPG-MC. RPIMC is proven to be a promising meshless alternative to FEM for transient diffusion problems. ",Konstantinos A. Mountris and Esther Pueyo,,,11,
A subtractive manufacturing constraint for level set topology   optimization,"  We present a method for enforcing manufacturability constraints in generated parts such that they will be automatically ready for fabrication using a subtractive approach. We primarily target multi-axis CNC milling approaches but the method should generalize to other subtractive methods as well. To this end, we take as user input: the radius of curvature of the tool bit, a coarse model of the tool head and optionally a set of milling directions. This allows us to enforce the following manufacturability conditions: 1) surface smoothness such that the radius of curvature of the part does not exceed the milling bit radius, 2) orientation such that every part of the surface to be milled is visible from at least one milling direction, 3) accessibility such that every surface patch can be reached by the tool bit without interference with the tool or head mount. We will show how to efficiently enforce the constraint during level set-based topology optimization modifying the advection velocity such that at each iteration the topology optimization maintains a descent optimization direction and does not violate any of the manufacturability conditions. This approach models the actual subtractive process by carving away material accessible to the machine at each iteration until a local optimum is achieved. ","Nigel Morris, Adrian Butscher and Francesco Iorio",,,11,
Linear-frictional contact model for 3D discrete element simulations of   granular systems,"  The linear-frictional contact model is the most commonly used contact mechanism for discrete element (DEM) simulations of granular materials. Linear springs with a frictional slider are used for modeling interactions in directions normal and tangential to the contact surface. Although the model is simple in two dimensions, its implementation in 3D faces certain subtle challenges, and the particle interactions that occur within a single time-step require careful modeling with a robust algorithm. The paper details a 3D algorithm that accounts for the changing direction of the tangential force within a time-step, the transition from elastic to slip behavior within a time-step, possible contact sliding during only part of a time-step, and twirling and rotation of the tangential force during a time-step. Without three of these adjustments, errors are introduced in the incremental stiffness of an assembly. Without the fourth adjustment, the resulting stress tensor is not only incorrect, it is no longer a tensor. The algorithm also computes the work increments during a time-step, both elastic and dissipative. ","Matthew R. Kuhn, Kiichi Suzuki, Ali Daouadji",,,11,
Capacity Value of Interconnection Between Two Systems,"  Concerns about system adequacy have led to the establishment of capacity mechanisms in a number of regulatory areas. Against this background, it is essential to accurately quantify the contribution to security of supply that results from interconnectors to neighbouring systems. This paper introduces a definition of capacity value for interconnection between two systems in the form of a capacity allocation curve. Four power flow policies are proposed to encompass the full range of possible market outcomes that may affect the capacity value. A convolution-based method is presented to efficiently compute and compare capacity allocation curves, and it is applied to a model system that is inspired by Great Britain's interconnection with the continental Europe. The results indicate areas of interest for the coordination of capacity mechanisms. ","Simon H. Tindemans, Matthew Woolf, Goran Strbac",,,11,
Study of autonomous conservative oscillator using an improved   perturbation method,"  In a recent article \cite{manimegalai2019}, Aboodh transform based homotopy perturbation method ($AT$) has been found to produce approximate analytical solutions in a simple way but with better accuracy in comparison to those obtained from some of the established approximation methods \cite{mehdipour2010application,nofal2013analytical} for some physically relevant anharmonic oscillators such as autonomous conservative oscillator (ACO). In the present article, expansion of frequency ($\omega$) and an auxiliary parameter ($h$) are incorporated in the framework of the homotopy perturbation method (HPM) to improve the accuracy by retaining its simplicity. Laplace transform is used to make the calculation simpler. This improved HPM ($LH$) is simple but provides highly accurate results for ACO in comparison to those obtained from $AT$. The error in the values of frequency and displacement calculated using the $LH$ is found to be one or two order of magnitude less than those obtained from $AT$ for the considered parameter sets. ",C. F. Sagar Zephania and Tapas Sil,,,11,
Automatic weak imposition of free slip boundary conditions via Nitsche's   method: application to nonlinear problems in geodynamics,"  Imposition of free slip boundary conditions in science and engineering simulations presents a challenge when the simulation domain is non-trivial. Inspired by recent progress in symbolic computation of discontinuous Galerkin finite element methods, we present a symmetric interior penalty form of Nitsche's method to weakly impose these slip boundary conditions and present examples of its use in the Stokes subsystem motivated by problems in geodynamics. We compare numerical results with well established benchmark problems. We also examine performance of the method with iterative solvers. ","Nathan Sime, Cian R. Wilson",,,11,
Uncertainty Quantification of Structural Systems with Subset of Data,"  Quantification of the impact of uncertainty in material properties as well as the input ground motion on structural responses is an important step in implementing a performance-based earthquake engineering (PBEE) framework. Among various sources of uncertainty, the variability in the input ground motions, a.k.a. record-to-record, greatly affects the assessment results. The objective of this paper is to quantify the uncertainty in structural response with hybrid uncertainty sources. In this paper, multiple matrix completion methods are proposed and applied on a case study structure. The matrix completion method is a means to estimate the analyses results for the entire set of input parameters by conducting analysis for only a small subset of analyses. The main algorithmic contributions of our proposed method are twofold. First, we develop a sampling technique for choosing a subset of representative simulations, which allows improving the accuracy of the estimated response. An unsupervised machine learning technique is used for this purpose. Next, the proposed matrix completion method for uncertainty quantification is further refined by incorporating a regression model that is trained on the available partial simulations. The regression model improves the initial sampling as it provides a rough estimation of the structural responses. Finally, the proposed algorithm is applied to a multi-degree-of-freedom system, and the structural responses (i.e., displacements and base shear) are estimated. Results show that the proposed algorithm can effectively estimate the response from a full set of nonlinear simulations by conducting analyses only on a small portion of the set. ","Mohammad Amin Hariri-Ardebili, Farhad Pourkamali-Anaraki, Siamak   Sattar",,,11,
A multilevel Monte Carlo method for high-dimensional uncertainty   quantification of low-frequency electromagnetic devices,"  This work addresses uncertainty quantification of electromagnetic devices determined by the eddy current problem. The multilevel Monte Carlo (MLMC) method is used for the treatment of uncertain parameters while the devices are discretized in space by the finite element method. Both methods yield numerical approximations such that the total errors is split into stochastic and spatial contributions. We propose a particular implementation where the spatial error is controlled based on a Richardson-extrapolation-based error indicator. The stochastic error in turn is efficiently reduced in the MLMC approach by distributing the samples on multiple grids. The method is applied to a toy problem with closed-form solution and a permanent magnet synchronous machine with uncertainties. The uncertainties under consideration are related to the material properties in the stator and the magnets in the rotor. The examples show that the error indicator works reliably, the meshes used for the different levels do not have to be nested and, most importantly, MLMC reduces the computational cost by at least one order of magnitude compared to standard Monte Carlo. ","Armin Galetzka, Zeger Bontinck, Ulrich R\""omer, Sebastian Sch\""ops",,,11,
An Efficient Gradient Projection Method for Structural Topology   Optimization,"  This paper presents an efficient gradient projection-based method for structural topological optimization problems characterized by a nonlinear objective function which is minimized over a feasible region defined by bilateral bounds and a single linear equality constraint. The specialty of the constraints type, as well as heuristic engineering experiences are exploited to improve the scaling scheme, projection, and searching step. In detail, gradient clipping and a modified projection of searching direction under certain condition are utilized to facilitate the efficiency of the proposed method. Besides, an analytical solution is proposed to approximate this projection with negligible computation and memory costs. Furthermore, the calculation of searching steps is largely simplified. Benchmark problems, including the MBB, the force inverter mechanism, and the 3D cantilever beam are used to validate the effectiveness of the method. The proposed method is implemented in MATLAB which is open-sourced for educational usage. ",Zhi Zeng and Fulei Ma,,,11,
Topology Optimization Design of Stretchable Metamaterials with Bezier   Skeleton Explicit Density (BSED) Representation Algorithm,"  A new density field representation technique called the Bezier skeleton explicit density (BSED) representation scheme for topology optimization of stretchable metamaterials under finite deformation is proposed for the first time. The proposed approach overcomes a key deficiency in existing density-based optimization methods that typically yield designs that do not have smooth surfaces but have large number of small intricate features, which are difficult to manufacture even by additive manufacturing. In the proposed approach, Bezier curves are utilized to describe the skeleton of the design being optimized where the description of the entire design is realized by assigning thickness along the curves. This geometric representation technique ensures that the optimized design is smooth and concise and can easily be tuned to be manufacturable by additive manufacturing. In the optimization method, the density field is described by the Heaviside function defined on the Bezier curves. Compared to NURBS or B-spline based models, Bezier curves have fewer control parameters and hence are easier to manipulate for sensitivity derivation, especially for distance sensitivities. Due to its powerful curve fitting ability, using Bezier curve to represent density field allows exploring design space effectively and generating concise structures without any intricate small features at the borders. Furthermore, this density representation method is mesh independent and design variables are reduced significantly so that optimization problem can be solved efficiently using small-scale optimization algorithms such as sequential quadratic programming. ","Hao Deng, Shawn Hinnebusch, and Albert C. To",,,11,
Fail-safe optimization of viscous dampers for seismic retrofitting,"  This paper presents a new optimization approach for designing minimum-cost fail-safe distributions of fluid viscous dampers for seismic retrofitting. Failure is modeled as either complete damage of the dampers or partial degradation of the dampers' properties. In general, this leads to optimization problems with large number of constraints. Thus, the use of a working-set optimization algorithm is proposed. The main idea is to solve a sequence of relaxed optimization sub-problems with a small sub-set of all constraints. The algorithm terminates once a solution of a sub-problem is found that satisfies all the constraints of the problem. The retrofitting cost is minimized with constraints on the inter-story drifts at the peripheries of frame structures. The structures considered are subjected to a realistic ensemble of ground motions, and their response is evaluated with time-history analyses. The transient optimization problem is efficiently solved with a gradient-based sequential linear programming algorithm. The gradients of the response functions are calculated with a consistent adjoint sensitivity analysis procedure. Promising results attained for 3-D irregular frames are presented and discussed. The numerical results highlight the fact that the optimized layout and size of the dampers can change significantly even for moderate levels of damage. ",Nicol\`o Pollini,,,11,
Variational phase-field continuum model uncovers adhesive wear   mechanisms in asperity junctions,"  Wear is well known for causing material loss in a sliding interface. Available macroscopic approaches are bound to empirical fitting parameters, which range several orders of magnitude. Major advances in tribology have recently been achieved via Molecular Dynamics, although its use is strongly limited by computational cost. Here, we propose a study of the physical processes that lead to wear at the scale of the surface roughness, where adhesive junctions are formed between the asperities on the surface of the materials. Using a brittle formulation of the variational phase-field approach to fracture, we demonstrate that the failure mechanisms of an adhesive junction can be linked to its geometry. By imposing specific couplings between the damage and the elastic energy, we further investigate the triggering processes underlying each failure mechanism. We show that a large debris formation is mostly triggered by tensile stresses while shear stresses lead to small or no particle formation. We also study groups of junctions and discuss how microcontact interactions can be favored in some geometries to form macro-particles. This leads us to propose a classification in terms of macroscopic wear rate. Although based on a continuum approach, our phase-field calculations are able to effectively capture the failure of adhesive junctions, as observed through discrete Molecular Dynamics simulations. ","Sylvain Collet, Jean-Fran\c{c}ois Molinari, Stella Brach",,,11,
VulnDS: Top-k Vulnerable SME Detection System in Networked-Loans,"  Groups of small and medium enterprises (SMEs) can back each other to obtain loans from banks and thus form guarantee networks. If the loan repayment of a small company in the network defaults, its backers are required to repay the loan. Therefore, risk over networked enterprises may cause significant contagious damage. In real-world applications, it is critical to detect top vulnerable nodes in such complex financial network with near real-time performance. To address this challenge, we introduce VulnDS: a top-k vulnerable SME detection system for large-scale financial networks, which is deployed in our collaborated bank. First, we model the risks of the guaranteed-loan network by a probabilistic graph, which consists of the guarantee-loan network structure, self-risk probability for the nodes and diffusion probability for the edges. Moreover, to identify the vulnerable enterprises, we propose a sampling-based approach with tight theoretical guarantee. Novel optimization techniques are developed in order to scale for large networks. We conduct extensive experiments on 3 real financial datasets, in addition with 5 large-scale benchmark networks. The evaluation results show that the proposed method can achieve up to 100x speedup ratio compared with baseline methods. Case studies are further conducted in the deployed system to demonstrate the effectiveness of proposed model. ","Dawei Cheng, Xiaoyang Wang, Chen Chen, Ying Zhang",,,11,
Three-dimensional convolutional neural network (3D-CNN) for   heterogeneous material homogenization,"  Homogenization is a technique commonly used in multiscale computational science and engineering for predicting collective response of heterogeneous materials and extracting effective mechanical properties. In this paper, a three-dimensional deep convolutional neural network (3D-CNN) is proposed to predict the effective material properties for representative volume elements (RVEs) with random spherical inclusions. The high-fidelity dataset generated by a computational homogenization approach is used for training the 3D-CNN models. The inference results of the trained networks on unseen data indicate that the network is capable of capturing the microstructural features of RVEs and produces an accurate prediction of effective stiffness and Poisson's ratio. The benefits of the 3D-CNN over conventional finite-element-based homogenization with regard to computational efficiency, uncertainty quantification and model's transferability are discussed in sequence. We find the salient features of the 3D-CNN approach make it a potentially suitable alternative for facilitating material design with fast product design iteration and efficient uncertainty quantification. ",Chengping Rao and Yang Liu,,,11,
MeSH descriptors indicate the knowledge growth in the   SARS-CoV-2/COVID-19 pandemic,"  The scientific papers dealing with the novel betacoronavirus SARS-CoV-2 and the coronavirus disease 2019 (COVID-19) caused by this virus, published in 2020 and recorded in the database PUBMED, were retrieved on April 27, 2020. About 20\% of the records contain Medical Subject Headings (MeSH), keywords assigned to records in the course of the indexing process in order to summarise the articles' contents. The temporal sequence of the first occurrences of the keywords was determined, thus giving insight into the growth of the knowledge base of the pandemic. ",Johannes Stegmann,,,11,
Team assembly mechanisms and the knowledge produced in the Mexico's   National Institute of Geriatrics: a network analysis and agent-based   modelling approach,"  Mexico's National Institute of Geriatrics (INGER) is the national research center of reference for matters related to human aging. INGER scientists perform basic, clinical and demographic research which may imply different scientific cultures working together in the same specialized institution. In this paper, by a combination of text mining, co-authorship network analysis and agent-based modeling we analyzed and modeled the team assembly practices and the structure of the knowledge produced by scientists from INGER. Our results showed a weak connection between basic and clinical research, and the emergence of a highly connected academic leadership. Importantly, basic and clinical-demographic researchers exhibited different team assembly strategies: Basic researchers tended to form larger teams mainly with external collaborators while clinical and demographic researchers formed smaller teams that very often incorporated internal (INGER) collaborators. We showed how these two different ways to form research teams impacted the organization of knowledge produced at INGER. Following these observations, we modeled, via agent-based modeling, the coexistence of different scientific cultures (basic and clinical research) exhibiting different team assembly strategies in the same institution. Our agent model successfully reproduced the current situation of INGER. Moreover, by modifying the values of homophily we obtain alternative scenarios in which multidisciplinary and interdisciplinary research could be done. ","Carmen Garc\'ia-Pe\~na, Luis Miguel Guti\'errez-Robledo, Augusto   Cabrera-Becerril and David Fajardo-Ortiz",,,11,
Towards a more realistic citation model: The key role of research team   sizes,"  We propose a new citation model which builds on the existing models that explicitly or implicitly include ""direct"" and ""indirect"" (learning about a cited paper's existence from references in another paper) citation mechanisms. Our model departs from the usual, unrealistic assumption of uniform probability of direct citation, in which initial differences in citation arise purely randomly. Instead, we demonstrate that a two-mechanism model in which the probability of direct citation is proportional to the number of authors on a paper (team size) is able to reproduce the empirical citation distributions of articles published in the field of astronomy remarkably well, and at different points in time. Interpretation of our model is that the intrinsic citation capacity, and hence the initial visibility of a paper, will be enhanced when more people are intimately familiar with some work, favoring papers from larger teams. While the intrinsic citation capacity cannot depend only on the team size, our model demonstrates that it must be to some degree correlated with it, and distributed in a similar way, i.e., having a power-law tail. Consequently, our team-size model qualitatively explains the existence of a correlation between the number of citations and the number of authors on a paper. ",Sta\v{s}a Milojevi\'c,,,11,
How to Investigate the Historical Roots and Evolution of Research Fields   in China? A Case Study on iMetrics Using RootCite,"  This paper aimed to provide an approach to investigate the historical roots and evolution of research fields in China by extending the reference publication year spectroscopy (RPYS). RootCite, an open source software accepts raw data from both the Web of Science and the China Social Science Citation Index (CSSCI), was developed using python. We took iMetrics in China as the research case. 5,141 Chinese iMetrics related publications with 73,376 non-distinct cited references (CR) collected from the CSSCI were analyzed using RootCite. The results showed that the first CR in the field can be dated back to 1882 and written in English; but the majority (64.2%) of the CR in the field were Chinese publications. 17 peaks referring to 18 seminal works (13 in English and 5 in Chinese) were located during the period from 1900 to 2017. The field shared the same roots with that in the English world but has its own characteristics, and it was then shaped by contributions from both the English world and China. The five Chinese works have played irreplaceable and positive roles in the historical evolutionary path of the field, which should not be ignored, especially for the evolution of the field. This research demonstrated how RootCite aided the task of identifying the origin and evolution of research fields in China, which could be valuable for extending RPYS for countries with other languages. ","Xin Li, Qiang Yao, Xuli Tang, Qian Li, Mengjia Wu",,,11,
Characterising authors on the extent of their paper acceptance: A case   study of the Journal of High Energy Physics,"  New researchers are usually very curious about the recipe that could accelerate the chances of their paper getting accepted in a reputed forum (journal/conference). In search of such a recipe, we investigate the profile and peer review text of authors whose papers almost always get accepted at a venue (Journal of High Energy Physics in our current work). We find authors with high acceptance rate are likely to have a high number of citations, high $h$-index, higher number of collaborators etc. We notice that they receive relatively lengthy and positive reviews for their papers. In addition, we also construct three networks -- co-reviewer, co-citation and collaboration network and study the network-centric features and intra- and inter-category edge interactions. We find that the authors with high acceptance rate are more `central' in these networks; the volume of intra- and inter-category interactions are also drastically different for the authors with high acceptance rate compared to the other authors. Finally, using the above set of features, we train standard machine learning models (random forest, XGBoost) and obtain very high class wise precision and recall. In a followup discussion we also narrate how apart from the author characteristics, the peer-review system might itself have a role in propelling the distinction among the different categories which could lead to potential discrimination and unfairness and calls for further investigation by the system admins. ",Rima Hazra and Aryan and Hardik Aggarwal and Matteo Marsili and   Animesh Mukherjee,,,11,
How do academic topics shift across altmetric sources? A case study of   the research area of Big Data,"  Taking the research area of Big Data as a case study, we propose an approach for exploring how academic topics shift through the interactions among audiences across different altmetric sources. Data used is obtained from Web of Science (WoS) and Altmetric.com, with a focus on Blog, News, Policy, Wikipedia, and Twitter. Author keywords from publications and terms from online events are extracted as the main topics of the publications and the online discussion of their audiences at Altmetric. Different measures are applied to determine the (dis)similarities between the topics put forward by the publication authors and those by the online audiences. Results show that overall there are substantial differences between the two sets of topics around Big Data scientific research. The main exception is Twitter, where high-frequency hashtags in tweets have a stronger concordance with the author keywords in publications. Among the online communities, Blogs and News show a strong similarity in the terms commonly used, while Policy documents and Wikipedia articles exhibit the strongest dissimilarity in considering and interpreting Big Data related research. Specifically, the audiences not only focus on more easy-to-understand academic topics related to social or general issues, but also extend them to a broader range of topics in their online discussions. This study lays the foundations for further investigations about the role of online audiences in the transformation of academic topics across altmetric sources, and the degree of concern and reception of scholarly contents by online communities. ",Xiaozan Lyu and Rodrigo Costas,,,11,
Comparing like with like: China ranks first in SCI-indexed research   articles since 2018,"  China's rising in scientific research output is impressive. The academic community is curious about the time when the cross-over in the number of annual scientific publication production between China and the USA can happen. By using Web of Science Core Collection's Science Citation Index Expanded database, this study finds that China still ranks the second in the production of SCI-indexed publications in 2019 but may leapfrog the USA to be the first in 2020 or 2021, if all document types are considered. Comparatively, China has already overtaken the USA and been the largest SCI-indexed original research article producer since 2018. However, China still lags behind the USA regarding the number of review paper production. In general, quantitative advantage does not equal quality or impact advantage. We think that the USA will continue to be the global scientific leader for a long time. ",Junwen Zhu and Weishu Liu,,,11,
Classification of abrupt changes along viewing profiles of scientific   articles,"  With the expansion of electronic publishing, a new dynamics of scientific articles dissemination was initiated. Nowadays, many works are widely disseminated even before publication, in the form of preprints. Another important new element concerns the views of published articles. Thanks to the availability of respective data by some journals, such as PLoS ONE, it became possible to develop investigations on how scientific works are viewed along time, often before the first citations appear. This provides the main theme of the present work. More specifically, our research was motivated by preliminary observations that the view profiles along time tend to present a piecewise linear nature. A methodology was then delineated in order to identify the main segments in the view profiles, which allowed several related measurements to be derived. In particular, we focused on the inclination and length of each subsequent segment. Basic statistics indicated that the inclination can vary substantially along subsequent segments, while the segment lengths resulted more stable. Complementary joint statistics analysis, considering pairwise correlations, provided further information about the properties of the views. In order to better understand the view profiles, we performed respective multivariate statistical analysis, including principal component analysis and hierarchical clustering. The results suggest that a portion of the polygonal views are organized into clusters or groups. These groups were characterized in terms of prototypes indicating the relative increase or decrease along subsequent segments. Four respective distinct models were then developed for representing the observed segments. It was found that models incorporating joint dependencies between the properties of the segments provided the most accurate results among the considered alternatives. ","Ana C. M. Brito, Filipi N. Silva, Henrique F. de Arruda, Cesar H.   Comin, Diego R. Amancio and Luciano da F. Costa",,,11,
Journal article publishing in the social sciences and humanities: a   comparison of Web of Science coverage for five European countries,"  This study compares publication pattern dynamics in the social sciences and humanities in five European countries. Three are Central and Eastern European countries that share a similar cultural and political heritage (the Czech Republic, Slovakia, and Poland). The other two are Flanders (Belgium) and Norway, representing Western Europe and the Nordics, respectively. We analysed 449,409 publications from 2013-2016 and found that, despite persisting differences between the two groups of countries across all disciplines, publication patterns in the Central and Eastern European countries are becoming more similar to those in their Western and Nordic counterparts. Articles from the Central and Eastern European countries are increasingly published in journals indexed in Web of Science and also in journals with the highest citation impacts. There are, however, clear differences between social science and humanities disciplines, which need to be considered in research evaluation and science policy. ","Michal Petr, Tim C.E. Engels, Emanuel Kulczycki, Marta Duskova, Raf   Guns, Monika Sieberova, Gunnar Sivertsen",,,11,
A Realistic Guide to Making Data Available Alongside Code to Improve   Reproducibility,"  Data makes science possible. Sharing data improves visibility, and makes the research process transparent. This increases trust in the work, and allows for independent reproduction of results. However, a large proportion of data from published research is often only available to the original authors. Despite the obvious benefits of sharing data, and scientists' advocating for the importance of sharing data, most advice on sharing data discusses its broader benefits, rather than the practical considerations of sharing. This paper provides practical, actionable advice on how to actually share data alongside research. The key message is sharing data falls on a continuum, and entering it should come with minimal barriers. ","Nicholas J Tierney, Karthik Ram",,,11,
Science through Wikipedia: A novel representation of open knowledge   through co-citation networks,"  This study provides an overview of science from the Wikipedia perspective. A methodology has been established for the analysis of how Wikipedia editors regard science through their references to scientific papers. The method of co-citation has been adapted to this context in order to generate Pathfinder networks (PFNET) that highlight the most relevant scientific journals and categories, and their interactions in order to find out how scientific literature is consumed through this open encyclopaedia. In addition to this, their obsolescence has been studied through Price index. A total of 1 433 457 references available at Altmetric.com have been initially taken into account. After pre-processing and linking them to the data from Elsevier's CiteScore Metrics the sample was reduced to 847 512 references made by 193 802 Wikipedia articles to 598 746 scientific articles belonging to 14 149 journals indexed in Scopus. As highlighted results we found a significative presence of ""Medicine"" and ""Biochemistry, Genetics and Molecular Biology"" papers and that the most important journals are multidisciplinary in nature, suggesting also that high-impact factor journals were more likely to be cited. Furthermore, only 13.44% of Wikipedia citations are to Open Access journals. ","Wenceslao Arroyo-Machado, Daniel Torres-Salinas, Enrique   Herrera-Viedma, Esteban Romero-Fr\'ias",,,11,
Similarity network fusion for scholarly journals,"  This paper explores intellectual and social proximity among scholarly journals by using network fusion techniques. Similarities among journals are initially represented by means of a three-layer network based on co-citations, common authors and common editors. The information contained in the three layers is combined by implementing a fused similarity network. Subsequently, partial distance correlations are adopted for measuring the contribution of each layer to the structure of the fused network. Finally, the community morphology of the fused network is explored by using modularity. In the three fields considered (i.e. economics, information and library sciences and statistics) the major contribution to the structure of the fused network arises from editors. This result suggests that the role of editors as gatekeepers of journals is the most relevant in defining the boundaries of scholarly communities. As to information and library sciences and statistics, the clusters of journals reflect sub-field specializations. As to economics, clusters of journals appear to be better interpreted in terms of alternative methodological approaches. Thus, the graphs representing the clusters of journals in the fused network are powerful explorative instruments for exploring research fields. ","Federica Baccini, Lucio Barabesi, Alberto Baccini, Mahdi Khelfaoui,   Yves Gingras",,,11,
Scientometric analysis and knowledge mapping of literature-based   discovery (1986-2020),"  Literature-based discovery (LBD) aims to discover valuable latent relationships between disparate sets of literatures. LBD research has undergone an evolution from being an emerging area to a mature research field. Hence it is timely and necessary to summarize the LBD literature and scrutinize general bibliographic characteristics, current and future publication trends, and its intellectual structure. This paper presents the first inclusive scientometric overview of LBD research. We utilize a comprehensive scientometric approach incorporating CiteSpace to systematically analyze the literature on LBD from the last four decades (1986-2020). After manual cleaning, we have retrieved a total of 409 documents from six bibliographic databases (Web of Science, Scopus, PubMed, IEEE Xplore, ACM Digital Library, and Springer Link) and two preprint servers (ArXiv and BiorXiv). The results have shown that Thomas C. Rindflesch published the highest number of LBD papers, followed by Don R. Swanson. The United States plays a leading role in LBD research with the University of Chicago as the dominant institution. To go deeper, we also perform science mapping including cascading citation expansion. The knowledge base of LBD research has changed significantly since its inception, with emerging topics including deep learning and explainable artificial intelligence. The results have indicated that LBD is still growing and evolving. Drawing on our insights, we now better understand the historical progress of LBD in the last 35 years and are able to improve publishing practices to contribute to the field in the future. ",Andrej Kastrin and Dimitar Hristovski,,,11,
Where Are We? Using Scopus to Map the Literature at the Intersection   Between Artificial Intelligence and Research on Crime,"  Research on Artificial Intelligence (AI) applications has spread over many scientific disciplines. Scientists have tested the power of intelligent algorithms developed to predict (or learn from) natural, physical and social phenomena. This also applies to crime-related research problems. Nonetheless, studies that map the current state of the art at the intersection between AI and crime are lacking. What are the current research trends in terms of topics in this area? What is the structure of scientific collaboration when considering works investigating criminal issues using machine learning, deep learning, and AI in general? What are the most active countries in this specific scientific sphere? Using data retrieved from the Scopus database, this work quantitatively analyzes 692 published works at the intersection between AI and crime employing network science to respond to these questions. Results show that researchers are mainly focusing on cyber-related criminal topics and that relevant themes such as algorithmic discrimination, fairness, and ethics are considerably overlooked. Furthermore, data highlight the extremely disconnected structure of co-authorship networks. Such disconnectedness may represent a substantial obstacle to a more solid community of scientists interested in these topics. Additionally, the graph of scientific collaboration indicates that countries that are more prone to engage in international partnerships are generally less central in the network. This means that scholars working in highly productive countries (e.g. the United States, China) tend to mostly collaborate domestically. Finally, current issues and future developments within this scientific area are also discussed. ",Gian Maria Campedelli,,,11,
Towards an RDF Knowledge Graph of Scholars from Early Modern History,"  The use of Semantic Web Technologies supports research in the field of digital humanities. In this paper we focus on the creation of semantic independent online databases such as those of historical prosopography. These databases contain biographical information of historical persons. We focus on this information with an interest in German professorial career patterns from the 16th to the 18th century. In that respect, we describe the process of building an Early Modern Scholarly Career RDF Knowledge Graph from two existing prosopography online databases: the Catalogus Professorum Lipsiensium and the Catalogus Professorum Helmstadiensium. Further, we provide an insight in how to query the information using KBox to answer research questions. ",Jennifer Blanke and Thomas Riechert,,,11,
t factor: A metric for measuring impact on Twitter,"  Based on the definition of the well-known h index we propose a t factor for measuring the impact of publications (and other entities) on Twitter. The new index combines tweet and retweet data in a balanced way whereby retweets are seen as data reflecting the impact of initial tweets. The t factor is defined as follows: A unit (single publication, journal, researcher, research group etc.) has factor t if t of its Nt tweets have at least t retweets each and the other (Nt-t) tweets have <=t retweets each. ",Lutz Bornmann and Robin Haunschild,,,11,
Are nationally oriented journals indexed in Scopus becoming more   international? The effect of publication language and access modality,"  An exploratory, descriptive analysis is presented of the national orientation of scientific, scholarly journals as reflected in the affiliations of publishing or citing authors. It calculates for journals covered in Scopus an Index of National Orientation (INO), and analyses the distribution of INO values across disciplines and countries, and the correlation between INO values and journal impact factors. The study did not find solid evidence that journal impact factors are good measures of journal internationality in terms of the geographical distribution of publishing or citing authors, as the relationship between a journal's national orientation and its citation impact is found to be inverse U-shaped. In addition, journals publishing in English are not necessarily internationally oriented in terms of the affiliations of publishing or citing authors; in social sciences and humanities also USA has their nationally oriented literatures. The paper examines the extent to which nationally oriented journals entering Scopus in earlier years, have become in recent years more international. It is found that in the study set about 40 per cent of such journals does reveal traces of internationalization, while the use of English as publication language and an Open Access (OA) status are important determinants. ","Henk F. Moed, Felix de Moya-Anegon, Vicente Guerrero-Bote, Carmen   Lopez-Illescas",,,11,
Operational Research Literature as a Use Case for the Open Research   Knowledge Graph,"  The Open Research Knowledge Graph (ORKG) provides machine-actionable access to scholarly literature that habitually is written in prose. Following the FAIR principles, the ORKG makes traditional, human-coded knowledge findable, accessible, interoperable, and reusable in a structured manner in accordance with the Linked Open Data paradigm. At the moment, in ORKG papers are described manually, but in the long run the semantic depth of the literature at scale needs automation. Operational Research is a suitable test case for this vision because the mathematical field and, hence, its publication habits are highly structured: A mundane problem is formulated as a mathematical model, solved or approximated numerically, and evaluated systematically. We study the existing literature with respect to the Assembly Line Balancing Problem and derive a semantic description in accordance with the ORKG. Eventually, selected papers are ingested to test the semantic description and refine it further. ","Mila Runnwerth, Markus Stocker, S\""oren Auer",,,11,
Research performance of UNU - A bibliometric analysis of the United   Nations University,"  The scientific paper output of the United Nations University (UNU) was bibliometrically analysed. It was found that (i) a noticeable continous paper output starts in 1995, (ii) about 65% of the research papers have been published as international cooperations and 18% as single-authored papers, (iv) the research papers rank above world average according to Pudovkin-Garfield Percentile Rank Index, and (v) paper content indicate the wide variety of scientific topics UNU has been and is working on. ",Johannes Stegmann,,,11,
"Statistical relationships between corresponding authorship,   international co-authorship and citation impact of national research systems","  This paper presents a statistical analysis of the relationship between three science indicators applied in earlier bibliometric studies, namely research leadership based on corresponding authorship, international collaboration using international co-authorship data, and field-normalized citation impact. Indicators at the level of countries are extracted from the SIR database created by SCImago Research Group from publication records indexed for Elsevier's Scopus. The relationship between authorship and citation-based indicators is found to be complex, as it reflects a country's phase of scientific development and the coverage policy of the database. Moreover, one should distinguish a genuine leadership effect from a purely statistical effect due to fractional counting. Further analyses at the level of institutions and qualitative validation studies are recommended. ","Felix de Moya-Anegon, Vicente P. Guerrero-Bote, Carmen Lopez-Illescas   and Henk F. Moed",,,11,
Investigating Correlations of Automatically Extracted Multimodal   Features and Lecture Video Quality,"  Ranking and recommendation of multimedia content such as videos is usually realized with respect to the relevance to a user query. However, for lecture videos and MOOCs (Massive Open Online Courses) it is not only required to retrieve relevant videos, but particularly to find lecture videos of high quality that facilitate learning, for instance, independent of the video's or speaker's popularity. Thus, metadata about a lecture video's quality are crucial features for learning contexts, e.g., lecture video recommendation in search as learning scenarios. In this paper, we investigate whether automatically extracted features are correlated to quality aspects of a video. A set of scholarly videos from a Mass Open Online Course (MOOC) is analyzed regarding audio, linguistic, and visual features. Furthermore, a set of cross-modal features is proposed which are derived by combining transcripts, audio, video, and slide content. A user study is conducted to investigate the correlations between the automatically collected features and human ratings of quality aspects of a lecture video. Finally, the impact of our features on the knowledge gain of the participants is discussed. ","Jianwei Shi, Christian Otto, Anett Hoppe, Peter Holtz, Ralph Ewerth",,,11,
Stego Quality Enhancement by Message Size Reduction and Fibonacci   Bit-Plane Mapping,"  An efficient 2-step steganography technique is proposed to enhance stego image quality and secret message un-detectability. The first step is a preprocessing algorithm that reduces the size of secret images without losing information. This results in improved stego image quality compared to other existing image steganography methods. The proposed secret image size reduction (SISR) algorithm is an efficient spatial domain technique. The second step is an embedding mechanism that relies on Fibonacci representation of pixel intensities to minimize the effect of embedding on the stego image quality. The improvement is attained by using bit-plane(s) mapping instead of bit-plane(s) replacement for embedding. The proposed embedding mechanism outperforms the binary based LSB randomly embedding in two ways: reduced effect on stego quality and increased robustness against statistical steganalysers. Experimental results demonstrate the benefits of the proposed scheme in terms of: 1) SISR ratio (indirectly results in increased capacity); 2) quality of the stego; and 3) robustness against steganalysers such as RS, and WS. Furthermore, experimental results show that the proposed SISR algorithm can be extended to be applicable on DICOM standard medical images. Future security standardization research is proposed that would focus on evaluating the security, performance, and effectiveness of steganography algorithms. ","Alan A. Abdulla, Harin Sellahewa, and Sabah A. Jassim",,,11,
Steganography using a 3 player game,"  Image steganography aims to securely embed secret information into cover images. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD, are among the most secure and most used methods for image steganography. With the arrival of deep learning and more specifically the Generative Adversarial Networks (GAN), new techniques have appeared. Among these techniques, there is the 3 player game approaches, where three networks compete against each other.In this paper, we propose three different architectures based on the 3 player game. The first-architecture is proposed as a rigorous alternative to two recent publications. The second takes into account stego noise power. Finally, our third architecture enriches the second one with a better interaction between the embedding and extracting networks. Our method achieves better results compared to the existing works GSIVAT, HiDDeN, and paves the way for future research on this topic. ",Mehdi Yedroudj and Fr\'ed\'eric Comby and Marc Chaumont,,,11,
An Automated and Robust Image Watermarking Scheme Based on Deep Neural   Networks,"  Digital image watermarking is the process of embedding and extracting a watermark covertly on a cover-image. To dynamically adapt image watermarking algorithms, deep learning-based image watermarking schemes have attracted increased attention during recent years. However, existing deep learning-based watermarking methods neither fully apply the fitting ability to learn and automate the embedding and extracting algorithms, nor achieve the properties of robustness and blindness simultaneously. In this paper, a robust and blind image watermarking scheme based on deep learning neural networks is proposed. To minimize the requirement of domain knowledge, the fitting ability of deep neural networks is exploited to learn and generalize an automated image watermarking algorithm. A deep learning architecture is specially designed for image watermarking tasks, which will be trained in an unsupervised manner to avoid human intervention and annotation. To facilitate flexible applications, the robustness of the proposed scheme is achieved without requiring any prior knowledge or adversarial examples of possible attacks. A challenging case of watermark extraction from phone camera-captured images demonstrates the robustness and practicality of the proposal. The experiments, evaluation, and application cases confirm the superiority of the proposed scheme. ","Xin Zhong, Pei-Chi Huang, Spyridon Mastorakis, Frank Y. Shih",,,11,
SMP Challenge: An Overview of Social Media Prediction Challenge 2019,"  ""SMP Challenge"" aims to discover novel prediction tasks for numerous data on social multimedia and seek excellent research teams. Making predictions via social multimedia data (e.g. photos, videos or news) is not only helps us to make better strategic decisions for the future, but also explores advanced predictive learning and analytic methods on various problems and scenarios, such as multimedia recommendation, advertising system, fashion analysis etc.   In the SMP Challenge at ACM Multimedia 2019, we introduce a novel prediction task Temporal Popularity Prediction, which focuses on predicting future interaction or attractiveness (in terms of clicks, views or likes etc.) of new online posts in social media feeds before uploading. We also collected and released a large-scale SMPD benchmark with over 480K posts from 69K users. In this paper, we define the challenge problem, give an overview of the dataset, present statistics of rich information for data and annotation and design the accuracy and correlation evaluation metrics for temporal popularity prediction to the challenge. ","Bo Wu, Wen-Huang Cheng, Peiye Liu, Bei Liu, Zhaoyang Zeng, Jiebo Luo",,,11,
DWT-GBT-SVD-based Robust Speech Steganography,"  Steganography is a method that can improve network security and make communications safer. In this method, a secret message is hidden in content like audio signals that should not be perceptible by listening to the audio or seeing the signal waves. Also, it should be robust against different common attacks such as noise and compression. In this paper, we propose a new speech steganography method based on a combination of Discrete Wavelet Transform, Graph-based Transform, and Singular Value Decomposition (SVD). In this method, we first find voiced frames based on energy and zero-crossing counts of the frames and then embed a binary message into voiced frames. Experimental results on the NOIZEUS database show that the proposed method is imperceptible and also robust against Gaussian noise, re-sampling, re-quantization, high pass filter, and low pass filter. Also, it is robust against MP3 compression and scaling for watermarking applications. ","Noshin Amiri, Iman Naderi",,,11,
Impact of the Number of Votes on the Reliability and Validity of   Subjective Speech Quality Assessment in the Crowdsourcing Approach,"  The subjective quality of transmitted speech is traditionally assessed in a controlled laboratory environment according to ITU-T Rec. P.800. In turn, with crowdsourcing, crowdworkers participate in a subjective online experiment using their own listening device, and in their own working environment. Despite such less controllable conditions, the increased use of crowdsourcing micro-task platforms for quality assessment tasks has pushed a high demand for standardized methods, resulting in ITU-T Rec. P.808. This work investigates the impact of the number of judgments on the reliability and the validity of quality ratings collected through crowdsourcing-based speech quality assessments, as an input to ITU-T Rec. P.808 . Three crowdsourcing experiments on different platforms were conducted to evaluate the overall quality of three different speech datasets, using the Absolute Category Rating procedure. For each dataset, the Mean Opinion Scores (MOS) are calculated using differing numbers of crowdsourcing judgements. Then the results are compared to MOS values collected in a standard laboratory experiment, to assess the validity of crowdsourcing approach as a function of number of votes. In addition, the reliability of the average scores is analyzed by checking inter-rater reliability, gain in certainty, and the confidence of the MOS. The results provide a suggestion on the required number of votes per condition, and allow to model its impact on validity and reliability. ","Babak Naderi, Tobias Hossfeld, Matthias Hirth, Florian Metzger,   Sebastian M\""oller, Rafael Zequeira Jim\'enez",,,11,
Adaptive Online Multi-modal Hashing via Hadamard Matrix,"  Hashing plays an important role in information retrieval, due to its low storage and high speed of processing. Among the techniques available in the literature, multi-modal hashing, which can encode heterogeneous multi-modal features into compact hash codes, has received particular attention. Existing multi-modal hashing methods introduce hyperparameters to balance many regularization terms designed to make the models more robust in the hash learning process. However, it is time-consuming and labor-intensive to set them proper values. In this paper, we propose a simple, yet effective method that is inspired by the Hadamard matrix, which captures the multi-modal feature information in an adaptive manner and preserves the discriminative semantic information in the hash codes. Our framework is flexible and involves a very few hyper-parameters. Extensive experimental results show the method is effective and achieves superior performance compared to state-of-the-art algorithms. ","Jun Yu, Xiao-JunWu, Donglin Zhang, Josef Kittler",,,11,
QoE-Driven UAV-Enabled Pseudo-Analog Wireless Video Broadcast: A Joint   Optimization of Power and Trajectory,"  The explosive demands for high quality mobile video services have caused heavy overload to the existing cellular networks. Although the small cell has been proposed to alleviate such a problem, the network operators may not be interested in deploying numerous base stations (BSs) due to expensive infrastructure construction and maintenance. The unmanned aerial vehicles (UAVs) can provide the low-cost and quick deployment, which can support high-quality line-of-sight communications and have become promising mobile BSs. In this paper, we propose a quality-of-experience (QoE)-driven UAV-enabled pseudo-analog wireless video broadcast scheme, which provides mobile video broadcast services for ground users (GUs). Due to limited energy available in UAV, the aim of the proposed scheme is to maximize the minimum peak signal-to-noise ratio (PSNR) of GUs' video reconstruction quality by jointly optimizing the transmission power allocation strategy and the UAV trajectory. Firstly, the reconstructed video quality at GUs is defined under the constraints of the UAV's total energy and motion mechanism, and the proposed scheme is formulated as a complex non-convex optimization problem. Then, the optimization problem is simplified to obtain a tractable suboptimal solution with the help of the block coordinate descent model and the successive convex approximation model. Finally, the experimental results are presented to show the effectiveness of the proposed scheme. Specifically, the proposed scheme can achieve over 1.6dB PSNR gains in terms of GUs' minimum PSNR, compared with the state-of-the-art schemes, e.g., DVB, SoftCast, and SharpCast. ","Xiao-Wei Tang, Xin-Lin Huang, Fei Hu",,,11,
Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with   Visual Computing for Improved Music Video Analysis,"  This thesis combines audio-analysis with computer vision to approach Music Information Retrieval (MIR) tasks from a multi-modal perspective. This thesis focuses on the information provided by the visual layer of music videos and how it can be harnessed to augment and improve tasks of the MIR research domain. The main hypothesis of this work is based on the observation that certain expressive categories such as genre or theme can be recognized on the basis of the visual content alone, without the sound being heard. This leads to the hypothesis that there exists a visual language that is used to express mood or genre. In a further consequence it can be concluded that this visual information is music related and thus should be beneficial for the corresponding MIR tasks such as music genre classification or mood recognition. A series of comprehensive experiments and evaluations are conducted which are focused on the extraction of visual information and its application in different MIR tasks. A custom dataset is created, suitable to develop and test visual features which are able to represent music related information. Evaluations range from low-level visual features to high-level concepts retrieved by means of Deep Convolutional Neural Networks. Additionally, new visual features are introduced capturing rhythmic visual patterns. In all of these experiments the audio-based results serve as benchmark for the visual and audio-visual approaches. The experiments are conducted for three MIR tasks Artist Identification, Music Genre Classification and Cross-Genre Classification. Experiments show that an audio-visual approach harnessing high-level semantic information gained from visual concept detection, outperforms audio-only genre-classification accuracy by 16.43%. ",Alexander Schindler,,,11,
Accessibility in 360-degree video players,"  Any media experience must be fully inclusive and accessible to all users regardless of their ability. With the current trend towards immersive experiences, such as Virtual Reality (VR) and 360-degree video, it becomes key that these environments are adapted to be fully accessible. However, until recently the focus has been mostly on adapting the existing techniques to fit immersive displays, rather than considering new approaches for accessibility designed specifically for these increasingly relevant media experiences. This paper surveys a wide range of 360-degree video players and examines the features they include for dealing with accessibility, such as Subtitles, Audio Description, Sign Language, User Interfaces, and other interaction features, like voice control and support for multi-screen scenarios. These features have been chosen based on guidelines from standardization contributions, like in the World Wide Web Consortium (W3C) and the International Communication Union (ITU), and from research contributions for making 360-degree video consumption experiences accessible. The in-depth analysis has been part of a research effort towards the development of a fully inclusive and accessible 360-degree video player. The paper concludes by discussing how the newly developed player has gone above and beyond the existing solutions and guidelines, by providing accessibility features that meet the expectations for a widely used immersive medium, like 360-degree video. ",Chris Hughes and Mario Montagud,,,11,
Performance of AV1 Real-Time Mode,"  With COVID-19, the interest for digital interactions has raised, putting in turn real-time (or low-latency) codecs into a new light. Most of the codec research has been traditionally focusing on coding efficiency, while very little literature exist on real-time codecs. It is shown how the speed at which content is made available impacts both latency and throughput. The authors introduce a new test set up, integrating a paced reader, which allows to run codec in the same condition as real-time media capture. Quality measurements using VMAF, as well as multiple speed measurements are made on encoding of HD and full HD video sequences, both at 25 fps and 50 fps to compare the respective performances of several implementations of the H.264, H.265, VP8, VP9 and AV1 codecs. ",Ludovic Roux and Alexandre Gouaillard,,,11,
Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality,"  Volumetric video allows viewers to experience highly-realistic 3D content with six degrees of freedom in mixed reality (MR) environments. Rendering complex volumetric videos can require a prohibitively high amount of computational power for mobile devices. A promising technique to reduce the computational burden on mobile devices is to perform the rendering at a cloud server. However, cloud-based rendering systems suffer from an increased interaction (motion-to-photon) latency that may cause registration errors in MR environments. One way of reducing the effective latency is to predict the viewer's head pose and render the corresponding view from the volumetric video in advance. In this paper, we design a Kalman filter for head motion prediction in our cloud-based volumetric video streaming system. We analyze the performance of our approach using recorded head motion traces and compare its performance to an autoregression model for different prediction intervals (look-ahead times). Our results show that the Kalman filter can predict head orientations 0.5 degrees more accurately than the autoregression model for a look-ahead time of 60 ms. ","Serhan G\""ul, Sebastian Bosse, Dimitri Podborski, Thomas Schierl,   Cornelius Hellge",,,11,
Improved Handling of Repeats and Jumps in Audio-Sheet Image   Synchronization,"  This paper studies the problem of automatically generating piano score following videos given an audio recording and raw sheet music images. Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP. We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats. We find that a significant bottleneck in system performance is handling jumps and repeats correctly. In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori. We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known. It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level. By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is. Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps. ",Mengyi Shan and TJ Tsai,,,11,
ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation   Networks,"  Loss function is crucial for model training and feature representation learning, conventional models usually regard facial attractiveness recognition task as a regression problem, and adopt MSE loss or Huber variant loss as supervision to train a deep convolutional neural network (CNN) to predict facial attractiveness score. Little work has been done to systematically compare the performance of diverse loss functions. In this paper, we firstly systematically analyze model performance under diverse loss functions. Then a novel loss function named ComboLoss is proposed to guide the SEResNeXt50 network. The proposed method achieves state-of-the-art performance on SCUT-FBP, HotOrNot and SCUT-FBP5500 datasets with an improvement of 1.13%, 2.1% and 0.57% compared with prior arts, respectively. Code and models are available at https://github.com/lucasxlu/ComboLoss.git. ",Lu Xu and Jinhai Xiang,,,11,
A Universal Framework to Construct a Huffman-Code-Mapping-based   Reversible Data Hiding Scheme for JPEG Images,"  Huffman code mapping (HCM) is a recent technique for reversible data hiding (RDH) in JPEG images. The existing HCM-based RDH schemes cause neither file-size increment nor visual distortion for the marked JPEG image, which is the superiority compared to the RDH schemes that use other techniques, such as histogram shifting (HS). However, the embedding capacity achieved by the HCM-based RDH schemes is much lower than the HS-based RDH schemes. Moreover, the existing HCM-based schemes are only applicable to the JPEG images coded with the default Huffman table. In this paper, we propose a universal framework to design the HCM-based RDH scheme. Under this framework, the key issue of HCM-based schemes, i.e., construct the optimal code mapping relationship, is converted to solve a combinatorial optimization problem. The high embedding capacity can be achieved with a slight increase in the file-size of the marked JPEG image. In addition, the problem of applicability is also solved by customizing the Huffman table. As a realization, we construct a new HCM-based scheme by employing the genetic algorithm to search the nearly optimal solution. Experiments show that the performance on the file-size preservation, visual quality, and computational complexity is superior to recent HS-based RDH schemes under the identical payload. ","Zhaoxia Yin, Yang Du and Yuan Ji",,,11,
A General Approach for Using Deep Neural Network for Digital   Watermarking,"  Technologies of the Internet of Things (IoT) facilitate digital contents such as images being acquired in a massive way. However, consideration from the privacy or legislation perspective still demands the need for intellectual content protection. In this paper, we propose a general deep neural network (DNN) based watermarking method to fulfill this goal. Instead of training a neural network for protecting a specific image, we train on an image set and use the trained model to protect a distinct test image set in a bulk manner. Respective evaluations both from the subjective and objective aspects confirm the supremacy and practicability of our proposed method. To demonstrate the robustness of this general neural watermarking mechanism, commonly used manipulations are applied to the watermarked image to examine the corresponding extracted watermark, which still retains sufficient recognizable traits. To the best of our knowledge, we are the first to propose a general way to perform watermarking using DNN. Considering its performance and economy, it is concluded that subsequent studies that generalize our work on utilizing DNN for intellectual content protection is a promising research trend. ","Yurui Ming, Weiping Ding, Zehong Cao, Chin-Teng Lin",,,11,
Multi-view data capture using edge-synchronised mobiles,"  Multi-view data capture permits free-viewpoint video (FVV) content creation. To this end, several users must capture video streams, calibrated in both time and pose, framing the same object/scene, from different viewpoints. New-generation network architectures (e.g. 5G) promise lower latency and larger bandwidth connections supported by powerful edge computing, properties that seem ideal for reliable FVV capture. We have explored this possibility, aiming to remove the need for bespoke synchronisation hardware when capturing a scene from multiple viewpoints, making it possible through off-the-shelf mobiles. We propose a novel and scalable data capture architecture that exploits edge resources to synchronise and harvest frame captures. We have designed an edge computing unit that supervises the relaying of timing triggers to and from multiple mobiles, in addition to synchronising frame harvesting. We empirically show the benefits of our edge computing unit by analysing latencies and show the quality of 3D reconstruction outputs against an alternative and popular centralised solution based on Unity3D. ","Matteo Bortolon, Paul Chippendale, Stefano Messelodi and Fabio Poiesi",,,11,
Hide Secret Information in Blocks: Minimum Distortion Embedding,"  In this paper, a new steganographic method is presented that provides minimum distortion in the stego image. The proposed encoding algorithm focuses on DCT rounding error and optimizes that in a way to reduce distortion in the stego image, and the proposed algorithm produces less distortion than existing methods (e.g., F5 algorithm). The proposed method is based on DCT rounding error which helps to lower distortion and higher embedding capacity. ",Md Amiruzzaman and Rizal Mohd Nor,,,11,
JQF: Optimal JPEG Quantization Table Fusion by Simulated Annealing on   Texture Images and Predicting Textures,"  JPEG has been a widely used lossy image compression codec for nearly three decades. The JPEG standard allows to use customized quantization table; however, it's still a challenging problem to find an optimal quantization table within acceptable computational cost. This work tries to solve the dilemma of balancing between computational cost and image specific optimality by introducing a new concept of texture mosaic images. Instead of optimizing a single image or a collection of representative images, the simulated annealing technique is applied to texture mosaic images to search for an optimal quantization table for each texture category. We use pre-trained VGG-16 CNN model to learn those texture features and predict the new image's texture distribution, then fuse optimal texture tables to come out with an image specific optimal quantization table. On the Kodak dataset with the quality setting $Q=95$, our experiment shows a size reduction of 23.5% over the JPEG standard table with a slightly 0.35% FSIM decrease, which is visually unperceivable. The proposed JQF method achieves per image optimality for JPEG encoding with less than one second additional timing cost. The online demo is available at https://matthorn.s3.amazonaws.com/JQF/qtbl_vis.html ",Chen-Hsiu Huang and Ja-Ling Wu,,,11,
Dagger: Towards Efficient RPCs in Cloud Microservices with Near-Memory   Reconfigurable NICs,"  Cloud applications are increasingly relying on hundreds of loosely-coupled microservices to complete user requests that meet an applications end-to-end QoS requirements. Communication time between services accounts for a large fraction of the end-to-end latency and can introduce performance unpredictability and QoS violations. This work presents our early work on Dagger, a hardware acceleration platform for networking, designed specifically with the unique qualities of microservices in mind. The Dagger architecture relies on an FPGA-based NIC, closely coupled with the processor over a configurable memory interconnect, designed to offload and accelerate RPC stacks. Unlike the traditional cloud systems that use PCIe links as the NIC I/O interface, we leverage memory-interconnected FPGAs as networking devices to provide the efficiency, transparency, and programmability needed for fine-grained microservices. We show that this considerably improves CPU utilization and performance for cloud RPCs. ","Nikita Lazarev, Neil Adit, Shaojie Xiang, Zhiru Zhang, and Christina   Delimitrou",,,11,
RapidLayout: Fast Hard Block Placement of FPGA-optimized Systolic Arrays   using Evolutionary Algorithms,"  Evolutionary algorithms can outperform conventional placement algorithms such as simulated annealing, analytical placement as well as manual placement on metrics such as runtime, wirelength, pipelining cost, and clock frequency when mapping FPGA hard block intensive designs such as systolic arrays on Xilinx UltraScale+ FPGAs. For certain hard-block intensive, systolic array accelerator designs, the commercial-grade Xilinx Vivado CAD tool is unable to provide a legal routing solution without tedious manual placement constraints. Instead, we formulate an automatic FPGA placement algorithm for these hard blocks as a multi-objective optimization problem that targets wirelength squared and maximum bounding box size metrics. We build an end-to-end placement and routing flow called RapidLayout using the Xilinx RapidWright framework. RapidLayout runs 5-6$\times$ faster than Vivado with manual constraints and eliminates the weeks-long effort to generate placement constraints manually for the hard blocks. We also perform automated post-placement pipelining of the long wires inside each convolution block to target 650MHz URAM-limited operation. RapidLayout outperforms (1) the simulated annealer in VPR by 33% in runtime, 1.9-2.4$\times$ in wirelength, and 3-4$\times$ in bounding box size, while also (2) beating the analytical placer UTPlaceF by 9.3$\times$ in runtime, 1.8-2.2$\times$ in wirelength, and 2-2.7$\times$ in bounding box size. We employ transfer learning from a base FPGA device to speed-up placement optimization for similar FPGA devices in the UltraScale+ family by 11-14$\times$ than learning the placements from scratch. ","Niansong Zhang, Xiang Chen, Nachiket Kapre",,,11,
Mitigating Write Disturbance Errors of Phase-Change Memory as In-Module   Approach,"  With the growing demand for technology scaling and storage capacity in server systems to support high-performance computing, phase-change memory (PCM) has garnered attention as the next-generation non-volatile memory to satisfy these requirements. However, write disturbance error (WDE) appears as a serious reliability problem preventing PCM from general commercialization. WDE occurs on the neighboring cells of a written cell due to heat dissipation. Previous studies for the prevention of WDEs are based on the write cache or verify-n-correction while they often suffer from significant area overhead and performance degradation, making it unsuitable for high-performance computing. Therefore, an on-demand correction is required to minimize the performance overhead. In this paper, an in-module disturbance barrier (IMDB) mitigating WDEs is proposed. IMDB includes two sets of SRAMs into two levels and evicts entries with a policy that leverages the characteristics of WDE. In this work, the comparator dedicated to the replacement policy requires significant hardware resources and latency. Thus, an approximate comparator is designed to reduce the area and latency considerably. Furthermore, the exploration of architecture parameters is conducted to obtain cost-effective design. The proposed work significantly reduces WDEs without a noticeable speed degradation and additional energy consumption compared to previous methods. ","Hyokeun Lee, Seungyong Lee, Byeongki Song, Moonsoo Kim, Seokbo Shim,   Hyun Kim, Hyuk-Jae Lee",,,11,
Cycle-Accurate Evaluation of Software-Hardware Co-Design of Decimal   Computation in RISC-V Ecosystem,"  Software-hardware co-design solutions for decimal computation can provide several Pareto points to development of embedded systems in terms of hardware cost and performance. This paper demonstrates how to accurately evaluate such co-design solutions using RISC-V ecosystem. In a software-hardware co-design solution, a part of solution requires dedicated hardware. In our evaluation framework, we develop new decimal oriented instructions supported by an accelerator. The framework can realize cycle-accurate analysis for performance as well as hardware overhead for co-design solutions for decimal computation. The obtained performance result is compared with an estimation with dummy functions. ",Riaz-ul-haque Mian and Michihiro Shintani and Michiko Inoue,,,11,
"Energy Efficient Computing Systems: Architectures, Abstractions and   Modeling to Techniques and Standards","  Computing systems have undergone several inflexion points - while Moore's law guided the semiconductor industry to cram more and more transistors and logic into the same volume, the limits of instruction-level parallelism (ILP) and the end of Dennard's scaling drove the industry towards multi-core chips. We have now entered the era of domain-specific architectures for new workloads like AI and ML. These trends continue, arguably with other limits, along with challenges imposed by tighter integration, extreme form factors and diverse workloads, making systems more complex from an energy efficiency perspective. Many research surveys have covered different aspects of techniques in hardware and microarchitecture across devices, servers, HPC, data center systems along with software, algorithms, frameworks for energy efficiency and thermal management. Somewhat in parallel, the semiconductor industry has developed techniques and standards around specification, modeling and verification of complex chips; these areas have not been addressed in detail by previous research surveys. This survey aims to bring these domains together and is composed of a systematic categorization of key aspects of building energy efficient systems - (a) specification - the ability to precisely specify the power intent or properties at different layers (b) modeling and simulation of the entire system or subsystem (hardware or software or both) so as to be able to perform what-if analysis, (c) techniques used for implementing energy efficiency at different levels of the stack, (d) verification techniques used to provide guarantees that the functionality of complex designs are preserved, and (e) energy efficiency standards and consortiums that aim to standardize different aspects of energy efficiency, including cross-layer optimizations. ",Rajeev Muralidhar and Renata Borovica-Gajic and Rajkumar Buyya,,,11,
Runtime Task Scheduling using Imitation Learning for Heterogeneous   Many-Core Systems,"  Domain-specific systems-on-chip, a class of heterogeneous many-core systems, are recognized as a key approach to narrow down the performance and energy-efficiency gap between custom hardware accelerators and programmable processors. Reaching the full potential of these architectures depends critically on optimally scheduling the applications to available resources at runtime. Existing optimization-based techniques cannot achieve this objective at runtime due to the combinatorial nature of the task scheduling problem. As the main theoretical contribution, this paper poses scheduling as a classification problem and proposes a hierarchical imitation learning (IL)-based scheduler that learns from an Oracle to maximize the performance of multiple domain-specific applications. Extensive evaluations with six streaming applications from wireless communications and radar domains show that the proposed IL-based scheduler approximates an offline Oracle policy with more than 99% accuracy for performance- and energy-based optimization objectives. Furthermore, it achieves almost identical performance to the Oracle with a low runtime overhead and successfully adapts to new applications, many-core system configurations, and runtime variations in application characteristics. ","Anish Krishnakumar, Samet E. Arda, A. Alper Goksoy, Sumit K. Mandal,   Umit Y. Ogras, Anderson L. Sartor, Radu Marculescu",,,11,
Synchronizer-Free Digital Link Controller,"  This work presents a producer-consumer link between two independent clock domains. The link allows for metastability-free, low-latency, high-throughput communication by slight adjustments to the clock frequencies of the producer and consumer domains steered by a controller circuit. Any such controller cannot deterministically avoid, detect, nor resolve metastability. Typically, this is addressed by synchronizers, incurring a larger dead time in the control loop. We follow the approach of Friedrichs et al. (TC 2018) who proposed metastability-containing circuits. The result is a simple control circuit that may become metastable, yet deterministically avoids buffer underrun or overflow. More specifically, the controller output may become metastable, but this may only affect oscillator speeds within specific bounds. In contrast, communication is guaranteed to remain metastability-free. We formally prove correctness of the producer-consumer link and a possible implementation that has only small overhead. With SPICE simulations of the proposed implementation we further substantiate our claims. The simulation uses 65nm process running at roughly 2GHz. ","Johannes Bund and Matthias F\""ugger and Christoph Lenzen and Moti   Medina",,,11,
A Ring Router Microarchitecture for NoCs,"  Network-on-Chip (NoC) has become a popular choice for connecting a large number of processing cores in chip multiprocessor design. In a conventional NoC design, most of the area in the router is occupied by the buffers and the crossbar switch. These two components also consume the majority of the router's power. Much of the research in NoC has been based on the conventional router microarchitecture. We propose a novel router microarchitecture that treats the router itself as a small network of the ring topology. It eliminates the large crossbar switch in the conventional design. In addition, network latency is much reduced. Simulation and circuit synthesis show that the proposed microarchitecture can reduce the latency, area and power by 53%, 34% and 27%, respectively, compared to the conventional design. ",Wo-Tak Wu,,,11,
ZnG: Architecting GPU Multi-Processors with New Flash for Scalable Data   Analysis,"  We propose ZnG, a new GPU-SSD integrated architecture, which can maximize the memory capacity in a GPU and address performance penalties imposed by an SSD. Specifically, ZnG replaces all GPU internal DRAMs with an ultra-low-latency SSD to maximize the GPU memory capacity. ZnG further removes performance bottleneck of the SSD by replacing its flash channels with a high-throughput flash network and integrating SSD firmware in the GPU's MMU to reap the benefits of hardware accelerations. Although flash arrays within the SSD can deliver high accumulated bandwidth, only a small fraction of such bandwidth can be utilized by GPU's memory requests due to mismatches of their access granularity. To address this, ZnG employs a large L2 cache and flash registers to buffer the memory requests. Our evaluation results indicate that ZnG can achieve 7.5x higher performance than prior work. ",Jie Zhang and Myoungsoo Jung,,,11,
TrappeD: DRAM Trojan Designs for Information Leakage and Fault Injection   Attacks,"  In this paper, we investigate the advanced circuit features such as wordline- (WL) underdrive (prevents retention failure) and overdrive (assists write) employed in the peripherals of Dynamic RAM (DRAM) memories from a security perspective. In an ideal environment, these features ensure fast and reliable read and write operations. However, an adversary can re-purpose them by inserting Trojans to deliver malicious payloads such as fault injections, Denial-of-Service (DoS), and information leakage attacks when activated by the adversary. Simulation results indicate that wordline voltage can be increased to cause retention failure and thereby launch a DoS attack in DRAM memory. Furthermore, two wordlines or bitlines can be shorted to leak information or inject faults by exploiting the DRAM's refresh operation. We demonstrate an information leakage system exploit by implementing TrappeD on RocketChip SoC. ","Karthikeyan Nagarajan, Asmit De, Mohammad Nasim Imtiaz Khan, Swaroop   Ghosh",,,11,
Addressing Variability in Reuse Prediction for Last-Level Caches,"  Last-Level Cache (LLC) represents the bulk of a modern CPU processor's transistor budget and is essential for application performance as LLC enables fast access to data in contrast to much slower main memory. However, applications with large working set size often exhibit streaming and/or thrashing access patterns at LLC. As a result, a large fraction of the LLC capacity is occupied by dead blocks that will not be referenced again, leading to inefficient utilization of the LLC capacity. To improve cache efficiency, the state-of-the-art cache management techniques employ prediction mechanisms that learn from the past access patterns with an aim to accurately identify as many dead blocks as possible. Once identified, dead blocks are evicted from LLC to make space for potentially high reuse cache blocks.   In this thesis, we identify variability in the reuse behavior of cache blocks as the key limiting factor in maximizing cache efficiency for state-of-the-art predictive techniques. Variability in reuse prediction is inevitable due to numerous factors that are outside the control of LLC. The sources of variability include control-flow variation, speculative execution and contention from cores sharing the cache, among others. Variability in reuse prediction challenges existing techniques in reliably identifying the end of a block's useful lifetime, thus causing lower prediction accuracy, coverage, or both. To address this challenge, this thesis aims to design robust cache management mechanisms and policies for LLC in the face of variability in reuse prediction to minimize cache misses, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, to improve cache efficiency by addressing variability in reuse prediction. ",Priyank Faldu,,,11,
FORECASTER: A Continual Lifelong Learning Approach to Improve Hardware   Efficiency,"  Computer applications are continuously evolving. However, significant knowledge can be harvested from older applications or versions and applied in the context of newer applications or versions. Such a vision can be realized with Continual Lifelong Learning. Therefore, we propose to employ continual lifelong learning to dynamically tune hardware configurations based on application behavior. The goal of such tuning is to maximize hardware efficiency (i.e., maximize an application performance while minimizing the hardware energy consumption). Our proposed approach, FORECASTER, uses deep reinforcement learning to continually learn during the execution of an application as well as propagate and utilize the accumulated knowledge during subsequent executions of the same or new application. We propose a novel hardware and ISA support to implement deep reinforcement learning. We implement FORECASTER and compare its performance against prior learning-based hardware reconfiguration approaches. Our results show that FORECASTER can save an average 16% of system power over the baseline setup with full usage of hardware while sacrificing an average of 4.7% of execution time. ","Phat Nguyen, Arnav Kansal, Abhishek Taur, Mohamed Zahran, Abdullah   Muzahid",,,11,
MGPU-TSM: A Multi-GPU System with Truly Shared Memory,"  The sizes of GPU applications are rapidly growing. They are exhausting the compute and memory resources of a single GPU, and are demanding the move to multiple GPUs. However, the performance of these applications scales sub-linearly with GPU count because of the overhead of data movement across multiple GPUs. Moreover, a lack of hardware support for coherency exacerbates the problem because a programmer must either replicate the data across GPUs or fetch the remote data using high-overhead off-chip links. To address these problems, we propose a multi-GPU system with truly shared memory (MGPU-TSM), where the main memory is physically shared across all the GPUs. We eliminate remote accesses and avoid data replication using an MGPU-TSM system, which simplifies the memory hierarchy. Our preliminary analysis shows that MGPU-TSM with 4 GPUs performs, on average, 3.9x? better than the current best performing multi-GPU configuration for standard application benchmarks. ","Saiful A. Mojumder, Yifan Sun, Leila Delshadtehrani, Yenai Ma,   Trinayan Baruah, Jos\'e L. Abell\'an, John Kim, David Kaeli and Ajay Joshi",,,11,
Hardware Acceleration of Sparse and Irregular Tensor Computations of ML   Models: A Survey and Insights,"  Machine learning (ML) models are widely used in many domains including media processing and generation, computer vision, medical diagnosis, embedded systems, high-performance and scientific computing, and recommendation systems. For efficiently processing these computational- and memory-intensive applications, tensors of these over-parameterized models are compressed by leveraging sparsity, size reduction, and quantization of tensors. Unstructured sparsity and tensors with varying dimensions yield irregular-shaped computation, communication, and memory access patterns; processing them on hardware accelerators in a conventional manner does not inherently leverage acceleration opportunities. This paper provides a comprehensive survey on how to efficiently execute sparse and irregular tensor computations of ML models on hardware accelerators. In particular, it discusses additional enhancement modules in architecture design and software support; categorizes different hardware designs and acceleration techniques and analyzes them in terms of hardware and execution costs; highlights further opportunities in terms of hardware/software/algorithm co-design optimizations and joint optimizations among described hardware and software enhancement modules. The takeaways from this paper include: understanding the key challenges in accelerating sparse, irregular-shaped, and quantized tensors; understanding enhancements in acceleration systems for supporting their efficient computations; analyzing trade-offs in opting for a specific type of design enhancement; understanding how to map and compile models with sparse tensors on the accelerators; understanding recent design trends for efficient accelerations and further opportunities. ","Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral   Shrivastava, Baoxin Li",,,11,
DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of   FPGA-based DNN Accelerator,"  Existing FPGA-based DNN accelerators typically fall into two design paradigms. Either they adopt a generic reusable architecture to support different DNN networks but leave some performance and efficiency on the table because of the sacrifice of design specificity. Or they apply a layer-wise tailor-made architecture to optimize layer-specific demands for computation and resources but loose the scalability of adaptation to a wide range of DNN networks. To overcome these drawbacks, this paper proposes a novel FPGA-based DNN accelerator design paradigm and its automation tool, called DNNExplorer, to enable fast exploration of various accelerator designs under the proposed paradigm and deliver optimized accelerator architectures for existing and emerging DNN networks. Three key techniques are essential for DNNExplorer's improved performance, better specificity, and scalability, including (1) a unique accelerator design paradigm with both high-dimensional design space support and fine-grained adjustability, (2) a dynamic design space to accommodate different combinations of DNN workloads and targeted FPGAs, and (3) a design space exploration (DSE) engine to generate optimized accelerator architectures following the proposed paradigm by simultaneously considering both FPGAs' computation and memory resources and DNN networks' layer-wise characteristics and overall complexity. Experimental results show that, for the same FPGAs, accelerators generated by DNNExplorer can deliver up to 4.2x higher performances (GOP/s) than the state-of-the-art layer-wise pipelined solutions generated by DNNBuilder for VGG-like DNN with 38 CONV layers. Compared to accelerators with generic reusable computation units, DNNExplorer achieves up to 2.0x and 4.4x DSP efficiency improvement than a recently published accelerator design from academia (HybridDNN) and a commercial DNN accelerator IP (Xilinx DPU), respectively. ","Xiaofan Zhang, Hanchen Ye, Junsong Wang, Yonghua Lin, Jinjun Xiong,   Wen-mei Hwu, Deming Chen",,,11,
HOBFLOPS CNNs: Hardware Optimized Bitsliced Floating-Point Operations   Convolutional Neural Networks,"  Convolutional neural network (CNN) inference is commonly performed with 8-bit integer values. However, higher precision floating-point inference is required. Existing processors support 16- or 32 bit FP but do not typically support custom precision FP. We propose hardware optimized bit-sliced floating-point operators (HOBFLOPS), a method of generating efficient custom-precision emulated bitsliced software FP arithmetic, for CNNs. We compare HOBFLOPS8-HOBFLOPS16 performance against SoftFP16 on Arm Neon and Intel architectures. HOBFLOPS allows researchers to prototype arbitrary-levels of FP arithmetic precision for CNN accelerators. Furthermore, HOBFLOPS fast custom-precision FP CNNs in software may be valuable in cases where memory bandwidth is limited. ","James Garland, David Gregg",,,11,
The MosaicSim Simulator (Full Technical Report),"  As Moore's Law has slowed and Dennard Scaling has ended, architects are increasingly turning to heterogeneous parallelism and domain-specific hardware-software co-designs. These trends present new challenges for simulation-based performance assessments that are central to early-stage architectural exploration. Simulators must be lightweight to support rich heterogeneous combinations of general purpose cores and specialized processing units. They must also support agile exploration of hardware-software co-design, i.e. changes in the programming model, compiler, ISA, and specialized hardware.   To meet these challenges, we introduce MosaicSim, a lightweight, modular simulator for heterogeneous systems, offering accuracy and agility designed specifically for hardware-software co-design explorations. By integrating the LLVM toolchain, MosaicSim enables efficient modeling of instruction dependencies and flexible additions across the stack. Its modularity also allows the composition and integration of different hardware components. We first demonstrate that MosaicSim captures architectural bottlenecks in applications, and accurately models both scaling trends in a multicore setting and accelerator behavior. We then present two case-studies where MosaicSim enables straightforward design space explorations for emerging systems, i.e. data science application acceleration and heterogeneous parallel architectures. ","Opeoluwa Matthews, Aninda Manocha, Davide Giri, Marcelo Orenes-Vera,   Esin Tureci, Tyler Sorensen, Tae Jun Ham, Juan L. Arag\'on, Luca P. Carloni,   Margaret Martonosi",,,11,
WoLFRaM: Enhancing Wear-Leveling and Fault Tolerance in Resistive   Memories using Programmable Address Decoders,"  Resistive memories have limited lifetime caused by limited write endurance and highly non-uniform write access patterns. Two main techniques to mitigate endurance-related memory failures are 1) wear-leveling, to evenly distribute the writes across the entire memory, and 2) fault tolerance, to correct memory cell failures. However, one of the main open challenges in extending the lifetime of existing resistive memories is to make both techniques work together seamlessly and efficiently. To address this challenge, we propose WoLFRaM, a new mechanism that combines both wear-leveling and fault tolerance techniques at low cost by using a programmable resistive address decoder (PRAD). The key idea of WoLFRaM is to use PRAD for implementing 1) a new efficient wear-leveling mechanism that remaps write accesses to random physical locations on the fly, and 2) a new efficient fault tolerance mechanism that recovers from faults by remapping failed memory blocks to available physical locations. Our evaluations show that, for a Phase Change Memory (PCM) based system with cell endurance of 108 writes, WoLFRaM increases the memory lifetime by 68% compared to a baseline that implements the best state-of-the-art wear-leveling and fault correction mechanisms. WoLFRaM's average / worst-case performance and energy overheads are 0.51% / 3.8% and 0.47% / 2.1% respectively. ","Leonid Yavits, Lois Orosa, Suyash Mahar, Jo\~ao Dinis Ferreira, Mattan   Erez, Ran Ginosar, Onur Mutlu",,,11,
A Survey of Resource Management for Processing-in-Memory and Near-Memory   Processing Architectures,"  Due to amount of data involved in emerging deep learning and big data applications, operations related to data movement have quickly become the bottleneck. Data-centric computing (DCC), as enabled by processing-in-memory (PIM) and near-memory processing (NMP) paradigms, aims to accelerate these types of applications by moving the computation closer to the data. Over the past few years, researchers have proposed various memory architectures that enable DCC systems, such as logic layers in 3D stacked memories or charge sharing based bitwise operations in DRAM. However, application-specific memory access patterns, power and thermal concerns, memory technology limitations, and inconsistent performance gains complicate the offloading of computation in DCC systems. Therefore, designing intelligent resource management techniques for computation offloading is vital for leveraging the potential offered by this new paradigm. In this article, we survey the major trends in managing PIM and NMP-based DCC systems and provide a review of the landscape of resource management techniques employed by system designers for such systems. Additionally, we discuss the future challenges and opportunities in DCC management. ","Kamil Khan, Sudeep Pasricha, Ryan Gary Kim",,,11,
"von Neumann's missing ""Second Draft"": what it should contain","  Computing science is based on a computing paradigm that is not valid anymore for today's technological conditions. The reason is that the transmission time even inside the processor chip, but especially between the components of the system, is not negligible anymore. The paper introduces a quantitative measure for dispersion, which is vital for both computing performance and energy consumption, and demonstrates how its value increased with the changing technology. The temporal behavior (including the dispersion of the commonly used synchronization clock time) of computing components has a critical impact on the system's performance at all levels, as demonstrated from gate-level operation to supercomputing. The same effect limits the utility of the researched new materials/effects if the related transfer time cannot be proportionally mitigated. von Neumann's model is perfect, but now it is used outside of its range of validity. The correct procedure to consider the transfer time for the present technological background is also derived. ",J\'anos V\'egh,,,11,
GENIEx: A Generalized Approach to Emulating Non-Ideality in Memristive   Xbars using Neural Networks,"  The analog nature of computing in Memristive crossbars poses significant issues due to various non-idealities such as: parasitic resistances, non-linear I-V characteristics of the device etc. The non-idealities can have a detrimental impact on the functionality i.e. computational accuracy of crossbars. Past works have explored modeling the non-idealities using analytical techniques. However, several non-idealities have data dependent behavior. This can not be captured using analytical (non data-dependent) models thereby, limiting their suitability in predicting application accuracy.   To address this, we propose a Generalized Approach to Emulating Non-Ideality in Memristive Crossbars using Neural Networks (GENIEx), which accurately captures the data-dependent nature of non-idealities. We perform extensive HSPICE simulations of crossbars with different voltage and conductance combinations. Following that, we train a neural network to learn the transfer characteristics of the non-ideal crossbar. Next, we build a functional simulator which includes key architectural facets such as \textit{tiling}, and \textit{bit-slicing} to analyze the impact of non-idealities on the classification accuracy of large-scale neural networks. We show that GENIEx achieves \textit{low} root mean square errors (RMSE) of $0.25$ and $0.7$ for low and high voltages, respectively, compared to HSPICE. Additionally, the GENIEx errors are $7\times$ and $12.8\times$ better than an analytical model which can only capture the linear non-idealities. Further, using the functional simulator and GENIEx, we demonstrate that an analytical model can overestimate the degradation in classification accuracy by $\ge 10\%$ on CIFAR-100 and $3.7\%$ on ImageNet datasets compared to GENIEx. ","Indranil Chakraborty, Mustafa Fayez Ali, Dong Eun Kim, Aayush Ankit   and Kaushik Roy",,,11,
Control of criticality and computation in spiking neuromorphic networks   with plasticity,"  The critical state is assumed to be optimal for any computation in recurrent neural networks, because criticality maximizes a number of abstract computational properties. We challenge this assumption by evaluating the performance of a spiking recurrent neural network on a set of tasks of varying complexity at - and away from critical network dynamics. To that end, we developed a spiking network with synaptic plasticity on a neuromorphic chip. We show that the distance to criticality can be easily adapted by changing the input strength, and then demonstrate a clear relation between criticality, task-performance and information-theoretic fingerprint. Whereas the information-theoretic measures all show that network capacity is maximal at criticality, this is not the case for performance on specific tasks: Only the complex, memory-intensive tasks profit from criticality, whereas the simple tasks suffer from it. Thereby, we challenge the general assumption that criticality would be beneficial for any task, and provide instead an understanding of how the collective network state should be tuned to task requirement to achieve optimal performance. ","Benjamin Cramer, David St\""ockel, Markus Kreft, Michael Wibral,   Johannes Schemmel, Karlheinz Meier, Viola Priesemann",,,11,
Design and Characterization of Superconducting Nanowire-Based Processors   for Acceleration of Deep Neural Network Training,"  Training of deep neural networks (DNNs) is a computationally intensive task and requires massive volumes of data transfer. Performing these operations with the conventional von Neumann architectures creates unmanageable time and power costs. Recent studies have shown that mixed-signal designs involving crossbar architectures are capable of achieving acceleration factors as high as 30,000x over the state of the art digital processors. These approaches involve utilization of non-volatile memory (NVM) elements as local processors. However, no technology has been developed to-date that can satisfy the strict device requirements for the unit cell. This paper presents the superconducting nanowire-based processing element as a cross-point device. The unit cell has many programmable non-volatile states that can be used to perform analog multiplication. Importantly, these states are intrinsically discrete due to quantization of flux, which provides symmetric switching characteristics. Operation of these devices in a crossbar is described and verified with electro-thermal circuit simulations. Finally, validation of the concept in an actual DNN training task is shown using an emulator. ","Murat Onen, Brenden A. Butters, Emily Toomey, Tayfun Gokmen, Karl K.   Berggren",,,11,
A Multilayer Neural Network Merging Image Preprocessing and Pattern   Recognition by Integrating Diffusion and Drift Memristors,"  With the development of research on novel memristor model and device, neural networks by integrating various memristor models have become a hot research topic recently. However, state-of-the-art works still build such neural networks using drift memristor only. Furthermore, some other related works are only applied to a few individual applications including pattern recognition and edge detection. In this paper, a novel kind of multilayer neural network is proposed, in which diffusion and drift memristor models are applied to construct a system merging image preprocessing and pattern recognition. Specifically, the entire network consists of two diffusion memristive cellular layers for image preprocessing and one drift memristive feedforward layer for pattern recognition. Experimental results show that good recognition accuracy of noisy MNIST is obtained due to the fusion of image preprocessing and pattern recognition. Moreover, owing to high-efficiency in-memory computing and brief spiking encoding methods, high processing speed, high throughput, and few hardware resources of the entire network are achieved. ","Zhiri Tang, Ruohua Zhu, Ruihan Hu, Yanhua Chen, Edmond Q. Wu, Hao   Wang, Jin He, Qijun Huang, Sheng Chang",,,11,
"Quantum Computer: Hello, Music!","  Quantum computing is emerging as a promising technology, which is built on the principles of subatomic physics. By the time of writing, fully fledged practical quantum computers are not widely available. But research and development are advancing rapidly. Various software simulators are already available. And a few companies have already started to provide access to quantum hardware via the cloud. These initiatives have enabled experiments with quantum computing to tackle some realistic problems in science; e.g., in chemistry and cryptography. In spite of continuing progress in developing increasingly more sophisticated hardware and software, research in quantum computing has been focusing primarily on developing scientific applications. Up till now there has been virtually no research activity aimed at widening the range of applications of this technology beyond science and engineering. In particular applications for the entertainment industry and creative economies. This article introduces a new field of research, which is referred to as Quantum Computer Music. This research is aimed at the development of quantum computing tools and approaches to creating, performing, listening to and distributing music. The article begins with a brief historical background. Then, it introduces the notion of algorithmic music and presents two quantum computer music systems: a singing voice synthesiser and a musical sequencer based on quantum walk. A primer on quantum computing is also given. The chapter ends with a concluding discussion and advice for further work to develop this new exciting area of research. ",Eduardo R. Miranda,,,11,
Receptor Saturation Modeling for Synaptic DMC,"  Synaptic communication is a natural Molecular Communication (MC) system which may serve as a blueprint for the design of synthetic MC systems. In particular, it features highly specialized mechanisms to enable inter-symbol interference (ISI)-free and energy efficient communication. The understanding of synaptic MC is furthermore critical for disruptive innovations in the context of brain-machine interfaces. However, the physical modeling of synaptic MC is complicated by the possible saturation of the molecular receiver arising from the competition of postsynaptic receptors for neurotransmitters. Saturation renders the system behavior nonlinear and is commonly neglected in existing analytical models. In this work, we propose a novel model for receptor saturation in terms of a nonlinear, state-dependent boundary condition for Fick's diffusion equation. We solve the resulting boundary-value problem using an eigenfunction expansion of the Laplace operator and the incorporation of the receiver memory as feedback system into the corresponding state-space description. The presented solution is numerically stable and computationally efficient. Furthermore, the proposed model is validated with particle-based stochastic computer simulations. ","Sebastian Lotter and Maximilian Sch\""afer and Johannes Zeitler and   Robert Schober",,,11,
High-Throughput In-Memory Computing for Binary Deep Neural Networks with   Monolithically Integrated RRAM and 90nm CMOS,"  Deep learning hardware designs have been bottlenecked by conventional memories such as SRAM due to density, leakage and parallel computing challenges. Resistive devices can address the density and volatility issues, but have been limited by peripheral circuit integration. In this work, we demonstrate a scalable RRAM based in-memory computing design, termed XNOR-RRAM, which is fabricated in a 90nm CMOS technology with monolithic integration of RRAM devices between metal 1 and 2. We integrated a 128x64 RRAM array with CMOS peripheral circuits including row/column decoders and flash analog-to-digital converters (ADCs), which collectively become a core component for scalable RRAM-based in-memory computing towards large deep neural networks (DNNs). To maximize the parallelism of in-memory computing, we assert all 128 wordlines of the RRAM array simultaneously, perform analog computing along the bitlines, and digitize the bitline voltages using ADCs. The resistance distribution of low resistance states is tightened by write-verify scheme, and the ADC offset is calibrated. Prototype chip measurements show that the proposed design achieves high binary DNN accuracy of 98.5% for MNIST and 83.5% for CIFAR-10 datasets, respectively, with energy efficiency of 24 TOPS/W and 158 GOPS throughput. This represents 5.6X, 3.2X, 14.1X improvements in throughput, energy-delay product (EDP), and energy-delay-squared product (ED2P), respectively, compared to the state-of-the-art literature. The proposed XNOR-RRAM can enable intelligent functionalities for area-/energy-constrained edge computing devices. ","Shihui Yin, Xiaoyu Sun, Shimeng Yu, Jae-sun Seo",,,11,
A Cost & Performance-Efficient Field-Programmable Pin-Constrained   Digital Microfluidic Biochip,"  Digital microfluidic biochips (DMFBs) constitute modern generation of Lab-on-Chip (LoC) devices aimed at automation, miniaturization and cost-affordability of biochemistry and laboratory procedures. Over the course of past few years there have been various application-specific and general-purpose DMFBs aimed at reduced manufacturing costs; following the same trend this study presents a general-purpose DMFB with highly competitive characteristics compared with the state-of-the-art DMFBs. The proposed DMFB architecture provides lower Layout / PCB fabrication costs thereby reducing the total manufacturing costs. While more cost-affordable the proposed design is competitive with the state-of-the-art DMFB architectures. ","Alireza Abdoli, Ali Jahanian",,,11,
A Novel Quantum Algorithm for Ant Colony Optimization,"  Ant colony optimization is one of the potential solutions to tackle intractable NP-Hard discrete combinatorial optimization problems. The metaphor of ant colony can be thought of as the evolution of the best path from a given graph as a globally optimal solution, which is unaffected by earlier local convergence to achieve improved optimization efficiency. Earlier Quantum Ant Colony Optimization research work was primarily based on Quantum-inspired Evolutionary Algorithms, which deals with customizing and improving the quantum rotation gate through upgraded formation of the lookup table of rotation angle. Instead of relying on evolutionary algorithms, we have proposed a discrete-time quantum algorithm based on adaptive quantum circuit for pheromone updation. The algorithm encodes all possible paths in the exhaustive search space as input to the ORACLE. Iterative model of exploration and exploitation of all possible paths by quantum ants results in global optimal path convergence through probabilistic measurement of selected path. Our novel approach attempts to accelerate the search space exploitation in a significant manner to obtain the best optimal path as a solution through quantum arallelization achieving polynomial time speed-up over its classical counter part. ","Mrityunjay Ghosh, Nivedita Dey, Debdeep Mitra and Amlan Chakrabarti",,,11,
ROS: Resource-constrained Oracle Synthesis for Quantum Computers,"  We present a completely automatic synthesis framework for oracle functions, a central part in many quantum algorithms. The proposed framework for resource-constrained oracle synthesis (ROS) is a LUT-based hierarchical method in which every step is specifically tailored to address hardware resource constraints. ROS embeds a LUT mapper designed to simplify the successive synthesis steps, costing each LUT according to the resources used by its corresponding quantum circuit. In addition, the framework exploits a SAT-based quantum garbage management technique. Those two characteristics give ROS the ability to beat the state-of-the-art hierarchical method both in number of qubits and in number of operations. The efficiency of the framework is demonstrated by synthesizing quantum oracles for Grover's algorithm. ","Giulia Meuli (EPFL), Mathias Soeken (EPFL), Martin Roetteler   (Microsoft), Giovanni De Micheli (EPFL)",,,11,
Membrane Fusion-Based Transmitter Design for Molecular Communication   Systems,"  This paper proposes a novel imperfect spherical transmitter (TX) model, namely the membrane fusion (MF)-based TX, that adopts MF between a vesicle and the TX membrane to release molecules encapsulated within the vesicle. For the MF-based TX, the molecule release probability and the fraction of molecules released from the TX membrane are derived. Incorporating molecular degradation and a fully-absorbing receiver (RX), the end-to-end molecule hitting probability at the RX is also derived. A simulation framework for the MF-based TX is proposed, where the released point on the TX membrane and the released time of each molecule are determined. Aided by the simulation framework, the derived analytical expressions are validated. Simulation results verify that a low MF probability or low vesicle mobility slows the release of molecules from the TX, extends time required to reach the peak release probability, and reduces the end-to-end molecule hitting probability at the RX. ","Xinyu Huang, Yuting Fang, Adam Noel, Nan Yang",,,11,
SLIM: Simultaneous Logic-in-Memory Computing Exploiting Bilayer Analog   OxRAM Devices,"  Von Neumann architecture based computers isolate/physically separate computation and storage units i.e. data is shuttled between computation unit (processor) and memory unit to realize logic/ arithmetic and storage functions. This to-and-fro movement of data leads to a fundamental limitation of modern computers, known as the memory wall. Logic in-Memory (LIM) approaches aim to address this bottleneck by computing inside the memory units and thereby eliminating the energy-intensive and time-consuming data movement. However, most LIM approaches reported in literature are not truly ""simultaneous"" as during LIM operation the bitcell can be used only as a Memory cell or only as a Logic cell. The bitcell is not capable of storing both the Memory/Logic outputs simultaneously. Here, we propose a novel 'Simultaneous Logic in-Memory' (SLIM) methodology that allows to implement both Memory and Logic operations simultaneously on the same bitcell in a non-destructive manner without losing the previously stored Memory state. Through extensive experiments we demonstrate the SLIM methodology using non-filamentary bilayer analog OxRAM devices with NMOS transistors (2T-1R bitcell). Detailed programming scheme, array level implementation and controller architecture are also proposed. Furthermore, to study the impact of introducing SLIM array in the memory hierarchy, a simple image processing application (edge detection) is also investigated. It has been estimated that by performing all computations inside the SLIM array, the total Energy Delay Product (EDP) reduces by ~ 40x in comparison to a modern-day computer. EDP saving owing to reduction in data transfer between CPU Memory is observed to be ~ 780x. ","Sandeep Kaur Kingra, Vivek Parmar, Che-Chia Chang, Boris Hudec,   Tuo-Hung Hou, and Manan Suri",,,11,
SHE-MTJ Circuits for Convolutional Neural Networks,"  We report the performance characteristics of a notional Convolutional Neural Network based on the previously-proposed Multiply-Accumulate-Activate-Pool set, an MTJ-based spintronic circuit made to compute multiple neural functionalities in parallel. A study of image classification with the MNIST handwritten digits dataset using this network is provided via simulation. The effect of changing the weight representation precision, the severity of device process variation within the MAAP sets and the computational redundancy are provided. The emulated network achieves between 90 and 95\% image classification accuracy at a cost of ~100 nJ per image. ",Andrew W. Stephan and Steven J. Koester,,,11,
Stochastic Magnetoelectric Neuron for Temporal Information Encoding,"  Emulating various facets of computing principles of the brain can potentially lead to the development of neuro-computers that are able to exhibit brain-like cognitive capabilities. In this letter, we propose a magnetoelectronic neuron that utilizes noise as a computing resource and is able to encode information over time through the independent control of external voltage signals. We extensively characterize the device operation using simulations and demonstrate its suitability for neuromorphic computing platforms performing temporal information encoding. ","Kezhou Yang, Abhronil Sengupta",,,11,
In-Memory Nearest Neighbor Search with FeFET Multi-Bit   Content-Addressable Memories,"  Nearest neighbor (NN) search is an essential operation in many applications, such as one/few-shot learning and image classification. As such, fast and low-energy hardware support for accurate NN search is highly desirable. Ternary content-addressable memories (TCAMs) have been proposed to accelerate NN search for few-shot learning tasks by implementing $L_\infty$ and Hamming distance metrics, but they cannot achieve software-comparable accuracies. This paper proposes a novel distance function that can be natively evaluated with multi-bit content-addressable memories (MCAMs) based on ferroelectric FETs (FeFETs) to perform a single-step, in-memory NN search. Moreover, this approach achieves accuracies comparable to floating-point precision implementations in software for NN classification and one/few-shot learning tasks. As an example, the proposed method achieves a 98.34% accuracy for a 5-way, 5-shot classification task for the Omniglot dataset (only 0.8% lower than software-based implementations) with a 3-bit MCAM. This represents a 13% accuracy improvement over state-of-the-art TCAM-based implementations at iso-energy and iso-delay. The presented distance function is resilient to the effects of FeFET device-to-device variations. Furthermore, this work experimentally demonstrates a 2-bit implementation of FeFET MCAM using AND arrays from GLOBALFOUNDRIES to further validate proof of concept. ","Arman Kazemi, Mohammad Mehdi Sharifi, Ann Franchesca Laguna, Franz   M\""uller, Ramin Rajaei, Ricardo Olivo, Thomas K\""ampfe, Michael Niemier, X.   Sharon Hu",,,11,
"Survey on STT-MRAM Testing: Failure Mechanisms, Fault Models, and Tests","  As one of the most promising emerging non-volatile memory (NVM) technologies, spin-transfer torque magnetic random access memory (STT-MRAM) has attracted significant research attention due to several features such as high density, zero standby leakage, and nearly unlimited endurance. However, a high-quality test solution is required prior to the commercialization of STT-MRAM. In this paper, we present all STT-MRAM failure mechanisms: manufacturing defects, extreme process variations, magnetic coupling, STT-switching stochasticity, and thermal fluctuation. The resultant fault models including permanent faults and transient faults are classified and discussed. Moreover, the limited test algorithms and design-for-testability (DfT) designs proposed in the literature are also covered. It is clear that test solutions for STT-MRAMs are far from well established yet, especially when considering a defective part per billion (DPPB) level requirement. We present the main challenges on the STT-MRAM testing topic at three levels: failure mechanisms, fault modeling, and test/DfT designs. ","Lizhou Wu, Mottaqiallah Taouil, Siddharth Rao, Erik Jan Marinissen,   Said Hamdioui",,,11,
Mixed-precision deep learning based on computational memory,"  Deep neural networks (DNNs) have revolutionized the field of artificial intelligence and have achieved unprecedented success in cognitive tasks such as image and speech recognition. Training of large DNNs, however, is computationally intensive and this has motivated the search for novel computing architectures targeting this application. A computational memory unit with nanoscale resistive memory devices organized in crossbar arrays could store the synaptic weights in their conductance states and perform the expensive weighted summations in place in a non-von Neumann manner. However, updating the conductance states in a reliable manner during the weight update process is a fundamental challenge that limits the training accuracy of such an implementation. Here, we propose a mixed-precision architecture that combines a computational memory unit performing the weighted summations and imprecise conductance updates with a digital processing unit that accumulates the weight updates in high precision. A combined hardware/software training experiment of a multilayer perceptron based on the proposed architecture using a phase-change memory (PCM) array achieves 97.73% test accuracy on the task of classifying handwritten digits (based on the MNIST dataset), within 0.6% of the software baseline. The architecture is further evaluated using accurate behavioral models of PCM on a wide class of networks, namely convolutional neural networks, long-short-term-memory networks, and generative-adversarial networks. Accuracies comparable to those of floating-point implementations are achieved without being constrained by the non-idealities associated with the PCM devices. A system-level study demonstrates 173x improvement in energy efficiency of the architecture when used for training a multilayer perceptron compared with a dedicated fully digital 32-bit implementation. ","S. R. Nandakumar, Manuel Le Gallo, Christophe Piveteau, Vinay Joshi,   Giovanni Mariani, Irem Boybat, Geethan Karunaratne, Riduan Khaddam-Aljameh,   Urs Egger, Anastasios Petropoulos, Theodore Antonakopoulos, Bipin Rajendran,   Abu Sebastian, Evangelos Eleftheriou",,,11,
sBSNN: Stochastic-Bits Enabled Binary Spiking Neural Network with   On-Chip Learning for Energy Efficient Neuromorphic Computing at the Edge,"  In this work, we propose stochastic Binary Spiking Neural Network (sBSNN) composed of stochastic spiking neurons and binary synapses (stochastic only during training) that computes probabilistically with one-bit precision for power-efficient and memory-compressed neuromorphic computing. We present an energy-efficient implementation of the proposed sBSNN using 'stochastic bit' as the core computational primitive to realize the stochastic neurons and synapses, which are fabricated in 90nm CMOS process, to achieve efficient on-chip training and inference for image recognition tasks. The measured data shows that the 'stochastic bit' can be programmed to mimic spiking neurons, and stochastic Spike Timing Dependent Plasticity (or sSTDP) rule for training the binary synaptic weights without expensive random number generators. Our results indicate that the proposed sBSNN realization offers possibility of up to 32x neuronal and synaptic memory compression compared to full precision (32-bit) SNN and energy efficiency of 89.49 TOPS/Watt for two-layer fully-connected SNN. ","Minsuk Koo, Gopalakrishnan Srinivasan, Yong Shim, and Kaushik Roy",,,11,
Deep Learning in Memristive Nanowire Networks,"  Analog crossbar architectures for accelerating neural network training and inference have made tremendous progress over the past several years. These architectures are ideal for dense layers with fewer than roughly a thousand neurons. However, for large sparse layers, crossbar architectures are highly inefficient. A new hardware architecture, dubbed the MN3 (Memristive Nanowire Neural Network), was recently described as an efficient architecture for simulating very wide, sparse neural network layers, on the order of millions of neurons per layer. The MN3 utilizes a high-density memristive nanowire mesh to efficiently connect large numbers of silicon neurons with modifiable weights. Here, in order to explore the MN3's ability to function as a deep neural network, we describe one algorithm for training deep MN3 models and benchmark simulations of the architecture on two deep learning tasks. We utilize a simple piecewise linear memristor model, since we seek to demonstrate that training is, in principle, possible for randomized nanowire architectures. In future work, we intend on utilizing more realistic memristor models, and we will adapt the presented algorithm appropriately. We show that the MN3 is capable of performing composition, gradient propagation, and weight updates, which together allow it to function as a deep neural network. We show that a simulated multilayer perceptron (MLP), built from MN3 networks, can obtain a 1.61% error rate on the popular MNIST dataset, comparable to equivalently sized software-based network. This work represents, to the authors' knowledge, the first randomized nanowire architecture capable of reproducing the backpropagation algorithm. ","Jack D. Kendall, Ross D. Pantone, and Juan C. Nino",,,11,
Distributed Injection-Locking in Analog Ising Machines to Solve   Combinatorial Optimizations,"  The oscillator-based Ising machine (OIM) is a network of coupled CMOS oscillators that solves combinatorial optimization problems. In this paper, the distribution of the injection-locking oscillations throughout the circuit is proposed to accelerate the phase-locking of the OIM. The implications of the proposed technique theoretically investigated and verified by extensive simulations in EDA tools with a $130~nm$ PTM model. By distributing the injective signal of the super-harmonic oscillator, the speed is increased by $219.8\%$ with negligible increase in the power dissipation and phase-locking error of the device due to the distributed technique. ",M. Ali Vosoughi,,,11,
An Agency-Directed Approach to Test Generation for Simulation-based   Autonomous Vehicle Verification,"  Simulation-based verification is beneficial for assessing otherwise dangerous or costly on-road testing of autonomous vehicles (AV). This paper addresses the challenge of efficiently generating effective tests for simulation-based AV verification using software testing agents. The multi-agent system (MAS) programming paradigm offers rational agency, causality and strategic planning between multiple agents. We exploit these aspects for test generation, focusing in particular on the generation of tests that trigger the precondition of an assertion. On the example of a key assertion we show that, by encoding a variety of different behaviours respondent to the agent's perceptions of the test environment, the agency-directed approach generates twice as many effective tests than pseudo-random test generation, while being both efficient and robust. Moreover, agents can be encoded to behave naturally without compromising the effectiveness of test generation. Our results suggest that generating tests using agency-directed testing significantly improves upon random and simultaneously provides more realistic driving scenarios. ","Greg Chance, Abanoub Ghobrial, Severin Lemaignan, Tony Pipe, Kerstin   Eder",,,11,
Convergence of Opinion Diffusion is PSPACE-complete,"  We analyse opinion diffusion in social networks, where a finite set of individuals is connected in a directed graph and each simultaneously changes their opinion to that of the majority of their influencers. We study the algorithmic properties of the fixed-point behaviour of such networks, showing that the problem of establishing whether individuals converge to stable opinions is PSPACE-complete. ","Dmitry Chistikov, Grzegorz Lisowski, Mike Paterson, Paolo Turrini",,,11,
A Novel Multi-Agent System for Complex Scheduling Problems,"  Complex scheduling problems require a large amount computation power and innovative solution methods. The objective of this paper is the conception and implementation of a multi-agent system that is applicable in various problem domains. Independent specialized agents handle small tasks, to reach a superordinate target. Effective coordination is therefore required to achieve productive cooperation. Role models and distributed artificial intelligence are employed to tackle the resulting challenges. We simulate a NP-hard scheduling problem to demonstrate the validity of our approach. In addition to the general agent based framework we propose new simulation-based optimization heuristics to given scheduling problems. Two of the described optimization algorithms are implemented using agents. This paper highlights the advantages of the agent-based approach, like the reduction in layout complexity, improved control of complicated systems, and extendability. ","Peter Hillmann, Tobias Uhlig, Gabi Dreo Rodosek, Oliver Rose",,,11,
Coordination of Autonomous Vehicles: Taxonomy and Survey,"  In the near future, our streets will be populated by myriads of autonomous self-driving vehicles to serve our diverse mobility needs. This will raise the need to coordinate their movements in order to properly handle both access to shared resources (e.g., intersections and parking slots) and the execution of mobility tasks (e.g., platooning and ramp merging). In this paper, we firstly introduce the general issues associated to coordination of autonomous vehicles, by identifying and framing the key classes of coordination problems. Following, we overview the different approaches that can be adopted to manage such coordination problems, by classifying them in terms of the degree of autonomy in decision making that is left to autonomous vehicles during coordination. Finally, we overview some further peculiar challenges that research will have to address before autonomously coordinated vehicles can safely hit our streets. ","Stefano Mariani, Giacomo Cabri, and Franco Zambonelli",,,11,
Adaptive Online Distributed Optimal Control of Very-Large-Scale Robotic   Systems,"  This paper presents an adaptive online distributed optimal control approach that is applicable to optimal planning for very-large-scale robotics systems in highly uncertain environments. This approach is developed based on the optimal mass transport theory. It is also viewed as an online reinforcement learning and approximate dynamic programming approach in the Wasserstein-GMM space, where a novel value functional is defined based on the probability density functions of robots and the time-varying obstacle map functions describing the changing environmental information. The proposed approach is demonstrated on the path planning problem of very-largescale robotic systems where the approximated layout of obstacles in the workspace is incrementally updated by the observations of robots, and compared with some existing state-of-the-art approaches. The numerical simulation results show that the proposed approach outperforms these approaches in aspects of the average traveling distance and the energy cost. ","Pingping Zhu, Chang Liu, Silvia Ferrari",,,11,
Adaptive Social Learning,"  This work proposes a novel strategy for social learning by introducing the critical feature of adaptation. In social learning, several distributed agents update continually their belief about a phenomenon of interest through: i) direct observation of streaming data that they gather locally; and ii) diffusion of their beliefs through local cooperation with their neighbors. Traditional social learning implementations are known to learn well the underlying hypothesis (which means that the belief of every individual agent peaks at the true hypothesis), achieving steady improvement in the learning accuracy under stationary conditions. However, these algorithms do not perform well under nonstationary conditions commonly encountered in online learning, exhibiting a significant inertia to track drifts in the streaming data. In order to address this gap, we propose an Adaptive Social Learning (ASL) strategy, which relies on a small step-size parameter to tune the adaptation degree. We provide a detailed characterization of the learning performance by means of a steady-state analysis. Focusing on the small step-size regime, we establish that the ASL strategy achieves consistent learning under standard global identifiability assumptions. We derive reliable Gaussian approximations for the probability of error (i.e., of choosing a wrong hypothesis) at each individual agent. We also carry out a large deviations analysis revealing the universal behavior of adaptive social learner: the error probabilities decrease exponentially fast with the inverse of the step-size, and we characterize the resulting exponential learning rate. ","Virginia Bordignon, Vincenzo Matta, Ali H. Sayed",,,11,
Multi-Agent Programming Contest 2019 FIT BUT Team solution,"  During our participation in MAPC 2019, we have developed two multi-agent systems that have been designed specifically for this competition. The first of the systems is pro-active system that works with pre-specified scenarios and tasks agents with generated goals designed for individual agents according to assigned role. The second system is designed as more reactive and employs layered architecture with highly dynamic behaviour, where agents select their own action based on their perception of usefulness of said action. ","Vaclav Uhlir, Frantisek Zboril, Frantisek Vidensky",,,11,
A Game-Theoretic Utility Network for Cooperative Multi-Agent Decisions   in Adversarial Environments,"  Many underlying relationships among multi-agent systems (MAS) in various scenarios, especially agents working on dangerous, hazardous, and risky situations, can be represented in terms of game theory. In adversarial environments, the adversaries can be intentional or unintentional based on their needs and motivations. Agents will adopt suitable decision-making strategies to maximize their current needs and minimize their expected costs. In this paper, we propose a new network model called Game-Theoretic Utility Tree (GUT) to achieve cooperative decision-making for MAS in adversarial environments combining the core principles of game theory, utility theory, and probabilistic graphical models. Through calculating multi-level Game-Theoretic computation units, GUT can decompose high-level strategies into executable lower levels. Then, we design Explorers and Monsters Game to validate our model against a cooperative decision-making algorithm based on the state-of-the-art QMIX approach. Also, we implement different predictive models for MAS working with incomplete information to estimate adversaries' state. Our experimental results demonstrate that the GUT significantly enhances cooperation among MAS to successfully complete the assigned tasks with lower costs and higher winning probabilities against adversaries. ",Qin Yang and Ramviyas Parasuraman,,,11,
Evidence Propagation and Consensus Formation in Noisy Environments,"  We study the effectiveness of consensus formation in multi-agent systems where there is both belief updating based on direct evidence and also belief combination between agents. In particular, we consider the scenario in which a population of agents collaborate on the best-of-n problem where the aim is to reach a consensus about which is the best (alternatively, true) state from amongst a set of states, each with a different quality value (or level of evidence). Agents' beliefs are represented within Dempster-Shafer theory by mass functions and we investigate the macro-level properties of four well-known belief combination operators for this multi-agent consensus formation problem: Dempster's rule, Yager's rule, Dubois & Prade's operator and the averaging operator. The convergence properties of the operators are considered and simulation experiments are conducted for different evidence rates and noise levels. Results show that a combination of updating on direct evidence and belief combination between agents results in better consensus to the best state than does evidence updating alone. We also find that in this framework the operators are robust to noise. Broadly, Yager's rule is shown to be the better operator under various parameter values, i.e. convergence to the best state, robustness to noise, and scalability. ","Michael Crosscombe, Jonathan Lawry, Palina Bartashevich",,,11,
Probabilistic Gathering Of Agents With Simple Sensors,"  Gathering is a fundamental task for multi-agent systems and the problem has been studied under various assumptions on the sensing capabilities of mobile agents. This paper addresses the problem for a group of agents that are identical and indistinguishable, oblivious, and lack the capacity of direct communication. At the beginning of unit-time intervals, the agents select random headings in the plane and then detect the presence of other agents behind them. Then they move forward only if no agents are detected in their sensing ""back half-plane"". Two types of motion are considered: when no peers are detected behind them, either the agents perform unit jumps forward, or they start to move with unit speed while continuously sensing their back half-plane, and stop whenever another agent appears there. For the first type of motion extensive empirical evidence suggests that with high probability clustering occurs in finite expected time to a small region with diameter of about the size of the unit jump, while for continuous sensing and motion we can prove gathering in finite expected time if a ""blind-zone"" is assumed in their sensing half-plane. Relationships between the number of agents or the size of the blind-zone and convergence time are empirically studied and compared to a theoretical upper-bound dependent on these factors. ","Ariel Barel, Thomas Dag\`es, Rotem Manor, Alfred M. Bruckstein",,,11,
Multi Type Mean Field Reinforcement Learning,"  Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework. ",Sriram Ganapathi Subramanian and Pascal Poupart and Matthew E. Taylor   and Nidhi Hegde,,,11,
Transfer among Agents: An Efficient Multiagent Transfer Learning   Framework,"  Transfer Learning has shown great potential to enhance the single-agent Reinforcement Learning (RL) efficiency, by sharing learned policies of previous tasks. Similarly, in multiagent settings, the learning performance can also be promoted if agents can share knowledge between each other. However, it remains an open question of how an agent should learn from other agents' knowledge. In this paper, we propose a novel multiagent option-based policy transfer (MAOPT) framework to improve multiagent learning efficiency. Our framework learns what advice to give to each agent and when to terminate it by modeling multiagent policy transfer as the option learning problem. MAOPT provides different kinds of variants which can be classified into two types in terms of the experience used during training. One type is the MAOPT with the Global Option Advisor which has the access to the global information of the environment. However, in many realistic scenarios, we can only obtain each agent's local information due to the partial observation. The other type contains MAOPT with the Local Option Advisor and MAOPT with the Successor Representation Option (SRO) which are suitable for this setting and collect each agent's local experience for the update. In many cases, each agent's experience is inconsistent with each other which causes the option-value estimation to oscillate and to become inaccurate. SRO is used to handle the experience inconsistency by decoupling the dynamics of the environment from the rewards to learn the option-value function under each agent's preference. MAOPT can be easily combined with existing deep RL approaches. Experimental results show it significantly boosts the performance of existing deep RL methods in both discrete and continuous state spaces. ","Tianpei Yang, Weixun Wang, Hongyao Tang, Jianye Hao, Zhaopeng Meng,   Hangyu Mao, Dong Li, Wulong Liu, Yujing Hu, Yingfeng Chen and Changjie Fan",,,11,
Quasi-synchronization of bounded confidence opinion dynamics with   stochastic asynchronous rule,"  Recently the theory of noise-induced synchronization of Hegselmann-Krause (HK) dynamics has been well developed. As a typical opinion dynamics of bounded confidence, the HK model obeys a synchronous updating rule, i.e., \emph{all} agents check and update their opinions at each time point. However, whether asynchronous bounded confidence models, including the famous Deffuant-Weisbuch (DW) model, can be synchronized by noise have not been theoretically proved. In this paper, we propose a generalized bounded confidence model which possesses a stochastic asynchronous rule. The model takes the DW model and the HK model as special cases and can significantly generalize the bounded confidence models to practical application. We discover that the asynchronous model possesses a different noise-based synchronization behavior compared to the synchronous HK model. Generally, the HK dynamics can achieve quasi-synchronization \emph{almost surely} under the drive of noise. For the asynchronous dynamics, we prove that the model can achieve quasi-synchronization \emph{in mean}, which is a new type of quasi-synchronization weaker than the ""almost surely"" sense. The results unify the theory of noise-induced synchronization of bounded confidence opinion dynamics and hence proves the noise-induced synchronization of DW model theoretically for the first time. Moreover, the results provide a theoretical foundation for developing noise-based control strategy of more complex social opinion systems with stochastic asynchronous rules. ","Wei Su, Xueqiao Wang, Ge Chen, Kai Shen",,,11,
Towards automatic estimation of conversation floors within F-formations,"  The detection of free-standing conversing groups has received significant attention in recent years. In the absence of a formal definition, most studies operationalize the notion of a conversation group either through a spatial or a temporal lens. Spatially, the most commonly used representation is the F-formation, defined by social scientists as the configuration in which people arrange themselves to sustain an interaction. However, the use of this representation is often accompanied with the simplifying assumption that a single conversation occurs within an F-formation. Temporally, various categories have been used to organize conversational units; these include, among others, turn, topic, and floor. Some of these concepts are hard to define objectively by themselves. The present work constitutes an initial exploration into unifying these perspectives by primarily posing the question: can we use the observation of simultaneous speaker turns to infer whether multiple conversation floors exist within an F-formation? We motivate a metric for the existence of distinct conversation floors based on simultaneous speaker turns, and provide an analysis using this metric to characterize conversations across F-formations of varying cardinality. We contribute two key findings: firstly, at the average speaking turn duration of about two seconds for humans, there is evidence for the existence of multiple floors within an F-formation; and secondly, an increase in the cardinality of an F-formation correlates with a decrease in duration of simultaneous speaking turns. ","Chirag Raman, Hayley Hung",,,11,
Reward Machines for Cooperative Multi-Agent Reinforcement Learning,"  In cooperative multi-agent reinforcement learning, a collection of agents learns to interact in a shared environment to achieve a common goal. We propose the use of reward machines (RM) -- Mealy machines used as structured representations of reward functions -- to encode the team's task. The proposed novel interpretation of RMs in the multi-agent setting explicitly encodes required teammate interdependencies and independencies, allowing the team-level task to be decomposed into sub-tasks for individual agents. We define such a notion of RM decomposition and present algorithmically verifiable conditions guaranteeing that distributed completion of the sub-tasks leads to team behavior accomplishing the original task. This framework for task decomposition provides a natural approach to decentralized learning: agents may learn to accomplish their sub-tasks while observing only their local state and abstracted representations of their teammates. We accordingly propose a decentralized q-learning algorithm. Furthermore, in the case of undiscounted rewards, we use local value functions to derive lower and upper bounds for the global value function corresponding to the team task. Experimental results in three discrete settings exemplify the effectiveness of the proposed RM decomposition approach, which converges to a successful team policy two orders of magnitude faster than a centralized learner and significantly outperforms hierarchical and independent q-learning approaches. ","Cyrus Neary, Zhe Xu, Bo Wu, and Ufuk Topcu",,,11,
Simulation of Real-time Routing for UAS traffic Management with   Communication and Airspace Safety Considerations,"  Small Unmanned Aircraft Systems (sUAS) will be an important component of the smart city and intelligent transportation environments of the near future. The demand for sUAS related applications, such as commercial delivery and land surveying, is expected to grow rapidly in next few years. In general, sUAS traffic routing and management functions are needed to coordinate the launching of sUAS from different launch sites and determine their trajectories to avoid conflict while considering several other constraints such as expected arrival time, minimum flight energy, and availability of communication resources. However, as the airborne sUAS density grows in a certain area, it is difficult to foresee the potential airspace and communications resource conflicts and make immediate decisions to avoid them. To address this challenge, we present a temporal and spatial routing algorithm and simulation platform for sUAS trajectory management in a high density urban area that plans sUAS movements in a spatial and temporal maze taking into account obstacles that are either static or dynamic in time. The routing allows the sUAS to avoid static no-fly areas (i.e. static obstacles) or other in-flight sUAS and areas that have congested communication resources (i.e. dynamic obstacles). The algorithm is evaluated using an agent-based simulation platform. The simulation results show that the proposed algorithm outperforms other route management algorithms in many areas, especially in processing speed and memory efficiency. Detailed comparisons are provided for the sUAS flight time, the overall throughput, conflict rate and communication resource utilization. The results demonstrate that our proposed algorithm can be used to address the airspace and communication resource utilization needs for a next generation smart city and smart transportation. ","Zhao Jin, Ziyi Zhao, Chen Luo, Franco Basti, Adrian Solomon, M. Cenk   Gursoy, Carlos Caicedo, Qinru Qiu",,,11,
Crowd simulation for crisis management: the outcomes of the last decade,"  The last few decades, crowd simulation for crisis management is highlighted as an important topic of interest for many scientific fields. As the continues evolution of computational resources increases, along with the capabilities of Artificial Intelligence, the demand for better and more realistic simulation has become more attractive and popular to scientists. Along those years, there have been published hundreds of research articles and have been created numerous different systems that aim to simulate crowd behaviors, crisis cases and emergency evacuation scenarios. For better outcomes, recent research has focused on the separation of the problem of crisis management, to multiple research sub-fields (categories), such as the navigation of the simulated pedestrians, their psychology, the group dynamics etc. There have been extended research works suggesting new methods and techniques for those categories of problems. In this paper, we propose three main research categories, each one consist of several sub-categories, relying on crowd simulation for crisis management aspects and we present the outcomes of the last decade, focusing mostly on works exploiting multi-agent technologies. We analyze a number of technologies, methodologies, techniques, tools and systems introduced throughout the last years. A comparative review and discussion of the proposed categories is presented towards the identification of the most efficient aspects of the proposed categories. A general framework, towards the future crowd simulation for crisis management is presented based on the most efficient to yield the most realistic outcomes of the last decades. The paper is concluded with some highlights and open questions for future directions. ","George Sidiropoulos, Chairi Kiourt, Lefteris Moussiades",,,11,
Active Deception using Factored Interactive POMDPs to Recognize Cyber   Attacker's Intent,"  This paper presents an intelligent and adaptive agent that employs deception to recognize a cyber adversary's intent. Unlike previous approaches to cyber deception, which mainly focus on delaying or confusing the attackers, we focus on engaging with them to learn their intent. We model cyber deception as a sequential decision-making problem in a two-agent context. We introduce factored finitely nested interactive POMDPs (I-POMDPx) and use this framework to model the problem with multiple attacker types. Our approach models cyber attacks on a single honeypot host across multiple phases from the attacker's initial entry to reaching its adversarial objective. The defending I-POMDPx-based agent uses decoys to engage with the attacker at multiple phases to form increasingly accurate predictions of the attacker's behavior and intent. The use of I-POMDPs also enables us to model the adversary's mental state and investigate how deception affects their beliefs. Our experiments in both simulation and on a real host show that the I-POMDPx-based agent performs significantly better at intent recognition than commonly used deception strategies on honeypots. ","Aditya Shinde, Prashant Doshi, Omid Setayeshfar",,,11,
GOAL-DTU: Development of Distributed Intelligence for the Multi-Agent   Programming Contest,"  We provide a brief description of the GOAL-DTU system for the agent contest, including the overall strategy and how the system is designed to apply this strategy. Our agents are implemented using the GOAL programming language. We evaluate the performance of our agents for the contest, and finally also discuss how to improve the system based on analysis of its strengths and weaknesses. ",Alexander Birch Jensen and J{\o}rgen Villadsen,,,11,
Finding the maximum-a-posteriori behaviour of agents in an agent-based   model,"  In this paper we consider the problem of finding the most probable set of events that could have led to a set of partial, noisy observations of some dynamical system. In particular, we consider the case of a dynamical system that is a (possibly stochastic) time-stepping agent-based model with a discrete state space, the (possibly noisy) observations are the number of agents that have some given property and the events we're interested in are the decisions made by the agents (their ``expressed behaviours'') as the model evolves.   We show that this problem can be reduced to an integer linear programming problem which can subsequently be solved numerically using a standard branch-and-cut algorithm. We describe two implementations, an ``offline'' algorithm that finds the maximum-a-posteriori expressed behaviours given a set of observations over a finite time window, and an ``online'' algorithm that incrementally builds a feasible set of behaviours from a stream of observations that may have no natural beginning or end.   We demonstrate both algorithms on a spatial predator-prey model on a 32x32 grid with an initial population of 100 agents. ",Daniel Tang,,,11,
toki: A Build- and Test-Platform for Prototyping and Evaluating   Operating System Concepts in Real-Time Environments,"  Typically, even low-level operating system concepts, such as resource sharing strategies and predictability measures, are evaluated with Linux on PC hardware. This leaves a large gap to real industrial applications. Hence, the direct transfer of the results might be difficult. As a solution, we present toki, a prototyping and evaluation platform based on FreeRTOS and several open-source libraries. toki comes with a unified build- and test-environment based on Yocto and Qemu, which makes it well suited for rapid prototyping. With its architecture chosen similar to production industrial systems, toki provides the ground work to implement early prototypes of real-time systems research results, up to technology readiness level 7, with little effort. ",Oliver Horst and Uwe Baumgarten,,,11,
Utilization Difference Based Partitioned Scheduling of Mixed-Criticality   Systems,"  Mixed-Criticality (MC) systems consolidate multiple functionalities with different criticalities onto a single hardware platform. Such systems improve the overall resource utilization while guaranteeing resources to critical tasks. In this paper, we focus on the problem of partitioned multiprocessor MC scheduling, in particular the problem of designing efficient partitioning strategies. We develop two new partitioning strategies based on the principle of evenly distributing the difference between total high-critical utilization and total low-critical utilization for the critical tasks among all processors. By balancing this difference, we are able to reduce the pessimism in uniprocessor MC schedulability tests that are applied on each processor, thus improving overall schedulability. To evaluate the schedulability performance of the proposed strategies, we compare them against existing partitioned algorithms using extensive experiments. We show that the proposed strategies are effective with both dynamic-priority Earliest Deadline First with Virtual Deadlines (EDF-VD) and fixed-priority Adaptive Mixed-Criticality (AMC) algorithms. Specifically, our results show that the proposed strategies improve schedulability by as much as 28.1% and 36.2% for implicit and constrained-deadline task systems respectively. ","Saravanan Ramanathan, Arvind Easwaran",,,11,
"Look Mum, no VM Exits! (Almost)","  Multi-core CPUs are a standard component in many modern embedded systems. Their virtualisation extensions enable the isolation of services, and gain popularity to implement mixed-criticality or otherwise split systems. We present Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses novel architectural approaches to combine Linux, a powerful general-purpose system, with strictly isolated special-purpose components. Our design goals favour simplicity over features, establish a minimal code base, and minimise hypervisor activity.   Direct assignment of hardware to guests, together with a deferred initialisation scheme, offloads any complex hardware handling and bootstrapping issues from the hypervisor to the general purpose OS. The hypervisor establishes isolated domains that directly access physical resources without the need for emulation or paravirtualisation. This retains, with negligible system overhead, Linux's feature-richness in uncritical parts, while frugal safety and real-time critical workloads execute in isolated, safe domains. ","Ralf Ramsauer, Jan Kiszka, Daniel Lohmann, Wolfgang Mauerer",,,11,
High Velocity Kernel File Systems with Bento,"  High development velocity is critical for modern systems. This is especially true for Linux file systems which are seeing increased pressure from new storage devices and new demands on storage systems. However, high velocity Linux kernel development is challenging due to the ease of introducing bugs, the difficulty of testing and debugging, and the lack of support for redeployment without service disruption. Existing approaches to high-velocity development of file systems for Linux have major downsides, such as the high performance penalty for FUSE file systems, slowing the deployment cycle for new file system functionality.   We propose Bento, a framework for high velocity development of Linux kernel file systems. It enables file systems written in safe Rust to be installed in the Linux kernel, with errors largely sandboxed to the file system. Bento file systems can be replaced with no disruption to running applications, allowing daily or weekly upgrades in a cloud server setting. Bento also supports userspace debugging. We implement a simple file system using Bento and show that it performs similarly to VFS-native ext4 on a variety of benchmarks and outperforms a FUSE version by 10x-60x on Filebench. We also show that we can dynamically add file provenance tracking to a running kernel file system with only 10ms of service interruption. ","Samantha Miller (1), Kaiyuan Zhang (1), Mengqi Chen (1), Ryan Jennings   (1), Ang Chen (2), Danyang Zhuo (3), Tom Anderson (1) ((1) University of   Washington, (2) Rice University, (3) Duke University)",,,11,
Dim Silicon and the Case for Improved DVFS Policies,"  Due to thermal and power supply limits, modern Intel CPUs reduce their frequency when AVX2 and AVX-512 instructions are executed. As the CPUs wait for 670{\mu}s before increasing the frequency again, the performance of some heterogeneous workloads is reduced. In this paper, we describe parallels between this situation and dynamic power management as well as between the policy implemented by these CPUs and fixed-timeout device shutdown policies. We show that the policy implemented by Intel CPUs is not optimal and describe potential better policies. In particular, we present a mechanism to classify applications based on their likeliness to cause frequency reduction. Our approach takes either the resulting classification information or information provided by the application and generates hints for the DVFS policy. We show that faster frequency changes based on these hints are able to improve performance for a web server using the OpenSSL library. ","Mathias Gottschlag, Yussuf Khalil, Frank Bellosa",,,11,
Virtual Gang based Scheduling of Real-Time Tasks on Multicore Platforms,"  We propose a virtual-gang based parallel real-time task scheduling approach for multicore platforms. Our approach is based on the notion of a virtual-gang, which is a group of parallel real-time tasks that are statically linked and scheduled together by a gang scheduler. We present a light-weight intra-gang synchronization framework, called RTG-Sync, and virtual gang formation algorithms that provide strong temporal isolation and high real-time schedulability in scheduling real-time tasks on multicore. We evaluate our approach both analytically, with generated tasksets against state-of-the-art approaches, and empirically with a case-study involving real-world workloads on a real embedded multicore platform. The results show that our approach provides simple but powerful compositional analysis framework, achieves better analytic schedulability, especially when the effect of interference is considered, and is a practical solution for COTS multicore platforms. ",Waqar Ali and Rodolfo Pellizzoni and Heechul Yun,,,11,
Period Adaptation for Continuous Security Monitoring in Multicore   Real-Time Systems,"  We propose a design-time framework (named HYDRA-C) for integrating security tasks into partitioned real-time systems (RTS) running on multicore platforms. Our goal is to opportunistically execute security monitoring mechanisms in a 'continuous' manner -- i.e., as often as possible, across cores, to ensure that security tasks run with as few interruptions as possible. Our framework will allow designers to integrate security mechanisms without perturbing existing real-time (RT) task properties or execution order. We demonstrate the framework using a proof-of-concept implementation with intrusion detection mechanisms as security tasks. We develop and use both, (a) a custom intrusion detection system (IDS), as well as (b) Tripwire -- an open source data integrity checking tool. These are implemented on a realistic rover platform designed using an ARM multicore chip. We compare the performance of HYDRA-C with a state-of-the-art RT security integration approach for multicore-based RTS and find that our method can, on average, detect intrusions 19.05% faster without impacting the performance of RT tasks. ","Monowar Hasan, Sibin Mohan, Rodolfo Pellizzoni, Rakesh B. Bobba",,,11,
HeRTA: Heaviside Real-Time Analysis,"  We investigate the mathematical properties of event bound functions as they are used in the worst-case response time analysis and utilization tests. We figure out the differences and similarities between the two approaches. Based on this analysis, we derive a more general form do describe events and event bounds. This new unified approach gives clear new insights in the investigation of real-time systems, simplifies the models and will support algebraic proofs in future work. In the end, we present a unified analysis which allows the algebraic definition of any scheduler. Introducing such functions to the real-time scheduling theory will lead two a more systematic way to integrate new concepts and applications to the theory. Last but not least, we show how the response time analysis in dynamic scheduling can be improved. ",Frank Slomka and Mohammadreza Sadeghi,,,11,
DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks,"  Real-time scheduling and locking protocols are fundamental facilities to construct time-critical systems. For parallel real-time tasks, predictable locking protocols are required when concurrent sub-jobs mutually exclusive access to shared resources. This paper for the first time studies the distributed synchronization framework of parallel real-time tasks, where both tasks and global resources are partitioned to designated processors, and requests to each global resource are conducted on the processor on which the resource is partitioned. We extend the Distributed Priority Ceiling Protocol (DPCP) for parallel tasks under federated scheduling, with which we proved that a request can be blocked by at most one lower-priority request. We develop task and resource partitioning heuristics and propose analysis techniques to safely bound the task response times. Numerical evaluation (with heavy tasks on 8-, 16-, and 32-core processors) indicates that the proposed methods improve the schedulability significantly compared to the state-of-the-art locking protocols under federated scheduling. ","Maolin Yang, Zewei Chen, Xu Jiang, Nan Guan, Hang Lei",,,11,
A File System For Write-Once Media,"  A file system standard for use with write-once media such as digital compact disks is proposed. The file system is designed to work with any operating system and a variety of physical media. Although the implementation is simple, it provides a a full-featured and high-performance alternative to conventional file systems on traditional, multiple-write media such as magnetic disks. ",Simson L. Garfinkel and J. Spencer Love,,,11,
Efficient Schedulability Test for Dynamic-Priority Scheduling of   Mixed-Criticality Real-Time Systems,"  Systems in many safety-critical application domains are subject to certification requirements. In such a system, there are typically different applications providing functionalities that have varying degrees of criticality. Consequently, the certification requirements for functionalities at these different criticality levels are also varying, with very high levels of assurance required for a highly critical functionality, whereas relatively low levels of assurance required for a less critical functionality. Considering the timing assurance given to various applications in the form of guaranteed budgets within deadlines, a theory of real-time scheduling for such multi-criticality systems has been under development in the recent past. In particular, an algorithm called Earliest Deadline First with Virtual Deadlines (EDF-VD) has shown a lot of promise for systems with two criticality levels, especially in terms of practical performance demonstrated through experiment results. In this paper we design a new schedulability test for EDF-VD that extend these performance benefits to multi-criticality systems. We propose a new test based on demand bound functions and also present a novel virtual deadline assignment strategy. Through extensive experiments we show that the proposed technique significantly outperforms existing strategies for a variety of generic real-time systems. ","Xiaozhe Gu, Arvind Easwaran",,,11,
Quantifying the Latency and Possible Throughput of External Interrupts   on Cyber-Physical Systems,"  An important characteristic of cyber-physical systems is their capability to respond, in-time, to events from their physical environment. However, to the best of our knowledge there exists no benchmark for assessing and comparing the interrupt handling performance of different software stacks. Hence, we present a flexible evaluation method for measuring the interrupt latency and throughput on ARMv8-A based platforms. We define and validate seven test-cases that stress individual parts of the overall process and combine them to three benchmark functions that provoke the minimal and maximal interrupt latency, and maximal interrupt throughput. ","Oliver Horst and Johannes Wiesb\""ock and Raphael Wild and Uwe   Baumgarten",,,11,
Disaggregated Accelerator Management System for Cloud Data Centers,"  A conventional data center that consists of monolithic-servers is confronted with limitations including lack of operational flexibility, low resource utilization, low maintainability, etc. Resource disaggregation is a promising solution to address the above issues. We propose a concept of disaggregated cloud data center architecture called Flow-in-Cloud (FiC) that enables an existing cluster computer system to expand an accelerator pool through a high-speed network. FlowOS-RM manages the entire pool resources, and deploys a user job on a dynamically constructed slice according to a user request. This slice consists of compute nodes and accelerators where each accelerator is attached to the corresponding compute node. This paper demonstrates the feasibility of FiC in a proof of concept experiment running a distributed deep learning application on the prototype system. The result successfully warrants the applicability of the proposed system. ",Ryousei Takano and Kuniyasu Suzaki,,,11,
Efficient Kernel Object Management for Tiered Memory Systems with KLOC,"  Software-controlled heterogeneous memory systems have the potential to improve performance, efficiency, and cost tradeoffs in emerging systems. Delivering on this promise requires an efficient operating system (OS) mechanisms and policies for data management. Unfortunately, modern OSes do not support efficient tiering of data between heterogeneous memories. While this problem is known (and is being studied) for application-level data pages, the question of how best to tier OS kernel objects has largely been ignored. We show that careful kernel object management is vital to the performance of software-controlled tiered memory systems. We find that the state-of-the-art OS page management research leaves considerable performance on the table by overlooking how best to tier, migrate, and manage kernel objects like inodes, dentry caches, journal blocks, network socket buffers, etc., associated with the filesystem and networking stack. In response, we characterize hotness, reuse, and liveness properties of kernel objects to develop appropriate tiering/migration mechanisms and policies. We evaluate our proposal using a real-system emulation framework on large-scale workloads like RocksDB, Redis, Cassandra, and Spark and achieve 1.4X to 4X higher throughput compared to the prior art. ","Sudarsun Kannan, Yujie Ren, Abhishek Bhatacharjee",,,11,
AppStreamer: Reducing Storage Requirements of Mobile Games through   Predictive Streaming,"  Storage has become a constrained resource on smartphones. Gaming is a popular activity on mobile devices and the explosive growth in the number of games coupled with their growing size contributes to the storage crunch. Even where storage is plentiful, it takes a long time to download and install a heavy app before it can be launched. This paper presents AppStreamer, a novel technique for reducing the storage requirements or startup delay of mobile games, and heavy mobile apps in general. AppStreamer is based on the intuition that most apps do not need the entirety of its files (images, audio and video clips, etc.) at any one time. AppStreamer can, therefore, keep only a small part of the files on the device, akin to a ""cache"", and download the remainder from a cloud storage server or a nearby edge server when it predicts that the app will need them in the near future. AppStreamer continuously predicts file blocks for the near future as the user uses the app, and fetches them from the storage server before the user sees a stall due to missing resources. We implement AppStreamer at the Android file system layer. This ensures that the apps require no source code or modification, and the approach generalizes across apps. We evaluate AppStreamer using two popular games: Dead Effect 2, a 3D first-person shooter, and Fire Emblem Heroes, a 2D turn-based strategy role-playing game. Through a user study, 75% and 87% of the users respectively find that AppStreamer provides the same quality of user experience as the baseline where all files are stored on the device. AppStreamer cuts down the storage requirement by 87% for Dead Effect 2 and 86% for Fire Emblem Heroes. ","Nawanol Theera-Ampornpunt, Shikhar Suryavansh, Sameer Manchanda,   Rajesh Panta, Kaustubh Joshi, Mostafa Ammar, Mung Chiang, Saurabh Bagchi",,,11,
MigrOS: Transparent Operating Systems Live Migration Support for   Containerised RDMA-applications,"  Major data centre providers are introducing RDMA-based networks for their tenants, as well as for operating the underlying infrastructure. In comparison to traditional socket-based network stacks, RDMA-based networks offer higher throughput, lower latency and reduced CPU overhead. However, transparent checkpoint and migration operations become much more difficult. The key reason is that the OS is removed from the critical path of communication. As a result, some of the communication state itself resides in the NIC hardware and is no more under the direct control of the OS. This control includes especially the support for virtualisation of communication which is needed for live migration of communication partners. In this paper, we propose the basic principles required to implement a migration-capable RDMA-based network. We recommend some changes at the software level and small changes at the hardware level. As a proof of concept, we integrate the proposed changes into SoftRoCE, an open-source kernel-level implementation of the RoCE protocol. We claim that these changes introduce no runtime overhead when migration does not happen. Finally, we develop a proof-of-concept implementation for migrating containerised applications that use RDMA-based networks. ","Maksym Planeta, Jan Bierbaum, Leo Sahaya Daphne Antony, Torsten   Hoefler, Hermann H\""artig",,,11,
SGX-LKL: Securing the Host OS Interface for Trusted Execution,"  Hardware support for trusted execution in modern CPUs enables tenants to shield their data processing workloads in otherwise untrusted cloud environments. Runtime systems for the trusted execution must rely on an interface to the untrusted host OS to use external resources such as storage, network, and other functions. Attackers may exploit this interface to leak data or corrupt the computation.   We describe SGX-LKL, a system for running Linux binaries inside of Intel SGX enclaves that only exposes a minimal, protected and oblivious host interface: the interface is (i) minimal because SGX-LKL uses a complete library OS inside the enclave, including file system and network stacks, which requires a host interface with only 7 calls; (ii) protected because SGX-LKL transparently encrypts and integrity-protects all data passed via low-level I/O operations; and (iii) oblivious because SGX-LKL performs host operations independently of the application workload. For oblivious disk I/O, SGX-LKL uses an encrypted ext4 file system with shuffled disk blocks. We show that SGX-LKL protects TensorFlow training with a 21% overhead. ","Christian Priebe, Divya Muthukumaran, Joshua Lind, Huanzhou Zhu,   Shujie Cui, Vasily A. Sartakov, Peter Pietzuch",,,11,
Study of Firecracker MicroVM,"  Firecracker is a virtualization technology that makes use of Kernel Virtual Machine (KVM). Firecracker belongs to a new virtualization class named the micro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweight MicroVMs in non-virtualized environments in a fraction of a second, at the same time offering the security and workload isolation provided by traditional VMs and also the resource efficiency that comes along with containers \cite{b1}. Firecracker aims to provide a slimmed-down MicroVM, comprised of approximately 50K lines of code in Rust and with a reduced attack surface for guest VMs. This report will examine the internals of Firecracker and understand why Firecracker is the next big thing going forward in virtualization and cloud computing. ",Madhur Jain,,,11,
Fissile Locks,"  Classic test-and-test (TS) mutual exclusion locks are simple, and enjoy high performance and low latency of ownership transfer under light or no contention. However, they do not scale gracefully under high contention and do not provide any admission order guarantees. Such concerns led to the development of scalable queue-based locks, such as a recent Compact NUMA-aware (CNA) lock, a variant of another popular queue-based MCS lock. CNA scales well under load and provides certain admission guarantees, but has more complicated lock handover operations than TS and incurs higher latencies at low contention. We propose Fissile locks, which capture the most desirable properties of both TS and CNA. A Fissile lock consists of two underlying locks: a TS lock, which serves as a fast path, and a CNA lock, which serves as a slow path. The key feature of Fissile locks is the ability of threads on the fast path to bypass threads enqueued on the slow path, and acquire the lock with less overhead than CNA. Bypass is bounded (by a tunable parameter) to avoid starvation and ensure long-term fairness. The result is a highly scalable NUMA-aware lock with progress guarantees that performs like TS at low contention and like CNA at high contention. ",Dave Dice and Alex Kogan,,,11,
Characterizing Synchronous Writes in Stable Memory Devices,"  Distributed algorithms that operate in the fail-recovery model rely on the state stored in stable memory to guarantee the irreversibility of operations even in the presence of failures. The performance of these algorithms lean heavily on the performance of stable memory. Current storage technologies have a defined performance profile: data is accessed in blocks of hundreds or thousands of bytes, random access to these blocks is expensive and sequential access is somewhat better. File system implementations hide some of the performance limitations of the underlying storage devices using buffers and caches. However, fail-recovery distributed algorithms bypass some of these techniques and perform synchronous writes to be able to tolerate a failure during the write itself. Assuming the distributed system designer is able to buffer the algorithm's writes, we ask how buffer size and latency complement each other. In this paper we start to answer this question by characterizing the performance (throughput and latency) of typical stable memory devices using a representative set of current file systems. ","William B. Mingardi, Gustavo M. D. Vieira",,,11,
On a caching system with object sharing,"  We consider a content-caching system thatis shared by a number of proxies. The cache could belocated in an edge-cloud datacenter and the proxies couldeach serve a large population of mobile end-users. Eachproxy operates its own LRU-list of a certain capacity inthe shared cache. The length of objects simultaneouslyappearing in plural LRU-lists is equally divided amongthem,i.e., object sharing among the LRUs. We provide a ""working-set"" approximation for this system to quicklyestimate the cache-hit probabilities under such objectsharing, which can be used to facilitate admission control.Also, a way to reduce ripple evictions,i.e.,setrequestoverhead, is suggested. We give numerical results for ourMemCacheD with Object Sharing (MCD-OS) prototype. ","George Kesidis, Nader Alfares, Xi Li, Bhuvan Urgaonkar, Mahmut   Kandemir, Takis Konstantopoulos",,,11,
ROOT I/O compression algorithms and their performance impact within Run   3,"  The LHCs Run3 will push the envelope on data-intensive workflows and, since at the lowest level this data is managed using the ROOT software framework, preparations for managing this data are starting already. At the beginning of LHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then, ROOT has added support for additional algorithms such as LZMA and LZ4, each with unique strengths. This work must continue as industry introduces new techniques - ROOT can benefit saving disk space or reducing the I/O and bandwidth for online and offline needs of experiments by introducing better compression algorithms. In addition to alternate algorithms, we have been exploring alternate techniques to improve parallelism and apply pre-conditioners to the serialized data.   We have performed a survey of the performance of the new compression techniques. Our survey includes various use cases of data compression of ROOT files provided by different LHC experiments. We also provide insight into solutions applied to resolve bottlenecks in compression algorithms, resulting in improved ROOT performance. ","Oksana Shadura (1), Brian Paul Bockelman (1) ((1) University of   Nebraska-Lincoln, (2) Morgridge Institute for Research)",,,11,
Pass-and-swap queues,"  Order-independent (OI) queues, introduced by Berezner, Kriel, and Krzesinski in 1995, expanded the family of multi-class queues that are known to have a product-form stationary distribution by allowing for intricate class-dependent service rates. This paper further broadens this family by introducing pass-and-swap (P&S) queues, an extension of OI queues where any customer that completes service is not necessarily the customer that leaves the system. More precisely, we supplement the OI queue model with an undirected graph on the customer classes, which we call a swapping graph, such that there is an edge between two classes if customers of these classes can be swapped with one another. When a customer completes service, it passes over customers in the remainder of the queue until it finds a customer it can swap position with, that is, a customer whose class is a neighbor in the graph. In its turn, the customer that is ejected from its position takes the position of the next customer it can swap with, and so on. This is repeated until a customer cannot find another customer to be swapped with anymore; this customer is the one that leaves the queue. After proving that P&S queues have a product-form stationary distribution, we derive a necessary and sufficient stability condition for (open networks of) P&S queues that also applies to OI queues. We then study irreducibility properties of closed networks of P&S queues and derive the corresponding product-form stationary distribution. Lastly, we demonstrate that closed networks of P&S queues can be applied to describe the dynamics of new and existing load-distribution and scheduling algorithms in machine pools. ","C\'eline Comte, Jan-Pieter Dorsman",,,11,
Age of Information in a Decentralized Network of Parallel Queues with   Routing and Packets Losses,"  The paper deals with Age of Information in a network of multiple sources and parallel servers/queues with buffering capabilities, preemption in service and losses in served packets. The servers do not communicate between each other and the packets are dispatched through the servers according to a predefined probabilistic routing. By making use of the Stochastic Hybrid System (SHS) method, we provide a derivation of the average Age of Information of a system of two parallel servers (with and without buffer capabilities) and compare the result with that of a single queue. We show known results of packets delay in Queuing Theory do not hold for Age of Information. Unfortunately, the complexity of computing the Age of Information using the SHS method increases highly with the number of queues. We therefore provide an upper bound of the average Age of Information in a parallel server system of an arbitrary number of M/M/1/(N + 1) queues and its tightness in various regimes. This upper bound allows providing a tight approximation of the Age of Information with a very low complexity. ",Josu Doncel and Mohamad Assaad,,,11,
Threshold-based rerouting and replication for resolving job-server   affinity relations,"  We consider a system with several job types and two parallel server pools. Within the pools the servers are homogeneous, but across pools possibly not in the sense that the service speed of a job may depend on its type as well as the server pool. Immediately upon arrival, jobs are assigned to a server pool. This could be based on (partial) knowledge of their type, but such knowledge might not be available. Information about the job type can however be obtained while the job is in service; as the service progresses, the likelihood that the service speed of this job type is low increases, creating an incentive to execute the job on different, possibly faster, server(s). Two policies are considered: reroute the job to the other server pool, or replicate it there.   We determine the effective load per server under both the rerouting and replication policy for completely unknown as well as partly known job types. We also examine the impact of these policies on the stability bound, and find that the uncertainty in job types may significantly degrade the performance. For (highly) unbalanced service speeds full replication achieves the largest stability bound while for (nearly) balanced service speeds no replication maximizes the stability bound. Finally, we discuss how the use of threshold-based policies can help improve the expected latency for completely or partly unknown job types. ",Youri Raaijmakers and Sem Borst and Onno Boxma,,,11,
Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic,"  We consider scheduling to minimize mean response time of the M/G/k queue with unknown job sizes. In the single-server case, the optimal policy is the Gittins policy, but it is not known whether Gittins or any other policy is optimal in the multiserver case. Exactly analyzing the M/G/k under any scheduling policy is intractable, and Gittins is a particularly complicated policy that is hard to analyze even in the single-server case.   In this work we introduce monotonic Gittins (M-Gittins), a new variation of the Gittins policy, and show that it minimizes mean response time in the heavy-traffic M/G/k for a wide class of finite-variance job size distributions. We also show that the monotonic shortest expected remaining processing time (M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for mean response time in the heavy traffic M/G/k under similar conditions. These results constitute the most general optimality results to date for the M/G/k with unknown job sizes. Our techniques build upon work by Grosof et al., who study simple policies, such as SRPT, in the M/G/k; Bansal et al., Kamphorst and Zwart, and Lin et al., who analyze mean response time scaling of simple policies in the heavy-traffic M/G/1; and Aalto et al. and Scully et al., who characterize and analyze the Gittins policy in the M/G/1. ","Ziv Scully, Isaac Grosof, Mor Harchol-Balter",,,11,
An approach to define Very High Capacity Networks with improved quality   at an affordable cost,"  This paper aims to propose one possible approach in the setting of VHCNs (Very High Capacity Networks) performance targets that should be capable of promoting efficient investments for operators and, at the same time, improving the benefits for end-users. To this aim, we suggest relying on some specific KPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth as perceived by the customer - valid at the application layer, instead of the physical layer data-rate. In this regard, the paper underlines that the bandwidth perceived is strictly linked to the latency. The most important implication is that some of the most demanding services envisaged for the future (e.g., mobile virtual and augmented reality, tactile internet) cannot be met by merely increasing the low-level protocol data-rate. Therefore, for the VHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory pre-requisite. ",Giovanni Santella and Francesco Vatalaro,,,11,
Load Balancing Performance in Distributed Storage with Regular Balanced   Redundancy,"  Contention at the storage nodes is the main cause of long and variable data access times in distributed storage systems. Offered load on the system must be balanced across the storage nodes in order to minimize contention, and load balance in the system should be robust against the skews and fluctuations in content popularities. Data objects are replicated across multiple nodes in practice to allow for load balancing. However redundancy increases the storage requirement and should be used efficiently. We evaluate load balancing performance of natural storage schemes in which each data object is stored at $d$ different nodes and each node stores the same number of objects. We find that load balance in a system of $n$ nodes improves multiplicatively with $d$ as long as $d = o\left(\log(n)\right)$, and improves exponentially as soon as $d = \Theta\left(\log(n)\right)$. We show that the load balance in the system improves the same way with $d$ when the service choices are created with XOR's of $r$ objects rather than object replicas, which also reduces the storage overhead multiplicatively by $r$. However, unlike accessing an object replica, access through a recovery set composed by an XOR'ed object copy requires downloading content from $r$ nodes, which increases the load imbalance in the system additively by $r$. ","Mehmet Fatih Aktas, Amir Behrouzi-Far, Emina Soljanin, Philip Whiting",,,11,
On Competitive Analysis for Polling Systems,"  Polling systems have been widely studied, however most of these studies focus on polling systems with renewal processes for arrivals and random variables for service times. There is a need driven by practical applications to study polling systems with arbitrary arrivals (not restricted to time-varying or in batches) and revealed service time upon a job's arrival. To address that need, our work considers a polling system with generic setting and for the first time provides the worst-case analysis for online scheduling policies in this system. We provide conditions for the existence of constant competitive ratios, and competitive lower bounds for general scheduling policies in polling systems. Our work also bridges the queueing and scheduling communities by proving the competitive ratios for several well-studied policies in the queueing literature, such as cyclic policies with exhaustive, gated or l-limited service disciplines for polling systems. ",Jin Xu and Natarajan Gautam,,,11,
Stability for Two-class Multiserver-job Systems,"  Multiserver-job systems, where jobs require concurrent service at many servers, occur widely in practice. Much is known in the dropping setting, where jobs are immediately discarded if they require more servers than are currently available. However, very little is known in the more practical setting where jobs queue instead.   In this paper, we derive a closed-form analytical expression for the stability region of a two-class (non-dropping) multiserver-job system where each class of jobs requires a distinct number of servers and requires a distinct exponential distribution of service time, and jobs are served in first-come-first-served (FCFS) order. This is the first result of any kind for an FCFS multiserver-job system where the classes have distinct service distributions. Our work is based on a technique that leverages the idea of a ""saturated"" system, in which an unlimited number of jobs are always available.   Our analytical formula provides insight into the behavior of FCFS multiserver-job systems, highlighting the huge wastage (idle servers while jobs are in the queue) that can occur, as well as the nonmonotonic effects of the service rates on wastage. ","Isaac Grosof, Mor Harchol-Balter, Alan Scheller-Wolf",,,11,
Asymptotically Optimal Load Balancing in Large-scale Heterogeneous   Systems with Multiple Dispatchers,"  We consider the load balancing problem in large-scale heterogeneous systems with multiple dispatchers. We introduce a general framework called Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps local (possibly outdated) estimates of queue lengths for all the servers, and the dispatching decision is made purely based on these local estimates. The local estimates are updated via infrequent communications between dispatchers and servers. We derive sufficient conditions for LED policies to achieve throughput optimality and delay optimality in heavy-traffic, respectively. These conditions directly imply delay optimality for many previous local-memory based policies in heavy traffic. Moreover, the results enable us to design new delay optimal policies for heterogeneous systems with multiple dispatchers. Finally, the heavy-traffic delay optimality of the LED framework directly resolves a recent open problem on how to design optimal load balancing schemes using delayed information. ","Xingyu Zhou, Ness Shroff and Adam Wierman",,,11,
Performance Analysis of an Interference-Limited RIS-Aided Network,"  In this work, the performance of reconfigurable intelligent surface (RIS)-aided communication systems corrupted by the co-channel interference (CCI) at the destination is investigated. Assuming Rayleigh fading and equal-power CCI, we present the analysis for the outage probability (OP), average bit error rate (BER), and ergodic capacity. In addition, an asymptotic outage analysis is carried in order to obtain further insights. Our analysis shows that the number of reflecting elements as well as the number of interferers have a great impact on the overall system performance. ","Liang Yang, Yin Yang, Daniel Benevides da Costa, and Imene Trigui",,,11,
A Prompt Report on the Performance of Intel Optane DC Persistent Memory   Module,"  In this prompt report, we present the basic performance evaluation of Intel Optane Data Center Persistent Memory Module (Optane DCPMM), which is the first commercially-available, byte-addressable non-volatile memory modules released in April 2019. Since at the moment of writing only a few reports on its performance were published, this letter is intended to complement other performance studies. Through experiments using our own measurement tools, we obtained that the latency of random read-only access was approximately 374 ns. That of random writeback-involving access was 391 ns. The bandwidths of read-only and writeback-involving access for interleaved memory modules were approximately 38 GB/s and 3 GB/s, respectively. ",Takahiro Hirofuchi and Ryousei Takano,,,11,
Self-Learning Threshold-Based Load Balancing,"  We consider a large-scale service system where incoming tasks have to be instantaneously dispatched to one out of many parallel server pools. The dispatcher uses a threshold for balancing the load and keeping the maximum number of concurrent tasks across server pools low. We demonstrate that such a policy is optimal on the fluid and diffusion scales for a suitable threshold value, while only involving a small communication overhead. In order to set the threshold optimally, it is important, however, to learn the load of the system, which may be uncertain or even time-varying. For that purpose, we design a control rule for tuning the threshold in an online manner. We provide conditions which guarantee that this adaptive threshold settles at the optimal value, along with estimates for the time until this happens. ","Diego Goldsztajn, Sem C. Borst, Johan S. H. van Leeuwaarden, Debankur   Mukherjee, Philip A. Whiting",,,11,
Proximity Based Load Balancing Policies on Graphs: A Simulation Study,"  Distributed load balancing is the act of allocating jobs among a set of servers as evenly as possible. There are mainly two versions of the load balancing problem that have been studied in the literature: static and dynamic. The static interpretation leads to formulating the load balancing problem as a case with jobs (balls) never leaving the system and accumulating at the servers (bins) whereas the dynamic setting deals with the case when jobs arrive and leave the system after service completion. This paper designs and evaluates server proximity aware job allocation policies for treating load balancing problems with a goal to reduce the communication cost associated with the jobs. We consider a class of proximity aware Power of Two (POT) choice based assignment policies for allocating jobs to servers, where servers are interconnected as an n-vertex graph G(V, E). For the static version, we assume each job arrives at one of the servers, u. For the dynamic setting, we assume G to be a circular graph and job arrival process at each server is described by a Poisson point process with the job service time exponentially distributed. For both settings, we then assign each job to the server with minimum load among servers u and v where v is chosen according to one of the following two policies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop neighborhood of u (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood of u with probability proportional to the inverse square of the distance between u and v. Our simulation results show that both the policies consistently produce a load distribution which is much similar to that of a classical proximity oblivious POT policy. ","Nitish K. Panigrahy, Thirupathaiah Vasantam, Prithwish Basu and Don   Towsley",,,11,
"Delay and Price Differentiation in Cloud Computing: A Service Model,   Supporting Architectures, and Performance","  Many cloud service providers (CSPs) provide on-demand service at a price with a small delay. We propose a QoS-differentiated model where multiple SLAs deliver both on-demand service for latency-critical users and delayed services for delay-tolerant users at lower prices. Two architectures are considered to fulfill SLAs. The first is based on priority queues. The second simply separates servers into multiple modules, each for one SLA. As an ecosystem, we show that the proposed framework is dominant-strategy incentive compatible. Although the first architecture appears more prevalent in the literature, we prove the superiority of the second architecture, under which we further leverage queueing theory to determine the optimal SLA delays and prices. Finally, the viability of the proposed framework is validated through numerical comparison with the on-demand service and it exhibits a revenue improvement in excess of 200%. Our results can help CSPs design optimal delay-differentiated services and choose appropriate serving architectures. ","Xiaohu Wu, Francesco De Pellegrini and Giuliano Casale",,,11,
QPS-r: A Cost-Effective Crossbar Scheduling Algorithm and Its Stability   and Delay Analysis,"  In an input-queued switch, a crossbar schedule, or a matching between the input ports and the output ports needs to be computed in each switching cycle, or time slot. Designing switching algorithms with very low computational complexity, that lead to high throughput and small delay is a challenging problem. There appears to be a fundamental tradeoff between the computational complexity of the switching algorithm and the resultants throughput and delay. Parallel maximal matching algorithms (adapted for switching) appear to have stricken a sweet spot in this tradeoff, and prior work has shown the following performance guarantees. Using maximal matchings in every time slot results in at least 50% switch throughput and order-optimal (i.e., independent of the switch size N) average delay bounds for various traffic arrival processes. On the other hand, their computational complexity can be as low as $O(log^2N)$ per port/processor, which is much lower than those of the algorithms such as maximum weighted matching which ensures better throughput performance.   In this work, we propose QPS-r, a parallel iterative switching algorithm that has the lowest possible computational complexity: O(1) per port. Using Lyapunov stability analysis, we show that the throughput and delay performance is identical to that of maximal matching algorithm. Although QPS-r builds upon an existing technique called Queue-Proportional Sampling (QPS), in this paper, we provide analytical guarantees on its throughput and delay under i.i.d. traffic as well as a Markovian traffic model which can model many realistic traffic patterns. We also demonstrate that QPS-3 (running 3 iterations) has comparable empirical throughput and delay performances as iSLIP (running $log_2 N$ iterations), a refined and optimized representative maximal matching algorithm adapted for switching. ","Long Gong, Jun Xu, Liang Liu, Siva Theja Maguluri",,,11,
A Learned Performance Model for the Tensor Processing Unit,"  Accurate hardware performance models are critical to efficient code generation. They can be used by compilers to make heuristic decisions, by superoptimizers as an minimization objective, or by autotuners to find an optimal configuration of a specific program. However, they are difficult to develop because contemporary processors are complex, and the recent proliferation of deep learning accelerators has increased the development burden. We demonstrate a method of learning performance models from a corpus of tensor computation graph programs for the Tensor Processing Unit (TPU). We train a neural network over kernel-level sub-graphs from the corpus and find that the learned model is competitive to a heavily-optimized analytical cost model used in the production XLA compiler. ","Samuel J. Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou, and   Mike Burrows",,,11,
MultiCloud Resource Management using Apache Mesos for Planned   Integration with Apache Airavata,"  We discuss initial results and our planned approach for incorporating Apache Mesos based resource management that will enable design and development of scheduling strategies for Apache Airavata jobs so that they can be launched on multiple clouds, wherein several VMs do not have Public IP addresses. We present initial work and next steps on the design of a meta-scheduler using Apache Mesos. Apache Mesos presents a unified view of resources available across several clouds and clusters. Our meta-scheduler can potentially examine and identify the cases where multiple small jobs have been submitted by the same scientists and then redirect job from the same community account or user to different clusters. Our approach uses a NAT firewall to make nodes/VMs, without a Public IP, visible to Mesos for the unified view. ","Pankaj Saha, Madhusudhan Govindaraju, Suresh Marru, Marlon Pierce",,,11,
Product Forms for FCFS Queueing Models with Arbitrary Server-Job   Compatibilities: An Overview,"  In recent years a number of models involving different compatibilities between jobs and servers in queueing systems, or between agents and resources in matching systems, have been studied, and, under Markov assumptions and appropriate stability conditions, the stationary distributions have been shown to have product forms. We survey these results and show how, under an appropriate detailed description of the state, many are corollaries of similar results for the Order Independent Queue. We also discuss how to use the product form results to determine distributions for steady-state response times. ",Kristen Gardner and Rhonda Righter,,,11,