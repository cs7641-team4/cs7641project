title,abstract,authors,pdf_url,url,label1,label2
Blank Language Models,"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.","['Tianxiao Shen', 'Victor Quach', 'Regina Barzilay', 'Tommi Jaakkola']",https://www.aclweb.org/anthology/2020.emnlp-main.420.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.420/,2,4
Acrostic Poem Generation,"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.","['Rajat Agarwal', 'Katharina Kann']",https://www.aclweb.org/anthology/2020.emnlp-main.94.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.94/,2,
Unsupervised Adaptation of Question Answering Systems via Generative Self-training,"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.","['Steven Rennie', 'Etienne Marcheret', 'Neil Mallinar', 'David Nahamoo', 'Vaibhava Goel']",https://www.aclweb.org/anthology/2020.emnlp-main.87.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.87/,5,
An Imitation Game for Learning Semantic Parsers from User Interaction,"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at https://github.com/sunlab-osu/MISP.","['Ziyu Yao', 'Yiqi Tang', 'Wen-tau Yih', 'Huan Sun', 'Yu Su']",https://www.aclweb.org/anthology/2020.emnlp-main.559.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.559/,8,1
Conversational Semantic Parsing,"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which also set state-of-the-art in ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.","['Armen Aghajanyan', 'Jean Maillard', 'Akshat Shrivastava', 'Keith Diedrick', 'Michael Haeger', 'Haoran Li', 'Yashar Mehdad', 'Veselin Stoyanov', 'Anuj Kumar', 'Mike Lewis', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.408.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.408/,8,1
Probing Task-Oriented Dialogue Representation from Language Models,"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.","['Chien-Sheng Wu', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.409.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.409/,4,5
A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support,"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.","['Ashish Sharma', 'Adam Miner', 'David Atkins', 'Tim Althoff']",https://www.aclweb.org/anthology/2020.emnlp-main.425.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.425/,6,
A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses,"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents’ verbal and non-verbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available.","['Hisashi Kamezawa', 'Noriki Nishida', 'Nobuyuki Shimizu', 'Takashi Miyazaki', 'Hideki Nakayama']",https://www.aclweb.org/anthology/2020.emnlp-main.267.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.267/,6,9
ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,"Given questions regarding some prototypical situation — such as Name something that people usually do before they leave the house for work? — a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show – Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.","['Michael Boratko', 'Xiang Li', 'Tim O’Gorman', 'Rajarshi Das', 'Dan Le', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.85.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.85/,5,
Interpretation of NLP models through input marginalization,"To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.","['Siwon Kim', 'Jihun Yi', 'Eunji Kim', 'Sungroh Yoon']",https://www.aclweb.org/anthology/2020.emnlp-main.255.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.255/,4,10
Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies,We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.,"['Maryam Aminian', 'Mohammad Sadegh Rasooli', 'Mona Diab']",https://www.aclweb.org/anthology/2020.emnlp-main.663.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.663/,10,
The importance of fillers for text representations of speech transcripts,"While being an essential component of spoken language, fillers (e.g. “um” or “uh”) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two downstream tasks — predicting a speaker’s stance and expressed confidence.","['Tanvi Dinkar', 'Pierre Colombo', 'Matthieu Labeau', 'Chloé Clavel']",https://www.aclweb.org/anthology/2020.emnlp-main.641.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.641/,4,
Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation,"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.","['Ruibo Liu', 'Guangxuan Xu', 'Chenyan Jia', 'Weicheng Ma', 'Lili Wang', 'Soroush Vosoughi']",https://www.aclweb.org/anthology/2020.emnlp-main.726.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.726/,2,10
Improving Neural Topic Models using Knowledge Distillation,"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.","['Alexander Miserlis Hoyle', 'Pranav Goel', 'Philip Resnik']",https://www.aclweb.org/anthology/2020.emnlp-main.137.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.137/,3,
Multi-Unit Transformers for Neural Machine Translation,"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.","['Jianhao Yan', 'Fandong Meng', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.77.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.77/,2,
Weakly Supervised Subevent Knowledge Acquisition,"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.","['Wenlin Yao', 'Zeyu Dai', 'Maitreyi Ramaswamy', 'Bonan Min', 'Ruihong Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.430.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.430/,1,4
Structured Attention for Unsupervised Dialogue Structure Induction,"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.","['Liang Qiu', 'Yizhou Zhao', 'Weiyan Shi', 'Yuan Liang', 'Feng Shi', 'Tao Yuan', 'Zhou Yu', 'Song-chun Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.148.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.148/,1,4
Towards Enhancing Faithfulness for Neural Machine Translation,"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.","['Rongxiang Weng', 'Heng Yu', 'Xiangpeng Wei', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.212.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.212/,2,10
AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","['Taylor Shin', 'Yasaman Razeghi', 'Robert L. Logan IV', 'Eric Wallace', 'Sameer Singh']",https://www.aclweb.org/anthology/2020.emnlp-main.346.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.346/,3,4
Multi-resolution Annotations for Emoji Prediction,"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.","['Weicheng Ma', 'Ruibo Liu', 'Lili Wang', 'Soroush Vosoughi']",https://www.aclweb.org/anthology/2020.emnlp-main.542.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.542/,1,3
LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool,"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for “strong” cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target “weak” alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at https://github.com/google-research-datasets/lareqa.","['Uma Roy', 'Noah Constant', 'Rami Al-Rfou’', 'Aditya Barua', 'Aaron Phillips', 'Yinfei Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.477.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.477/,2,5
Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments,"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims.","['Sungho Jeon', 'Michael Strube']",https://www.aclweb.org/anthology/2020.emnlp-main.604.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.604/,1,
Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art,"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.","['Jing Lu', 'Vincent Ng']",https://www.aclweb.org/anthology/2020.emnlp-main.536.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.536/,4,
"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family – LXMERT – finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT’s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.","['Jaemin Cho', 'Jiasen Lu', 'Dustin Schwenk', 'Hannaneh Hajishirzi', 'Aniruddha Kembhavi']",https://www.aclweb.org/anthology/2020.emnlp-main.707.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.707/,2,4
HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media,"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.","['Hsin-Yu Chen', 'Cheng-Te Li']",https://www.aclweb.org/anthology/2020.emnlp-main.200.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.200/,4,7
Generationary or “How We Went beyond Word Sense Inventories and Learned to Gloss”,"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.","['Michele Bevilacqua', 'Marco Maru', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.585.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.585/,2,
Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks,"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient — when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.","['Trapit Bansal', 'Rishikesh Jha', 'Tsendsuren Munkhdalai', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.38.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.38/,6,
ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.","['Shiyue Zhang', 'Benjamin Frey', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.43.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.43/,2,10
Imitation Attacks and Defenses for Black-box Machine Translation Systems,"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary’s BLEU score and attack success rate at some cost in the defender’s BLEU and inference speed.","['Eric Wallace', 'Mitchell Stern', 'Dawn Song']",https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.446/,2,10
Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection,"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).","['Shaolei Wang', 'Zhongyuan Wang', 'Wanxiang Che', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.142.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.142/,4,
Semantic Label Smoothing for Sequence to Sequence Problems,"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.","['Michal Lukasik', 'Himanshu Jain', 'Aditya Menon', 'Seungyeon Kim', 'Srinadh Bhojanapalli', 'Felix Yu', 'Sanjiv Kumar']",https://www.aclweb.org/anthology/2020.emnlp-main.405.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.405/,2,
Modularized Transfomer-based Ranking Framework,"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.","['Luyu Gao', 'Zhuyun Dai', 'Jamie Callan']",https://www.aclweb.org/anthology/2020.emnlp-main.342.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.342/,8,
Accurate Word Alignment Induction from Neural Machine Translation,"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.","['Yun Chen', 'Yang Liu', 'Guanhua Chen', 'Xin Jiang', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.42.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.42/,4,
Multi-hop Inference for Question-driven Summarization,"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.","['Yang Deng', 'Wenxuan Zhang', 'Wai Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.547.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.547/,2,5
Scaling Hidden Markov Language Models,"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.","['Justin Chiu', 'Alexander M. Rush']",https://www.aclweb.org/anthology/2020.emnlp-main.103.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.103/,4,
Unsupervised Text Style Transfer with Padded Masked Language Models,"We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source–target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that Masker performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods’ accuracy by over 10 percentage points when pre-training them on silver training data generated by Masker.","['Eric Malmi', 'Aliaksei Severyn', 'Sascha Rothe']",https://www.aclweb.org/anthology/2020.emnlp-main.699.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.699/,3,10
MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification,"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.","['Qianli Ma', 'Zhenxi Lin', 'Jiangyue Yan', 'Zipeng Chen', 'Liuhong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.544.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.544/,4,
Let’s Stop Incorrect Comparisons in End-to-end Relation Extraction!,"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation. We then propose a small empirical study to quantify the most common mistake’s impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05. We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER. This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics. We finally call for unifying the evaluation setting in end-to-end RE.","['Bruno Taillé', 'Vincent Guigue', 'Geoffrey Scoutheeten', 'Patrick Gallinari']",https://www.aclweb.org/anthology/2020.emnlp-main.301.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.301/,4,8
Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages,"Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks. To address the issues, we propose a meta graph learning (MGL) method. Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages. Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer. Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.","['Zheng Li', 'Mukul Kumar', 'William Headden', 'Bing Yin', 'Ying Wei', 'Yu Zhang', 'Qiang Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.179.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.179/,2,10
A Preliminary Exploration of GANs for Keyphrase Generation,"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs). For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard benchmark datasets. We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques. Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models. To our knowledge, this is one of the first works that use GANs for keyphrase generation. We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.","['Avinash Swaminathan', 'Haimin Zhang', 'Debanjan Mahata', 'Rakesh Gosangi', 'Rajiv Shah', 'Amanda Stent']",https://www.aclweb.org/anthology/2020.emnlp-main.645.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.645/,2,4
Active Learning for BERT: An Empirical Study,"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.","['Liat Ein Dor', 'Alon Halfon', 'Ariel Gera', 'Eyal Shnarch', 'Lena Dankin', 'Leshem Choshen', 'Marina Danilevsky', 'Ranit Aharonov', 'Yoav Katz', 'Noam Slonim']",https://www.aclweb.org/anthology/2020.emnlp-main.638.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.638/,10,
Context-Aware Answer Extraction in Question Answering,"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases. To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task. With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases. We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.","['Yeon Seonwoo', 'Ji-Hoon Kim', 'Jung-Woo Ha', 'Alice Oh']",https://www.aclweb.org/anthology/2020.emnlp-main.189.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.189/,5,
Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach.","['Dong Zhang', 'Xincheng Ju', 'Junhui Li', 'Shoushan Li', 'Qiaoming Zhu', 'Guodong Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.291.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.291/,10,
Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing,"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.","['Piotr Szymański', 'Kyle Gorman']",https://www.aclweb.org/anthology/2020.emnlp-main.172.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.172/,9,10
HABERTOR: An Efficient and Effective Deep Hatespeech Detector,"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT’s architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.","['Thanh Tran', 'Yifan Hu', 'Changwei Hu', 'Kevin Yen', 'Fei Tan', 'Kyumin Lee', 'Se Rim Park']",https://www.aclweb.org/anthology/2020.emnlp-main.606.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.606/,3,4
Textual Data Augmentation for Efficient Active Learning on Tiny Datasets,"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.","['Husam Quteineh', 'Spyridon Samothrakis', 'Richard Sutcliffe']",https://www.aclweb.org/anthology/2020.emnlp-main.600.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.600/,2,3
Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction,"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven, composed, Ode to Joy>. In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases). Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.","['Patrick Hohenecker', 'Frank Mtumbuka', 'Vid Kocijan', 'Thomas Lukasiewicz']",https://www.aclweb.org/anthology/2020.emnlp-main.690.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.690/,8,
SRLGRN: Semantic Role Labeling Graph Reasoning Network,"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.","['Chen Zheng', 'Parisa Kordjamshidi']",https://www.aclweb.org/anthology/2020.emnlp-main.714.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.714/,3,5
Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT,"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.","['Akshay Smit', 'Saahil Jain', 'Pranav Rajpurkar', 'Anuj Pareek', 'Andrew Y. Ng', 'Matthew Lungren']",https://www.aclweb.org/anthology/2020.emnlp-main.117.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.117/,2,8
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods—including the quantity of gendered words, a dialogue safety classifier, and human assessments—all of which show that our models generate less gendered, but equally engaging chit-chat responses.","['Emily Dinan', 'Angela Fan', 'Adina Williams', 'Jack Urbanek', 'Douwe Kiela', 'Jason Weston']",https://www.aclweb.org/anthology/2020.emnlp-main.656.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.656/,7,
Re-evaluating Evaluation in Text Summarization,"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).","['Manik Bhandari', 'Pranav Narayan Gour', 'Atabak Ashfaq', 'Pengfei Liu', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.751.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.751/,2,10
An information theoretic view on selecting linguistic probes,"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier – or ”probe” – to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either ”the representation being rich in knowledge”, or ”the probe learning the task”, which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the ”good probe” criteria proposed by the two papers, *selectivity* (Hewitt and Liang, 2019) and *information gain* (Pimentel et al., 2020), are equivalent – the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.","['Zining Zhu', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.744.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.744/,1,
An Element-aware Multi-representation Model for Law Article Prediction,"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.","['Huilin Zhong', 'Junsheng Zhou', 'Weiguang Qu', 'Yunfei Long', 'Yanhui Gu']",https://www.aclweb.org/anthology/2020.emnlp-main.540.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.540/,4,10
Homophonic Pun Generation with Lexically Constrained Rewriting,"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.","['Zhiwei Yu', 'Hongyu Zang', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.229.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.229/,2,4
On Losses for Modern Language Models,"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP’s effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks – sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant – that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERTBase on the GLUE benchmark using fewer than a quarter of the training tokens.","['Stéphane Aroca-Ouellette', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.403.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.403/,4,10
Named Entity Recognition for Social Media Texts with Semantic Augmentation,"Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.","['Yuyang Nie', 'Yuanhe Tian', 'Xiang Wan', 'Yan Song', 'Bo Dai']",https://www.aclweb.org/anthology/2020.emnlp-main.107.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.107/,7,10
Effectively pretraining a speech translation decoder with Machine Translation data,"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.","['Ashkan Alinejad', 'Anoop Sarkar']",https://www.aclweb.org/anthology/2020.emnlp-main.644.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.644/,2,9
Hate-Speech and Offensive Language Detection in Roman Urdu,"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.","['Hammad Rizwan', 'Muhammad Haroon Shakeel', 'Asim Karim']",https://www.aclweb.org/anthology/2020.emnlp-main.197.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.197/,7,9
Are All Good Word Vector Spaces Isomorphic?,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).","['Ivan Vulić', 'Sebastian Ruder', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.257.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.257/,10,
A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation,"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.","['Ming Wang', 'Yinglin Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.504.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.504/,4,10
AmbigQA: Answering Ambiguous Open-domain Questions,"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.","['Sewon Min', 'Julian Michael', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.466.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.466/,3,5
Text Graph Transformer for Document Classification,"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.","['Haopeng Zhang', 'Jiawei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.668.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.668/,4,
Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.","['Yuncheng Hua', 'Yuan-Fang Li', 'Gholamreza Haffari', 'Guilin Qi', 'Tongtong Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.469.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.469/,5,
Where Are You? Localization from Embodied Dialog,"We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans – an Observer and a Locator – complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task – providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer’s location within 3m in unseen buildings, vs. 70.4% for human Locators.","['Meera Hahn', 'Jacob Krantz', 'Dhruv Batra', 'Devi Parikh', 'James Rehg', 'Stefan Lee', 'Peter Anderson']",https://www.aclweb.org/anthology/2020.emnlp-main.59.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.59/,5,
Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product,"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github.com/jd-aig/JAVE.","['Tiangang Zhu', 'Yue Wang', 'Haoran Li', 'Youzheng Wu', 'Xiaodong He', 'Bowen Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.166.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.166/,6,8
Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets,"Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.","['Nedjma Ousidhoum', 'Yangqiu Song', 'Dit-Yan Yeung']",https://www.aclweb.org/anthology/2020.emnlp-main.199.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.199/,3,9
Selection and Generation: Learning towards Multi-Product Advertisement Post Generation,"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic. A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products. Hence, multi-product AD post generation is meaningful and important. We propose a novel end-to-end model named S-MG Net to generate the AD post. Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network). (2) generate a post including selected products via the MGenNet (Multi-Generator Network). Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products. Then, MGenNet generates the description copywriting of each product. Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.","['Zhangming Chan', 'Yuchi Zhang', 'Xiuying Chen', 'Shen Gao', 'Zhiqiang Zhang', 'Dongyan Zhao', 'Rui Yan']",https://www.aclweb.org/anthology/2020.emnlp-main.313.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.313/,2,3
DualTKB: A Dual Learning Bridge between Text and Knowledge Base,"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.","['Pierre Dognin', 'Igor Melnyk', 'Inkit Padhi', 'Cicero dos Santos', 'Payel Das']",https://www.aclweb.org/anthology/2020.emnlp-main.694.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.694/,10,
Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling,"In recent years, there has been an increasing interest in the application of Artificial Intelligence – and especially Machine Learning – to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic UserPerceived Value classification task. We release Stories2Insights, an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.","['Costanza Conforti', 'Stephanie Hirmer', 'Dai Morgan', 'Marco Basaldella', 'Yau Ben Or']",https://www.aclweb.org/anthology/2020.emnlp-main.677.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.677/,4,6
Pre-Training Transformers as Energy-Based Cloze Models,"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.","['Kevin Clark', 'Minh-Thang Luong', 'Quoc Le', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.20.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.20/,9,
Multi-view Story Characterization from Movie Plot Synopses and Reviews,"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events). Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses. Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision. We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.","['Sudipta Kar', 'Gustavo Aguilar', 'Mirella Lapata', 'Thamar Solorio']",https://www.aclweb.org/anthology/2020.emnlp-main.454.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.454/,3,
SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction,"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.","['Xuming Hu', 'Lijie Wen', 'Yusong Xu', 'Chenwei Zhang', 'Philip S. Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.299.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.299/,8,
XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization,"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.","['Alessandro Raganato', 'Tommaso Pasini', 'Jose Camacho-Collados', 'Mohammad Taher Pilehvar']",https://www.aclweb.org/anthology/2020.emnlp-main.584.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.584/,2,10
Beyond [CLS] through Ranking by Generation,"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.","['Cicero dos Santos', 'Xiaofei Ma', 'Ramesh Nallapati', 'Zhiheng Huang', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.134.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.134/,5,8
Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations,"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.","['Arturo Oncevay', 'Barry Haddow', 'Alexandra Birch']",https://www.aclweb.org/anthology/2020.emnlp-main.187.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.187/,1,2
MAVEN: A Massive General Domain Event Detection Dataset,"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.","['Xiaozhi Wang', 'Ziqi Wang', 'Xu Han', 'Wangyi Jiang', 'Rong Han', 'Zhiyuan Liu', 'Juanzi Li', 'Peng Li', 'Yankai Lin', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.129.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.129/,6,10
Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(nˆ6) down to O(nˆ3),"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.",['Caio Corro'],https://www.aclweb.org/anthology/2020.emnlp-main.219.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.219/,1,
Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models,"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC). Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines. As a result, a high Kappa score of 61% is achieved for inter-annotator agreement. Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2). Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.","['Changmao Li', 'Elaine Fisher', 'Rebecca Thomas', 'Steve Pittard', 'Vicki Hertzberg', 'Jinho D. Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.679.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.679/,4,6
Multi-Fact Correction in Abstractive Text Summarization,"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.","['Yue Dong', 'Shuohang Wang', 'Zhe Gan', 'Yu Cheng', 'Jackie Chi Kit Cheung', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.749.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.749/,2,5
Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging,"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model. Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.","['Semih Yavuz', 'Kazuma Hashimoto', 'Wenhao Liu', 'Nitish Shirish Keskar', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.412.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.412/,6,4
"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three","Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs. However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available. In this paper, we study ensemble distillation as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles. We validate this framework on two tasks: named-entity recognition and machine translation. We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.","['Steven Reich', 'David Mueller', 'Nicholas Andrews']",https://www.aclweb.org/anthology/2020.emnlp-main.450.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.450/,2,
SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery,"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have a similar likelihood of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities’ infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.","['Jiaming Shen', 'Wenda Qiu', 'Jingbo Shang', 'Michelle Vanni', 'Xiang Ren', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.666.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.666/,1,
Toward Micro-Dialect Identification in Diaglossic and Code-Switched Environments,"Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message. For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models. To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT predicts micro-dialects with 9.9% F1, 76 better than a majority class baseline. Our new language model also establishes new state-of-the-art on several external tasks.","['Muhammad Abdul-Mageed', 'Chiyu Zhang', 'AbdelRahim Elmadany', 'Lyle Ungar']",https://www.aclweb.org/anthology/2020.emnlp-main.472.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.472/,1,6
Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.","['Reina Akama', 'Sho Yokoi', 'Jun Suzuki', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.68.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.68/,1,2
Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training,"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.","['Hai Ye', 'Qingyu Tan', 'Ruidan He', 'Juntao Li', 'Hwee Tou Ng', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.599.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.599/,2,
BERT-enhanced Relational Sentence Ordering Network,"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences. Extensive evaluations are conducted on six public datasets. The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.","['Baiyun Cui', 'Yingming Li', 'Zhongfei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.511.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.511/,10,
Writing Strategies for Science Communication: Data and Computational Analysis,"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies’ use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.","['Tal August', 'Lauren Kim', 'Katharina Reinecke', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.429.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.429/,2,
Text Classification Using Label Names Only: A Language Model Self-Training Approach,"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.","['Yu Meng', 'Yunyi Zhang', 'Jiaxin Huang', 'Chenyan Xiong', 'Heng Ji', 'Chao Zhang', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.724.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.724/,1,3
Zero-Shot Cross-Lingual Transfer with Meta Learning,"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.","['Farhad Nooralahzadeh', 'Giannis Bekoulis', 'Johannes Bjerva', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.368.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.368/,2,3
HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction,"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity. To address this issue, we present a novel nested NER model named HIT. Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary. Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary. Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.","['Yu Wang', 'Yun Li', 'Hanghang Tong', 'Ziye Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.486.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.486/,4,
Training Question Answering Models From Synthetic Data,"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.","['Raul Puri', 'Ryan Spring', 'Mohammad Shoeybi', 'Mostofa Patwary', 'Bryan Catanzaro']",https://www.aclweb.org/anthology/2020.emnlp-main.468.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.468/,2,5
Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models,"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.","['Isabel Papadimitriou', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.554.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.554/,1,
On the Sentence Embeddings from Pre-trained Language Models,"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.","['Bohan Li', 'Hao Zhou', 'Junxian He', 'Mingxuan Wang', 'Yiming Yang', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.733.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.733/,4,10
Biomedical Event Extraction as Sequence Labeling,"We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL’s speed and accuracy makes it a viable approach for large-scale real-world scenarios.","['Alan Ramponi', 'Rob van der Goot', 'Rosario Lombardo', 'Barbara Plank']",https://www.aclweb.org/anthology/2020.emnlp-main.431.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.431/,7,8
KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework.","['Wenhu Chen', 'Yu Su', 'Xifeng Yan', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.697.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.697/,2,6
Structure Aware Negative Sampling in Knowledge Graphs,"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.","['Kian Ahrabian', 'Aarash Feizi', 'Yasmin Salehi', 'William L. Hamilton', 'Avishek Joey Bose']",https://www.aclweb.org/anthology/2020.emnlp-main.492.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.492/,4,
Autoregressive Knowledge Distillation through Imitation Learning,"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.","['Alexander Lin', 'Jeremy Wohlwend', 'Howard Chen', 'Tao Lei']",https://www.aclweb.org/anthology/2020.emnlp-main.494.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.494/,2,
NwQM: A neural quality assessment framework for Wikipedia,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.","['Bhanu Prakash Reddy Guda', 'Sasi Bhushan Seelaboyina', 'Soumya Sarkar', 'Animesh Mukherjee']",https://www.aclweb.org/anthology/2020.emnlp-main.674.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.674/,3,
SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness,"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.","['Nathan Ng', 'Kyunghyun Cho', 'Marzyeh Ghassemi']",https://www.aclweb.org/anthology/2020.emnlp-main.97.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.97/,4,10
On the Sparsity of Neural Machine Translation Models,"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.","['Yong Wang', 'Longyue Wang', 'Victor Li', 'Zhaopeng Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.78.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.78/,2,10
The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures,"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.","['Haim Dubossarsky', 'Ivan Vulić', 'Roi Reichart', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.186.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.186/,2,
An Unsupervised Sentence Embedding Method by Mutual Information Maximization,"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.","['Yan Zhang', 'Ruidan He', 'Zuozhu Liu', 'Kwan Hui Lim', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.124.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.124/,6,
Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations,"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.","['Kai Sun', 'Richong Zhang', 'Samuel Mensah', 'Yongyi Mao', 'Xudong Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.304.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.304/,8,
BLEU might be Guilty but References are not Innocent,"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.","['Markus Freitag', 'David Grangier', 'Isaac Caswell']",https://www.aclweb.org/anthology/2020.emnlp-main.5.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.5/,2,
Profile Consistency Identification for Open-domain Dialogue Agents,"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.","['Haoyu Song', 'Yan Wang', 'Weinan Zhang', 'Zhengyu Zhao', 'Ting Liu', 'Xiaojiang Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.539.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.539/,10,
Annotating Temporal Dependency Graphs via Crowdsourcing,"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good trade-off between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. The data set is publicly available.","['Jiarui Yao', 'Haoling Qiu', 'Bonan Min', 'Nianwen Xue']",https://www.aclweb.org/anthology/2020.emnlp-main.432.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.432/,10,
DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,"Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.","['Bosheng Ding', 'Linlin Liu', 'Lidong Bing', 'Canasai Kruengkrai', 'Thien Hai Nguyen', 'Shafiq Joty', 'Luo Si', 'Chunyan Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.488.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.488/,3,4
Shallow-to-Deep Training for Neural Machine Translation,"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/SDT-Training.","['Bei Li', 'Ziyang Wang', 'Hui Liu', 'Yufan Jiang', 'Quan Du', 'Tong Xiao', 'Huizhen Wang', 'Jingbo Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.72.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.72/,2,
Reading Between the Lines: Exploring Infilling in Visual Narratives,"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.","['Khyathi Raghavi Chandu', 'Ruo-Ping Dong', 'Alan W. Black']",https://www.aclweb.org/anthology/2020.emnlp-main.93.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.93/,2,
Predicting Clinical Trial Results by Implicit Evidence Integration,"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.","['Qiao Jin', 'Chuanqi Tan', 'Mosha Chen', 'Xiaozhong Liu', 'Songfang Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.114.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.114/,6,
Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.","['Jianguo Zhang', 'Kazuma Hashimoto', 'Wenhao Liu', 'Chien-Sheng Wu', 'Yao Wan', 'Philip S. Yu', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.411.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.411/,8,
An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training,"Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.","['Kristjan Arumae', 'Qing Sun', 'Parminder Bhatia']",https://www.aclweb.org/anthology/2020.emnlp-main.394.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.394/,8,
Statistical Power and Translationese in Machine Translation Evaluation,"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.","['Yvette Graham', 'Barry Haddow', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.6.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.6/,2,4
Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks,"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy","['Shubham Toshniwal', 'Sam Wiseman', 'Allyson Ettinger', 'Karen Livescu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.685.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.685/,6,10
DGST: a Dual-Generator Network for Text Style Transfer,"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.","['Xiao Li', 'Guanyi Chen', 'Chenghua Lin', 'Ruizhe Li']",https://www.aclweb.org/anthology/2020.emnlp-main.578.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.578/,2,
An Empirical Study of Generation Order for Machine Translation,"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT’14 English → German and WMT’18 English → Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English → German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.","['William Chan', 'Mitchell Stern', 'Jamie Kiros', 'Jakob Uszkoreit']",https://www.aclweb.org/anthology/2020.emnlp-main.464.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.464/,2,
DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion,"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.","['Zhen Han', 'Peng Chen', 'Yunpu Ma', 'Volker Tresp']",https://www.aclweb.org/anthology/2020.emnlp-main.593.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.593/,4,
Translation Quality Estimation by Jointly Learning to Score and Rank,"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.","['Jingyi Zhang', 'Josef van Genabith']",https://www.aclweb.org/anthology/2020.emnlp-main.205.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.205/,2,10
Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations,"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations. We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets. We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks. Our results indicate a significant improvement over the application of the dense word representations.",['Gábor Berend'],https://www.aclweb.org/anthology/2020.emnlp-main.683.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.683/,6,9
Counterfactual Off-Policy Training for Neural Dialogue Generation,"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.","['Qingfu Zhu', 'Weinan Zhang', 'Ting Liu', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.276.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.276/,2,
"If beam search is the answer, what was the question?","Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","['Clara Meister', 'Ryan Cotterell', 'Tim Vieira']",https://www.aclweb.org/anthology/2020.emnlp-main.170.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.170/,2,5
PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.","['Xinyu Hua', 'Lu Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.57.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.57/,2,10
Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning,"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.","['Lianhui Qin', 'Vered Shwartz', 'Peter West', 'Chandra Bhagavatula', 'Jena D. Hwang', 'Ronan Le Bras', 'Antoine Bosselut', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.58.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.58/,2,10
Compositional Phrase Alignment and Beyond,"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.","['Yuki Arase', 'Jun’ichi Tsujii']",https://www.aclweb.org/anthology/2020.emnlp-main.125.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.125/,8,4
Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data,"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.","['Shachar Rosenman', 'Alon Jacovi', 'Yoav Goldberg']",https://www.aclweb.org/anthology/2020.emnlp-main.302.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.302/,5,8
Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.","['Rujun Han', 'Yichao Zhou', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.461.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.461/,4,8
Enhancing Aspect Term Extraction with Soft Prototypes,"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.","['Zhuang Chen', 'Tieyun Qian']",https://www.aclweb.org/anthology/2020.emnlp-main.164.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.164/,1,8
Extracting Implicitly Asserted Propositions in Argumentation,"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.","['Yohan Jo', 'Jacky Visser', 'Chris Reed', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.2.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.2/,1,3
Learning to Pronounce Chinese Without a Pronunciation Dictionary,"We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.","['Christopher Chu', 'Scot Fang', 'Kevin Knight']",https://www.aclweb.org/anthology/2020.emnlp-main.458.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.458/,9,
Cross Copy Network for Dialogue Generation,"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances’ logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.","['Changzhen Ji', 'Xin Zhou', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Conghui Zhu', 'Tiejun Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.149.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.149/,2,
MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.","['Jonas Pfeiffer', 'Ivan Vulić', 'Iryna Gurevych', 'Sebastian Ruder']",https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.617/,2,5
Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding,"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and information elicitation in such settings, but has been limited to manual review of small corpora. We introduce **Interview**—a large-scale (105K conversations) media dialog dataset collected from news interview transcripts—which allows us to investigate such patterns at scale. We present a dialog model that leverages external knowledge as well as dialog acts via auxiliary losses and demonstrate that our model quantitatively and qualitatively outperforms strong discourse-agnostic baselines for dialog modeling—generating more specific and topical responses in interview-style conversations.","['Bodhisattwa Prasad Majumder', 'Shuyang Li', 'Jianmo Ni', 'Julian McAuley']",https://www.aclweb.org/anthology/2020.emnlp-main.653.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.653/,1,3
Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.","['Nayu Liu', 'Xian Sun', 'Hongfeng Yu', 'Wenkai Zhang', 'Guangluan Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.144.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.144/,2,4
Supervised Seeded Iterated Learning for Interactive Language Learning,"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of in the language-drift translation game.","['Yuchen Lu', 'Soumye Singhal', 'Florian Strub', 'Olivier Pietquin', 'Aaron Courville']",https://www.aclweb.org/anthology/2020.emnlp-main.325.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.325/,2,5
Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization,"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.","['Zhenjie Zhao', 'Evangelos Papalexakis', 'Xiaojuan Ma']",https://www.aclweb.org/anthology/2020.emnlp-main.266.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.266/,4,8
Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact,"Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the tree model. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.",['Yugo Murawaki'],https://www.aclweb.org/anthology/2020.emnlp-main.69.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.69/,4,6
End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems,"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.","['Siamak Shakeri', 'Cicero dos Santos', 'Henghui Zhu', 'Patrick Ng', 'Feng Nan', 'Zhiguo Wang', 'Ramesh Nallapati', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.439.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.439/,2,5
Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise.","['Xiaoan Ding', 'Tianyu Liu', 'Baobao Chang', 'Zhifang Sui', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.657.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.657/,4,
Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions,"The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.","['Ritam Dutt', 'Rishabh Joshi', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.605.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.605/,4,
On the weak link between importance and prunability of attention heads,"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.","['Aakriti Budhraja', 'Madhura Pande', 'Preksha Nema', 'Pratyush Kumar', 'Mitesh M. Khapra']",https://www.aclweb.org/anthology/2020.emnlp-main.260.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.260/,4,10
Detecting Attackable Sentences in Arguments,"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.","['Yohan Jo', 'Seojin Bang', 'Emaad Manzoor', 'Eduard Hovy', 'Chris Reed']",https://www.aclweb.org/anthology/2020.emnlp-main.1.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.1/,3,4
Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity,"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user’s pre-existing knowledge. Assuming a correlation between engagement and user responses such as “liking” messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user’s prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a bert content model by 13 mean reciprocal rank points.","['Pedro Rodriguez', 'Paul A. Crook', 'Seungwhan Moon', 'Zhiguang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.655.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.655/,3,5
Personal Information Leakage Detection in Conversations,"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants. The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation. In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems. The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model. In those cases, a significant number of information leaking utterances can be detected by our models with high precision.","['Qiongkai Xu', 'Lizhen Qu', 'Zeyu Gao', 'Gholamreza Haffari']",https://www.aclweb.org/anthology/2020.emnlp-main.532.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.532/,4,10
Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models,"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.","['Pierangelo Lombardo', 'Alessio Boiardi', 'Luca Colombo', 'Angelo Schiavone', 'Nicolò Tamagnone']",https://www.aclweb.org/anthology/2020.emnlp-main.249.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.249/,6,10
Entities as Experts: Sparse Memory Access with Entity Supervision,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EaE’s entity representations are learned directly from text. We show that EaE’s learned representations capture sufficient knowledge to answer TriviaQA questions such as “Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?”, outperforming an encoder-generator Transformer model with 10x the parameters on this task. According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE’s performance.","['Thibault Févry', 'Livio Baldini Soares', 'Nicholas Fitzgerald', 'Eunsol Choi', 'Tom Kwiatkowski']",https://www.aclweb.org/anthology/2020.emnlp-main.400.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.400/,5,
Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis,"The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like “nothing special”. Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation “food-was” is treated equally as an adjectival complement relation “was-okay” in “food was okay”. To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs. Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information. Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs. Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs. Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.","['Mi Zhang', 'Tieyun Qian']",https://www.aclweb.org/anthology/2020.emnlp-main.286.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.286/,1,3
BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.","['Canwen Xu', 'Wangchunshu Zhou', 'Tao Ge', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.633.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.633/,6,4
Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example—the model’s confidence in the true class, and the variability of this confidence across epochs—obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of “ambiguous” regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are “easy to learn” for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds “hard to learn”; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","['Swabha Swayamdipta', 'Roy Schwartz', 'Nicholas Lourie', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Noah A. Smith', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.746.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.746/,10,6
Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems,"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.","['Jindřich Libovický', 'Alexander Fraser']",https://www.aclweb.org/anthology/2020.emnlp-main.203.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.203/,2,
Asking without Telling: Exploring Latent Ontologies in Contextual Representations,"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.","['Julian Michael', 'Jan A. Botha', 'Ian Tenney']",https://www.aclweb.org/anthology/2020.emnlp-main.552.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.552/,1,3
Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent,"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners’ performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent’s performance. To enable the agent to behave like a learner, we leverage entailment modeling’s capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning.","['Yun-Hsuan Jen', 'Chieh-Yang Huang', 'MeiHua Chen', 'Ting-Hao Huang', 'Lun-Wei Ku']",https://www.aclweb.org/anthology/2020.emnlp-main.312.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.312/,6,
Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.","['Amir Pouran Ben Veyseh', 'Nasim Nouri', 'Franck Dernoncourt', 'Dejing Dou', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.719.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.719/,1,
Towards More Accurate Uncertainty Estimation In Text Classification,"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as “mix-up”, “self-ensembling”, “distinctiveness score”, is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.","['Jianfeng He', 'Xuchao Zhang', 'Shuo Lei', 'Zhiqian Chen', 'Fanglan Chen', 'Abdulaziz Alhamadani', 'Bei Xiao', 'ChangTien Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.671.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.671/,4,
Iterative Domain-Repaired Back-Translation,"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.","['Hao-Ran Wei', 'Zhirui Zhang', 'Boxing Chen', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.474.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.474/,2,
Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations,"The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.","['Jianfei Yu', 'Jing Jiang', 'Ling Min Serena Khoo', 'Hai Leong Chieu', 'Rui Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.108.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.108/,7,
Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts. We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.","['Jiaxin Huang', 'Yu Meng', 'Fang Guo', 'Heng Ji', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.568.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.568/,3,
Retrofitting Structure-aware Transformer Language Model for End Tasks,"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.","['Hao Fei', 'Yafeng Ren', 'Donghong Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.168.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.168/,4,
Can Automatic Post-Editing Improve NMT?,"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.","['Shamil Chollampatt', 'Raymond Hendy Susanto', 'Liling Tan', 'Ewa Szymanska']",https://www.aclweb.org/anthology/2020.emnlp-main.217.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.217/,2,
Multi-Dimensional Gender Bias Classification,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.","['Emily Dinan', 'Angela Fan', 'Ledell Wu', 'Jason Weston', 'Douwe Kiela', 'Adina Williams']",https://www.aclweb.org/anthology/2020.emnlp-main.23.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.23/,7,
Experience Grounds Language,"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.","['Yonatan Bisk', 'Ari Holtzman', 'Jesse Thomason', 'Jacob Andreas', 'Yoshua Bengio', 'Joyce Chai', 'Mirella Lapata', 'Angeliki Lazaridou', 'Jonathan May', 'Aleksandr Nisnevich', 'Nicolas Pinto', 'Joseph Turian']",https://www.aclweb.org/anthology/2020.emnlp-main.703.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.703/,1,
A Method for Building a Commonsense Inference Dataset based on Basic Events,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a high-performance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.","['Kazumasa Omura', 'Daisuke Kawahara', 'Sadao Kurohashi']",https://www.aclweb.org/anthology/2020.emnlp-main.192.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.192/,10,
Fortifying Toxic Speech Detectors Against Veiled Toxicity,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.","['Xiaochuang Han', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.622.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.622/,7,
Revealing the Myth of Higher-Order Inference in Coreference Resolution,"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an end-to-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.","['Liyan Xu', 'Jinho D. Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.686.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.686/,1,
Semantic Role Labeling as Syntactic Dependency Parsing,"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.","['Tianze Shi', 'Igor Malioutov', 'Ozan İrsoy']",https://www.aclweb.org/anthology/2020.emnlp-main.610.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.610/,1,
"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40,000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.","['AmirAli Bagher Zadeh', 'Yansheng Cao', 'Simon Hessner', 'Paul Pu Liang', 'Soujanya Poria', 'Louis-Philippe Morency']",https://www.aclweb.org/anthology/2020.emnlp-main.141.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.141/,10,
Which *BERT? A Survey Organizing Contextualized Encoders,"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.","['Patrick Xia', 'Shijie Wu', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.608.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.608/,4,
Adversarial Attack and Defense of Structured Prediction Models,"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.","['Wenjuan Han', 'Liwen Zhang', 'Yong Jiang', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.182.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.182/,4,
An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks,"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.","['Lifu Tu', 'Tianyu Liu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.449.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.449/,4,
COD3S: Diverse Generation with Discrete Semantic Signatures,"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.","['Nathaniel Weir', 'João Sedoc', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.421.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.421/,2,
Benchmarking Meaning Representations in Neural Semantic Parsing,"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work’s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose , a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets × four meaning representations. A thorough experimental study on Unimer reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.","['Jiaqi Guo', 'Qian Liu', 'Jian-Guang Lou', 'Zhenwen Li', 'Xueqing Liu', 'Tao Xie', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.118.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.118/,1,
From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers,"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.","['Anne Lauscher', 'Vinit Ravishankar', 'Ivan Vulić', 'Goran Glavaš']",https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.363/,2,
Improving AMR Parsing with Sequence-to-Sequence Pre-training,"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.","['Dongqin Xu', 'Junhui Li', 'Muhua Zhu', 'Min Zhang', 'Guodong Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.196.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.196/,8,
CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","['Nikita Nangia', 'Clara Vania', 'Rasika Bhalerao', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.154.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.154/,10,
DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset,"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.","['Lijie Wang', 'Ao Zhang', 'Kun Wu', 'Ke Sun', 'Zhenghua Li', 'Hua Wu', 'Min Zhang', 'Haifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.562.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.562/,10,
Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays,"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.","['Wei Song', 'Ziyao Song', 'Ruiji Fu', 'Lizhen Liu', 'Miaomiao Cheng', 'Ting Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.225.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.225/,3,
Causal Inference of Script Knowledge,"When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.","['Noah Weber', 'Rachel Rudinger', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.612.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.612/,8,
Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble,"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as “black boxes”. We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.","['Peerat Limkonchotiwat', 'Wannaphong Phatthiyaphaibun', 'Raheem Sarwar', 'Ekapol Chuangsuwanich', 'Sarana Nutanong']",https://www.aclweb.org/anthology/2020.emnlp-main.315.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.315/,4,
Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling,"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by ‘composing’ word representations of the first and last words in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.","['Diego Marcheggiani', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.322.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.322/,1,
Template Guided Text Generation for Task-Oriented Dialogue,"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.","['Mihir Kale', 'Abhinav Rastogi']",https://www.aclweb.org/anthology/2020.emnlp-main.527.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.527/,2,
An Analysis of Natural Language Inference Benchmarks through the Lens of Negation,"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.","['Md Mosharaf Hossain', 'Venelin Kovatchev', 'Pranoy Dutta', 'Tiffany Kao', 'Elizabeth Wei', 'Eduardo Blanco']",https://www.aclweb.org/anthology/2020.emnlp-main.732.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.732/,4,
Pareto Probing: Trading Off Accuracy for Complexity,"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe’s performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.","['Tiago Pimentel', 'Naomi Saphra', 'Adina Williams', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.254.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.254/,4,
Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder,"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.","['Wenyue Zhang', 'Xiaoli Li', 'Yang Li', 'Suge Wang', 'Deyu Li', 'Jian Liao', 'Jianxing Zheng']",https://www.aclweb.org/anthology/2020.emnlp-main.307.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.307/,3,
Analyzing Individual Neurons in Pre-trained Language Models,"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.","['Nadir Durrani', 'Hassan Sajjad', 'Fahim Dalvi', 'Yonatan Belinkov']",https://www.aclweb.org/anthology/2020.emnlp-main.395.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.395/,4,
End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning,"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.","['Zixiang Ding', 'Rui Xia', 'Jianfei Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.290.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.290/,3,
Simulated multiple reference training improves low-resource machine translation,"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser’s distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation.","['Huda Khayrallah', 'Brian Thompson', 'Matt Post', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.7.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.7/,2,
A Spectral Method for Unsupervised Multi-Document Summarization,"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.","['Kexiang Wang', 'Baobao Chang', 'Zhifang Sui']",https://www.aclweb.org/anthology/2020.emnlp-main.32.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.32/,2,
Efficient One-Pass End-to-End Entity Linking for Questions,"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.","['Belinda Z. Li', 'Sewon Min', 'Srinivasan Iyer', 'Yashar Mehdad', 'Wen-tau Yih']",https://www.aclweb.org/anthology/2020.emnlp-main.522.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.522/,5,
Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness,"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.","['Stefan Larson', 'Anthony Zheng', 'Anish Mahendran', 'Rishi Tekriwal', 'Adrian Cheung', 'Eric Guldan', 'Kevin Leach', 'Jonathan K. Kummerfeld']",https://www.aclweb.org/anthology/2020.emnlp-main.650.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.650/,4,
Scene Restoring for Narrative Machine Reading Comprehension,"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.","['Zhixing Tian', 'Yuanzhe Zhang', 'Kang Liu', 'Jun Zhao', 'Yantao Jia', 'Zhicheng Sheng']",https://www.aclweb.org/anthology/2020.emnlp-main.247.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.247/,5,
Dissecting Span Identification Tasks with Performance Prediction,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks’ properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.","['Sean Papay', 'Roman Klinger', 'Sebastian Padó']",https://www.aclweb.org/anthology/2020.emnlp-main.396.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.396/,1,
Pretrained Language Model Embryology: The Birth of ALBERT,"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.","['Cheng-Han Chiang', 'Sung-Feng Huang', 'Hung-Yi Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.553.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.553/,4,
Investigating representations of verb bias in neural language models,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker’s choice of construction is known to depend on multiple factors, including the choice of main verb – a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.","['Robert Hawkins', 'Takateru Yamakoshi', 'Thomas L. Griffiths', 'Adele Goldberg']",https://www.aclweb.org/anthology/2020.emnlp-main.376.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.376/,1,
Neural Deepfake Detection with Factual Structure of Text,"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.","['Wanjun Zhong', 'Duyu Tang', 'Zenan Xu', 'Ruize Wang', 'Nan Duan', 'Ming Zhou', 'Jiahai Wang', 'Jian Yin']",https://www.aclweb.org/anthology/2020.emnlp-main.193.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.193/,6,
Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection,"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies. In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments. We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection. Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.","['Jingfeng Yang', 'Diyi Yang', 'Zhaoran Ma']",https://www.aclweb.org/anthology/2020.emnlp-main.113.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.113/,2,
Semantic Evaluation for Text-to-SQL with Distilled Test Suites,"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.","['Ruiqi Zhong', 'Tao Yu', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.29.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.29/,1,
Modeling Content Importance for Summarization with Pre-trained Language Models,"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences. Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.","['Liqiang Xiao', 'Lu Wang', 'Hao He', 'Yaohui Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.293.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.293/,2,
EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering,"We propose EXAMS – a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.EXAMS offers unique fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of the proposed models. We perform various experiments with existing top-performing multilingual pre-trained models and show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible by now. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.","['Momchil Hardalov', 'Todor Mihaylov', 'Dimitrina Zlatkova', 'Yoan Dinkov', 'Ivan Koychev', 'Preslav Nakov']",https://www.aclweb.org/anthology/2020.emnlp-main.438.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.438/,5,
Unsupervised Commonsense Question Answering with Self-Talk,"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as “what is the definition of...” to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.","['Vered Shwartz', 'Peter West', 'Ronan Le Bras', 'Chandra Bhagavatula', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.373.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.373/,5,
The Grammar of Emergent Languages,"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.","['Oskar van der Wal', 'Silvan de Boer', 'Elia Bruni', 'Dieuwke Hupkes']",https://www.aclweb.org/anthology/2020.emnlp-main.270.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.270/,1,
An Empirical Investigation of Contextualized Number Prediction,"We conduct a large scale empirical investigation of contextualized number prediction in running text. Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value within a sentence. We experiment with novel combinations of contextual encoders and output distributions over the real number line. Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures. We evaluate these models on two numeric datasets in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection. We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.","['Taylor Berg-Kirkpatrick', 'Daniel Spokoyny']",https://www.aclweb.org/anthology/2020.emnlp-main.385.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.385/,6,
Direct Segmentation Models for Streaming Speech Translation,"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account. This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk. An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.","['Javier Iranzo-Sánchez', 'Adrià Giménez Pastor', 'Joan Albert Silvestre-Cerdà', 'Pau Baquero-Arnal', 'Jorge Civera Saiz', 'Alfons Juan']",https://www.aclweb.org/anthology/2020.emnlp-main.206.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.206/,9,
Generating Fact Checking Briefs,"Fact checking at scale is difficult—while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs — in particular QABriefs — increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.","['Angela Fan', 'Aleksandra Piktus', 'Fabio Petroni', 'Guillaume Wenzek', 'Marzieh Saeidi', 'Andreas Vlachos', 'Antoine Bordes', 'Sebastian Riedel']",https://www.aclweb.org/anthology/2020.emnlp-main.580.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.580/,2,
Keep it Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit,"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.","['Amrith Krishna', 'Ashim Gupta', 'Deepak Garasangi', 'Pavankumar Satuluri', 'Pawan Goyal']",https://www.aclweb.org/anthology/2020.emnlp-main.388.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.388/,1,
Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders,"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.","['Jue Wang', 'Wei Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.133.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.133/,1,
"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions","We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.","['Xiang Zhou', 'Yixin Nie', 'Hao Tan', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.659.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.659/,10,
Knowledge Graph Alignment with Entity-Pair Embedding,"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations. Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.","['Zhichun Wang', 'Jinjian Yang', 'Xiaoju Ye']",https://www.aclweb.org/anthology/2020.emnlp-main.130.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.130/,8,
Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model,"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.","['Bugeun Kim', 'Kyung Seo Ki', 'Donggeon Lee', 'Gahgene Gweon']",https://www.aclweb.org/anthology/2020.emnlp-main.308.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.308/,6,
Unsupervised Discovery of Implicit Gender Bias,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.","['Anjalie Field', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.44.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.44/,7,
Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions,"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","['Arya D. McCarthy', 'Adina Williams', 'Shijia Liu', 'David Yarowsky', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.456.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.456/,1,
GLUCOSE: GeneraLized and COntextualized Story Explanations,"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of ~670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE’s rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans’ mental models.","['Nasrin Mostafazadeh', 'Aditya Kalyanpur', 'Lori Moon', 'David Buchanan', 'Lauren Berkowitz', 'Or Biran', 'Jennifer Chu-Carroll']",https://www.aclweb.org/anthology/2020.emnlp-main.370.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.370/,2,
"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.","['Zhiyuan Liu', 'Yixin Cao', 'Liangming Pan', 'Juanzi Li', 'Zhiyuan Liu', 'Tat-Seng Chua']",https://www.aclweb.org/anthology/2020.emnlp-main.515.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.515/,8,
Adversarial Semantic Collisions,"We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.","['Congzheng Song', 'Alexander M. Rush', 'Vitaly Shmatikov']",https://www.aclweb.org/anthology/2020.emnlp-main.344.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.344/,4,
MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.","['Andreas Rücklé', 'Jonas Pfeiffer', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.194.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.194/,5,
MLSUM: The Multilingual Summarization Corpus,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages – namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.","['Thomas Scialom', 'Paul-Alexis Dray', 'Sylvain Lamprier', 'Benjamin Piwowarski', 'Jacopo Staiano']",https://www.aclweb.org/anthology/2020.emnlp-main.647.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.647/,10,
Don’t Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings,"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.","['Phillip Keung', 'Yichao Lu', 'Julian Salazar', 'Vikas Bhardwaj']",https://www.aclweb.org/anthology/2020.emnlp-main.40.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.40/,2,
How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task. The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.","['Henry Elder', 'Alexander O’Connor', 'Jennifer Foster']",https://www.aclweb.org/anthology/2020.emnlp-main.230.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.230/,2,
Table Fact Verification with Structure-Aware Transformer,"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.","['Hongzhi Zhang', 'Yingyao Wang', 'Sirui Wang', 'Xuezhi Cao', 'Fuzheng Zhang', 'Zhongyuan Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.126.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.126/,6,
Global-to-Local Neural Networks for Document-Level Relation Extraction,"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.","['Difeng Wang', 'Wei Hu', 'Ermei Cao', 'Weijian Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.303.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.303/,8,
Help! Need Advice on Identifying Advice,"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research.","['Venkata Subrahmanyan Govindarajan', 'Benjamin Chen', 'Rebecca Warholic', 'Katrin Erk', 'Junyi Jessy Li']",https://www.aclweb.org/anthology/2020.emnlp-main.427.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.427/,7,
Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.","['Xiaomian Kang', 'Yang Zhao', 'Jiajun Zhang', 'Chengqing Zong']",https://www.aclweb.org/anthology/2020.emnlp-main.175.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.175/,2,
Stepwise Extractive Summarization and Planning with Structured Transformers,"We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.","['Shashi Narayan', 'Joshua Maynez', 'Jakub Adamek', 'Daniele Pighin', 'Blaz Bratanic', 'Ryan McDonald']",https://www.aclweb.org/anthology/2020.emnlp-main.339.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.339/,2,
Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT,"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.","['Rik van Noord', 'Antonio. Toral', 'Johan Bos']",https://www.aclweb.org/anthology/2020.emnlp-main.371.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.371/,1,
Investigating African-American Vernacular English in Transformer-Based Text Generation,"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to text corpora availability. We investigate the performance of GPT-2 on AAVE text by creating a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating syntactic structure and AAVE- or SAE-specific language for each pair. We evaluate each sample and its GPT-2 generated text with pretrained sentiment classifiers and find that while AAVE text results in more classifications of negative sentiment than SAE, the use of GPT-2 generally increases occurrences of positive sentiment for both. Additionally, we conduct human evaluation of AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall quality.","['Sophie Groenwold', 'Lily Ou', 'Aesha Parekh', 'Samhita Honnavalli', 'Sharon Levy', 'Diba Mirza', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.473.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.473/,2,
Please Mind the Root: Decoding Arborescences for Dependency Parsing,"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.","['Ran Zmigrod', 'Tim Vieira', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.390.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.390/,1,
SubjQA: A Dataset for Subjectivity and Review Comprehension,"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.","['Johannes Bjerva', 'Nikita Bhutani', 'Behzad Golshan', 'Wang-Chiew Tan', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.442.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.442/,10,
Look at the First Sentence: Position Bias in Question Answering,"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.","['Miyoung Ko', 'Jinhyuk Lee', 'Hyunjae Kim', 'Gangwoo Kim', 'Jaewoo Kang']",https://www.aclweb.org/anthology/2020.emnlp-main.84.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.84/,5,
MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.","['Anthony Chen', 'Gabriel Stanovsky', 'Sameer Singh', 'Matt Gardner']",https://www.aclweb.org/anthology/2020.emnlp-main.528.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.528/,10,
MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge,"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles. Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue. In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task. We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve). Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.","['Sopan Khosla', 'Shikhar Vashishth', 'Jill Fain Lehman', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.626.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.626/,8,
Connecting the Dots: Event Graph Schema Induction with Path Language Modeling,"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.","['Manling Li', 'Qi Zeng', 'Ying Lin', 'Kyunghyun Cho', 'Heng Ji', 'Jonathan May', 'Nathanael Chambers', 'Clare Voss']",https://www.aclweb.org/anthology/2020.emnlp-main.50.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.50/,6,
AnswerFact: Fact Checking in Product Question Answering,"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.","['Wenxuan Zhang', 'Yang Deng', 'Jing Ma', 'Wai Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.188.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.188/,5,
Surprisal Predicts Code-Switching in Chinese-English Bilingual Text,"Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese. The model includes known control variables together with word surprisal and word entropy. We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors. We also found sentence length to be a significant predictor, which has been related to sentence complexity. We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language. We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.","['Jesús Calvillo', 'Le Fang', 'Jeremy Cole', 'David Reitter']",https://www.aclweb.org/anthology/2020.emnlp-main.330.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.330/,2,
Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction,"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models. Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport. Experimental results on MUSE and VecMap datasets show significant improvement of our models. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed method.","['Xu Zhao', 'Zihao Wang', 'Hao Wu', 'Yong Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.238.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.238/,2,
"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow","We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.","['Li Zhang', 'Qing Lyu', 'Chris Callison-Burch']",https://www.aclweb.org/anthology/2020.emnlp-main.374.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.374/,5,
INSPIRED: Toward Sociable Recommendation Dialog Systems,"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories.","['Shirley Anugrah Hayati', 'Dongyeop Kang', 'Qingxiaoyang Zhu', 'Weiyan Shi', 'Zhou Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.654.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.654/,5,
Recurrent Event Network: Autoregressive Structure Inferenceover Temporal Knowledge Graphs,"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.","['Woojeong Jin', 'Meng Qu', 'Xisen Jin', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.541.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.541/,4,
CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage cross-lingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.","['Ahmed El-Kishky', 'Vishrav Chaudhary', 'Francisco Guzmán', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.480.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.480/,10,
Coarse-to-Fine Query Focused Multi-Document Summarization,"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.","['Yumo Xu', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.296/,2,
A State-independent and Time-evolving Network for Early Rumor Detection in Social Media,"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event’s life cycle. Such coarse-grained methods failed to capture the event’s unique features in different states. To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation. Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events. For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction. This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection. Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems. We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.","['Rui Xia', 'Kaizhou Xuan', 'Jianfei Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.727.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.727/,7,
"BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations","Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.","['Aina Garí Soler', 'Marianna Apidianaki']",https://www.aclweb.org/anthology/2020.emnlp-main.598.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.598/,1,
The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual APIs,"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.","['Nitish Shirish Keskar', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",https://www.aclweb.org/anthology/2020.emnlp-main.501.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.501/,2,
RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling,"In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.","['Jun Quan', 'Shian Zhang', 'Qian Cao', 'Zizhong Li', 'Deyi Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.67.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.67/,5,
X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset,"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.","['Angel Daza', 'Anette Frank']",https://www.aclweb.org/anthology/2020.emnlp-main.321.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.321/,10,
Probing Pretrained Language Models for Lexical Semantics,"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.","['Ivan Vulić', 'Edoardo Maria Ponti', 'Robert Litschko', 'Goran Glavaš', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.586.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.586/,1,
Mention Extraction and Linking for SQL Query Generation,"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.","['Jianqiang Ma', 'Zeyu Yan', 'Shuai Pang', 'Yang Zhang', 'Jianping Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.563.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.563/,8,
Solving Historical Dictionary Codes with a Neural Language Model,"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1% of the cipher-word tokens correctly.","['Christopher Chu', 'Raphael Valenti', 'Kevin Knight']",https://www.aclweb.org/anthology/2020.emnlp-main.471.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.471/,6,
Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.","['Yanlin Feng', 'Xinyue Chen', 'Bill Yuchen Lin', 'Peifeng Wang', 'Jun Yan', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.99.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.99/,5,
Multi-task Learning for Multilingual Neural Machine Translation,"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.","['Yiren Wang', 'ChengXiang Zhai', 'Hany Hassan']",https://www.aclweb.org/anthology/2020.emnlp-main.75.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.75/,2,
Learning from Context or Names? An Empirical Study on Neural Relation Extraction,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.","['Hao Peng', 'Tianyu Gao', 'Xu Han', 'Yankai Lin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.298.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.298/,8,
Seq2Edits: Sequence Transduction Using Span-level Edit Operations,"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.","['Felix Stahlberg', 'Shankar Kumar']",https://www.aclweb.org/anthology/2020.emnlp-main.418.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.418/,4,
Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC contains over 98K explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: “X is a Y” AND “Y has Z” IMPLIES “X has Z”). We find that generalized chains maintain performance while also being more robust to certain perturbations.","['Harsh Jhamtani', 'Peter Clark']",https://www.aclweb.org/anthology/2020.emnlp-main.10.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.10/,5,
Pre-training Entity Relation Encoder with Intra-span and Inter-span Information,"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).","['Yijun Wang', 'Changzhi Sun', 'Yuanbin Wu', 'Junchi Yan', 'Peng Gao', 'Guotong Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.132.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.132/,8,
Calibration of Pre-trained Transformers,"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.","['Shrey Desai', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.21.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.21/,4,
Generating Dialogue Responses from a Semantic Latent Space,"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.","['Wei-Jen Ko', 'Avik Ray', 'Yilin Shen', 'Hongxia Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.352.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.352/,5,
Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people’s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.","['Haochen Liu', 'Wentao Wang', 'Yiqi Wang', 'Hui Liu', 'Zitao Liu', 'Jiliang Tang']",https://www.aclweb.org/anthology/2020.emnlp-main.64.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.64/,2,
An Empirical Study of Pre-trained Transformers for Arabic Information Extraction,"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.","['Wuwei Lan', 'Yang Chen', 'Wei Xu', 'Alan Ritter']",https://www.aclweb.org/anthology/2020.emnlp-main.382.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.382/,8,
A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression,"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution – Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.","['Mingming Sun', 'Wenyue Hua', 'Zoey Liu', 'Xin Wang', 'Kangjie Zheng', 'Ping Li']",https://www.aclweb.org/anthology/2020.emnlp-main.167.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.167/,8,
TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue,"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.","['Chien-Sheng Wu', 'Steven C.H. Hoi', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.66.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.66/,5,
Learning from Task Descriptions,"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model’s ability to solve each task. Moreover, the dataset’s structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.","['Orion Weller', 'Nicholas Lourie', 'Matt Gardner', 'Matthew Peters']",https://www.aclweb.org/anthology/2020.emnlp-main.105.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.105/,8,
Question Directed Graph Attention Network for Numerical Reasoning over Text,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.","['Kunlong Chen', 'Weidi Xu', 'Xingyi Cheng', 'Zou Xiaochuan', 'Yuyu Zhang', 'Le Song', 'Taifeng Wang', 'Yuan Qi', 'Wei Chu']",https://www.aclweb.org/anthology/2020.emnlp-main.549.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.549/,6,
UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues,"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.","['Hung Le', 'Doyen Sahoo', 'Chenghao Liu', 'Nancy Chen', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.146.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.146/,5,
Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding,"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.","['Samson Tan', 'Shafiq Joty', 'Lav Varshney', 'Min-Yen Kan']",https://www.aclweb.org/anthology/2020.emnlp-main.455.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.455/,1,
Data and Representation for Turkish Natural Language Inference,"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.","['Emrah Budur', 'Rıza Özçelik', 'Tunga Güngör', 'Christopher Potts']",https://www.aclweb.org/anthology/2020.emnlp-main.662.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.662/,8,
Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction,"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner’s perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.","['Tara Safavi', 'Danai Koutra', 'Edgar Meij']",https://www.aclweb.org/anthology/2020.emnlp-main.667.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.667/,4,
XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.","['Edoardo Maria Ponti', 'Goran Glavaš', 'Olga Majewska', 'Qianchu Liu', 'Ivan Vulić', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.185.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.185/,10,
AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network,"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.","['Xinyu Wang', 'Yong Jiang', 'Nguyen Bach', 'Tao Wang', 'Zhongqiang Huang', 'Fei Huang', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.485.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.485/,4,
Fˆ2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax,"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, Fˆ2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. Fˆ2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.","['Byung-Ju Choi', 'Jimin Hong', 'David Park', 'Sang Wan Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.737.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.737/,2,
PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction,"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more proactive and powerful (“_He pursues his dream of being a doctor_”). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.","['Xinyao Ma', 'Maarten Sap', 'Hannah Rashkin', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.602.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.602/,2,
How Much Knowledge Can You Pack Into the Parameters of a Language Model?,"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.","['Adam Roberts', 'Colin Raffel', 'Noam Shazeer']",https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.437/,4,
Hierarchical Graph Network for Multi-hop Question Answering,"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.","['Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Rohit Pillai', 'Shuohang Wang', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.710.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.710/,5,
Position-Aware Tagging for Aspect Sentiment Triplet Extraction,"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness.","['Lu Xu', 'Hao Li', 'Wei Lu', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.183.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.183/,3,
Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze,"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.","['Ece Takmaz', 'Sandro Pezzelle', 'Lisa Beinborn', 'Raquel Fernández']",https://www.aclweb.org/anthology/2020.emnlp-main.377.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.377/,6,
RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark – Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language.","['Tatiana Shavrina', 'Alena Fenogenova', 'Emelyanov Anton', 'Denis Shevelev', 'Ekaterina Artemova', 'Valentin Malykh', 'Vladislav Mikhailov', 'Maria Tikhonova', 'Andrey Chertok', 'Andrey Evlampiev']",https://www.aclweb.org/anthology/2020.emnlp-main.381.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.381/,10,
What do Models Learn from Question Answering Datasets?,"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter","['Priyanka Sen', 'Amir Saffari']",https://www.aclweb.org/anthology/2020.emnlp-main.190.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.190/,5,
Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.","['Nils Reimers', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.365.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.365/,2,
A Bilingual Generative Transformer for Semantic Sentence Embedding,"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.","['John Wieting', 'Graham Neubig', 'Taylor Berg-Kirkpatrick']",https://www.aclweb.org/anthology/2020.emnlp-main.122.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.122/,2,
KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks","['Fabio Massimo Zanzotto', 'Andrea Santilli', 'Leonardo Ranaldi', 'Dario Onorati', 'Pierfrancesco Tommasino', 'Francesca Fallucchi']",https://www.aclweb.org/anthology/2020.emnlp-main.18.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.18/,1,
Multi-Step Inference for Reasoning Over Paragraphs,"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.","['Jiangming Liu', 'Matt Gardner', 'Shay B. Cohen', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.245.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.245/,8,
Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots,"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.","['Yuanmeng Yan', 'Keqing He', 'Hong Xu', 'Sihong Liu', 'Fanyu Meng', 'Min Hu', 'Weiran Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.490.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.490/,1,
Modeling the Music Genre Perception across Language-Bound Cultures,"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.","['Elena V. Epure', 'Guillaume Salha', 'Manuel Moussallam', 'Romain Hennequin']",https://www.aclweb.org/anthology/2020.emnlp-main.386.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.386/,6,
Automatic Extraction of Rules Governing Morphological Agreement,"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/","['Aditi Chaudhary', 'Antonios Anastasopoulos', 'Adithya Pratapa', 'David R. Mortensen', 'Zaid Sheikh', 'Yulia Tsvetkov', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.422.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.422/,1,
Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.","['Brian Thompson', 'Matt Post']",https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.8/,2,
Word Frequency Does Not Predict Grammatical Knowledge in Language Models,"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks. Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.","['Charles Yu', 'Ryan Sie', 'Nicolas Tedeschi', 'Leon Bergen']",https://www.aclweb.org/anthology/2020.emnlp-main.331.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.331/,1,
EmoTag1200: Understanding the Association between Emojis and Emotions,"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available.","['Abu Awal Md Shoeb', 'Gerard de Melo']",https://www.aclweb.org/anthology/2020.emnlp-main.720.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.720/,10,
An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing,"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.","['Martin Schmitt', 'Sahand Sharifzadeh', 'Volker Tresp', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.577.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.577/,2,
On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment,"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers’ generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.","['Zirui Wang', 'Zachary C. Lipton', 'Yulia Tsvetkov']",https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.359/,2,
PRover: Proof Generation for Interpretable Reasoning over Rules,"Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for “depth 5”, indicating significant scope for future work.","['Swarnadeep Saha', 'Sayan Ghosh', 'Shashank Srivastava', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.9.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.9/,8,
BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance,"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model’s performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression","['Jianquan Li', 'Xiaokang Liu', 'Honghong Zhao', 'Ruifeng Xu', 'Min Yang', 'Yaohong Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.242.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.242/,4,
Assessing Phrasal Representation and Composition in Transformers,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.","['Lang Yu', 'Allyson Ettinger']",https://www.aclweb.org/anthology/2020.emnlp-main.397.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.397/,1,
Transformer Based Multi-Source Domain Adaptation,"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple source domains and must make predictions on a domain for which no labelled data has been seen. Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training, to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their performance, suggesting that large transformer-based models are already relatively robust across domains. Additionally, we show that mixture of experts leads to significant performance improvements by comparing several variants of mixing functions, including one novel metric based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective metrics for mixing their predictions.","['Dustin Wright', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.639.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.639/,4,
HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training,"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.","['Linjie Li', 'Yen-Chun Chen', 'Yu Cheng', 'Zhe Gan', 'Licheng Yu', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.161.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.161/,4,
With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation,"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.","['Bianca Scarlini', 'Tommaso Pasini', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.285.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.285/,1,
Less is More: Attention Supervision with Counterfactuals for Text Classification,"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.","['Seungtaek Choi', 'Haeju Park', 'Jinyoung Yeo', 'Seung-won Hwang']",https://www.aclweb.org/anthology/2020.emnlp-main.543.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.543/,4,
Be More with Less: Hypergraph Attention Networks for Inductive Text Classification,"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model – hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.","['Kaize Ding', 'Jianling Wang', 'Jundong Li', 'Dingcheng Li', 'Huan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.399.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.399/,4,
TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion,"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.","['Jiapeng Wu', 'Meng Cao', 'Jackie Chi Kit Cheung', 'William L. Hamilton']",https://www.aclweb.org/anthology/2020.emnlp-main.462.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.462/,4,
Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation,"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.","['Wenxiang Jiao', 'Xing Wang', 'Shilin He', 'Irwin King', 'Michael Lyu', 'Zhaopeng Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.176.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.176/,2,
Sound Natural: Content Rephrasing in Dialog Systems,"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user. For example, for queries like ‘ask my wife if she can pick up the kids’ or ‘remind me to take my pills’, we need to rephrase the content to ‘can you pick up the kids’ and ‘take your pills’. In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query. We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it. We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model","['Arash Einolghozati', 'Anchit Gupta', 'Keith Diedrick', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.414.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.414/,5,
UDapter: Language Adaptation for Truly Universal Dependency Parsing,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.","['Ahmet Üstün', 'Arianna Bisazza', 'Gosse Bouma', 'Gertjan Van Noord']",https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.180/,1,
Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages,"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.","['Michael A. Hedderich', 'David Adelani', 'Dawei Zhu', 'Jesujoba Alabi', 'Udia Markus', 'Dietrich Klakow']",https://www.aclweb.org/anthology/2020.emnlp-main.204.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.204/,2,
Inference Strategies for Machine Translation with Conditional Masking,"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks.","['Julia Kreutzer', 'George Foster', 'Colin Cherry']",https://www.aclweb.org/anthology/2020.emnlp-main.465.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.465/,2,
Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems,"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols’ semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.","['Jinghui Qin', 'Lihui Lin', 'Xiaodan Liang', 'Rumin Zhang', 'Liang Lin']",https://www.aclweb.org/anthology/2020.emnlp-main.309.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.309/,6,1
Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models,"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.","['Thuy Vu', 'Dinh Phung', 'Gholamreza Haffari']",https://www.aclweb.org/anthology/2020.emnlp-main.497.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.497/,,
Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis,"The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation. To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper. Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling. Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.","['Chenggong Gong', 'Jianfei Yu', 'Rui Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.572.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.572/,,
Self-Paced Learning for Neural Machine Translation,"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.","['Yu Wan', 'Baosong Yang', 'Derek F. Wong', 'Yikai Zhou', 'Lidia S. Chao', 'Haibo Zhang', 'Boxing Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.80.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.80/,,
Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","['Francisco Vargas', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.232.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.232/,,
"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.","['Yun He', 'Ziwei Zhu', 'Yin Zhang', 'Qin Chen', 'James Caverlee']",https://www.aclweb.org/anthology/2020.emnlp-main.372.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.372/,6,4
Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks,"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.","['Yufan Zhao', 'Can Xu', 'Wei Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.279.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.279/,,
Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs,"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.",['Marius Pasca'],https://www.aclweb.org/anthology/2020.emnlp-main.503.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.503/,,
Named Entity Recognition Only from Word Embeddings,"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features. However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.","['Ying Luo', 'Hai Zhao', 'Junlang Zhan']",https://www.aclweb.org/anthology/2020.emnlp-main.723.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.723/,,
Exploring Contextualized Neural Language Models for Temporal Dependency Parsing,"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.","['Hayley Ross', 'Jonathon Cai', 'Bonan Min']",https://www.aclweb.org/anthology/2020.emnlp-main.689.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.689/,,
Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes,"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes. The classification is carried out by mapping text embeddings to the word graph embeddings of the classes. Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved. In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.","['Paulo Cavalin', 'Victor Henrique Alves Ribeiro', 'Ana Appel', 'Claudio Pinhanez']",https://www.aclweb.org/anthology/2020.emnlp-main.324.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.324/,1,4
Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks,"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.","['Viet Dac Lai', 'Tuan Ngo Nguyen', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.435.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.435/,4,
Utility is in the Eye of the User: A Critique of NLP Leaderboards,"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards – in their current form – can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model’s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).","['Kawin Ethayarajh', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.393.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.393/,4,
FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction,"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.","['Dianbo Sui', 'Yubo Chen', 'Jun Zhao', 'Yantao Jia', 'Yuantao Xie', 'Weijian Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.165/,,
The role of context in neural pitch accent detection in English,"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.","['Elizabeth Nielsen', 'Mark Steedman', 'Sharon Goldwater']",https://www.aclweb.org/anthology/2020.emnlp-main.642.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.642/,,
Monolingual Adapters for Zero-Shot Neural Machine Translation,"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.","['Jerin Philip', 'Alexandre Bérard', 'Matthias Gallé', 'Laurent Besacier']",https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.361/,2,
VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls,"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.","['Ramit Sawhney', 'Piyush Khanna', 'Arshiya Aggarwal', 'Taru Jain', 'Puneet Mathur', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.643.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.643/,,
GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems,"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.","['Lishan Huang', 'Zheng Ye', 'Jinghui Qin', 'Liang Lin', 'Xiaodan Liang']",https://www.aclweb.org/anthology/2020.emnlp-main.742.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.742/,,
Predicting Reference: What do Language Models Learn about Discourse Models?,"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.","['Shiva Upadhye', 'Leon Bergen', 'Andrew Kehler']",https://www.aclweb.org/anthology/2020.emnlp-main.70.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.70/,,
Interpretable Multi-dataset Evaluation for Named Entity Recognition,"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval","['Jinlan Fu', 'Pengfei Liu', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.489.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.489/,,
Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning,"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.","['Yuning Mao', 'Yanru Qu', 'Yiqing Xie', 'Xiang Ren', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.136.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.136/,,
CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.","['Tara Safavi', 'Danai Koutra']",https://www.aclweb.org/anthology/2020.emnlp-main.669.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.669/,,
Inquisitive Question Generation for High Level Text Comprehension,"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.","['Wei-Jen Ko', 'Te-yuan Chen', 'Yiyan Huang', 'Greg Durrett', 'Junyi Jessy Li']",https://www.aclweb.org/anthology/2020.emnlp-main.530.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.530/,,
ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.","['Liying Cheng', 'Dekun Wu', 'Lidong Bing', 'Yan Zhang', 'Zhanming Jie', 'Wei Lu', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.90.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.90/,,
F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.","['Hendrik Schuff', 'Heike Adel', 'Ngoc Thang Vu']",https://www.aclweb.org/anthology/2020.emnlp-main.575.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.575/,,
Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness,"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.","['Hyunwoo Kim', 'Byeongchang Kim', 'Gunhee Kim']",https://www.aclweb.org/anthology/2020.emnlp-main.65.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.65/,,
A matter of framing: The impact of linguistic formalism on probing results,"Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.","['Ilia Kuznetsov', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.13.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.13/,,
What Can We Learn from Collective Human Opinions on Natural Language Inference Data?,"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in αNLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.","['Yixin Nie', 'Xiang Zhou', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.734.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.734/,,
A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT,"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.","['Masaaki Nagata', 'Katsuki Chousa', 'Masaaki Nishino']",https://www.aclweb.org/anthology/2020.emnlp-main.41.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.41/,,
Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning,"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.","['Hanlu Wu', 'Tengfei Ma', 'Lingfei Wu', 'Tariro Manyumwa', 'Shouling Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.294.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.294/,,
Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning,"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to guide a sophisticated walk-based reinforcement learning (RL) model. An alternate approach is to use traditional symbolic methods (e.g., rule induction), which achieve good performance but can be hard to generalize due to the limitation of symbolic representation. In this paper, we propose RuleGuider, which leverages high-quality rules generated by symbolic-based methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets shows that RuleGuider clearly improves the performance of walk-based models without losing interpretability.","['Deren Lei', 'Gangrong Jiang', 'Xiaotao Gu', 'Kexuan Sun', 'Yuning Mao', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.688.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.688/,,
Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding,"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.","['Jiaming Shen', 'Heng Ji', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.22.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.22/,,
Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.","['Chengyu Wang', 'Minghui Qiu', 'Jun Huang', 'Xiaofeng He']",https://www.aclweb.org/anthology/2020.emnlp-main.250.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.250/,,
A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis,"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.","['Zehui Dai', 'Cheng Peng', 'Huajie Chen', 'Yadong Ding']",https://www.aclweb.org/anthology/2020.emnlp-main.565.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.565/,,
Quantifying Intimacy in Language,"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.","['Jiaxin Pei', 'David Jurgens']",https://www.aclweb.org/anthology/2020.emnlp-main.428.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.428/,7,
Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.","['Xiangji Zeng', 'Yunliang Li', 'Yuchen Zhai', 'Yin Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.590.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.590/,,
Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation,"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.","['Minki Kang', 'Moonsu Han', 'Sung Ju Hwang']",https://www.aclweb.org/anthology/2020.emnlp-main.493.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.493/,,
ConjNLI: Natural Language Inference Over Conjunctive Sentences,"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions (“and”, “or”, “but”, “nor”) with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions.","['Swarnadeep Saha', 'Yixin Nie', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.661.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.661/,,
Condolence and Empathy in Online Communities,"Offering condolence is a natural reaction to hearing someone’s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal—trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.","['Naitian Zhou', 'David Jurgens']",https://www.aclweb.org/anthology/2020.emnlp-main.45.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.45/,,
Multi-Stage Pre-training for Low-Resource Domain Adaptation,"Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.","['Rong Zhang', 'Revanth Gangi Reddy', 'Md Arafat Sultan', 'Vittorio Castelli', 'Anthony Ferritto', 'Radu Florian', 'Efsun Sarioglu Kayi', 'Salim Roukos', 'Avirup Sil', 'Todd Ward']",https://www.aclweb.org/anthology/2020.emnlp-main.440.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.440/,10,4
Knowledge Association with Hyperbolic Knowledge Graph Embeddings,"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.","['Zequn Sun', 'Muhao Chen', 'Wei Hu', 'Chengming Wang', 'Jian Dai', 'Wei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.460.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.460/,,
Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task,"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.","['Dongyeop Kang', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.529.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.529/,,
CancerEmo: A Dataset for Fine-Grained Emotion Detection,"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce CancerEmo, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.","['Tiberiu Sosea', 'Cornelia Caragea']",https://www.aclweb.org/anthology/2020.emnlp-main.715.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.715/,,
TeaForN: Teacher-Forcing with N-grams,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.","['Sebastian Goodman', 'Nan Ding', 'Radu Soricut']",https://www.aclweb.org/anthology/2020.emnlp-main.702.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.702/,,
BAE: BERT-based Adversarial Examples for Text Classification,"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.","['Siddhant Garg', 'Goutham Ramakrishnan']",https://www.aclweb.org/anthology/2020.emnlp-main.498.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.498/,,
SetConv: A New Approach for Learning from Imbalanced Data,"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.","['Yang Gao', 'Yi-Fan Li', 'Yu Lin', 'Charu Aggarwal', 'Latifur Khan']",https://www.aclweb.org/anthology/2020.emnlp-main.98.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.98/,,
AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue,"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.","['Jaehun Jung', 'Bokyung Son', 'Sungwon Lyu']",https://www.aclweb.org/anthology/2020.emnlp-main.280.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.280/,,
Keep CALM and Explore: Language Models for Action Generation in Text-based Games,"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame.","['Shunyu Yao', 'Rohan Rao', 'Matthew Hausknecht', 'Karthik Narasimhan']",https://www.aclweb.org/anthology/2020.emnlp-main.704.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.704/,,
RethinkCWS: Is Chinese Word Segmentation a Solved Task?,"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user’s models: https://github.com/neulab/InterpretEval","['Jinlan Fu', 'Pengfei Liu', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.457.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.457/,,
Task-oriented Domain-specific Meta-Embedding for Text Classification,"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains. Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem. In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings. We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.","['Xin Wu', 'Yi Cai', 'Yang Kai', 'Tao Wang', 'Qing Li']",https://www.aclweb.org/anthology/2020.emnlp-main.282.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.282/,,
Improving Word Sense Disambiguation with Translations,"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation. Since our approach is language independent, we perform WSD experiments on several languages. The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD. To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.","['Yixing Luan', 'Bradley Hauer', 'Lili Mou', 'Grzegorz Kondrak']",https://www.aclweb.org/anthology/2020.emnlp-main.332.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.332/,2,4
STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering,"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.","['Hrituraj Singh', 'Sumit Shekhar']",https://www.aclweb.org/anthology/2020.emnlp-main.264.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.264/,,
Design Challenges in Low-resource Cross-lingual Entity Linking,"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia’s interlanguage links and thus suffer when the foreign language’s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.","['Xingyu Fu', 'Weijia Shi', 'Xiaodong Yu', 'Zian Zhao', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.521.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.521/,,
COMETA: A Corpus for Medical Entity Linking in the Social Media,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman’s language. Meanwhile, there is a growing need for applications that can understand the public’s voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.","['Marco Basaldella', 'Fangyu Liu', 'Ehsan Shareghi', 'Nigel Collier']",https://www.aclweb.org/anthology/2020.emnlp-main.253.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.253/,,
Suicidal Risk Detection for Military Personnel,"We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q&A site, we collect posts containing military-relevant content written by active-duty military personnel. We then annotate the posts with two groups of experts: military experts and mental health experts. Our dataset includes 2,791 posts with 13,955 corresponding expert annotations of suicidal risk levels, and this dataset is available to researchers who consent to research ethics agreement. Using various fine-tuned state-of-the-art language models, we predict the level of suicide risk, reaching .88 F1 score for classifying the risks.","['Sungjoon Park', 'Kiwoong Park', 'Jaimeen Ahn', 'Alice Oh']",https://www.aclweb.org/anthology/2020.emnlp-main.198.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.198/,,
Modularized Syntactic Neural Networks for Sentence Classification,"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each node of a syntax tree is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation. We design a tree-parallel mini-batch strategy for efficient training and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.","['Haiyan Wu', 'Ying Liu', 'Shaoyun Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.222.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.222/,,
CHARM: Inferring Personal Attributes from Conversations,"Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.","['Anna Tigunova', 'Andrew Yates', 'Paramita Mirza', 'Gerhard Weikum']",https://www.aclweb.org/anthology/2020.emnlp-main.434.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.434/,4,8
Cross-lingual Spoken Language Understanding with Regularized Representation Alignment,"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.","['Zihan Liu', 'Genta Indra Winata', 'Peng Xu', 'Zhaojiang Lin', 'Pascale Fung']",https://www.aclweb.org/anthology/2020.emnlp-main.587.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.587/,,
Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.","['Alex Warstadt', 'Yian Zhang', 'Xiaocheng Li', 'Haokun Liu', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.16.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.16/,,
Uncertainty-Aware Semantic Augmentation for Neural Machine Translation,"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.","['Xiangpeng Wei', 'Heng Yu', 'Yue Hu', 'Rongxiang Weng', 'Luxi Xing', 'Weihua Luo']",https://www.aclweb.org/anthology/2020.emnlp-main.216.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.216/,,
Human-centric dialog training via offline reinforcement learning,"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.","['Natasha Jaques', 'Judy Hanwen Shen', 'Asma Ghandeharioun', 'Craig Ferguson', 'Agata Lapedriza', 'Noah Jones', 'Shixiang Gu', 'Rosalind Picard']",https://www.aclweb.org/anthology/2020.emnlp-main.327.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.327/,5,
Double Graph Based Reasoning for Document-level Relation Extraction,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.","['Shuang Zeng', 'Runxin Xu', 'Baobao Chang', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.127.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.127/,,
An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels,"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.","['Ilias Chalkidis', 'Manos Fergadiotis', 'Sotiris Kotitsas', 'Prodromos Malakasiotis', 'Nikolaos Aletras', 'Ion Androutsopoulos']",https://www.aclweb.org/anthology/2020.emnlp-main.607.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.607/,,
Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.","['Ramy Eskander', 'Smaranda Muresan', 'Michael Collins']",https://www.aclweb.org/anthology/2020.emnlp-main.391.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.391/,2,4
IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system’s performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.","['James Ferguson', 'Matt Gardner', 'Hannaneh Hajishirzi', 'Tushar Khot', 'Pradeep Dasigi']",https://www.aclweb.org/anthology/2020.emnlp-main.86.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.86/,,
Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The “omni-directional” BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.","['Brielen Madureira', 'David Schlangen']",https://www.aclweb.org/anthology/2020.emnlp-main.26.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.26/,,
Parsing Gapping Constructions Based on Grammatical and Semantic Roles,A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.,"['Yoshihide Kato', 'Shigeki Matsubara']",https://www.aclweb.org/anthology/2020.emnlp-main.218.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.218/,,
Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses,"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.","['Prathyusha Jwalapuram', 'Shafiq Joty', 'Youlin Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.177.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.177/,,
Intrinsic Evaluation of Summarization Datasets,"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.","['Rishi Bommasani', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.649.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.649/,,
Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing,"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.","['Xilun Chen', 'Asish Ghoshal', 'Yashar Mehdad', 'Luke Zettlemoyer', 'Sonal Gupta']",https://www.aclweb.org/anthology/2020.emnlp-main.413.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.413/,1,4
SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge,"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.","['Pei Ke', 'Haozhe Ji', 'Siyang Liu', 'Xiaoyan Zhu', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.567.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.567/,,
"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation","In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.","['Yaobo Liang', 'Nan Duan', 'Yeyun Gong', 'Ning Wu', 'Fenfei Guo', 'Weizhen Qi', 'Ming Gong', 'Linjun Shou', 'Daxin Jiang', 'Guihong Cao', 'Xiaodong Fan', 'Ruofei Zhang', 'Rahul Agrawal', 'Edward Cui', 'Sining Wei', 'Taroon Bharti', 'Ying Qiao', 'Jiun-Hung Chen', 'Winnie Wu', 'Shuguang Liu', 'Fan Yang', 'Daniel Campos', 'Rangan Majumder', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.484.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.484/,,
"Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis","Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect’s sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models’ performance on ARTS by up to 32.85%. Our code and new test set are available at https://github.com/zhijing-jin/ARTS_TestSet","['Xiaoyu Xing', 'Zhijing Jin', 'Di Jin', 'Bingning Wang', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.292.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.292/,,
Neural Conversational QA: Learning to Reason vs Exploiting Patterns,"Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based program, built to exploit these patterns, had comparative performance to that of the neural models. In this paper we share our findings about the four types of patterns in the ShARC corpus and how the neural models exploit them. Motivated by the above findings, we create and share a modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.","['Nikhil Verma', 'Abhishek Sharma', 'Dhiraj Madan', 'Danish Contractor', 'Harshit Kumar', 'Sachindra Joshi']",https://www.aclweb.org/anthology/2020.emnlp-main.589.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.589/,,
Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis,"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.","['Yuncong Li', 'Cunxiang Yin', 'Sheng-hua Zhong', 'Xu Pan']",https://www.aclweb.org/anthology/2020.emnlp-main.287.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.287/,,
On the Role of Supervision in Unsupervised Constituency Parsing,"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Few-shot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.","['Haoyue Shi', 'Karen Livescu', 'Kevin Gimpel']",https://www.aclweb.org/anthology/2020.emnlp-main.614.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.614/,,
AxCell: Automatic Extraction of Results from Machine Learning Papers,"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.","['Marcin Kardas', 'Piotr Czapla', 'Pontus Stenetorp', 'Sebastian Ruder', 'Sebastian Riedel', 'Ross Taylor', 'Robert Stojnic']",https://www.aclweb.org/anthology/2020.emnlp-main.692.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.692/,,
The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection,"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.","['Zibo Lin', 'Deng Cai', 'Yan Wang', 'Xiaojiang Liu', 'Haitao Zheng', 'Shuming Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.741.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.741/,,
Knowledge-Grounded Dialogue Generation with Pre-trained Language Models,"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.","['Xueliang Zhao', 'Wei Wu', 'Can Xu', 'Chongyang Tao', 'Dongyan Zhao', 'Rui Yan']",https://www.aclweb.org/anthology/2020.emnlp-main.272.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.272/,,
Evaluating the Factual Consistency of Abstractive Text Summarization,"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.","['Wojciech Kryściński', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",https://www.aclweb.org/anthology/2020.emnlp-main.750.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.750/,,
MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models,"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","['Peng Xu', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Raul Puri', 'Pascale Fung', 'Animashree Anandkumar', 'Bryan Catanzaro']",https://www.aclweb.org/anthology/2020.emnlp-main.226.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.226/,,
SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning,"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.","['Tsu-Jui Fu', 'Xin Wang', 'Scott Grafton', 'Miguel Eckstein', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.357.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.357/,6,4
Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data,"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.","['Lingkai Kong', 'Haoming Jiang', 'Yuchen Zhuang', 'Jie Lyu', 'Tuo Zhao', 'Chao Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.102.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.102/,,
Exploring Semantic Capacity of Terms,"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.","['Jie Huang', 'Zilong Wang', 'Kevin Chang', 'Wen-Mei Hwu', 'JinJun Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.684.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.684/,,
Attention Is All You Need for Chinese Word Segmentation,"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.","['Sufeng Duan', 'Hai Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.317.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.317/,4,1
Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News,"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users’ consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters’ followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.","['Nguyen Vo', 'Kyumin Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.621.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.621/,,
Parallel Interactive Networks for Multi-Domain Dialogue State Generation,"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.","['Junfan Chen', 'Richong Zhang', 'Yongyi Mao', 'Jie Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.151.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.151/,,
The Multilingual Amazon Reviews Corpus,"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.","['Phillip Keung', 'Yichao Lu', 'György Szarvas', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.369.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.369/,2,4
Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.","['Loitongbam Gyanendro Singh', 'Anasua Mitra', 'Sanasam Ranbir Singh']",https://www.aclweb.org/anthology/2020.emnlp-main.718.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.718/,,
Visually Grounded Continual Learning of Compositional Phrases,"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work.","['Xisen Jin', 'Junyi Du', 'Arka Sadhu', 'Ram Nevatia', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.158.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.158/,,
Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.","['Miguel Ballesteros', 'Rishita Anubhai', 'Shuai Wang', 'Nima Pourdamghani', 'Yogarshi Vyas', 'Jie Ma', 'Parminder Bhatia', 'Kathleen McKeown', 'Yaser Al-Onaizan']",https://www.aclweb.org/anthology/2020.emnlp-main.436.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.436/,4,1
How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.","['Nicola De Cao', 'Michael Sejr Schlichtkrull', 'Wilker Aziz', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.262.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.262/,,
Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.","['Mengjie Zhao', 'Tao Lin', 'Fei Mi', 'Martin Jaggi', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.174.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.174/,,
Incremental Event Detection via Knowledge Consolidation Networks,"Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.","['Pengfei Cao', 'Yubo Chen', 'Jun Zhao', 'Taifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.52.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.52/,,
Inducing Target-Specific Latent Structures for Aspect Sentiment Classification,"Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our model can complement supervised syntactic features with latent semantic dependencies. Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.","['Chenhua Chen', 'Zhiyang Teng', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.451.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.451/,3,4
TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization,"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as ‘London’ and ‘Paris’ with different expressions: “the major cities”, “the capital cities” and “two European cities”. Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of Person, Organization, and Location named entities. The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.","['Clément Jumel', 'Annie Louis', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.646.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.646/,,
TernaryBERT: Distillation-aware Ultra-low Bit BERT,"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.","['Wei Zhang', 'Lu Hou', 'Yichun Yin', 'Lifeng Shang', 'Xiao Chen', 'Xin Jiang', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.37.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.37/,,
Generating Label Cohesive and Well-Formed Adversarial Claims,"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.","['Pepa Atanasova', 'Dustin Wright', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.256.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.256/,,
Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.","['Xiaoxiao Guo', 'Mo Yu', 'Yupeng Gao', 'Chuang Gan', 'Murray Campbell', 'Shiyu Chang']",https://www.aclweb.org/anthology/2020.emnlp-main.624.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.624/,,
PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.","['Hannah Rashkin', 'Asli Celikyilmaz', 'Yejin Choi', 'Jianfeng Gao']",https://www.aclweb.org/anthology/2020.emnlp-main.349.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.349/,2,4
Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme,"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26% (p<0.001) in F1 measure.","['Chaofa Yuan', 'Chuang Fan', 'Jianzhu Bao', 'Ruifeng Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.289.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.289/,,
Re-examining the Role of Schema Linking in Text-to-SQL,"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking. We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider despite its structural simplicity. Many remaining errors are attributable to corpus noise. This suggests schema linking is the crux for the current text-to-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks.","['Wenqiang Lei', 'Weixin Wang', 'Zhixin Ma', 'Tian Gan', 'Wei Lu', 'Min-Yen Kan', 'Tat-Seng Chua']",https://www.aclweb.org/anthology/2020.emnlp-main.564.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.564/,,
Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors,"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.","['Sida Gao', 'Matthew R. Gormley']",https://www.aclweb.org/anthology/2020.emnlp-main.406.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.406/,4,
Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization,"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.","['Jiaao Chen', 'Diyi Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.336.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.336/,2,4
Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering,"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability. We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.","['Yuxiang Wu', 'Sebastian Riedel', 'Pasquale Minervini', 'Pontus Stenetorp']",https://www.aclweb.org/anthology/2020.emnlp-main.244.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.244/,,
Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification,"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.","['Yunjie Ji', 'Hao Liu', 'Bolei He', 'Xinyan Xiao', 'Hua Wu', 'Yanhua Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.570.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.570/,,
Dynamic Data Selection and Weighting for Iterative Back-Translation,"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.","['Zi-Yi Dou', 'Antonios Anastasopoulos', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.475.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.475/,,
TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4-way classification is 60%. This is a dramatic drop of 35% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines will be made freely available.","['Wanqiu Long', 'Bonnie Webber', 'Deyi Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.223.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.223/,,
Local Additivity Based Data Augmentation for Semi-supervised NER,"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA","['Jiaao Chen', 'Zhenghui Wang', 'Ran Tian', 'Zichao Yang', 'Diyi Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.95.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.95/,,
Chapter Captor: Text Segmentation in Novels,"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.","['Charuta Pethe', 'Allen Kim', 'Steven Skiena']",https://www.aclweb.org/anthology/2020.emnlp-main.672.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.672/,,
Learning Adaptive Segmentation Policy for Simultaneous Translation,"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.","['Ruiqing Zhang', 'Chuanqiang Zhang', 'Zhongjun He', 'Hua Wu', 'Haifeng Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.178.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.178/,,
Train No Evil: Selective Masking for Task-Guided Pre-Training,"Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient. The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.","['Yuxian Gu', 'Zhengyan Zhang', 'Xiaozhi Wang', 'Zhiyuan Liu', 'Maosong Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.566.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.566/,,
Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification,"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.","['Prithviraj Sen', 'Marina Danilevsky', 'Yunyao Li', 'Siddhartha Brahma', 'Matthias Boehm', 'Laura Chiticariu', 'Rajasekar Krishnamurthy']",https://www.aclweb.org/anthology/2020.emnlp-main.345.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.345/,1,4
A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving,"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on local features while neglecting global information. To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph. Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations. Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information. Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.","['Qinzhuo Wu', 'Qi Zhang', 'Jinlan Fu', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.579.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.579/,,
IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation,"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.","['Yitao Cai', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.560.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.560/,,
Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model’s representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model’s expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.","['Bang An', 'Jie Lyu', 'Zhenyi Wang', 'Chunyuan Li', 'Changwei Hu', 'Fei Tan', 'Ruiyi Zhang', 'Yifan Hu', 'Changyou Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.17.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.17/,,
"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source wide-coverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.","['Valentina Pyatkin', 'Ayal Klein', 'Reut Tsarfaty', 'Ido Dagan']",https://www.aclweb.org/anthology/2020.emnlp-main.224.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.224/,,
Efficient Meta Lifelong-Learning with Limited Memory,"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.","['Zirui Wang', 'Sanket Vaibhav Mehta', 'Barnabás Poczós', 'Jaime G. Carbonell']",https://www.aclweb.org/anthology/2020.emnlp-main.39.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.39/,,
Conversational Semantic Parsing for Dialog State Tracking,"We consider a new perspective on dialog state tracking (DST), the task of estimating a user’s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to ~20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.","['Jianpeng Cheng', 'Devang Agrawal', 'Héctor Martínez Alonso', 'Shruti Bhargava', 'Joris Driesen', 'Federico Flego', 'Dain Kaplan', 'Dimitri Kartsaklis', 'Lin Li', 'Dhivya Piraviperumal', 'Jason D. Williams', 'Hong Yu', 'Diarmuid Ó Séaghdha', 'Anders Johannsen']",https://www.aclweb.org/anthology/2020.emnlp-main.651.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.651/,,
Non-Autoregressive Machine Translation with Latent Alignments,"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates. In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline. Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model. On the competitive WMT’14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.","['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Mohammad Norouzi']",https://www.aclweb.org/anthology/2020.emnlp-main.83.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.83/,,
A Streaming Approach For Efficient Batched Beam Search,"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines’ BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.","['Kevin Yang', 'Violet Yao', 'John DeNero', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.366.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.366/,4,
Incorporating a Local Translation Mechanism into Non-autoregressive Translation,"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. Our code will be released to the public.","['Xiang Kong', 'Zhisong Zhang', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.79.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.79/,,
Exploring Logically Dependent Multi-task Learning with Causal Inference,"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks. Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks. In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models. We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors. In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL. We conduct experiments on two English datasets and one Chinese dataset. Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions’ consistency.","['Wenqing Chen', 'Jidong Tian', 'Liqiang Xiao', 'Hao He', 'Yaohui Jin']",https://www.aclweb.org/anthology/2020.emnlp-main.173.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.173/,,
AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data,"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.","['Silei Xu', 'Sina Semnani', 'Giovanni Campagna', 'Monica Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.31.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.31/,,
Querying Across Genres for Medical Claims in News,"We present a query-based biomedical information retrieval task across two vastly different genres – newswire and research literature – where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.","['Chaoyuan Zuo', 'Narayan Acharya', 'Ritwik Banerjee']",https://www.aclweb.org/anthology/2020.emnlp-main.139.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.139/,,
Improving Low Compute Language Modeling with In-Domain Embedding Initialisation,"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.","['Charles Welch', 'Rada Mihalcea', 'Jonathan K. Kummerfeld']",https://www.aclweb.org/anthology/2020.emnlp-main.696.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.696/,,
Reactive Supervision: A New Method for Collecting Sarcasm Data,"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.","['Boaz Shmueli', 'Lun-Wei Ku', 'Soumya Ray']",https://www.aclweb.org/anthology/2020.emnlp-main.201.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.201/,,
Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings,"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We release these dictionaries to the research community. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60%. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.","['Naoki Otani', 'Satoru Ozaki', 'Xingyuan Zhao', 'Yucen Li', 'Micaelah St Johns', 'Lori Levin']",https://www.aclweb.org/anthology/2020.emnlp-main.360.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.360/,2,
Introducing a New Dataset for Event Detection in Cybersecurity Texts,"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.","['Hieu Man Duc Trong', 'Duc-Trong Le', 'Amir Pouran Ben Veyseh', 'Thuat Nguyen', 'Thien Huu Nguyen']",https://www.aclweb.org/anthology/2020.emnlp-main.433.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.433/,6,
"Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts","Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.","['Ece Takmaz', 'Mario Giulianelli', 'Sandro Pezzelle', 'Arabella Sinclair', 'Raquel Fernández']",https://www.aclweb.org/anthology/2020.emnlp-main.353.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.353/,9,2
Simultaneous Machine Translation with Visual Context,"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.","['Ozan Caglayan', 'Julia Ive', 'Veneta Haralampieva', 'Pranava Swaroop Madhyastha', 'Loïc Barrault', 'Lucia Specia']",https://www.aclweb.org/anthology/2020.emnlp-main.184.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.184/,,
Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.","['Suzanna Sia', 'Ayush Dalmia', 'Sabrina J. Mielke']",https://www.aclweb.org/anthology/2020.emnlp-main.135.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.135/,,
Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.","['Eleftheria Briakou', 'Marine Carpuat']",https://www.aclweb.org/anthology/2020.emnlp-main.121.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.121/,,
"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision","Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named “vokenization” that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call “vokens”). The “vokenizer” is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.","['Hao Tan', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.162.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.162/,,
VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling,"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, “Cambridge” and the first non-English corpus “Robert”, which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.","['Machel Reid', 'Edison Marrese-Taylor', 'Yutaka Matsuo']",https://www.aclweb.org/anthology/2020.emnlp-main.513.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.513/,,
COMET: A Neural Framework for MT Evaluation,"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.","['Ricardo Rei', 'Craig Stewart', 'Ana C Farinha', 'Alon Lavie']",https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.213/,,
Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.","['Chris Kedzie', 'Kathleen McKeown']",https://www.aclweb.org/anthology/2020.emnlp-main.419.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.419/,4,2
Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning,"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.","['Ye Liu', 'Sheng Zhang', 'Rui Song', 'Suo Feng', 'Yanghua Xiao']",https://www.aclweb.org/anthology/2020.emnlp-main.693.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.693/,,
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation","AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.","['Yan Zhang', 'Zhijiang Guo', 'Zhiyang Teng', 'Wei Lu', 'Shay B. Cohen', 'Zuozhu Liu', 'Lidong Bing']",https://www.aclweb.org/anthology/2020.emnlp-main.169.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.169/,,
Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88% novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.","['Tuhin Chakrabarty', 'Smaranda Muresan', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.524.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.524/,,
Generating Radiology Reports via Memory-driven Transformer,"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.","['Zhihong Chen', 'Yan Song', 'Tsung-Hui Chang', 'Xiang Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.112/,,
RNNs can generate bounded hierarchical languages with optimal memory,"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-(k,m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use O(km⁄2) memory (hidden units) to generate these languages. We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.","['John Hewitt', 'Michael Hahn', 'Surya Ganguli', 'Percy Liang', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.156.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.156/,,
Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction,"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding. Effective designs encode within the layout and formatting signals that point to where the important information can be found. In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection. Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these features function within the model.","['Yansen Wang', 'Zhen Fan', 'Carolyn Rose']",https://www.aclweb.org/anthology/2020.emnlp-main.140.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.140/,,
doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset,"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.","['Song Feng', 'Hui Wan', 'Chulaka Gunasekara', 'Siva Patel', 'Sachindra Joshi', 'Luis Lastras']",https://www.aclweb.org/anthology/2020.emnlp-main.652.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.652/,,
Evaluating and Characterizing Human Rationales,"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using “fidelity curves” to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.","['Samuel Carton', 'Anirudh Rathore', 'Chenhao Tan']",https://www.aclweb.org/anthology/2020.emnlp-main.747.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.747/,,
Understanding Neural Abstractive Summarization Models via Uncertainty,"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model’s token-level predictions. For two strong pre-trained models, PEGASUS and BART on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder’s uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model’s next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.","['Jiacheng Xu', 'Shrey Desai', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.508.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.508/,,
Online Back-Parsing for AMR-to-Text Generation,"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.","['Xuefeng Bai', 'Linfeng Song', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.92.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.92/,,
Towards Debiasing NLU Models from Unknown Biases,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.","['Prasetya Ajie Utama', 'Nafise Sadat Moosavi', 'Iryna Gurevych']",https://www.aclweb.org/anthology/2020.emnlp-main.613.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.613/,,
Precise Task Formalization Matters in Winograd Schema Evaluations,"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalization—the combination of input specification, loss function, and reuse of pretrained parameters—by users of the dataset, rather than improvements in the pretrained model’s reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance dramatically and (ii)several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model’s extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.","['Haokun Liu', 'William Huang', 'Dhara Mungra', 'Samuel Bowman']",https://www.aclweb.org/anthology/2020.emnlp-main.664.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.664/,,
Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,"Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at https://github.com/JiaweiSheng/FAAN.","['Jiawei Sheng', 'Shu Guo', 'Zhenyu Chen', 'Juwei Yue', 'Lihong Wang', 'Tingwen Liu', 'Hongbo Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.131.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.131/,,
Coreferential Reasoning Learning for Language Representation,"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.","['Deming Ye', 'Yankai Lin', 'Jiaju Du', 'Zhenghao Liu', 'Peng Li', 'Maosong Sun', 'Zhiyuan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.582.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.582/,1,
Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network,"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others’ responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.","['Sihan Wang', 'Kaijie Zhou', 'Kunfeng Lai', 'Jianping Shen']",https://www.aclweb.org/anthology/2020.emnlp-main.278.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.278/,5,
Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection,"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.","['Ruize Wang', 'Duyu Tang', 'Nan Duan', 'Wanjun Zhong', 'Zhongyu Wei', 'Xuan-Jing Huang', 'Daxin Jiang', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.320.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.320/,6,
Learning to Represent Image and Text with Denotation Graph,"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.","['Bowen Zhang', 'Hexiang Hu', 'Vihan Jain', 'Eugene Ie', 'Fei Sha']",https://www.aclweb.org/anthology/2020.emnlp-main.60.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.60/,9,
Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT,"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.","['Alexandra Chronopoulou', 'Dario Stojanovski', 'Alexander Fraser']",https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.214/,2,
Incorporating Behavioral Hypotheses for Query Generation,"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.","['Ruey-Cheng Chen', 'Chia-Jung Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.251.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.251/,4,
Coding Textual Inputs Boosts the Accuracy of Neural Networks,"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding. Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs. Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging. The source code is available at https://github.com/abdulrafae/coding_nmt.","['Abdul Rafae Khan', 'Jia Xu', 'Weiwei Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.104.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.104/,8,
PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation,"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context. This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.","['Bin Bi', 'Chenliang Li', 'Chen Wu', 'Ming Yan', 'Wei Wang', 'Songfang Huang', 'Fei Huang', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.700.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.700/,4,
MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision,"The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.","['Patrick Huber', 'Giuseppe Carenini']",https://www.aclweb.org/anthology/2020.emnlp-main.603.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.603/,1,
Multilingual AMR-to-Text Generation,"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics. We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.","['Angela Fan', 'Claire Gardent']",https://www.aclweb.org/anthology/2020.emnlp-main.231.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.231/,2,
Message Passing for Hyper-Relational Knowledge Graphs,"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.","['Mikhail Galkin', 'Priyansh Trivedi', 'Gaurav Maheshwari', 'Ricardo Usbeck', 'Jens Lehmann']",https://www.aclweb.org/anthology/2020.emnlp-main.596.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.596/,10,
Semi-supervised New Event Type Induction and Event Detection,"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.","['Lifu Huang', 'Heng Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.53.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.53/,8,
Do sequence-to-sequence VAEs learn global features of sentences?,"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.","['Tom Bosc', 'Pascal Vincent']",https://www.aclweb.org/anthology/2020.emnlp-main.350.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.350/,4,
Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis,"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality factors. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.","['Yao-Hung Hubert Tsai', 'Martin Ma', 'Muqiao Yang', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency']",https://www.aclweb.org/anthology/2020.emnlp-main.143.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.143/,9,
What time is it? Temporal Analysis of Novels,"Recognizing the flow of time in a story is a crucial aspect of understanding it. Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hour. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.","['Allen Kim', 'Charuta Pethe', 'Steven Skiena']",https://www.aclweb.org/anthology/2020.emnlp-main.730.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.730/,6,
MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering,"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.","['Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral.', 'Yezhou Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.63.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.63/,4,
TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as “what happened before/after [some event]?” We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.","['Qiang Ning', 'Hao Wu', 'Rujun Han', 'Nanyun Peng', 'Matt Gardner', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.88.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.88/,10,
Incremental Neural Coreference Resolution in Constant Memory,"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity’s representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3% relative loss in F1 on OntoNotes 5.0.","['Patrick Xia', 'João Sedoc', 'Benjamin Van Durme']",https://www.aclweb.org/anthology/2020.emnlp-main.695.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.695/,4,
Exploring the Role of Argument Structure in Online Debate Persuasion,"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the best predictive performance in assessing the persuasiveness of the arguments on online debates.","['Jialu Li', 'Esin Durmus', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.716.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.716/,3,
Quantitative argument summarization and beyond: Cross-domain key point analysis,"When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data. Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.","['Roy Bar-Haim', 'Yoav Kantor', 'Lilach Eden', 'Roni Friedman', 'Dan Lahav', 'Noam Slonim']",https://www.aclweb.org/anthology/2020.emnlp-main.3.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.3/,3,
Towards Interpretable Reasoning over Paragraph Effects in Situation,"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step “black box” model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.","['Mucheng Ren', 'Xiubo Geng', 'Tao Qin', 'He-Yan Huang', 'Daxin Jiang']",https://www.aclweb.org/anthology/2020.emnlp-main.548.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.548/,1,
Better Highlighting: Creating Sub-Sentence Summary Highlights,"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.","['Sangwoo Cho', 'Kaiqiang Song', 'Chen Li', 'Dong Yu', 'Hassan Foroosh', 'Fei Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.509.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.509/,2,
META: Metadata-Empowered Weak Supervision for Text Classification,"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","['Dheeraj Mekala', 'Xinyang Zhang', 'Jingbo Shang']",https://www.aclweb.org/anthology/2020.emnlp-main.670.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.670/,4,
MIME: MIMicking Emotions for Empathetic Response Generation,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.","['Navonil Majumder', 'Pengfei Hong', 'Shanshan Peng', 'Jiankun Lu', 'Deepanway Ghosal', 'Alexander Gelbukh', 'Rada Mihalcea', 'Soujanya Poria']",https://www.aclweb.org/anthology/2020.emnlp-main.721.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.721/,2,
Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering,"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.","['Zujie Liang', 'Weitao Jiang', 'Haifeng Hu', 'Jiaying Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.265.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.265/,5,
Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph,"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.","['Xin Lv', 'Xu Han', 'Lei Hou', 'Juanzi Li', 'Zhiyuan Liu', 'Wei Zhang', 'Yichi Zhang', 'Hao Kong', 'Suhui Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.459.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.459/,4,
MovieChats: Chat like Humans in a Closed Domain,"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work","['Hui Su', 'Xiaoyu Shen', 'Zhou Xiao', 'Zheng Zhang', 'Ernie Chang', 'Cheng Zhang', 'Cheng Niu', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.535.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.535/,5,
Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models,"Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision. We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.","['Ethan Wilcox', 'Peng Qian', 'Richard Futrell', 'Ryosuke Kohita', 'Roger Levy', 'Miguel Ballesteros']",https://www.aclweb.org/anthology/2020.emnlp-main.375.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.375/,10,
Improving Multilingual Models with Language-Clustered Vocabularies,"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.","['Hyung Won Chung', 'Dan Garrette', 'Kiat Chuan Tan', 'Jason Riesa']",https://www.aclweb.org/anthology/2020.emnlp-main.367.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.367/,2,
"Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics","This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers’ responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chinese microblog Sina Weibo with over 13 thousand trending topics, emotion votes in 24 fine-grained types from massive participants, and user comments to allow context understanding. In experiments, we examine baseline performance to predict a topic’s possible social emotions in a multilabel classification setting. The results show that a seq2seq model with user comment modeling performs the best, even surpassing human prediction. More analyses shed light on the effects of emotion types, topic description lengths, contexts from user comments, and the limited capacity of the existing models.","['Keyang Ding', 'Jing Li', 'Yuji Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.106.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.106/,7,
A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?,"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist. And therefore it raises a critical question of whether previous creditable approaches can still work well when facing these challenges. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. To further verify our conclusions, we also construct a new open NER dataset that focuses on entity types with weaker name regularity and lower mention coverage to verify our conclusion. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is critical for the models to generalize to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained encoders.","['Hongyu Lin', 'Yaojie Lu', 'Jialong Tang', 'Xianpei Han', 'Le Sun', 'Zhicheng Wei', 'Nicholas Jing Yuan']",https://www.aclweb.org/anthology/2020.emnlp-main.592.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.592/,10,
Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!,"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.","['Jack Hessel', 'Lillian Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.62.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.62/,10,
Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training,"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.","['Joe Stacey', 'Pasquale Minervini', 'Haim Dubossarsky', 'Sebastian Riedel', 'Tim Rocktäschel']",https://www.aclweb.org/anthology/2020.emnlp-main.665.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.665/,4,
Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements,"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,860 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.","['Yang Li', 'Gang Li', 'Luheng He', 'Jingjie Zheng', 'Hong Li', 'Zhiwei Guan']",https://www.aclweb.org/anthology/2020.emnlp-main.443.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.443/,6,
Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach,"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.","['Bowen Tan', 'Lianhui Qin', 'Eric Xing', 'Zhiting Hu']",https://www.aclweb.org/anthology/2020.emnlp-main.510.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.510/,2,
Multilevel Text Alignment with Cross-Document Attention,"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.","['Xuhui Zhou', 'Nikolaos Pappas', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.407.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.407/,2,
Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.","['Dayiheng Liu', 'Yeyun Gong', 'Jie Fu', 'Yu Yan', 'Jiusheng Chen', 'Jiancheng Lv', 'Nan Duan', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.467.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.467/,5,
Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.","['Weishi Wang', 'Steven C.H. Hoi', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.533.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.533/,5,
Generating Diverse Translation from Model Distribution with Dropout,"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.","['Xuanfu Wu', 'Yang Feng', 'Chenze Shao']",https://www.aclweb.org/anthology/2020.emnlp-main.82.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.82/,2,
A Diagnostic Study of Explainability Techniques for Text Classification,"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.","['Pepa Atanasova', 'Jakob Grue Simonsen', 'Christina Lioma', 'Isabelle Augenstein']",https://www.aclweb.org/anthology/2020.emnlp-main.263.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.263/,4,
BERT-ATTACK: Adversarial Attack Against BERT Using BERT,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.","['Linyang Li', 'Ruotian Ma', 'Qipeng Guo', 'Xiangyang Xue', 'Xipeng Qiu']",https://www.aclweb.org/anthology/2020.emnlp-main.500.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.500/,4,
ToTTo: A Controlled Table-To-Text Generation Dataset,"We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.","['Ankur Parikh', 'Xuezhi Wang', 'Sebastian Gehrmann', 'Manaal Faruqui', 'Bhuwan Dhingra', 'Diyi Yang', 'Dipanjan Das']",https://www.aclweb.org/anthology/2020.emnlp-main.89.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.89/,10,
STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.","['Nader Akoury', 'Shufan Wang', 'Josh Whiting', 'Stephen Hood', 'Nanyun Peng', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.525.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.525/,2,
Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder,"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts. Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics. We observe that our model can highly improve short text topic modeling performance. Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.","['Xiaobao Wu', 'Chunping Li', 'Yan Zhu', 'Yishu Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.138.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.138/,4,
"Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations","Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn’s existing contents. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.","['Lingzhi Wang', 'Jing Li', 'Xingshan Zeng', 'Haisong Zhang', 'Kam-Fai Wong']",https://www.aclweb.org/anthology/2020.emnlp-main.538.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.538/,5,
Exploiting Sentence Order in Document Alignment,"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala–English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.","['Brian Thompson', 'Philipp Koehn']",https://www.aclweb.org/anthology/2020.emnlp-main.483.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.483/,2,
DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.","['Valentin Hofmann', 'Janet Pierrehumbert', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.316.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.316/,4,
Affective Event Classification with Discourse-enhanced Self-training,"Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective polarity to event phrases. First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base. Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data. The key idea is to exploit event phrases that occur with a coreferent sentiment expression. The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier’s predictions and the polarities of the event’s coreferent sentiment expressions. Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.","['Yuan Zhuang', 'Tianyu Jiang', 'Ellen Riloff']",https://www.aclweb.org/anthology/2020.emnlp-main.452.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.452/,4,
ALICE: Active Learning with Contrastive Natural Language Explanations,"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model’s structure. We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.","['Weixin Liang', 'James Zou', 'Zhou Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.355.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.355/,8,
OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction,"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost. On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time. Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.","['Keshav Kolluru', 'Vaibhav Adlakha', 'Samarth Aggarwal', 'Mausam', 'Soumen Chakrabarti']",https://www.aclweb.org/anthology/2020.emnlp-main.306.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.306/,8,
Methods for Numeracy-Preserving Word Embeddings,"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.","['Dhanasekar Sundararaman', 'Shijing Si', 'Vivek Subramanian', 'Guoyin Wang', 'Devamanyu Hazarika', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.384.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.384/,8,
Constrained Fact Verification for FEVER,"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.","['Adithya Pratapa', 'Sai Muralidhar Jayanthi', 'Kavya Nerella']",https://www.aclweb.org/anthology/2020.emnlp-main.629.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.629/,6,
BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues,"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.","['Hung Le', 'Doyen Sahoo', 'Nancy Chen', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.145.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.145/,9,
Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation,"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.","['Dana Ruiter', 'Josef van Genabith', 'Cristina España-Bonet']",https://www.aclweb.org/anthology/2020.emnlp-main.202.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.202/,2,
Translation Artifacts in Cross-lingual Transfer Learning,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.","['Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre']",https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.618/,2,
Semantically Inspired AMR Alignment for the Portuguese Language,"Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.","['Rafael Anchiêta', 'Thiago Pardo']",https://www.aclweb.org/anthology/2020.emnlp-main.123.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.123/,8,
Language Model Prior for Low-Resource Neural Machine Translation,"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.","['Christos Baziotis', 'Barry Haddow', 'Alexandra Birch']",https://www.aclweb.org/anthology/2020.emnlp-main.615.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.615/,2,
A Simple Approach to Learning Unsupervised Multilingual Embeddings,"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space. In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques. We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.","['Pratik Jawanpuria', 'Mayank Meghwanshi', 'Bamdev Mishra']",https://www.aclweb.org/anthology/2020.emnlp-main.240.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.240/,2,
De-Biased Court’s View Generation with Causality,"Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court’s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.","['Yiquan Wu', 'Kun Kuang', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Jun Xiao', 'Yueting Zhuang', 'Luo Si', 'Fei Wu']",https://www.aclweb.org/anthology/2020.emnlp-main.56.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.56/,6,
Pre-training for Abstractive Document Summarization by Reinstating Source Text,"Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.","['Yanyan Zou', 'Xingxing Zhang', 'Wei Lu', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.297.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.297/,2,
BioMegatron: Larger Biomedical Domain Language Model,"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].","['Hoo-Chang Shin', 'Yang Zhang', 'Evelina Bakhturina', 'Raul Puri', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Raghav Mani']",https://www.aclweb.org/anthology/2020.emnlp-main.379.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.379/,10,
Towards Interpreting BERT for Reading Comprehension Based QA,"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.","['Sahana Ramnath', 'Preksha Nema', 'Deep Sahni', 'Mitesh M. Khapra']",https://www.aclweb.org/anthology/2020.emnlp-main.261.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.261/,4,
Entity Enhanced BERT Pre-training for Chinese NER,"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.","['Chen Jia', 'Yuefeng Shi', 'Qinrong Yang', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.518.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.518/,8,
Alignment-free Cross-lingual Semantic Role Labeling,"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings. The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence. It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.","['Rui Cai', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.319.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.319/,1,
On the Reliability and Validity of Detecting Approval of Political Actors in Tweets,"Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on social media. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from tweets.","['Indira Sen', 'Fabian Flöck', 'Claudia Wagner']",https://www.aclweb.org/anthology/2020.emnlp-main.110.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.110/,10,3
Learning to Fuse Sentences with Transformers for Summarization,"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning. In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer’s performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.","['Logan Lebanoff', 'Franck Dernoncourt', 'Doo Soon Kim', 'Lidan Wang', 'Walter Chang', 'Fei Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.338.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.338/,2,
COGS: A Compositional Generalization Challenge Based on Semantic Interpretation,"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.","['Najoung Kim', 'Tal Linzen']",https://www.aclweb.org/anthology/2020.emnlp-main.731.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.731/,10,
Aspect Sentiment Classification with Aspect-Specific Opinion Spans,"Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans. In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs. Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.","['Lu Xu', 'Lidong Bing', 'Wei Lu', 'Fei Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.288.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.288/,3,
PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge,"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.","['Yun He', 'Zhuoer Wang', 'Yin Zhang', 'Ruihong Huang', 'James Caverlee']",https://www.aclweb.org/anthology/2020.emnlp-main.611.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.611/,10,
Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations,"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.","['Ramit Sawhney', 'Shivam Agarwal', 'Arnav Wadhwa', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.676.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.676/,6,
Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning,"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.","['Yi Yang', 'Arzoo Katiyar']",https://www.aclweb.org/anthology/2020.emnlp-main.516.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.516/,4,
XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques,"Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xl-amr.","['Rexhina Blloshmi', 'Rocco Tripodi', 'Roberto Navigli']",https://www.aclweb.org/anthology/2020.emnlp-main.195.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.195/,2,
Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.","['Yosi Mass', 'Haggai Roitman']",https://www.aclweb.org/anthology/2020.emnlp-main.343.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.343/,4,
Text Segmentation by Cross Segment Attention,"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.","['Michal Lukasik', 'Boris Dadachev', 'Kishore Papineni', 'Gonçalo Simões']",https://www.aclweb.org/anthology/2020.emnlp-main.380.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.380/,1,
Sub-Instruction Aware Vision-and-Language Navigation,"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent’s performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.","['Yicong Hong', 'Cristian Rodriguez', 'Qi Wu', 'Stephen Gould']",https://www.aclweb.org/anthology/2020.emnlp-main.271.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.271/,6,
What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding,"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Hence, we carry out an empirical study on position embedding of mainstream pre-trained Transformers mainly focusing on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings by feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future works to choose the suitable positional encoding function for specific tasks given the application property.","['Yu-An Wang', 'Yun-Nung Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.555.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.555/,10,
MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems,"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.","['Zhaojiang Lin', 'Andrea Madotto', 'Genta Indra Winata', 'Pascale Fung']",https://www.aclweb.org/anthology/2020.emnlp-main.273.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.273/,5,
What is More Likely to Happen Next? Video-and-Language Future Event Prediction,"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work.","['Jie Lei', 'Licheng Yu', 'Tamara Berg', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.706.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.706/,10,9
Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks,"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.","['Yuanhe Tian', 'Yan Song', 'Fei Xia']",https://www.aclweb.org/anthology/2020.emnlp-main.487.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.487/,1,
Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.","['Xiuyi Chen', 'Fandong Meng', 'Peng Li', 'Feilong Chen', 'Shuang Xu', 'Bo Xu', 'Jie Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.275.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.275/,5,2
SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search,"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.","['Sean MacAvaney', 'Arman Cohan', 'Nazli Goharian']",https://www.aclweb.org/anthology/2020.emnlp-main.341.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.341/,6,
Multi-turn Response Selection using Dialogue Dependency Relations,"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.","['Qi Jia', 'Yizhu Liu', 'Siyu Ren', 'Kenny Zhu', 'Haifeng Tang']",https://www.aclweb.org/anthology/2020.emnlp-main.150.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.150/,1,5
Information-Theoretic Probing with Minimum Description Length,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates “the amount of effort” needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.","['Elena Voita', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.14/,8,
“What Do You Mean by That?” A Parser-Independent Interactive Approach for Enhancing Text-to-SQL,"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users’ natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.","['Yuntao Li', 'Bei Chen', 'Qian Liu', 'Yan Gao', 'Jian-Guang Lou', 'Yan Zhang', 'Dongmei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.561.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.561/,6,
Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,"Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account. In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT). However, graph-based neural networks do not take sequential information into account. In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure. Accordingly, our RGAT model can capture both the speaker dependency and the sequential information. Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations. In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.","['Taichi Ishiwatari', 'Yuki Yasuda', 'Taro Miyazaki', 'Jun Goto']",https://www.aclweb.org/anthology/2020.emnlp-main.597.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.597/,1,
Event Extraction as Machine Reading Comprehension,"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.","['Jian Liu', 'Yubo Chen', 'Kang Liu', 'Wei Bi', 'Xiaojiang Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.128.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.128/,8,
Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions,"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.","['Bodhisattwa Prasad Majumder', 'Harsh Jhamtani', 'Taylor Berg-Kirkpatrick', 'Julian McAuley']",https://www.aclweb.org/anthology/2020.emnlp-main.739.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.739/,5,
Routing Enforced Generative Model for Recipe Generation,"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.","['Zhiwei Yu', 'Hongyu Zang', 'Xiaojun Wan']",https://www.aclweb.org/anthology/2020.emnlp-main.311.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.311/,6,
More Bang for Your Buck: Natural Perturbation for Robust Question Answering,"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.","['Daniel Khashabi', 'Tushar Khot', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.12.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.12/,10,5
Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube,"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively “easy:” speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are “grounded” and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.","['Jack Hessel', 'Zhenhai Zhu', 'Bo Pang', 'Radu Soricut']",https://www.aclweb.org/anthology/2020.emnlp-main.709.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.709/,10,9
ETC: Encoding Long and Structured Inputs in Transformers,"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a “Contrastive Predictive Coding” (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.","['Joshua Ainslie', 'Santiago Ontanon', 'Chris Alberti', 'Vaclav Cvicek', 'Zachary Fisher', 'Philip Pham', 'Anirudh Ravula', 'Sumit Sanghai', 'Qifan Wang', 'Li Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.19/,4,
Unsupervised stance detection for arguments from consequences,"Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.","['Jonathan Kobbe', 'Ioana Hulpuş', 'Heiner Stuckenschmidt']",https://www.aclweb.org/anthology/2020.emnlp-main.4.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.4/,3,7
Distilling Multiple Domains for Neural Machine Translation,"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.","['Anna Currey', 'Prashant Mathur', 'Georgiana Dinu']",https://www.aclweb.org/anthology/2020.emnlp-main.364.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.364/,2,
On the importance of pre-training data volume for compact language models,"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.","['Vincent Micheli', 'Martin d’Hoffschmidt', 'François Fleuret']",https://www.aclweb.org/anthology/2020.emnlp-main.632.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.632/,4,
Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles,"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results—using several state-of-the-art models trained on the Multi-XScience dataset—reveal that Multi-XScience is well suited for abstractive models.","['Yao Lu', 'Yue Dong', 'Laurent Charlin']",https://www.aclweb.org/anthology/2020.emnlp-main.648.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.648/,10,2
X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.","['Zhengbao Jiang', 'Antonios Anastasopoulos', 'Jun Araki', 'Haibo Ding', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.479/,2,4
Coarse-to-Fine Pre-training for Named Entity Recognition,"More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.","['Xue Mengge', 'Bowen Yu', 'Zhenyu Zhang', 'Tingwen Liu', 'Yue Zhang', 'Bin Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.514.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.514/,4,
Gradient-guided Unsupervised Lexically Constrained Text Generation,"Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",['Lei Sha'],https://www.aclweb.org/anthology/2020.emnlp-main.701.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.701/,2,
"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation","News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.","['Dayiheng Liu', 'Yeyun Gong', 'Yu Yan', 'Jie Fu', 'Bo Shao', 'Daxin Jiang', 'Jiancheng Lv', 'Nan Duan']",https://www.aclweb.org/anthology/2020.emnlp-main.505.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.505/,2,
Plug and Play Autoencoders for Conditional Text Generation,"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.","['Florian Mai', 'Nikolaos Pappas', 'Ivan Montero', 'Noah A. Smith', 'James Henderson']",https://www.aclweb.org/anthology/2020.emnlp-main.491.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.491/,2,
“I’d rather just go to bed”: Understanding Indirect Answers,"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret ‘I’m starving.’ in response to ‘Hungry?’, even without direct cue words such as ‘yes’ and ‘no’. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today’s systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus ‘Circa’ with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes.","['Annie Louis', 'Dan Roth', 'Filip Radlinski']",https://www.aclweb.org/anthology/2020.emnlp-main.601.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.601/,5,
Joint Constrained Learning for Event-Event Relation Extraction,"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.","['Haoyu Wang', 'Muhao Chen', 'Hongming Zhang', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.51.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.51/,8,
Friendly Topic Assistant for Transformer Based Abstractive Summarization,"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.","['Zhengjue Wang', 'Zhibin Duan', 'Hao Zhang', 'Chaojie Wang', 'Long Tian', 'Bo Chen', 'Mingyuan Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.35.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.35/,2,
Deconstructing word embedding algorithms,"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.","['Kian Kenyon-Dean', 'Edward Newell', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.681.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.681/,4,
SLM: Learning a Discourse Language Representation with Sentence Unshuffling,"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.","['Haejun Lee', 'Drew A. Hudson', 'Kangwook Lee', 'Christopher D. Manning']",https://www.aclweb.org/anthology/2020.emnlp-main.120.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.120/,4,
Identifying Elements Essential for BERT’s Multilinguality,"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.","['Philipp Dufter', 'Hinrich Schütze']",https://www.aclweb.org/anthology/2020.emnlp-main.358.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.358/,4,
When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models,"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.","['Changlong Yu', 'Jialong Han', 'Peifeng Wang', 'Yangqiu Song', 'Hongming Zhang', 'Wilfred Ng', 'Shuming Shi']",https://www.aclweb.org/anthology/2020.emnlp-main.502.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.502/,1,
Debiasing knowledge graph embeddings,"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.","['Joseph Fisher', 'Arpit Mittal', 'Dave Palfrey', 'Christos Christodoulopoulos']",https://www.aclweb.org/anthology/2020.emnlp-main.595.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.595/,4,
Interactive Refinement of Cross-Lingual Word Embeddings,"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.","['Michelle Yuan', 'Mozhi Zhang', 'Benjamin Van Durme', 'Leah Findlater', 'Jordan Boyd-Graber']",https://www.aclweb.org/anthology/2020.emnlp-main.482.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.482/,2,
Adversarial Self-Supervised Data-Free Distillation for Text Classification,"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge. Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.","['Xinyin Ma', 'Yongliang Shen', 'Gongfan Fang', 'Chen Chen', 'Chenghao Jia', 'Weiming Lu']",https://www.aclweb.org/anthology/2020.emnlp-main.499.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.499/,10,
VD-BERT: A Unified Vision and Dialog Transformer with BERT,"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.","['Yue Wang', 'Shafiq Joty', 'Michael Lyu', 'Irwin King', 'Caiming Xiong', 'Steven C.H. Hoi']",https://www.aclweb.org/anthology/2020.emnlp-main.269.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.269/,5,9
Identifying Exaggerated Language,"While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.","['Li Kong', 'Chuanyi Li', 'Jidong Ge', 'Bin Luo', 'Vincent Ng']",https://www.aclweb.org/anthology/2020.emnlp-main.571.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.571/,10,1
“You are grounded!”: Latent Name Artifacts in Pre-trained Language Models,"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.","['Vered Shwartz', 'Rachel Rudinger', 'Oyvind Tafjord']",https://www.aclweb.org/anthology/2020.emnlp-main.556.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.556/,10,
Learning Structured Representations of Entity Names using ActiveLearning and Weak Supervision,"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.","['Kun Qian', 'Poornima Chozhiyath Raman', 'Yunyao Li', 'Lucian Popa']",https://www.aclweb.org/anthology/2020.emnlp-main.517.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.517/,8,
Neural Topic Modeling with Cycle-Consistent Adversarial Training,"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.","['Xuemeng Hu', 'Rui Wang', 'Deyu Zhou', 'Yuxuan Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.725.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.725/,8,
SLURP: A Spoken Language Understanding Resource Package,"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.","['Emanuele Bastianelli', 'Andrea Vanzo', 'Pawel Swietojanski', 'Verena Rieser']",https://www.aclweb.org/anthology/2020.emnlp-main.588.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.588/,10,9
Speakers Fill Lexical Semantic Gaps with Context,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear—resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this—one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. 𝜌 = 0.40 in English). We then test our main hypothesis—that a word’s lexical ambiguity should negatively correlate with its contextual uncertainty—and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.","['Tiago Pimentel', 'Rowan Hall Maudslay', 'Damian Blasi', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.328.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.328/,1,
Token-level Adaptive Training for Neural Machine Translation,"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.","['Shuhao Gu', 'Jinchao Zhang', 'Fandong Meng', 'Yang Feng', 'Wanying Xie', 'Jie Zhou', 'Dong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.76.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.76/,2,
Scalable Zero-shot Entity Linking with Dense Entity Retrieval,"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.","['Ledell Wu', 'Fabio Petroni', 'Martin Josifoski', 'Sebastian Riedel', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.519.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.519/,8,
Lifelong Language Knowledge Distillation,"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.","['Yung-Sung Chuang', 'Shang-Yu Su', 'Yun-Nung Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.233.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.233/,4,
New Protocols and Negative Results for Textual Entailment Data Collection,"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit *pre-filled* text boxes, reduce previously observed issues with annotation artifacts.","['Samuel Bowman', 'Jennimaria Palomaki', 'Livio Baldini Soares', 'Emily Pitler']",https://www.aclweb.org/anthology/2020.emnlp-main.658.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.658/,10,
Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection,"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.","['Adam Tsakalidis', 'Maria Liakata']",https://www.aclweb.org/anthology/2020.emnlp-main.682.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.682/,1,
Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications,"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.","['Matthew Khoury', 'Rumen Dangovski', 'Longwu Ou', 'Preslav Nakov', 'Yichen Shen', 'Li Jing']",https://www.aclweb.org/anthology/2020.emnlp-main.640.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.640/,4,
Sparse Text Generation,"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: 𝜖-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.","['Pedro Henrique Martins', 'Zita Marinho', 'André F. T. Martins']",https://www.aclweb.org/anthology/2020.emnlp-main.348.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.348/,10,2
Multi-Stage Pre-training for Automated Chinese Essay Scoring,"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..","['Wei Song', 'Kai Zhang', 'Ruiji Fu', 'Lizhen Liu', 'Ting Liu', 'Miaomiao Cheng']",https://www.aclweb.org/anthology/2020.emnlp-main.546.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.546/,6,
"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation","Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt.","['Tahmid Hasan', 'Abhik Bhattacharjee', 'Kazi Samin', 'Masum Hasan', 'Madhusudan Basak', 'M. Sohel Rahman', 'Rifat Shahriyar']",https://www.aclweb.org/anthology/2020.emnlp-main.207.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.207/,10,2
Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning,"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT – a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.","['Tsvetomila Mihaylova', 'Vlad Niculae', 'André F. T. Martins']",https://www.aclweb.org/anthology/2020.emnlp-main.171.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.171/,4,
Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.","['Ruipeng Jia', 'Yanan Cao', 'Hengzhu Tang', 'Fang Fang', 'Cong Cao', 'Shi Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.295.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.295/,2,
ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention,"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.","['José Manuel Gómez-Pérez', 'Raúl Ortega']",https://www.aclweb.org/anthology/2020.emnlp-main.441.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.441/,5,9
Event Extraction by Answering (Almost) Natural Questions,"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).","['Xinya Du', 'Claire Cardie']",https://www.aclweb.org/anthology/2020.emnlp-main.49.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.49/,8,5
"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.","['Ieva Staliūnaitė', 'Ignacio Iacobacci']",https://www.aclweb.org/anthology/2020.emnlp-main.573.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.573/,4,5
APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning,"Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multitask learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.","['Liying Cheng', 'Lidong Bing', 'Qian Yu', 'Wei Lu', 'Luo Si']",https://www.aclweb.org/anthology/2020.emnlp-main.569.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.569/,3,
Conditional Causal Relationships between Emotions and Causes in Texts,"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.","['Xinhong Chen', 'Qing Li', 'Jianping Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.252.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.252/,3,
Distilling Structured Knowledge for Text-Based Relational Reasoning,"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.","['Jin Dong', 'Marc-Antoine Rondeau', 'William L. Hamilton']",https://www.aclweb.org/anthology/2020.emnlp-main.551.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.551/,4,
Discontinuous Constituent Parsing as Sequence Labeling,"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.","['David Vilares', 'Carlos Gómez-Rodríguez']",https://www.aclweb.org/anthology/2020.emnlp-main.221.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.221/,1,
PathQG: Neural Question Generation from Facts,"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.","['Siyuan Wang', 'Zhongyu Wei', 'Zhihao Fan', 'Zengfeng Huang', 'Weijian Sun', 'Qi Zhang', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.729.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.729/,2,
LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.","['Ikuya Yamada', 'Akari Asai', 'Hiroyuki Shindo', 'Hideaki Takeda', 'Yuji Matsumoto']",https://www.aclweb.org/anthology/2020.emnlp-main.523.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.523/,1,
Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms – greedy search, beam search, top-k sampling, and nucleus sampling – are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.","['Sean Welleck', 'Ilia Kulikov', 'Jaedeok Kim', 'Richard Yuanzhe Pang', 'Kyunghyun Cho']",https://www.aclweb.org/anthology/2020.emnlp-main.448.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.448/,4,
MedDialog: Large-scale Medical Dialogue Datasets,"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets – MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System","['Guangtao Zeng', 'Wenmian Yang', 'Zeqian Ju', 'Yue Yang', 'Sicheng Wang', 'Ruisi Zhang', 'Meng Zhou', 'Jiaqi Zeng', 'Xiangyu Dong', 'Ruoyu Zhang', 'Hongchao Fang', 'Penghui Zhu', 'Shu Chen', 'Pengtao Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.743.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.743/,10,
Improving Text Generation with Student-Forcing Optimal Transport,"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.","['Jianqiao Li', 'Chunyuan Li', 'Guoyin Wang', 'Hao Fu', 'Yuhchen Lin', 'Liqun Chen', 'Yizhe Zhang', 'Chenyang Tao', 'Ruiyi Zhang', 'Wenlin Wang', 'Dinghan Shen', 'Qian Yang', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.735.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.735/,2,
To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints,"State of the art research for date-time entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.","['Barun Patra', 'Chala Fufa', 'Pamela Bhattacharya', 'Charles C. Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.678.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.678/,6,1
Pre-training Mention Representations in Coreference Models,"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.","['Yuval Varkel', 'Amir Globerson']",https://www.aclweb.org/anthology/2020.emnlp-main.687.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.687/,4,
Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples,"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.","['Lihao Wang', 'Xiaoqing Zheng']",https://www.aclweb.org/anthology/2020.emnlp-main.228.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.228/,4,
Reformulating Unsupervised Style Transfer as Paraphrase Generation,"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input’s meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.","['Kalpesh Krishna', 'John Wieting', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.55.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.55/,2,
Grounded Compositional Outputs for Adaptive Language Modeling,"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.","['Nikolaos Pappas', 'Phoebe Mulcaire', 'Noah A. Smith']",https://www.aclweb.org/anthology/2020.emnlp-main.96.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.96/,1,
On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.","['Jonathan Pilault', 'Raymond Li', 'Sandeep Subramanian', 'Christopher Pal']",https://www.aclweb.org/anthology/2020.emnlp-main.748.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.748/,2,
With Little Power Comes Great Responsibility,"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.","['Dallas Card', 'Peter Henderson', 'Urvashi Khandelwal', 'Robin Jia', 'Kyle Mahowald', 'Dan Jurafsky']",https://www.aclweb.org/anthology/2020.emnlp-main.745.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.745/,10,
Explainable Automated Fact-Checking for Public Health Claims,"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.","['Neema Kotonya', 'Francesca Toni']",https://www.aclweb.org/anthology/2020.emnlp-main.623.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.623/,6,
Word Rotator’s Distance,"One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover’s distance (optimal transport), which we refer to as word rotator’s distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines. The source code is avaliable at https://github.com/eumesy/wrd","['Sho Yokoi', 'Ryo Takahashi', 'Reina Akama', 'Jun Suzuki', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.236.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.236/,8,
Word class flexibility: A deep contextualized approach,"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.","['Bai Li', 'Guillaume Thomas', 'Yang Xu', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.71.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.71/,1,
A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning,"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.","['Yichi Zhang', 'Zhijian Ou', 'Min Hu', 'Junlan Feng']",https://www.aclweb.org/anthology/2020.emnlp-main.740.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.740/,5,4
Denoising Relation Extraction from Document-level Distant Supervision,"Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE. To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks. The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.","['Chaojun Xiao', 'Yuan Yao', 'Ruobing Xie', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun', 'Fen Lin', 'Leyu Lin']",https://www.aclweb.org/anthology/2020.emnlp-main.300.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.300/,8,4
Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers,"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.","['Hanjie Chen', 'Yangfeng Ji']",https://www.aclweb.org/anthology/2020.emnlp-main.347.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.347/,4,6
Dialogue Response Ranking Training with Large-Scale Human Feedback Data,"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.","['Xiang Gao', 'Yizhe Zhang', 'Michel Galley', 'Chris Brockett', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.28.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.28/,5,7
Program Enhanced Fact Verification with Verbalization and Graph Attention Network,"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.","['Xiaoyu Yang', 'Feng Nie', 'Yufei Feng', 'Quan Liu', 'Zhigang Chen', 'Xiaodan Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.628.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.628/,8,4
Measuring Information Propagation in Literary Social Networks,"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.","['Matthew Sims', 'David Bamman']",https://www.aclweb.org/anthology/2020.emnlp-main.47.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.47/,10,7
Authorship Attribution for Neural Text Generation,"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the “author” of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T1 and T2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/ 302zWdz","['Adaku Uchendu', 'Thai Le', 'Kai Shu', 'Dongwon Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.673.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.673/,2,
Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs,"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations. Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.","['Jueqing Lu', 'Lan Du', 'Ming Liu', 'Joanna Dipnall']",https://www.aclweb.org/anthology/2020.emnlp-main.235.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.235/,8,4
FIND: Human-in-the-Loop Debugging Deep Text Classifiers,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND – a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).","['Piyawat Lertvittayakumjorn', 'Lucia Specia', 'Francesca Toni']",https://www.aclweb.org/anthology/2020.emnlp-main.24.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.24/,10,
Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks,"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.","['Denis Emelin', 'Ivan Titov', 'Rico Sennrich']",https://www.aclweb.org/anthology/2020.emnlp-main.616.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.616/,2,
Explainable Clinical Decision Support from Text,"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.","['Jinyue Feng', 'Chantal Shaib', 'Frank Rudzicz']",https://www.aclweb.org/anthology/2020.emnlp-main.115.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.115/,8,
Do Explicit Alignments Robustly Improve Multilingual Encoders?,"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.","['Shijie Wu', 'Mark Dredze']",https://www.aclweb.org/anthology/2020.emnlp-main.362.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.362/,10,2
CapWAP: Image Captioning with a Purpose,"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs—a natural expression of information need—from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.","['Adam Fisch', 'Kenton Lee', 'Ming-Wei Chang', 'Jonathan H. Clark', 'Regina Barzilay']",https://www.aclweb.org/anthology/2020.emnlp-main.705.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.705/,6,5
Uncertainty-Aware Label Refinement for Sequence Labeling,"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.","['Tao Gui', 'Jiacheng Ye', 'Qi Zhang', 'Zhengyan Li', 'Zichu Fei', 'Yeyun Gong', 'Xuan-Jing Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.181.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.181/,8,4
Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks,"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.","['Chengyue Jiang', 'Yinggong Zhao', 'Shanbo Chu', 'Libin Shen', 'Kewei Tu']",https://www.aclweb.org/anthology/2020.emnlp-main.258.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.258/,4,1
Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking,"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.","['Yexiang Wang', 'Yi Guo', 'Siqi Zhu']",https://www.aclweb.org/anthology/2020.emnlp-main.243.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.243/,5,
Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text,"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.","['Dongfang Li', 'Baotian Hu', 'Qingcai Chen', 'Weihua Peng', 'Anqi Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.111.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.111/,5,8
Deep Weighted MaxSAT for Aspect-based Opinion Extraction,"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.","['Meixi Wu', 'Wenya Wang', 'Sinno Jialin Pan']",https://www.aclweb.org/anthology/2020.emnlp-main.453.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.453/,8,4
Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification. Our source code is available at https://github.com/ShyamSubramanian/HESM.","['Shyam Subramanian', 'Kyumin Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.627.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.627/,8,
Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games,"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model’s action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.","['Subhajit Chaudhury', 'Daiki Kimura', 'Kartik Talamadupula', 'Michiaki Tatsubori', 'Asim Munawar', 'Ryuki Tachibana']",https://www.aclweb.org/anthology/2020.emnlp-main.241.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.241/,4,6
Within-Between Lexical Relation Classification,"We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.","['Oren Barkan', 'Avi Caciularu', 'Ido Dagan']",https://www.aclweb.org/anthology/2020.emnlp-main.284.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.284/,8,1
Social Media Attributions in the Context of Water Crisis,"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of attribution tie detection of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34% on attribution detection and 81.37% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain.","['Rupak Sarkar', 'Sayantan Mahinder', 'Hirak Sarkar', 'Ashiqur KhudaBukhsh']",https://www.aclweb.org/anthology/2020.emnlp-main.109.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.109/,7,6
Compressive Summarization with Plausibility and Salience Modeling,"Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.","['Shrey Desai', 'Jiacheng Xu', 'Greg Durrett']",https://www.aclweb.org/anthology/2020.emnlp-main.507.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.507/,2,
What Have We Achieved on Text Summarization?,"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.","['Dandan Huang', 'Leyang Cui', 'Sen Yang', 'Guangsheng Bao', 'Kun Wang', 'Jun Xie', 'Yue Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.33.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.33/,2,10
Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation,"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.","['Pei Zhang', 'Boxing Chen', 'Niyu Ge', 'Kai Fan']",https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.81/,2,10
Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited.","['Wenpeng Yin', 'Nazneen Fatema Rajani', 'Dragomir Radev', 'Richard Socher', 'Caiming Xiong']",https://www.aclweb.org/anthology/2020.emnlp-main.660.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.660/,8,4
A Simple Yet Strong Pipeline for HotpotQA,"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named , performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.","['Dirk Groeneveld', 'Tushar Khot', 'Mausam', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.711/,5,
Entity Linking in 100 Languages,"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.","['Jan A. Botha', 'Zifei Shan', 'Dan Gillick']",https://www.aclweb.org/anthology/2020.emnlp-main.630.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.630/,8,2
Incomplete Utterance Rewriting as Semantic Segmentation,"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.","['Qian Liu', 'Bei Chen', 'Jian-Guang Lou', 'Bin Zhou', 'Dongmei Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.227.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.227/,2,
LOGAN: Local Group Bias Detection by Clustering,"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.","['Jieyu Zhao', 'Kai-Wei Chang']",https://www.aclweb.org/anthology/2020.emnlp-main.155.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.155/,7,4
Improving Bilingual Lexicon Induction for Low Frequency Words,"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.","['Jiaji Huang', 'Xingyu Cai', 'Kenneth Church']",https://www.aclweb.org/anthology/2020.emnlp-main.100.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.100/,1,
Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering,"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.","['Pratyay Banerjee', 'Chitta Baral.']",https://www.aclweb.org/anthology/2020.emnlp-main.11.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.11/,5,4
Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents,"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as “kitchen” and “bedroom”, and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating “granite” with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.","['Gregory Yauney', 'Jack Hessel', 'David Mimno']",https://www.aclweb.org/anthology/2020.emnlp-main.160.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.160/,6,4
Structured Pruning of Large Language Models,"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.","['Ziheng Wang', 'Jeremy Wohlwend', 'Tao Lei']",https://www.aclweb.org/anthology/2020.emnlp-main.496.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.496/,4,1
Conversational Document Prediction to Assist Customer Care Agents,"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs. We study the task of predicting the documents that customer care agents can use to facilitate users’ needs. We also introduce a new public dataset which supports the aforementioned problem. Using this dataset and two others, we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task. Additionally, we analyze the practicality of such systems in terms of inference time complexity. Our show that an hybrid IR+DL approach provides the best of both worlds.","['Jatin Ganhotra', 'Haggai Roitman', 'Doron Cohen', 'Nathaniel Mills', 'Chulaka Gunasekara', 'Yosi Mass', 'Sachindra Joshi', 'Luis Lastras', 'David Konopnicki']",https://www.aclweb.org/anthology/2020.emnlp-main.25.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.25/,6,10
Learning VAE-LDA Models with Rounded Reparameterization Trick,"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.","['Runzhi Tian', 'Yongyi Mao', 'Richong Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.101.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.101/,4,
An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.","['Bhargavi Paranjape', 'Mandar Joshi', 'John Thickstun', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.153.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.153/,4,
Cold-start Active Learning through Self-supervised Language Modeling,"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.","['Michelle Yuan', 'Hsuan-Tien Lin', 'Jordan Boyd-Graber']",https://www.aclweb.org/anthology/2020.emnlp-main.637.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.637/,4,
Facilitating the Communication of Politeness through Fine-Grained Paraphrasing,"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.","['Liye Fu', 'Susan Fussell', 'Cristian Danescu-Niculescu-Mizil']",https://www.aclweb.org/anthology/2020.emnlp-main.416.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.416/,2,
Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems,"The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool.","['Jan Milan Deriu', 'Don Tuggener', 'Pius von Däniken', 'Jon Ander Campos', 'Álvaro Rodrigo', 'Thiziri Belkacem', 'Aitor Soroa', 'Eneko Agirre', 'Mark Cieliebak']",https://www.aclweb.org/anthology/2020.emnlp-main.326.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.326/,10,5
GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems,"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.","['Shiquan Yang', 'Rui Zhang', 'Sarah Erfani']",https://www.aclweb.org/anthology/2020.emnlp-main.147.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.147/,5,8
Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.","['Goro Kobayashi', 'Tatsuki Kuribayashi', 'Sho Yokoi', 'Kentaro Inui']",https://www.aclweb.org/anthology/2020.emnlp-main.574.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.574/,2,4
SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.","['Rongzhi Zhang', 'Yue Yu', 'Chao Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.691.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.691/,4,10
Analogous Process Structure Induction for Sub-event Sequence Prediction,"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.","['Hongming Zhang', 'Muhao Chen', 'Haoyu Wang', 'Yangqiu Song', 'Dan Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.119.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.119/,8,
CSP:Code-Switching Pre-training for Neural Machine Translation,"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.","['Zhen Yang', 'Bojie Hu', 'Ambyera Han', 'Shen Huang', 'Qi Ju']",https://www.aclweb.org/anthology/2020.emnlp-main.208.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.208/,2,
A Dataset for Tracking Entities in Open Domain Procedural Text,"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI, a high-quality (91.5% coverage as judged by humans and completely vetted), and large-scale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel model architectures.","['Niket Tandon', 'Keisuke Sakaguchi', 'Bhavana Dalvi', 'Dheeraj Rajagopal', 'Peter Clark', 'Michal Guerquin', 'Kyle Richardson', 'Eduard Hovy']",https://www.aclweb.org/anthology/2020.emnlp-main.520.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.520/,10,
Fact or Fiction: Verifying Scientific Claims,"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.","['David Wadden', 'Shanchuan Lin', 'Kyle Lo', 'Lucy Lu Wang', 'Madeleine van Zuylen', 'Arman Cohan', 'Hannaneh Hajishirzi']",https://www.aclweb.org/anthology/2020.emnlp-main.609.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.609/,10,
Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models,"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as “neural knowledge bases” via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).","['Bill Yuchen Lin', 'Seyeon Lee', 'Rahul Khanna', 'Xiang Ren']",https://www.aclweb.org/anthology/2020.emnlp-main.557.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.557/,6,8
Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers,"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese→English, Turkish→English, and English→German directions. Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.","['Yimeng Wu', 'Peyman Passban', 'Mehdi Rezagholizadeh', 'Qun Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.74.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.74/,2,4
Neural Topic Modeling by Incorporating Document Relationship Graph,"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.","['Deyu Zhou', 'Xuemeng Hu', 'Rui Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.310.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.310/,4,8
CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval,"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url]. We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.","['Shuo Sun', 'Kevin Duh']",https://www.aclweb.org/anthology/2020.emnlp-main.340.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.340/,10,
Analyzing Redundancy in Pretrained Transformer Models,"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.","['Fahim Dalvi', 'Hassan Sajjad', 'Nadir Durrani', 'Yonatan Belinkov']",https://www.aclweb.org/anthology/2020.emnlp-main.398.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.398/,10,
Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation,"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers’ robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs – dialog response generation and user simulation, where our model outperforms previous strong baselines.","['Kang Min Yoo', 'Hanbit Lee', 'Franck Dernoncourt', 'Trung Bui', 'Walter Chang', 'Sang-goo Lee']",https://www.aclweb.org/anthology/2020.emnlp-main.274.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.274/,5,10
Tackling the Low-resource Challenge for Canonical Segmentation,"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches out-perform existing ones on all languages by up to 11.4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly low-resource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages.","['Manuel Mager', 'Özlem Çetinoğlu', 'Katharina Kann']",https://www.aclweb.org/anthology/2020.emnlp-main.423.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.423/,4,1
Digital Voicing of Silent Speech,"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.","['David Gaddy', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.445.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.445/,9,
Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.","['Mengyun Chen', 'Tao Ge', 'Xingxing Zhang', 'Furu Wei', 'Ming Zhou']",https://www.aclweb.org/anthology/2020.emnlp-main.581.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.581/,2,10
Towards Modeling Revision Requirements in wikiHow Instructions,"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.","['Irshad Bhat', 'Talita Anthonio', 'Michael Roth']",https://www.aclweb.org/anthology/2020.emnlp-main.675.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.675/,10,4
Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.","['Simon Flachs', 'Ophélie Lacroix', 'Helen Yannakoudakis', 'Marek Rei', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.680.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.680/,10,
Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.","['Zehui Lin', 'Xiao Pan', 'Mingxuan Wang', 'Xipeng Qiu', 'Jiangtao Feng', 'Hao Zhou', 'Lei Li']",https://www.aclweb.org/anthology/2020.emnlp-main.210.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.210/,2,
IGT2P: From Interlinear Glossed Texts to Paradigms,"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.","['Sarah Moeller', 'Ling Liu', 'Changbing Yang', 'Katharina Kann', 'Mans Hulden']",https://www.aclweb.org/anthology/2020.emnlp-main.424.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.424/,1,
Is Graph Structure Necessary for Multi-hop Question Answering?,"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for textual multi-hop reasoning. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.","['Nan Shao', 'Yiming Cui', 'Ting Liu', 'Shijin Wang', 'Guoping Hu']",https://www.aclweb.org/anthology/2020.emnlp-main.583.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.583/,5,10
Intrinsic Probing through Dimension Selection,"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","['Lucas Torroba Hennigen', 'Adina Williams', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.15.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.15/,1,2
Embedding Words in Non-Vector Space with Unsupervised Graph Learning,"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.","['Max Ryabinin', 'Sergei Popov', 'Liudmila Prokhorenkova', 'Elena Voita']",https://www.aclweb.org/anthology/2020.emnlp-main.594.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.594/,4,
Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments.","['Alexander Ku', 'Peter Anderson', 'Roma Patel', 'Eugene Ie', 'Jason Baldridge']",https://www.aclweb.org/anthology/2020.emnlp-main.356.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.356/,6,2
HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification,"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.","['Wenshuo Yang', 'Jiyi Li', 'Fumiyo Fukumoto', 'Yanming Ye']",https://www.aclweb.org/anthology/2020.emnlp-main.545.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.545/,4,
Multilingual Offensive Language Identification with Cross-lingual Embeddings,"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources. We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish. Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.","['Tharindu Ranasinghe', 'Marcos Zampieri']",https://www.aclweb.org/anthology/2020.emnlp-main.470.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.470/,7,2
Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings,"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.","['Yue Wang', 'Jing Li', 'Michael Lyu', 'Irwin King']",https://www.aclweb.org/anthology/2020.emnlp-main.268.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.268/,7,8
Understanding Procedural Text using Interactive Entity Networks,"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned. In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking. In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions. Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes. We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.","['Jizhi Tang', 'Yansong Feng', 'Dongyan Zhao']",https://www.aclweb.org/anthology/2020.emnlp-main.591.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.591/,2,
Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning,"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent’s actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating commonsense captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset “Video-to-Commonsense (V2C)” that contains \sim9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.","['Zhiyuan Fang', 'Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral.', 'Yezhou Yang']",https://www.aclweb.org/anthology/2020.emnlp-main.61.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.61/,6,5
Exploring and Predicting Transferability across NLP Tasks,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.","['Tu Vu', 'Tong Wang', 'Tsendsuren Munkhdalai', 'Alessandro Sordoni', 'Adam Trischler', 'Andrew Mattarella-Micke', 'Subhransu Maji', 'Mohit Iyyer']",https://www.aclweb.org/anthology/2020.emnlp-main.635.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.635/,10,
Towards Better Context-aware Lexical Semantics:Adjusting Contextualized Representations through Static Anchors,"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors. Our method requires only another pre-trained model and no labeled data is needed. We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.","['Qianchu Liu', 'Diana McCarthy', 'Anna Korhonen']",https://www.aclweb.org/anthology/2020.emnlp-main.333.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.333/,4,1
SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling,"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77). In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.","['Di Wu', 'Liang Ding', 'Fan Lu', 'Jian Xie']",https://www.aclweb.org/anthology/2020.emnlp-main.152.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.152/,3,
Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations,"Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.","['Emily Allaway', 'Kathleen McKeown']",https://www.aclweb.org/anthology/2020.emnlp-main.717.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.717/,10,4
Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation,"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.","['Mehrad Moradshahi', 'Giovanni Campagna', 'Sina Semnani', 'Silei Xu', 'Monica Lam']",https://www.aclweb.org/anthology/2020.emnlp-main.481.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.481/,2,5
Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media,"In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.","['Shamik Roy', 'Dan Goldwasser']",https://www.aclweb.org/anthology/2020.emnlp-main.620.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.620/,7,6
Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation,"We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.","['Robert Munro', 'Alex (Carmen) Morrison']",https://www.aclweb.org/anthology/2020.emnlp-main.157.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.157/,3,10
Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution,"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.","['Yordan Yordanov', 'Oana-Maria Camburu', 'Vid Kocijan', 'Thomas Lukasiewicz']",https://www.aclweb.org/anthology/2020.emnlp-main.402.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.402/,7,10
Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning,"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.","['Wanyun Cui', 'Guangyu Zheng', 'Wei Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.444.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.444/,6,9
Social Chemistry 101: Learning to Reason about Social and Moral Norms,"Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives. For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct, such as “It is expected that you report crimes.” We present SOCIAL CHEMISTRY, a new conceptual formalism to study people’s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","['Maxwell Forbes', 'Jena D. Hwang', 'Vered Shwartz', 'Maarten Sap', 'Yejin Choi']",https://www.aclweb.org/anthology/2020.emnlp-main.48.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.48/,7,10
Partially-Aligned Data-to-Text Generation with Distant Supervision,"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data’s supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.","['Zihao Fu', 'Bei Shi', 'Wai Lam', 'Lidong Bing', 'Zhiyuan Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.738.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.738/,8,4
Zero-Shot Crosslingual Sentence Simplification,"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.","['Jonathan Mallinson', 'Rico Sennrich', 'Mirella Lapata']",https://www.aclweb.org/anthology/2020.emnlp-main.415.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.415/,2,
Fast semantic parsing with well-typedness guarantees,"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.","['Matthias Lindemann', 'Jonas Groschwitz', 'Alexander Koller']",https://www.aclweb.org/anthology/2020.emnlp-main.323.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.323/,1,
Sequence-Level Mixed Sample Data Augmentation,"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.","['Demi Guo', 'Yoon Kim', 'Alexander M. Rush']",https://www.aclweb.org/anthology/2020.emnlp-main.447.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.447/,4,
A Simple and Effective Model for Answering Multi-span Questions,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.","['Elad Segal', 'Avia Efrat', 'Mor Shoham', 'Amir Globerson', 'Jonathan Berant']",https://www.aclweb.org/anthology/2020.emnlp-main.248.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.248/,5,2
MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding,"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.","['Qinxin Wang', 'Hao Tan', 'Sheng Shen', 'Michael Mahoney', 'Zhewei Yao']",https://www.aclweb.org/anthology/2020.emnlp-main.159.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.159/,6,
Content Planning for Neural Story Generation with Aristotelian Rescoring,"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle’s Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.","['Seraphina Goldfarb-Tarrant', 'Tuhin Chakrabarty', 'Ralph Weischedel', 'Nanyun Peng']",https://www.aclweb.org/anthology/2020.emnlp-main.351.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.351/,2,
Are “Undocumented Workers” the Same as “Illegal Aliens”? Disentangling Denotation and Connotation in Vector Spaces,"In politics, neologisms are frequently invented for partisan objectives. For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., “immigrants” vs. “aliens”, “estate tax” vs. “death tax”) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.","['Albert Webson', 'Zhizhong Chen', 'Carsten Eickhoff', 'Ellie Pavlick']",https://www.aclweb.org/anthology/2020.emnlp-main.335.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.335/,7,4
Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations,"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models’ performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.","['Wanrong Zhu', 'Xin Wang', 'Pradyumna Narayana', 'Kazoo Sone', 'Sugato Basu', 'William Yang Wang']",https://www.aclweb.org/anthology/2020.emnlp-main.708.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.708/,10,
Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of ‘the doctor removed his mask’ is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.","['Ana Valeria González', 'Maria Barrett', 'Rasmus Hvingelby', 'Kellie Webster', 'Anders Søgaard']",https://www.aclweb.org/anthology/2020.emnlp-main.209.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.209/,2,
Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.","['Weijie Yu', 'Chen Xu', 'Jun Xu', 'Liang Pang', 'Xiaopeng Gao', 'Xiaozhao Wang', 'Ji-Rong Wen']",https://www.aclweb.org/anthology/2020.emnlp-main.239.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.239/,4,1
PyMT5: multi-mode translation of natural language and Python code with transformers,"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.","['Colin Clement', 'Dawn Drain', 'Jonathan Timcheck', 'Alexey Svyatkovskiy', 'Neel Sundaresan']",https://www.aclweb.org/anthology/2020.emnlp-main.728.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.728/,2,10
Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model,"Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.","['Jun Yen Leung', 'Guy Emerson', 'Ryan Cotterell']",https://www.aclweb.org/anthology/2020.emnlp-main.329.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.329/,2,
OCR Post Correction for Endangered Language Texts,"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.","['Shruti Rijhwani', 'Antonios Anastasopoulos', 'Graham Neubig']",https://www.aclweb.org/anthology/2020.emnlp-main.478.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.478/,10,2
"When BERT Plays the Lottery, All Tickets Are Winning","Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful. We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.","['Sai Prasanna', 'Anna Rogers', 'Anna Rumshisky']",https://www.aclweb.org/anthology/2020.emnlp-main.259.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.259/,4,
A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media,"Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user’s historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.","['Ramit Sawhney', 'Harshit Joshi', 'Saumya Gandhi', 'Rajiv Shah']",https://www.aclweb.org/anthology/2020.emnlp-main.619.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.619/,7,6
Contrastive Distillation on Intermediate Representations for Language Model Compression,"Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.","['Siqi Sun', 'Zhe Gan', 'Yuwei Fang', 'Yu Cheng', 'Shuohang Wang', 'Jingjing Liu']",https://www.aclweb.org/anthology/2020.emnlp-main.36.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.36/,4,
Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.","['Haozhe Ji', 'Pei Ke', 'Shaohan Huang', 'Furu Wei', 'Xiaoyan Zhu', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.54.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.54/,2,5
Unsupervised Question Decomposition for Question Answering,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.","['Ethan Perez', 'Patrick Lewis', 'Wen-tau Yih', 'Kyunghyun Cho', 'Douwe Kiela']",https://www.aclweb.org/anthology/2020.emnlp-main.713.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.713/,5,
Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character’s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.","['Victor Martinez', 'Krishna Somandepalli', 'Yalda Tehranian-Uhls', 'Shrikanth Narayanan']",https://www.aclweb.org/anthology/2020.emnlp-main.387.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.387/,3,7
Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.","['Chunyuan Li', 'Xiang Gao', 'Yuan Li', 'Baolin Peng', 'Xiujun Li', 'Yizhe Zhang', 'Jianfeng Gao']",https://www.aclweb.org/anthology/2020.emnlp-main.378.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.378/,4,
Some Languages Seem Easier to Parse Because Their Treebanks Leak,"Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data? We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.",['Anders Søgaard'],https://www.aclweb.org/anthology/2020.emnlp-main.220.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.220/,1,
Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.","['Reuben Tan', 'Bryan Plummer', 'Kate Saenko']",https://www.aclweb.org/anthology/2020.emnlp-main.163.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.163/,10,7
Learning a Cost-Effective Annotation Policy for Question Answering,"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.","['Bernhard Kratzwald', 'Stefan Feuerriegel', 'Huan Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.246.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.246/,5,
Semantic Role Labeling Guided Multi-turn Dialogue ReWriter,"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.","['Kun Xu', 'Haochen Tan', 'Linfeng Song', 'Han Wu', 'Haisong Zhang', 'Linqi Song', 'Dong Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.537.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.537/,1,4
LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space,"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.","['Muhammad Tasnim Mohiuddin', 'M Saiful Bari', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.215.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.215/,2,
Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments suggest that there hasn’t been much progress in multi-hop QA in the reading comprehension setting. For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning (19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction.","['Harsh Trivedi', 'Niranjan Balasubramanian', 'Tushar Khot', 'Ashish Sabharwal']",https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.712/,5,
Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation,"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).","['Jason Lee', 'Raphael Shu', 'Kyunghyun Cho']",https://www.aclweb.org/anthology/2020.emnlp-main.73.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.73/,2,
A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization,"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.","['Jinghui Yan', 'Yining Wang', 'Lu Xiang', 'Yu Zhou', 'Chengqing Zong']",https://www.aclweb.org/anthology/2020.emnlp-main.116.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.116/,8,
Dense Passage Retrieval for Open-Domain Question Answering,"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.","['Vladimir Karpukhin', 'Barlas Oguz', 'Sewon Min', 'Patrick Lewis', 'Ledell Wu', 'Sergey Edunov', 'Danqi Chen', 'Wen-tau Yih']",https://www.aclweb.org/anthology/2020.emnlp-main.550.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.550/,5,2
Grounded Adaptation for Zero-shot Executable Semantic Parsing,"We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.","['Victor Zhong', 'Mike Lewis', 'Sida I. Wang', 'Luke Zettlemoyer']",https://www.aclweb.org/anthology/2020.emnlp-main.558.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.558/,8,
Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation,"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training. Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.","['Maximiliana Behnke', 'Kenneth Heafield']",https://www.aclweb.org/anthology/2020.emnlp-main.211.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.211/,2,
Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training,"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.","['Wanwei He', 'Min Yang', 'Rui Yan', 'Chengming Li', 'Ying Shen', 'Ruifeng Xu']",https://www.aclweb.org/anthology/2020.emnlp-main.281.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.281/,8,4
Understanding the Difficulty of Training Transformers,"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance","['Liyuan Liu', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen', 'Jiawei Han']",https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.463/,4,1
Regularizing Dialogue Generation by Imitating Implicit Scenarios,"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.","['Shaoxiong Feng', 'Xuancheng Ren', 'Hongshen Chen', 'Bin Sun', 'Kan Li', 'Xu Sun']",https://www.aclweb.org/anthology/2020.emnlp-main.534.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.534/,5,
We Can Detect Your Bias: Predicting the Political Ideology of News Articles,"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology –left, center, or right–, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.","['Ramy Baly', 'Giovanni Da San Martino', 'James Glass', 'Preslav Nakov']",https://www.aclweb.org/anthology/2020.emnlp-main.404.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.404/,10,7
Unsupervised Parsing via Constituency Tests,"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.","['Steven Cao', 'Nikita Kitaev', 'Dan Klein']",https://www.aclweb.org/anthology/2020.emnlp-main.389.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.389/,1,
Visually Grounded Compound PCFGs,"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs).","['Yanpeng Zhao', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.354.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.354/,6,1
Disentangle-based Continual Graph Representation Learning,"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human’s ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models. The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.","['Xiaoyu Kou', 'Yankai Lin', 'Shaobo Liu', 'Peng Li', 'Jie Zhou', 'Yan Zhang']",https://www.aclweb.org/anthology/2020.emnlp-main.237.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.237/,4,10
Form2Seq : A Framework for Higher-Order Form Structure Extraction,"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures. We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms. To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task. We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation. Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines. Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.","['Milan Aggarwal', 'Hiresh Gupta', 'Mausoom Sarkar', 'Balaji Krishnamurthy']",https://www.aclweb.org/anthology/2020.emnlp-main.314.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.314/,8,
On the Ability and Limitations of Transformers to Recognize Formal Languages,"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.","['Satwik Bhattamishra', 'Kabir Ahuja', 'Navin Goyal']",https://www.aclweb.org/anthology/2020.emnlp-main.576.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.576/,4,2
A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation,"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model. Besides, we utilize a transfer learning method to improve the performance of OOV words. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our method achieves the state-of-the-art performances on all datasets. Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.","['Kaiyu Huang', 'Degen Huang', 'Zhuang Liu', 'Fengran Mo']",https://www.aclweb.org/anthology/2020.emnlp-main.318.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.318/,1,
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.","['Tianlu Wang', 'Xuezhi Wang', 'Yao Qin', 'Ben Packer', 'Kang Li', 'Jilin Chen', 'Alex Beutel', 'Ed Chi']",https://www.aclweb.org/anthology/2020.emnlp-main.417.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.417/,2,
An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets,"Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators’ attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.","['Gregory Spell', 'Brian Guay', 'Sunshine Hillygus', 'Lawrence Carin']",https://www.aclweb.org/anthology/2020.emnlp-main.46.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.46/,3,7
Q-learning with Language Model for Edit-based Unsupervised Summarization,"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.","['Ryosuke Kohita', 'Akifumi Wachi', 'Yang Zhao', 'Ryuki Tachibana']",https://www.aclweb.org/anthology/2020.emnlp-main.34.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.34/,4,2
Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.","['Sanyuan Chen', 'Yutai Hou', 'Yiming Cui', 'Wanxiang Che', 'Ting Liu', 'Xiangzhan Yu']",https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.634/,4,
UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.","['Jian Guan', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.736.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.736/,10,
Modeling Protagonist Emotions for Emotion-Aware Storytelling,"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.","['Faeze Brahman', 'Snigdha Chaturvedi']",https://www.aclweb.org/anthology/2020.emnlp-main.426.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.426/,3,2
Towards Persona-Based Empathetic Conversational Models,"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.","['Peixiang Zhong', 'Chen Zhang', 'Hao Wang', 'Yong Liu', 'Chunyan Miao']",https://www.aclweb.org/anthology/2020.emnlp-main.531.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.531/,7,
Cross-Thought for Sentence Encoder Pre-training,"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.","['Shuohang Wang', 'Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Yu Cheng', 'Jingjing Liu', 'Jing Jiang']",https://www.aclweb.org/anthology/2020.emnlp-main.30.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.30/,8,5
"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching","Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.","['Sangwhan Moon', 'Naoaki Okazaki']",https://www.aclweb.org/anthology/2020.emnlp-main.631.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.631/,6,4
Revisiting Modularized Multilingual NMT to Meet Industrial Demands,"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.","['Sungwon Lyu', 'Bokyung Son', 'Kichang Yang', 'Jaekyoung Bae']",https://www.aclweb.org/anthology/2020.emnlp-main.476.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.476/,2,
T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack,"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models. Our code is publicly available at https://github.com/AI-secure/T3/.","['Boxin Wang', 'Hengzhi Pei', 'Boyuan Pan', 'Qian Chen', 'Shuohang Wang', 'Bo Li']",https://www.aclweb.org/anthology/2020.emnlp-main.495.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.495/,3,5
Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data,"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.","['Rongsheng Zhang', 'Yinhe Zheng', 'Jianzhi Shao', 'Xiaoxi Mao', 'Yadong Xi', 'Minlie Huang']",https://www.aclweb.org/anthology/2020.emnlp-main.277.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.277/,10,5
Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning,"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.","['Tao Shen', 'Yi Mao', 'Pengcheng He', 'Guodong Long', 'Adam Trischler', 'Weizhu Chen']",https://www.aclweb.org/anthology/2020.emnlp-main.722.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.722/,4,8
Few-Shot Learning for Opinion Summarization,"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.","['Arthur Bražinskas', 'Mirella Lapata', 'Ivan Titov']",https://www.aclweb.org/anthology/2020.emnlp-main.337.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.337/,2,
To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging,"Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.","['Kasturi Bhattacharjee', 'Miguel Ballesteros', 'Rishita Anubhai', 'Smaranda Muresan', 'Jie Ma', 'Faisal Ladhak', 'Yaser Al-Onaizan']",https://www.aclweb.org/anthology/2020.emnlp-main.636.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.636/,2,8
End-to-End Slot Alignment and Recognition for Cross-Lingual NLU,"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.","['Weijia Xu', 'Batool Haider', 'Saab Mansour']",https://www.aclweb.org/anthology/2020.emnlp-main.410.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.410/,2,10
Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols,"Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.","['Prachi Jain', 'Sushant Rathi', 'Mausam', 'Soumen Chakrabarti']",https://www.aclweb.org/anthology/2020.emnlp-main.305.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.305/,10,
Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading,"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern.","['Yifan Gao', 'Chien-Sheng Wu', 'Jingjing Li', 'Shafiq Joty', 'Steven C.H. Hoi', 'Caiming Xiong', 'Irwin King', 'Michael Lyu']",https://www.aclweb.org/anthology/2020.emnlp-main.191.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.191/,5,
TNT: Text Normalization based Pre-training of Transformers for Content Moderation,"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.","['Fei Tan', 'Yifan Hu', 'Changwei Hu', 'Keqian Li', 'Kevin Yen']",https://www.aclweb.org/anthology/2020.emnlp-main.383.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.383/,1,4
Online Conversation Disentanglement with Pointer Networks,"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.","['Tao Yu', 'Shafiq Joty']",https://www.aclweb.org/anthology/2020.emnlp-main.512.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.512/,5,4
Factual Error Correction for Abstractive Summarization Models,"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.","['Meng Cao', 'Yue Dong', 'Jiapeng Wu', 'Jackie Chi Kit Cheung']",https://www.aclweb.org/anthology/2020.emnlp-main.506.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.506/,2,
POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training,"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields a logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research.","['Yizhe Zhang', 'Guoyin Wang', 'Chunyuan Li', 'Zhe Gan', 'Chris Brockett', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.698.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.698/,2,
Augmented Natural Language for Generative Sequence Labeling,"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results. Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.","['Ben Athiwaratkun', 'Cicero dos Santos', 'Jason Krone', 'Bing Xiang']",https://www.aclweb.org/anthology/2020.emnlp-main.27.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.27/,8,
Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models,"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.","['Alexander Terenin', 'Måns Magnusson', 'Leif Jonsson']",https://www.aclweb.org/anthology/2020.emnlp-main.234.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.234/,8,
DORB: Dynamically Optimizing Multiple Rewards with Bandits,"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.","['Ramakanth Pasunuru', 'Han Guo', 'Mohit Bansal']",https://www.aclweb.org/anthology/2020.emnlp-main.625.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.625/,4,
Substance over Style: Document-Level Targeted Content Transfer,"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model’s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.","['Allison Hegel', 'Sudha Rao', 'Asli Celikyilmaz', 'William B. Dolan']",https://www.aclweb.org/anthology/2020.emnlp-main.526.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.526/,2,
Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders,"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.","['Andrew Drozdov', 'Subendhu Rongali', 'Yi-Pei Chen', 'Tim O’Gorman', 'Mohit Iyyer', 'Andrew McCallum']",https://www.aclweb.org/anthology/2020.emnlp-main.392.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.392/,1,4
Small but Mighty: New Benchmarks for Split and Rephrase,"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable syntactic cues caused by its automatic generation process. Taking advantage of such cues, we show that even a simple rule-based model can perform on par with the state-of-the-art model. To remedy such limitations, we collect and release two crowdsourced benchmark datasets. We not only make sure that they contain significantly more diverse syntax, but also carefully control for their quality according to a well-defined set of criteria. While no satisfactory automatic metric exists, we apply fine-grained manual evaluation based on these criteria using crowdsourcing, showing that our datasets better represent the task and are significantly more challenging for the models.","['Li Zhang', 'Huaiyu Zhu', 'Siddhartha Brahma', 'Yunyao Li']",https://www.aclweb.org/anthology/2020.emnlp-main.91.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.91/,2,10
Don’t Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation,"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.","['Daniel Loureiro', 'Jose Camacho-Collados']",https://www.aclweb.org/anthology/2020.emnlp-main.283.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.283/,8,10
Compositional Demographic Word Embeddings,"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.","['Charles Welch', 'Jonathan K. Kummerfeld', 'Verónica Pérez-Rosas', 'Rada Mihalcea']",https://www.aclweb.org/anthology/2020.emnlp-main.334.pdf,https://www.aclweb.org/anthology/2020.emnlp-main.334/,7,10
"Challenges and strategies for running controlled crowdsourcing
  experiments","  This paper reports on the challenges and lessons we learned while runningcontrolled experiments in crowdsourcing platforms. Crowdsourcing is becoming anattractive technique to engage a diverse and large pool of subjects inexperimental research, allowing researchers to achieve levels of scale andcompletion times that would otherwise not be feasible in lab settings. However,the scale and flexibility comes at the cost of multiple and sometimes unknownsources of bias and confounding factors that arise from technical limitationsof crowdsourcing platforms and from the challenges of running controlledexperiments in the ""wild"". In this paper, we take our experience in runningsystematic evaluations of task design as a motivating example to explore,describe, and quantify the potential impact of running uncontrolledcrowdsourcing experiments and derive possible coping strategies. Among thechallenges identified, we can mention sampling bias, controlling the assignmentof subjects to experimental conditions, learning effects, and reliability ofcrowdsourcing results. According to our empirical studies, the impact ofpotential biases and confounding factors can amount to a 38\% loss in theutility of the data collected in uncontrolled settings; and it cansignificantly change the outcome of experiments. These issues ultimatelyinspired us to implement CrowdHub, a system that sits on top of majorcrowdsourcing platforms and allows researchers and practitioners to runcontrolled crowdsourcing projects.","Jorge Ram\'irez, Marcos Baez, Fabio Casati, Luca Cernuzzi, Boualem
  Benatallah",,,11,
"Evaluating Saliency Map Explanations for Convolutional Neural Networks:
  A User Study","  Convolutional neural networks (CNNs) offer great machine learning performanceover a range of applications, but their operation is hard to interpret, evenfor experts. Various explanation algorithms have been proposed to address thisissue, yet limited research effort has been reported concerning their userevaluation. In this paper, we report on an online between-group user studydesigned to evaluate the performance of ""saliency maps"" - a popular explanationalgorithm for image classification applications of CNNs. Our results indicatethat saliency maps produced by the LRP algorithm helped participants to learnabout some specific image features the system is sensitive to. However, themaps seem to provide very limited help for participants to anticipate thenetwork's output for new images. Drawing on our findings, we highlightimplications for design and further research on explainable AI. In particular,we argue the HCI and AI communities should look beyond instance-levelexplanations.","Ahmed Alqaraawi, Martin Schuessler, Philipp Wei{\ss}, Enrico Costanza
  and Nadia Berthouze",,,11,
"A systematic literature review on machine learning applications for
  consumer sentiment analysis using online reviews","  Consumer sentiment analysis is a recent fad for social media relatedapplications such as healthcare, crime, finance, travel, and academics.Disentangling consumer perception to gain insight into the desired objectiveand reviews is significant. With the advancement of technology, a massiveamount of social web-data increasing in terms of volume, subjectivity, andheterogeneity, becomes challenging to process it manually. Machine learningtechniques have been utilized to handle this difficulty in real-lifeapplications. This paper presents the study to find out the usefulness, scope,and applicability of this alliance of Machine Learning techniques for consumersentiment analysis on online reviews in the domain of hospitality and tourism.We have shown a systematic literature review to compare, analyze, explore, andunderstand the attempts and direction in a proper way to find research gaps toillustrating the future scope of this pairing. This work is contributing to theextant literature in two ways; firstly, the primary objective is to read andanalyze the use of machine learning techniques for consumer sentiment analysison online reviews in the domain of hospitality and tourism. Secondly, in thiswork, we presented a systematic approach to identify, collect observationalevidence, results from the analysis, and assimilate observations of all relatedhigh-quality research to address particular research queries referring to thedescribed research area.",Praphula Kumar Jain and Rajendra Pamula,,,11,
"Supporting the Supporters of Unaccompanied Migrant Youth: Designing for
  Social-ecological Resilience","  Unaccompanied migrant youth, fleeing to a new country without their parents,are exposed to mental health risks. Resilience interventions mitigate suchrisks, but access can be hindered by systemic and personal barriers. While muchwork has recently addressed designing technology to promote mental health, nonehas focused on the needs of these populations. This paper presents the resultsof interviews with 18 professional/ volunteer support workers and 5unaccompanied migrant youths, followed by three design workshops. The resultspoint to the diverse systems that can facilitate youths' resiliencedevelopment. The relationship between the youth and volunteers acting asmentors is particularly important for increasing resilience but comes withchallenges. This suggests the relevance of a social-ecological model ofresilience with a focus on designing technology to support the mentors in orderto help them better support the youth. We conclude by mapping out the designspace for mentor support.","Franziska Tachtler, Toni Michel, Petr Slov\'ak, Geraldine Fitzpatrick",,,11,
Overview of Surgical Simulation,"  Motivated by the current demand of clinical governance, surgical simulationis now a well-established modality for basic skills training and assessment.The practical deployment of the technique is a multi-disciplinary ventureencompassing areas in engineering, medicine and psychology. This paper providesan overview of the key topics involved in surgical simulation and associatedtechnical challenges. The paper discusses the clinical motivation for surgicalsimulation, the use of virtual environments for surgical training, modelacquisition and simplification, deformable models, collision detection, tissueproperty measurement, haptic rendering and image synthesis. Additional topicsinclude surgical skill training and assessment metrics as well as challengesfacing the incorporation of surgical simulation into medical educationcurricula.",Mohamed A. ElHelw,,,11,
"Improving Usability of User Centric Decision Making of Multi-Attribute
  Products on E-commerce Websites","  The high number of products available makes it difficult for a user to findthe most suitable products according to their needs. This problem is especiallyexacerbated when the user is trying to optimize multiple attributes duringproduct selection, e.g. memory size and camera resolution requirements in caseof smartphones. Previous studies have shown that such users search extensivelyto find a product that best meets their needs. In this paper, we propose aninterface that will help users in selecting a multi-attribute product through aseries of visualizations. This interface is especially targeted for users thatdesire to purchase the best possible product according to some criteria. Theinterface works by allowing the user to progressively shortlist products andultimately select the most appropriate product from a very small considerationset. We evaluated our proposed interface by conducting a controlled experimentthat empirically measures the efficiency, effectiveness and satisfaction of ourvisualization based interface and a typical e-commerce interface. The resultsshowed that our proposed interface allowed the user to find a desired productquickly and correctly, moreover, the subjective opinion of the users alsofavored our proposed interface.","Roquia Mushtaq, Naveed Ahmad, Aimal Rextin, and Muhammad Muddassir
  Malik",,,11,
"Designing Personalized Interaction of a Socially Assistive Robot for
  Stroke Rehabilitation Therapy","  The research of a socially assistive robot has a potential to augment andassist physical therapy sessions for patients with neurological andmusculoskeletal problems (e.g. stroke). During a physical therapy session,generating personalized feedback is critical to improve patient's engagement.However, prior work on socially assistive robotics for physical therapy hasmainly utilized pre-defined corrective feedback even if patients have variousphysical and functional abilities. This paper presents an interactive approachof a socially assistive robot that can dynamically select kinematic features ofassessment on individual patient's exercises to predict the quality of motionand provide patient-specific corrective feedback for personalized interactionof a robot exercise coach.","Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre
  Bernardino, and Sergi Berm\'udez i Badia",,,11,
How Visualization PhD Students Cope with Paper Rejections,"  We conducted a questionnaire study aimed towards PhD students in the field ofvisualization research to understand how they cope with paper rejections. Wecollected responses from 24 participants and performed a qualitative analysisof the data in relation to the provided support by collaborators, resubmissionstrategies, handling multiple rejects, and personal impression of the reviews.The results indicate that the PhD students in the visualization communitygenerally cope well with the negative reviews and, with experience, learn howto act accordingly to improve and resubmit their work. Our results reveal themain coping strategies that can be applied for constructively handling rejectedvisualization papers. The most prominent strategies include: discussing reviewswith collaborators and making a resubmission plan, doing a major revision toimprove the work, shortening the work, and seeing rejection as a positivelearning experience.","Shivam Agarwal, Shahid Latif, and Fabian Beck",,,11,
"Revealing Neural Network Bias to Non-Experts Through Interactive
  Counterfactual Examples","  AI algorithms are not immune to biases. Traditionally, non-experts havelittle control in uncovering potential social bias (e.g., gender bias) in thealgorithms that may impact their lives. We present a preliminary design for aninteractive visualization tool CEB to reveal biases in a commonly used AImethod, Neural Networks (NN). CEB combines counterfactual examples andabstraction of an NN decision process to empower non-experts to detect bias.This paper presents the design of CEB and initial findings of an expert panel(n=6) with AI, HCI, and Social science experts.","Chelsea M. Myers, Evan Freed, Luis Fernando Laris Pardo, Anushay
  Furqan, Sebastian Risi, Jichen Zhu",,,11,
"Eliciting User Preferences for Personalized Explanations for Video
  Summaries","  Video summaries or highlights are a compelling alternative for exploring andcontextualizing unprecedented amounts of video material. However, thesummarization process is commonly automatic, non-transparent and potentiallybiased towards particular aspects depicted in the original video. Therefore,our aim is to help users like archivists or collection managers to quicklyunderstand which summaries are the most representative for an original video.In this paper, we present empirical results on the utility of different typesof visual explanations to achieve transparency for end users on howrepresentative video summaries are, with respect to the original video. Weconsider four types of video summary explanations, which use in different waysthe concepts extracted from the original video subtitles and the video stream,and their prominence. The explanations are generated to meet target userpreferences and express different dimensions of transparency: conceptprominence, semantic coverage, distance and quantity of coverage. In two userstudies we evaluate the utility of the visual explanations for achievingtransparency for end users. Our results show that explanations representing allof the dimensions have the highest utility for transparency, and consequently,for understanding the representativeness of video summaries.","Oana Inel, Nava Tintarev and Lora Aroyo",,,11,
"Driver Identification by Neural Network on Extracted Statistical
  Features from Smartphone Data","  The future of transportation is driven by the use of artificial intelligenceto improve living and transportation. This paper presents a neuralnetwork-based system for driver identification using data collected by asmartphone. This system identifies the driver automatically, reliably and inreal-time without the need for facial recognition and also does not violateprivacy. The system architecture consists of three modules data collection,preprocessing and identification. In the data collection module, the data ofthe accelerometer and gyroscope sensors are collected using a smartphone. Thepreprocessing module includes noise removal, data cleaning, and segmentation.In this module, lost values will be retrieved and data of stopped vehicle willbe deleted. Finally, effective statistical properties are extracted fromdata-windows. In the identification module, machine learning algorithms areused to identify drivers' patterns. According to experiments, the bestalgorithm for driver identification is MLP with a maximum accuracy of 96%. Thissolution can be used in future transportation to develop driver-based insurancesystems as well as the development of systems used to apply penalties andincentives.","Ruhallah Ahmadian, Mehdi Ghatee",,,11,
"Integrating Prior Knowledge in Mixed Initiative Social Network
  Clustering","  We propose a new paradigm---called PK-clustering---to help social scientistscreate meaningful clusters in social networks. Many clustering algorithms existbut most social scientists find them difficult to understand, and tools do notprovide any guidance to choose algorithms, or to evaluate results taking intoaccount the prior knowledge of the scientists. Our work introduces a newclustering paradigm and a visual analytics user interface that address thisissue. It is based on a process that 1) captures the prior knowledge of thescientists as a set of incomplete clusters, 2) runs multiple clusteringalgorithms (similarly to clustering ensemble methods), 3) visualizes theresults of all the algorithms ranked and summarized by how well each algorithmmatches the prior knowledge, 5) evaluates the consensus between user-selectedalgorithms and 6) allows users to review details and iteratively update theacquired knowledge. We describe our paradigm using an initial functionalprototype, then provide two examples of use and early feedback from socialscientists. We believe our clustering paradigm offers a novel constructivemethod to iteratively build knowledge while avoiding being overly influenced bythe results of often-randomly selected black-box clustering algorithms.","Alexis Pister, Paolo Buono, Jean-Daniel Fekete, Catherine Plaisant,
  Paola Valdivia",,,11,
A Design Space of Vision Science Methods for Visualization Research,"  A growing number of efforts aim to understand what people see when using avisualization. These efforts provide scientific grounding to complement designintuitions, leading to more effective visualization practice. However,published visualization research currently reflects a limited set of availablemethods for understanding how people process visualized data. Alternativemethods from vision science offer a rich suite of tools for understandingvisualizations, but no curated collection of these methods exists in eitherperception or visualization research. We introduce a design space ofexperimental methods for empirically investigating the perceptual processesinvolved with viewing data visualizations to ultimately inform visualizationdesign guidelines. This paper provides a shared lexicon for facilitatingexperimental visualization research. We discuss popular experimental paradigms,adjustment types, response types, and dependent measures used in vision scienceresearch, rooting each in visualization examples. We then discuss theadvantages and limitations of each technique. Researchers can use this designspace to create innovative studies and progress scientific understanding ofdesign choices and evaluations in visualization. We highlight a history ofcollaborative success between visualization and vision science research andadvocate for a deeper relationship between the two fields that can elaborate onand extend the methodological design space for understanding visualization andvision.","Madison Elliott, Christine Nothelfer, Cindy Xiong and Danielle Szafir",,,11,
"Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching
  with Chalktalk","  Teaching that uses projected presentation media such as slide-shows lackssupport for dynamic content whose form and behaviors require live changesduring a lecture. Recent software alternatives such as the Chalktalk softwareplatform allow the creation of interactive simulations in arbitrary sequencesand combinations within presentations. These more dynamic solutions, however,do not optimize for face-to-face interactions: eye-contact, gaze direction, andconcurrent awareness of another person's movements together with the presentedcontent. To explore the extent to which these face-to-face interactions mayimprove learning and engagement during a lecture, we propose a Mixed Reality(MR) platform that places Chalktalk's behaviors and simulations within amirrored virtual world environment designed for face-to-face, one-on-oneinteractions. We compare our system with projected Chalktalk to evaluate itsrelative effectiveness for learning, retention, and level of engagement.","Zhenyi He, Ken Perlin",,,11,
"Data Requests and Scenarios for Data Design of Unobserved Events in
  Corona-related Confusion Using TEEDA","  Due to the global violence of the novel coronavirus, various industries havebeen affected and the breakdown between systems has been apparent. Tounderstand and overcome the phenomenon related to this unprecedented crisiscaused by the coronavirus infectious disease (COVID-19), the importance of dataexchange and sharing across fields has gained social attention. In this study,we use the interactive platform called treasuring every encounter of dataaffairs (TEEDA) to externalize data requests from data users, which is a toolto exchange not only the information on data that can be provided but also thecall for data, what data users want and for what purpose. Further, we analyzethe characteristics of missing data in the corona-related confusion stemmingfrom both the data requests and the providable data obtained in the workshop.We also create three scenarios for the data design of unobserved eventsfocusing on variables.","Teruaki Hayashi, Nao Uehara, Daisuke Hase, Yukio Ohsawa",,,11,
"OralCam: Enabling Self-Examination and Awareness of Oral Health Using a
  Smartphone Camera","  Due to a lack of medical resources or oral health awareness, oral diseasesare often left unexamined and untreated, affecting a large populationworldwide. With the advent of low-cost, sensor-equipped smartphones, mobileapps offer a promising possibility for promoting oral health. However, to thebest of our knowledge, no mobile health (mHealth) solutions can directlysupport a user to self-examine their oral health condition. This paper presentsOralCam, the first interactive app that enables end-users' self-examination offive common oral conditions (diseases or early disease signals) by takingsmartphone photos of one's oral cavity. OralCam allows a user to annotateadditional information (e.g. living habits, pain, and bleeding) to augment theinput image, and presents the output hierarchically, probabilistically and withvisual explanations to help a laymen user understand examination results.Developed on our in-house dataset that consists of 3,182 oral photos annotatedby dental experts, our deep learning based framework achieved an averagedetection sensitivity of 0.787 over five conditions with high localizationaccuracy. In a week-long in-the-wild user study (N=18), most participants hadno trouble using OralCam and interpreting the examination results. Two expertinterviews further validate the feasibility of OralCam for promoting users'awareness of oral health.","Yuan Liang, Hsuan-Wei Fan, Zhujun Fang, Leiying Miao, Wen Li, Xuan
  Zhang, Weibin Sun, Kun Wang, Lei He, Xiang Anthony Chen",,,11,
EmoG- Towards Emojifying Gmail Conversations,"  Emails are one of the most frequently used medium of communication in thepresent day across multiple domains including industry and educationalinstitutions. Understanding sentiments being expressed in an email could have aconsiderable impact on the recipients' action or response to the email.However, it is difficult to interpret emotions of the sender from pure text inwhich emotions are not explicitly present. Researchers have tried to predictcustomer attrition by integrating emails in client-company environment withemotions. However, most of the existing works deal with static assessment ofemail emotions. Presenting sentiments of emails dynamically to the reader couldhelp in understanding senders' emotion and as well have an impact on readers'action. Hence, in this paper, we present EmoG as a Google Chrome Extensionwhich is intended to support university students. It augments emails withemojis based on the sentiment being conveyed in the email, which might alsooffer faster overview of email sentiments and act as tags that could help inautomatic sorting and processing of emails. Currently, EmoG has been developedto support Gmail inbox on a Google Chrome browser, and could be extended toother inboxes and browsers with ease. We have conducted a user survey with 15university students to understand the usefulness of EmoG and received positivefeedback.",Akhila Sri Manasa Venigalla and Sridhar Chimalakonda,,,11,
"Submitting surveys via a conversational interface: an evaluation of user
  acceptance and approach effectiveness","  Conversational interfaces are currently on the rise: more and moreapplications rely on a chat-like interaction pattern to increase theiracceptability and to improve user experience. Also in the area of questionnairedesign and administration, interaction design is increasingly looked at as animportant ingredient of a digital solution. For those reasons, we designed anddeveloped a conversational survey tool to administer questionnaires with acolloquial form through a chat-like Web interface.  In this paper, we present the evaluation results of our approach, taking intoaccount both the user point of view - by assessing user acceptance andpreferences in terms of survey compilation experience - and the survey designperspective - by investigating the effectiveness of a conversational survey incomparison to a traditional questionnaire. We show that users clearlyappreciate the conversational form and prefer it over a traditional approachand that, from a data collection point of view, the conversational method showsthe same reliability and a higher response quality with respect to atraditional questionnaire.",Irene Celino and Gloria Re Calegari,,,11,
"Gaze-based Autism Detection for Adolescents and Young Adults using
  Prosaic Videos","  Autism often remains undiagnosed in adolescents and adults. Prior researchhas indicated that an autistic individual often shows atypical fixation andgaze patterns. In this short paper, we demonstrate that by monitoring a user'sgaze as they watch commonplace (i.e., not specialized, structured or coded)video, we can identify individuals with autism spectrum disorder. We recruited35 autistic and 25 non-autistic individuals, and captured their gaze using anoff-the-shelf eye tracker connected to a laptop. Within 15 seconds, ourapproach was 92.5% accurate at identifying individuals with an autismdiagnosis. We envision such automatic detection being applied during e.g., theconsumption of web media, which could allow for passive screening andadaptation of user interfaces.","Karan Ahuja, Abhishek Bose, Mohit Jain, Kuntal Dey, Anil Joshi,
  Krishnaveni Achary, Blessin Varkey, Chris Harrison and Mayank Goel",,,11,
Medical Selfies: Emotional Impacts and Practical Challenges,"  Medical images taken with mobile phones by patients, i.e. medical selfies,allow screening, monitoring and diagnosis of skin lesions. While mobileteledermatology can provide good diagnostic accuracy for skin tumours, there islittle research about emotional and physical aspects when taking medicalselfies of body parts. We conducted a survey with 100 participants and aqualitative study with twelve participants, in which they took images of eightbody parts including intimate areas. Participants had difficulties takingmedical selfies of their shoulder blades and buttocks. For the genitals, theyprefer to visit a doctor rather than sending images. Taking the imagestriggered privacy concerns, memories of past experiences with body parts andraised awareness of the bodily medical state. We present recommendations forthe design of mobile apps to address the usability and emotional impacts oftaking medical selfies.","Daniel Diethei, Ashley Colley, Matilda Kalving, Tarja Salmela, Jonna
  H\""akkil\""a, Johannes Sch\""oning",,,11,
"Minimizing The Maximum Distance Traveled To Form Patterns With Systems
  of Mobile Robots","  In the pattern formation problem, robots in a system must self-coordinate toform a given pattern, regardless of translation, rotation, uniform-scaling,and/or reflection. In other words, a valid final configuration of the system isa formation that is \textit{similar} to the desired pattern. While there hasbeen no shortage of research in the pattern formation problem under a varietyof assumptions, models, and contexts, we consider the additional constraintthat the maximum distance traveled among all robots in the system is minimum.Existing work in pattern formation and closely related problems are typicallyapplication-specific or not concerned with optimality (but rather feasibility).We show the necessary conditions any optimal solution must satisfy and presenta solution for systems of three robots. Our work also led to an interestingresult that has applications beyond pattern formation. Namely, a metric forcomparing two triangles where a distance of $0$ indicates the triangles aresimilar, and $1$ indicates they are \emph{fully dissimilar}.","Jared Coleman, Evangelos Kranakis, Oscar Morales-Ponce, Jaroslav
  Opatrny, Jorge Urrutia, Birgit Vogtenhuber",,,11,
Nonobtuse triangulations of PSLGs,"  We show that any planar straight line graph (PSLG) with $n$ vertices has aconforming triangulation by $O(n^{2.5})$ nonobtuse triangles (all angles $\leq90^\circ$), answering the question of whether any polynomial bound exists. Anonobtuse triangulation is Delaunay, so this result also improves a previous$O(n^3)$ bound of Eldesbrunner and Tan for conforming Delaunay triangulationsof PSLGs. In the special case that the PSLG is the triangulation of a simplepolygon, we will show that only $O(n^2)$ triangles are needed, improving an$O(n^4)$ bound of Bern and Eppstein. We also show that for any $\epsilon >0$,every PSLG has a conforming triangulation with $O(n^2/\epsilon^2)$ elements andwith all angles bounded above by $90^\circ + \epsilon$. This improves a resultof S. Mitchell when $\epsilon = 3 \pi /8 = 67.5^\circ $ and Tan when $\epsilon= 7\pi/30 =42^\circ$.",Christopher J. Bishop,,,11,
Bounded-Degree Spanners in the Presence of Polygonal Obstacles,"  Let $V$ be a finite set of vertices in the plane and $S$ be a finite set ofpolygonal obstacles. We show how to construct a plane $2$-spanner of thevisibility graph of $V$ with respect to $S$. As this graph can have unboundeddegree, we modify it in three easy-to-follow steps, in order to bound thedegree to $7$ at the cost of slightly increasing the spanning ratio to 6.",Andr\'e van Renssen and Gladys Wong,,,11,
"Obtaining a Canonical Polygonal Schema from a Greedy Homotopy Basis with
  Minimal Mesh Refinement","  Any closed manifold of genus g can be cut open to form a topological disk andthen mapped to a regular polygon with 4g sides. This construction is called thecanonical polygonal schema of the manifold, and is a key ingredient for manyapplications in graphics and engineering, where a parameterization between twoshapes with same topology is often needed. The sides of the 4g-gon define onthe manifold a system of loops, which all intersect at a single point and aredisjoint elsewhere. Computing a shortest system of loops of this kind isNP-hard. A computationally tractable alternative consists in computing a set ofshortest loops that are not fully disjoint in polynomial time, using the greedyhomotopy basis algorithm proposed by Erickson and Whittlesey, and then detachthem in post processing via mesh refinement. Despite this operation isconceptually simple, known refinement strategies do not scale well for highgenus shapes, triggering a mesh growth that may exceed the amount of memoryavailable in modern computers, leading to failures. In this paper we studyvarious local refinement operators to detach cycles in a system of loops, andshow that there are important differences between them, both in terms of meshcomplexity and preservation of the original surface. We ultimately propose twonovel refinement approaches: the former minimizes the number of new elements inthe mesh, possibly at the cost of a deviation from the input geometry. Thelatter allows to trade mesh complexity for geometric accuracy, boundingdeviation from the input surface. Both strategies are trivial to implement, andexperiments confirm that they allow to realize canonical polygonal schemas evenfor extremely high genus shapes where previous methods fail.",Marco Livesu,,,11,
On the VC-dimension of half-spaces with respect to convex sets,"  A family S of convex sets in the plane defines a hypergraph H = (S, E) asfollows. Every subfamily S' of S defines a hyperedge of H if and only if thereexists a halfspace h that fully contains S' , and no other set of S is fullycontained in h. In this case, we say that h realizes S'. We say a set S isshattered, if all its subsets are realized. The VC-dimension of a hypergraph His the size of the largest shattered set.  We show that the VC-dimension for pairwise disjoint convex sets in the planeis bounded by 3, and this is tight. In contrast, we show the VC-dimension ofconvex sets in the plane (not necessarily disjoint) is unbounded. We provide aquadratic lower bound in the number of pairs of intersecting sets in ashattered family of convex sets in the plane. We also show that theVC-dimension is unbounded for pairwise disjoint convex sets in R^d , for d > 2.We focus on, possibly intersecting, segments in the plane and determine thatthe VC-dimension is always at most 5. And this is tight, as we construct a setof five segments that can be shattered. We give two exemplary applications. Onefor a geometric set cover problem and one for a range-query data structureproblem, to motivate our findings.","Nicolas Grelier, Saeed Gh. Ilchi, Tillmann Miltzow, Shakhar
  Smorodinsky",,,11,
Arrangements of Pseudocircles: On Circularizability,"  An arrangement of pseudocircles is a collection of simple closed curves onthe sphere or in the plane such that any two of the curves are either disjointor intersect in exactly two crossing points. We call an arrangementintersecting if every pair of pseudocircles intersects twice. An arrangement iscircularizable if there is a combinatorially equivalent arrangement of circles.  In this paper we present the results of the first thorough study ofcircularizability. We show that there are exactly four non-circularizablearrangements of 5 pseudocircles (one of them was known before). In the set of2131 digon-free intersecting arrangements of 6 pseudocircles we identify thethree non-circularizable examples. We also show non-circularizability of 8additional arrangements of 6 pseudocircles which have a group of symmetries ofsize at least 4.  Most of our non-circularizability proofs depend on incidence theorems likeMiquel's. In other cases we contradict circularizability by considering acontinuous deformation where the circles of an assumed circle representationgrow or shrink in a controlled way.  The claims that we have all non-circularizable arrangements with the givenproperties are based on a program that generated all arrangements up to acertain size. Given the complete lists of arrangements, we used heuristics tofind circle representations. Examples where the heuristics failed were examinedby hand.",Stefan Felsner and Manfred Scheucher,,,11,
Computing the Real Isolated Points of an Algebraic Hypersurface,"  Let $\mathbb{R}$ be the field of real numbers. We consider the problem ofcomputing the real isolated points of a real algebraic set in $\mathbb{R}^n$given as the vanishing set of a polynomial system. This problem plays animportant role for studying rigidity properties of mechanism in materialdesigns. In this paper, we design an algorithm which solves this problem. It isbased on the computations of critical points as well as roadmaps for answeringconnectivity queries in real algebraic sets. This leads to a probabilisticalgorithm of complexity $(nd)^{O(n\log(n))}$ for computing the real isolatedpoints of real algebraic hypersurfaces of degree $d$. It allows us to solve inpractice instances which are out of reach of the state-of-the-art.","Huu Phuoc Le, Mohab Safey El Din, Timo de Wolff",,,11,
A tropical geometry approach to BIBO stability,Given a Laurent polynomial F and its amoeba AF. We relate here the questionof the BIBO stability of a multi linear time invariant system with a rationaltransfer function. We formulate very simple criteria for BIBO strong or weakstability in terms of the position of the origin 0 with respect to the amoebaAF and suggest an algorithmic procedure in order to test such property.,"Bossoto Bossoto (PAUSTI), M Mboup (CRESTIC), A Yger (IMB)",,,11,
Polygons with Prescribed Angles in 2D and 3D,"  We consider the construction of a polygon $P$ with $n$ vertices whose turningangles at the vertices are given by a sequence $A=(\alpha_0,\ldots,\alpha_{n-1})$, $\alpha_i\in (-\pi,\pi)$, for $i\in\{0,\ldots, n-1\}$. Theproblem of realizing $A$ by a polygon can be seen as that of constructing astraight-line drawing of a graph with prescribed angles at vertices, and hence,it is a special case of the well studied problem of constructing an \emph{anglegraph}.  In 2D, we characterize sequences $A$ for which every generic polygon$P\subset \mathbb{R}^2$ realizing $A$ has at least $c$ crossings, for every$c\in \mathbb{N}$, and describe an efficient algorithm that constructs, for agiven sequence $A$, a generic polygon $P\subset \mathbb{R}^2$ that realizes $A$with the minimum number of crossings.  In 3D, we describe an efficient algorithm that tests whether a given sequence$A$ can be realized by a (not necessarily generic) polygon $P\subset\mathbb{R}^3$, and for every realizable sequence the algorithm finds arealization.","Alon Efrat, Radoslav Fulek, Stephen Kobourov, Csaba D. T\'oth",,,11,
Approximating the (continuous) Fr\'echet distance,"  We describe the first strongly subquadratic time algorithm withsubexponential approximation ratio for approximately computing the Fr\'echetdistance between two polygonal chains. Specifically, let $P$ and $Q$ be twopolygonal chains with $n$ vertices in $d$-dimensional Euclidean space, and let$\alpha \in [\sqrt{n}, n]$. Our algorithm deterministically finds an$O(\alpha)$-approximate Fr\'echet correspondence in time $O((n^3 / \alpha^2)\log n)$. In particular, we get an $O(n)$-approximation in near-linear $O(n\log n)$ time, a vast improvement over the previously best know result, alinear time $2^{O(n)}$-approximation. As part of our algorithm, we alsodescribe how to turn any approximate decision procedure for the Fr\'echetdistance into an approximate optimization algorithm whose approximation ratiois the same up to arbitrarily small constant factors. The transformation intoan approximate optimization algorithm increases the running time of thedecision procedure by only an $O(\log n)$ factor.",Connor Colombe and Kyle Fox,,,11,
Escaping a Polygon,"  Suppose an ""escaping"" player moves continuously at maximum speed 1 in theinterior of a region, while a ""pursuing"" player moves continuously at maximumspeed $r$ outside the region. For what $r$ can the first player escape theregion, that is, reach the boundary a positive distance away from the pursuingplayer, assuming optimal play by both players? We formalize a model for thisinfinitesimally alternating 2-player game that we prove has a unique winner inany region with locally rectifiable boundary, avoiding pathological behaviors(where both players can have ""winning strategies"") previously identified forpursuit-evasion games such as the Lion and Man problem in certain metricspaces. For some regions, including both equilateral triangle and square, wegive exact results for the critical speed ratio, above which the pursuingplayer can win and below which the escaping player can win (and at which thepursuing player can win). For simple polygons, we give a simple formula andpolynomial-time algorithm that is guaranteed to give a 10.89898-approximationto the critical speed ratio, and we give a pseudopolynomial-time approximationscheme for arbitrarily approximating the critical speed ratio. On the negativeside, we prove NP-hardness of the problem for polyhedral domains in 3D, andprove stronger results (PSPACE-hardness and NP-hardness even to approximate)for generalizations to multiple escaping and pursuing players.","Zachary Abel, Hugo Akitaya, Erik D. Demaine, Martin L. Demaine, Adam
  Hesterberg, Jason S. Ku, Jayson Lynch",,,11,
"Isotopic Arrangement of Simple Curves: an Exact Numerical Approach based
  on Subdivision","  This paper presents the first purely numerical (i.e., non-algebraic)subdivision algorithm for the isotopic approximation of a simple arrangement ofcurves. The arrangement is ""simple"" in the sense that any three curves have nocommon intersection, any two curves intersect transversally, and each curve isnon-singular. A curve is given as the zero set of an analytic function$f:\mathbb{R}^2\rightarrow \mathbb{R}^2$, and effective interval forms of $f,\frac{\partial{f}}{\partial{x}}, \frac{\partial{f}}{\partial{y}}$ areavailable. Our solution generalizes the isotopic curve approximation algorithmsof Plantinga-Vegter (2004) and Lin-Yap (2009).  We use certified numerical primitives based on interval methods. Suchalgorithms have many favorable properties: they are practical, easy toimplement, suffer no implementation gaps, integrate topological with geometriccomputation, and have adaptive as well as local complexity.  A version of this paper without the appendices appeared in Lien et al.(2014).",Jyh-Ming Lien and Vikram Sharma and Gert Vegter and Chee Yap,,,11,
Reconstructing Embedded Graphs from Persistence Diagrams,"  The persistence diagram (PD) is an increasingly popular topologicaldescriptor. By encoding the size and prominence of topological features atvarying scales, the PD provides important geometric and topological informationabout a space. Recent work has shown that well-chosen (finite) sets of PDs candifferentiate between geometric simplicial complexes, providing a method forrepresenting complex shapes using a finite set of descriptors. A relatedinverse problem is the following: given a set of PDs (or an oracle we can queryfor persistence diagrams), what is underlying geometric simplicial complex? Inthis paper, we present an algorithm for reconstructing embedded graphs in$\mathbb{R}^d$ (plane graphs in $\mathbb{R}^2$) with $n$ vertices from $n^2 - n+ d + 1$ directional (augmented) PDs. Additionally, we empirically validate thecorrectness and time-complexity of our algorithm in $\mathbb{R}^2$ on randomlygenerated plane graphs using our implementation, and explain the numericallimitations of implementing our algorithm.","Robin Lynne Belton, Brittany Terese Fasy, Rostik Mertz, Samuel Micka,
  David L. Millman, Daniel Salinas, Anna Schenfisch, Jordan Schupbach, Lucia
  Williams",,,11,
Geometric Systems of Unbiased Representatives,"  Let $P$ be a set of points in $\mathbb{R}^d$, $B$ a bicoloring of $P$ and$\Oo$ a family of geometric objects (that is, intervals, boxes, balls, etc). Anobject from $\Oo$ is called balanced with respect to $B$ if it contains thesame number of points from each color of $B$. For a collection $\B$ ofbicolorings of $P$, a geometric system of unbiased representatives (G-SUR) is asubset $\Oo'\subseteq\Oo$ such that for any bicoloring $B$ of $\B$ there is anobject in $\Oo'$ that is balanced with respect to $B$.  We study the problem of finding G-SURs. We obtain general bounds on the sizeof G-SURs consisting of intervals, size-restricted intervals, axis-parallelboxes and Euclidean balls. We show that the G-SUR problem is NP-hard even inthe simple case of points on a line and interval ranges. Furthermore, we studya related problem on determining the size of the largest and smallest balancedintervals for points on the real line with a random distribution and coloring.  Our results are a natural extension to a geometric context of the workinitiated by Balachandran et al. on arbitrary systems of unbiasedrepresentatives.","Aritra Banik, Bhaswar B. Bhattacharya, Sujoy Bhore, Leonardo
  Mart\'inez-Sandoval",,,11,
"Fast and Scalable Complex Network Descriptor Using PageRank and
  Persistent Homology","  The PageRank of a graph is a scalar function defined on the node set of thegraph which encodes nodes centrality information of the graph. In this article,we use the PageRank function along with persistent homology to obtain ascalable graph descriptor and utilize it to compare the similarities betweengraphs. For a given graph $G(V,E)$, our descriptor can be computed in$O(|E|\alpha(|V|))$, where $\alpha$ is the inverse Ackermann function whichmakes it scalable and computable on massive graphs. We show the effectivenessof our method by utilizing it on multiple shape mesh datasets.","Mustafa Hajij, Elizabeth Munch, Paul Rosen",,,11,
Shortest Paths Among Obstacles in the Plane Revisited,"  Given a set of pairwise disjoint polygonal obstacles in the plane, finding anobstacle-avoiding Euclidean shortest path between two points is a classicalproblem in computational geometry and has been studied extensively. Theprevious best algorithm was given by Hershberger and Suri [FOCS 1993, SIAM J.Comput. 1999] and the algorithm runs in $O(n\log n)$ time and $O(n\log n)$space, where $n$ is the total number of vertices of all obstacles. Thealgorithm is time-optimal because $\Omega(n\log n)$ is a lower bound. It hasbeen an open problem for over two decades whether the space can be reduced to$O(n)$. In this paper, we settle it by solving the problem in $O(n\log n)$ timeand $O(n)$ space, which is optimal in both time and space; we achieve this bymodifying the algorithm of Hershberger and Suri. Like their original algorithm,our new algorithm can build a shortest path map for a source point $s$ in$O(n\log n)$ time and $O(n)$ space, such that given any query point $t$, thelength of a shortest path from $s$ to $t$ can be computed in $O(\log n)$ timeand a shortest path can be produced in additional time linear in the number ofedges of the path.",Haitao Wang,,,11,
Computing a maximum clique in geometric superclasses of disk graphs,"  In the 90's Clark, Colbourn and Johnson wrote a seminal paper, where theyproved that maximum clique can be solved in polynomial time in unit diskgraphs. Since then, the complexity of maximum clique in intersection graphs of(unit) d-dimensional balls has been investigated. For ball graphs, the problemis NP-hard, as shown by Bonamy et al. (FOCS '18). They also gave an efficientpolynomial time approximation scheme (EPTAS) for disk graphs, however thecomplexity of maximum clique in this setting remains unknown. In this paper, weshow the existence of a polynomial time algorithm for solving maximum clique ina geometric superclass of unit disk graphs. Moreover, we give partial resultstoward obtaining an EPTAS for intersection graphs of convex pseudo-disks.",Nicolas Grelier,,,11,
Approximating Surfaces in $R^3$ by Meshes with Guaranteed Regularity,"  We study the problem of approximating a surface $F$ in $R^3$ by a highquality mesh, a piecewise-flat triangulated surface whose triangles are asclose as possible to equilateral. The MidNormal algorithm generates atriangular mesh that is guaranteed to have angles in the interval $[49.1^o,81.8^o]$. As the mesh size $e\rightarrow 0$, the mesh converges pointwise to$F$ through surfaces that are isotopic to $F$. The GradNormal algorithm gives apiecewise-$C^1$ approximation of $F$, with angles in the interval $[35.2^o,101.5^o]$ as $e\rightarrow 0$. Previously achieved angle bounds were in theinterval $[30^o, 120^o]$.","Joel Hass, Maria Trnkova",,,11,
"A Fast and Efficient algorithm for Many-To-Many Matching of Points with
  Demands in One Dimension","  Given two point sets S and T, a many-to-many matching with demands (MMD)problem is the problem of finding a minimum-cost many-to-many matching betweenS and T such that each point of S (respectively T) is matched to at least agiven number of the points of T (respectively S). We propose the first O(n^2)time algorithm for computing a one dimensional MMD (OMMD) of minimum costbetween S and T, where |S|+|T| = n. In an OMMD problem, the input point sets Sand T lie on the real line and the cost of matching a point to another pointequals the distance between the two points. We also study a generalized versionof the MMD problem, the many-to-many matching with demands and capacities(MMDC) problem, that in which each point has a limited capacity in addition toa demand. We give the first O(n^2) time algorithm for the minimum-cost onedimensional MMDC (OMMDC) problem.",Fatemeh Rajabi-Alni and Alireza Bagheri,,,11,
Planar L-Drawings of Bimodal Graphs,"  In a planar L-drawing of a directed graph (digraph) each edge e isrepresented as a polyline composed of a vertical segment starting at the tailof e and a horizontal segment ending at the head of e. Distinct edges mayoverlap, but not cross. Our main focus is on bimodal graphs, i.e., digraphsadmitting a planar embedding in which the incoming and outgoing edges aroundeach vertex are contiguous. We show that every plane bimodal graph without2-cycles admits a planar L-drawing. This includes the class of upward-planegraphs. Finally, outerplanar digraphs admit a planar L-drawing - although theydo not always have a bimodal embedding - but not necessarily with anouterplanar embedding.","Patrizio Angelini, Steven Chaplick, Sabine Cornelsen, and Giordano Da
  Lozzo",,,11,
"Deep Learning for Distributed Channel Feedback and Multiuser Precoding
  in FDD Massive MIMO","  This paper shows that deep neural network (DNN) can be used for efficient anddistributed channel estimation, quantization, feedback, and downlink multiuserprecoding for a frequency-division duplex massive multiple-inputmultiple-output system in which a base station (BS) serves multiple mobileusers, but with rate-limited feedback from the users to the BS. A keyobservation is that the multiuser channel estimation and feedback problem canbe thought of as a distributed source coding problem. In contrast to thetraditional approach where the channel state information (CSI) is estimated andquantized at each user independently, this paper shows that a joint design ofpilot and a new DNN architecture that maps the received pilots directly intofeedback bits at the user side, then maps the feedback bits from all the usersdirectly into the precoding matrix at the BS, can significantly improve theoverall performance. This paper further proposes robust design strategies withrespect to channel parameters and also generalizable DNN architecture forvarying number of users and number of feedback bits. Numerical results showthat the DNN-based approach with short pilot sequences and very limitedfeedback overhead can already approach the performance of conventional linearprecoding schemes with full CSI.","Foad Sohrabi, Kareem M. Attiah, Wei Yu",,,11,
"Intelligent Reflecting Surface Operation under Predictable Receiver
  Mobility: A Continuous Time Propagation Model","  The operation of an intelligent reflecting surface (IRS) under predictablereceiver mobility is investigated. We develop a continuous time system modelfor multipath channels and discuss the optimal IRS configuration with respectto received power, Doppler spread, and delay spread. It is shown that thereceived power can be maximized without adding Doppler spread to the system. Ina numerical case study, we show that an IRS having the size of just two largebillboards can improve the link budget of ground to Low Earth Orbit (LEO)satellite links by up to 6 dB. It also adds a second, almost equivalentlystrong, communication path that improves the link reliability.","Bho Matthiesen and Emil Bj\""ornson and Elisabeth De Carvalho and Petar
  Popovski",,,11,
Spectral Efficiency of One-Bit Sigma-Delta Massive MIMO,"  We examine the uplink spectral efficiency of a massive MIMO base stationemploying a one-bit Sigma-Delta sampling scheme implemented in the spatialrather than the temporal domain. Using spatial rather than temporaloversampling, and feedback of the quantization error between adjacent antennas,the method shapes the spatial spectrum of the quantization noise away from anangular sector where the signals of interest are assumed to lie. It is shownthat, while a direct Bussgang analysis of the Sigma-Delta approach is notsuitable, an alternative equivalent linear model can be formulated tofacilitate an analysis of the system performance. The theoretical properties ofthe spatial quantization noise power spectrum are derived for the Sigma-Deltaarray, as well as an expression for the spectral efficiency of maximum ratiocombining (MRC). Simulations verify the theoretical results and illustrate thesignificant performance gains offered by the Sigma-Delta approach for both MRCand zero-forcing receivers.","Hessam Pirzadeh, Gonzalo Seco-Granados, Shilpa Rao, A. Lee
  Swindlehurst",,,11,
"Bayesian definition of random sequences with respect to conditional
  probabilities",We show that the posterior distribution converges weakly to ML-randomparameters when the model is consistent and define conditional random set bythe inverse of the Bayes estimator to validate consistent theorem in Bayesmodels for individual ML-random parameters and conditional random sequences.,Hayato Takahashi,,,11,
OTFS-NOMA based on SCMA,"  Orthogonal Time Frequency Space (OTFS) is a $\text{2-D}$ modulation techniquethat has the potential to overcome the challenges faced by orthogonal frequencydivision multiplexing (OFDM) in high Doppler environments. The performance ofOTFS in a multi-user scenario with orthogonal multiple access (OMA) techniqueshas been impressive. Due to the requirement of massive connectivity in 5G andbeyond, it is immensely essential to devise and examine the OTFS system withthe existing Non-orthogonal Multiple Access (NOMA) techniques.  In this paper, we propose a multi-user OTFS system based on a code-domainNOMA technique called Sparse Code Multiple Access (SCMA). This system isreferred to as the OTFS-SCMA model. The framework for OTFS-SCMA is designed forboth downlink and uplink. First, the sparse SCMA codewords are strategicallyplaced on the delay-Doppler plane such that the overall overloading factor ofthe OTFS-SCMA system is equal to that of the underlying basic SCMA system. Thereceiver in downlink performs the detection in two sequential phases: first,the conventional OTFS detection using the method of linear minimum mean squareerror (LMMSE), and then the conventional SCMA detection. For uplink, we proposea single-phase detector based on message-passing algorithm (MPA) to detect themultiple users' symbols. The performance of the proposed OTFS-SCMA system isvalidated through extensive simulations both in downlink and uplink. Weconsider delay-Doppler planes of different parameters and various SCMA systemsof overloading factor up to 200$\%$. The performance of OTFS-SCMA is comparedwith those of existing OTFS-OMA techniques. The comprehensive investigationdemonstrates the usefulness of OTFS-SCMA in future wireless communicationstandards.","Kuntal Deka, Anna Thomas, and Sanjeev Sharma",,,11,
Age-Energy Tradeoff in Fading Channels with Packet-Based Transmissions,"  The optimal transmission strategy to minimize the weighted combination of ageof information (AoI) and total energy consumption is studied in this paper. Itis assumed that the status update information is obtained and transmitted atfixed rate over a Rayleigh fading channel in a packet-based wirelesscommunication system. A maximum transmission round on each packet is enforcedto guarantee certain reliability of the update packets. Given fixed averagetransmission power, the age-energy tradeoff can be formulated as a constrainedMarkov decision process (CMDP) problem considering the sensing powerconsumption as well. Employing the Lagrangian relaxation, the CMDP problem istransformed into a Markov decision process (MDP) problem. An algorithm isproposed to obtain the optimal power allocation policy. Through simulationresults, it is shown that both age and energy efficiency can be improved by theproposed optimal policy compared with two benchmark schemes. Also, age can beeffectively reduced at the expense of higher energy cost, and more emphasis onenergy consumption leads to higher average age at the same energy efficiency.Overall, the tradeoff between average age and energy efficiency is identified.","Haitao Huang, Deli Qiao and M. Cenk Gursoy",,,11,
"Weighted Sum Power Maximization for Intelligent Reflecting Surface Aided
  SWIPT","  The low efficiency of far-field wireless power transfer (WPT) limits thefundamental rate-energy (R-E) performance trade-off of the simultaneouswireless information and power transfer (SWIPT) system. To address thischallenge, we propose in this letter a new SWIPT system aided by the emergingintelligent reflecting surface (IRS) technology. By leveraging massive low-costpassive elements that are able to reflect the signals with adjustable phaseshifts, IRS achieve a high passive beamforming gain, which is appealing fordrastically enhancing the WPT efficiency and thereby the R-E trade-off of SWIPTsystems. We consider an IRS being deployed to assist a multi-antenna accesspoint (AP) to serve multiple information decoding receivers (IDRs) and energyharvesting receivers (EHRs). We aim to maximize the weighted sum-power receivedby EHRs via jointly optimizing the transmit precoders at the AP and reflectphase shifts at the IRS, subject to the individualsignal-to-interference-plus-noise ratio (SINR) constraints for IDRs. Since thisproblem is non-convex, we propose efficient algorithms to obtain suboptimalsolutions for it. In particular, we prove that it is sufficient to sendinformation signals only at the AP to serve both IDRs and EHRs regardless oftheir channel realizations. Moreover, simulation results show significantperformance gains achieved by our proposed designs over benchmark schemes.",Qingqing Wu and Rui Zhang,,,11,
On Age and Value of Information in Status Update Systems,"  Motivated by the inherent value of packets arising in many cyber-physicalapplications (e.g., due to precision of the information content or an alarmmessage), we consider status update systems with update packets carrying valuesas well as their generation time stamps. Once generated, a status update packethas a random initial value and a deterministic deadline after which it is notuseful (ultimate staleness). In our model, value of a packet decreases in time(even after reception) starting from its generation to ultimate staleness whenit vanishes. The value of information (VoI) at the receiver is additive in thatthe VoI is the sum of the current values of all packets held by the receiver.We investigate various queuing disciplines under potential dependence betweenvalue and service time and provide closed form expressions for average VoI atthe receiver. Numerical results illustrate the average VoI for differentscenarios and the contrast between average age of information (AoI) and averageVoI.","Peng Zou, Omur Ozel, Suresh Subramaniam",,,11,
"Joint Constellation Design for the Two-User Non-Coherent Multiple-Access
  Channel","  We consider the joint constellation design problem for the two-usernon-coherent multiple-access channel (MAC). Based on an analysis on thenon-coherent maximum-likelihood (ML) detection error, we propose novel designcriteria so as to minimize the error probability. Based on these criteria, wepropose a simple and efficient construction consisting in partitioning asingle-user constellation. Numerical results show that our proposed metrics aremeaningful, and can be used as objectives to generate constellations throughnumerical optimization that perform better than other schemes for the sametransmission rate and power.","Khac-Hoang Ngo, Sheng Yang, Maxime Guillaud, Alexis Decurninge",,,11,
"Coordinating Complementary Waveforms for Suppressing Range Sidelobes in
  a Doppler Band","  We present a general method for constructing radar transmit pulse trains andreceive filters for which the radar point-spread function in delay and Doppler(radar cross-ambiguity function) is essentially free of range sidelobes insidea Doppler interval around the zero-Doppler axis. The transmit and receive pulsetrains are constructed by coordinating the transmission of a pair of Golaycomplementary waveforms across time according to zeros and ones in a binarysequence $P$. In the receive pulse train filter, each waveform is weightedaccording to an element from another sequence $Q$. We show that the spectrum ofessentially the product of $P$ and $Q$ sequences controls the size of the rangesidelobes of the cross-ambiguity function. We annihilate the range sidelobes atlow Doppler by designing the $(P,Q)$ pairs such that their products havehigh-order spectral nulls around zero Doppler. We specify the subspace, alongwith a basis, for such sequences, thereby providing a general way ofconstructing $(P,Q)$ pairs. At the same time, the signal-to-noise ratio (SNR)at the receiver output, for a single point target in white noise, depends onlyon the choice of $Q$. By jointly designing the transmit-receive sequences$(P,Q)$, we can maximize the output SNR subject to achieving a given order ofthe spectral null. The proposed $(P,Q)$ constructions can also be extended tosequences consisting of more than two complementary waveforms; this is doneexplicitly for a library of Golay complementary quads. Finally, we extend theconstruction of $(P,Q)$ pairs to multiple-input-multiple-output (MIMO) radar,by designing transmit-receive pairs of paraunitary waveform matrices whosematrix-valued cross-ambiguity function is essentially free of range sidelobesinside a Doppler interval around the zero-Doppler axis.","Wenbing Dang, Ali Pezeshki, Stephen D. Howard, William Moran, and
  Robert Calderbank",,,11,
"Efficient ML Direction of Arrival Estimation assuming Unknown Sensor
  Noise Powers","  This paper presents an efficient method for computing maximum likelihood (ML)direction of arrival (DOA) estimates assuming unknown sensor noise powers. Themethod combines efficient Alternate Projection (AP) procedures with Newtoniterations. The efficiency of the method lies in the fact that all itsintermediate steps have low complexity. The main contribution of this paper isthe method's last step, in which a concentrated cost function is maximized inboth the DOAs and noise powers in a few iterations through a Newton procedure.This step has low complexity because it employs closed-form expressions of thecost function's gradients and Hessians, which are presented in the paper. Themethod's total computational burden is of just a few mega-flops in typicalcases. We present the method for the deterministic and stochastic MLestimators. An analysis of the deterministic ML cost function's gradientreveals an unexpected drawback of its associated estimator: if the noise powersare unknown, then it is either degenerate or inconsistent. The root-mean-square(RMS) error performance and computational burden of the method are assessednumerically.",J. Selva,,,11,
"Cross Z-Complementary Pairs for Optimal Training in Spatial Modulation
  over Frequency Selective Channels","  The contributions of this paper are twofold: Firstly, we introduce a novelclass of sequence pairs, called ""cross Z-complementary pairs (CZCPs)"", eachdisplaying zero-correlation zone (ZCZ) properties for both their aperiodicautocorrelation sums and crosscorrelation sums. Systematic constructions ofperfect CZCPs based on selected Golay complementary pairs (GCPs) are presented.Secondly, we point out that CZCPs can be utilized as a key component indesigning training sequences for broadband spatial modulation (SM) systems. Weshow that our proposed SM training sequences derived from CZCPs lead to optimalchannel estimation performance over frequency-selective channels.","Zilong Liu, Ping Yang, Yong Liang Guan, Pei Xiao",,,11,
An Efficiently Generated Family of Binary de Bruijn Sequences,"  We study how to generate binary de Bruijn sequences efficiently from theclass of simple linear feedback shift registers with characteristic polynomial$f(x)=x^n+x^{n-1}+x+1 \in \mathbb{F}_2[x]$, for $n \geq 3$, using the cyclejoining method. Based on the properties of this class of LFSRs, we propose twoclasses of successor rules, each of which generates $O(2^{n-3})$ de Bruijnsequences. The cost to produce the next bit is $O(n)$ time and $O(n)$ space fora fixed $n$.","Yunlong Zhu, Zuling Chang, Martianus Frederic Ezerman, and Qiang Wang",,,11,
"Analysis and Optimization of an Intelligent Reflecting Surface-assisted
  System with Interference","  In this paper, we study an intelligent reflecting surface (IRS)-assistedsystem where a multi-antenna base station (BS) serves a single-antenna userwith the help of a multi-element IRS in the presence of interference generatedby a multi-antenna BS serving its own single-antenna user. The signal andinterference links via the IRS are modeled with Rician fading. To reduce phaseadjustment cost, we adopt quasi-static phase shift design where the phaseshifts do not change with the instantaneous channel state information (CSI). Weinvestigate two cases of CSI at the BSs, namely, the instantaneous CSI case andthe statistical CSI case, and apply Maximum Ratio Transmission (MRT) based onthe complete CSI and the CSI of the Line-of-sight (LoS) components,respectively. Different costs on channel estimation and beamforming adjustmentare incurred in the two CSI cases. First, we obtain a tractable expression ofthe average rate in the instantaneous CSI case and a tractable expression ofthe ergodic rate in the statistical CSI case. We also provide sufficientconditions for the average rate in the instantaneous CSI case to surpass theergodic rate in the statistical CSI case, at any phase shifts. Then, wemaximize the average rate and ergodic rate, both with respect to the phaseshifts, leading to two non-convex optimization problems. For each problem, weobtain a globally optimal solution under certain system parameters, and proposean iterative algorithm based on parallel coordinate descent (PCD) to obtain astationary point under arbitrary system parameters. Next, in each CSI case, weprovide sufficient conditions under which the optimal quasi-static phase shiftdesign is beneficial, compared to the system without IRS. Finally, wenumerically verify the analytical results and demonstrate notable gains of theproposal solutions over existing ones.","Yuhang Jia, Chencheng Ye, and Ying Cui",,,11,
Weight Distributions for Successive Cancellation Decoding of Polar Codes,"  In this paper, we derive the exact weight distributions that emerge duringeach stage of successive cancellation decoding of polar codes. Though we do notcompute the distance spectrum of polar codes, the results allow us to get anestimate of the decoding error probability and to show a link between the firstnonzero components of the weight distribution and the partial order between thesynthetic channels. Also, we establish the minimal distance between two cosetsassociated with two paths that differ in two positions. This can be regarded asa first step toward analyzing the weight distributions for the successivecancellation list decoding.","Rina Polyanskaya, Mars Davletshin, and Nikita Polyanskii",,,11,
"Integrated Sensing, Computation and Communication in B5G Cellular
  Internet of Things","  In this paper, we investigate the issue of integrated sensing, computationand communication (SCC) in beyond fifth-generation (B5G) cellular internet ofthings (IoT) networks. According to the characteristics of B5G cellular IoT, acomprehensive design framework integrating SCC is put forward for massive IoT.For sensing, highly accurate sensed information at IoT devices are sent to thebase station (BS) by using non-orthogonal communication over wireless multipleaccess channels. Meanwhile, for computation, a novel technique, namelyover-the-air computation (AirComp), is adopted to substantially reduce thelatency of massive data aggregation via exploiting the superposition propertyof wireless multiple access channels. To coordinate the co-channel interferencefor enhancing the overall performance of B5G cellular IoT integrating SCC, twojoint beamforming design algorithms are proposed from the perspectives of thecomputation error minimization and the weighted sum-rate maximization,respectively. Finally, extensive simulation results validate the effectivenessof the proposed algorithms for B5G cellular IoT over the baseline ones.","Qiao Qi, Xiaoming Chen, Caijun Zhong, Zhaoyang Zhang",,,11,
Throughput Analysis of Small Cell Networks under D-TDD and FFR,"  Dynamic time-division duplex (D-TDD) has emerged as an effective solution toaccommodate the unaligned downlink and uplink traffic in small cell networks.However, the flexibility of traffic configuration also introduces additionalinter-cell interference. In this letter, we study the effectiveness of applyingfractional frequency reuse (FFR) as an interference coordination technique forD-TDD small cell networks. We derive the analytical expressions of downlink anduplink mean packet throughput (MPT), then study a network parameteroptimization problem to maximize MPT while guaranteeing each user's throughput.Numerical results corroborate the benefits of the proposed FFR-based D-TDD interms of improving throughput.","Meiyan Song, Hangguan Shan, Howard H. Yang, and Tony Q. S. Quek",,,11,
Investigations on $c$-(almost) perfect nonlinear functions,"  In a prior paper \cite{EFRST20}, two of us, along with P. Ellingsen, P. Felkeand A. Tkachenko, 1defined a new (output) multiplicative differential, and thecorresponding $c$-differential uniformity, which has the potential of extendingdifferential cryptanalysis. Here, we continue the work, by looking at some APNfunctions through the mentioned concept and showing that their $c$-differentialuniformity increases significantly, in some cases.","Sihem Mesnager, Constanza Riera, Pantelimon Stanica, Haode Yan,
  Zhengchun Zhou",,,11,
"Physical Layer Security in a SISO Communication using Frequency-Domain
  Time-Reversal OFDM Precoding and Artificial Noise Injection","  A frequency domain (FD) time-reversal (TR) precoder is proposed to performphysical layer security (PLS) in single-input single-output (SISO) systemsusing orthogonal frequency-division multiplexing (OFDM) and artificial noise(AN) signal injection. The AN signal does not corrupt the data transmission tothe legitimate receiver but degrades the decoding performance of theeavesdropper. This scheme guarantees the secrecy of a communication towards alegitimate user when the transmitter knows the instantaneous channel stateinformation (CSI) of the legitimate link thanks to the channel reciprocity intime division duplex (TDD) systems, but does not know the instantaneous CSI ofa potential eavesdropper. Three optimal decoding structures at the eavesdropperare considered in a fast fading (FF) environment depending on the handshakeprocedure between Alice and Bob. Closed-form approximations of the AN energy toinject in order to maximize the SR of the communication are derived. Inaddition, the required conditions at the legitimate receiver's end to guaranteea given SR are determined when Eve's signal-to-noise ratio (SNR) is infinite.Furthermore, a waterfilling power allocation strategy is presented to furtherenhance the secrecy of the scheme. Simulation results are presented todemonstrate the security performance of the proposed secure system.","Sidney Jonathan Golstein, Fran\c{c}ois Rottenberg, Fran\c{c}ois
  Horlin, Philippe De Doncker, Julien Sarrazin",,,11,
Decoding of NB-LDPC codes over Subfields,"  The non-binary low-density parity-check (NB-LDPC) codes can offer promisingperformance advantages but suffer from high decoding complexity. To tackle thischallenge, in this paper, we consider NB-LDPC codes over finite fields as codesover \textit{subfields} as a means of reducing decoding complexity. Inparticular, our approach is based on a novel method of expanding a non-binaryTanner graph over a finite field into a graph over a subfield. This approachoffers several decoding strategies for a single NB-LDPC code, with varyinglevels of performance-complexity trade-offs. Simulation results demonstratethat in a majority of cases, performance loss is minimal when compared with thecomplexity gains.","V. B. Wijekoon, Emanuele Viterbo, Yi Hong",,,11,
"On the $k$ Nearest-Neighbor Path Distance from the Typical Intersection
  in the Manhattan Poisson Line Cox Process","  In this paper, we consider a Cox point process driven by the ManhattanPoisson line process. We calculate the exact cumulative distribution function(CDF) of the path distance (L1 norm) between a randomly selected intersectionand the $k$-th nearest node of the Cox process. The CDF is expressed as a sumover the integer partition function $p\!\left(k\right)$, which allows us tonumerically evaluate the CDF in a simple manner for practical values of $k$.These distance distributions can be used to study the $k$-coverage of broadcastsignals transmitted from a \ac{RSU} located at an intersection in intelligenttransport systems (ITS). Also, they can be insightful for network dimensioningin vehicle-to-everything (V2X) systems, because they can yield the exactdistribution of network load within a cell, provided that the \ac{RSU} isplaced at an intersection. Finally, they can find useful applications in otherbranches of science like spatial databases, emergency response planning, anddistricting.","Konstantinos Koufos, Harpreet S. Dhillon, Mehrdad Dianati and Carl P.
  Dettmann",,,11,
"Distributed Artificial Intelligence Solution for D2D Communication in 5G
  Networks","  Device to Device (D2D) Communication is one of the technology components ofthe evolving 5G architecture, as it promises improvements in energy efficiency,spectral efficiency, overall system capacity, and higher data rates. The abovenoted improvements in network performance spearheaded a vast amount of researchin D2D, which have identified significant challenges that need to be addressedbefore realizing their full potential in emerging 5G Networks. Towards thisend, this paper proposes the use of a distributed intelligent approach tocontrol the generation of D2D networks. More precisely, the proposed approachuses Belief-Desire-Intention (BDI) intelligent agents with extendedcapabilities (BDIx) to manage each D2D node independently and autonomously,without the help of the Base Station. The paper includes detailed algorithmicdescription for the decision of transmission mode, which maximizes the datarate, minimizes the power consumptions, while taking into consideration thecomputational load. Simulations show the applicability of BDI agents in jointlysolving D2D challenges.","Iacovos Ioannou, Vasos Vassiliou, Christophoros Christophorou, and
  Andreas Pitsillides",,,11,
COMONet: Community Mobile Network,"  The density of mobile phones has increased rapidly in recent years. Onedrawback of the current mobile telephone technology is that it forces all thecalls to go through cellular base stations even if the caller and the calleeare within the radio range of each other. Hybrid cellular networks andUnlicensed Mobile Access (UMA) have been proposed as solutions that enablemobile phone users to bypass cellular base stations. However, thesetechnologies either require special hardware or in some cases have to rely onthe service providers. We identified that most of the Commodity-off-the-Shelfmobile phones are Wi-Fi (and Bluetooth) enabled. We propose a Community MobileNetwork (COMONet) which utilizes Wi-Fi (and Bluetooth) to build ad hoc networkamong mobile phone users to bypass GSM base stations whenever possible. COMONetdoes not depend on special noncommodity hardware and it is a software basedsolution. COMONet monitors all the available paths over the ad hoc network andit transparently switches to the regular path over the service provider's GSMbase station if a path is not available over the ad hoc network. In COMONet thecaller and the callee do not have to be within the Wi-Fi or Bluetooth range ofeach other to make a call since the COMONet is capable of routing calls throughthe other mobile nodes that are participating in the COMONet.",Primal Wijesekera and Chamath I. Keppitiyagama,,,11,
"User Preference Learning-Aided Collaborative Edge Caching for Small Cell
  Networks","  While next-generation wireless communication networks intend leveraging edgecaching for enhanced spectral efficiency, quality of service, end-to-endlatency, content sharing cost, etc., several aspects of it are yet to beaddressed to make it a reality. One of the fundamental mysteries in acache-enabled network is predicting what content to cache and where to cache sothat high caching content availability is accomplished. For simplicity, most ofthe legacy systems utilize a static estimation - based on Zipf distribution,which, in reality, may not be adequate to capture the dynamic behaviors of thecontents popularities. Forecasting user's preferences can proactively allocatecaching resources and cache the needed contents, which is especially importantin a dynamic environment with real-time service needs. Motivated by this, wepropose a long short-term memory (LSTM) based sequential model that is capableof capturing the temporal dynamics of the users' preferences for the availablecontents in the content library. Besides, for a more efficient edge cachingsolution, different nodes in proximity can collaborate to help each other.Based on the forecast, a non-convex optimization problem is formulated tominimize content sharing costs among these nodes. Moreover, a greedy algorithmis used to achieve a sub-optimal solution. By using mathematical analysis andsimulation results, we validate that the proposed algorithm performs betterthan other existing schemes.","Md Ferdous Pervej, Le Thanh Tan, Rose Qingyang Hu",,,11,
"A Survey on Future Railway Radio Communications Services: Challenges and
  Opportunities","  Radio communications is one of the most disruptive technologies in railways,enabling a huge set of value-added services that greatly improve many aspectsof railways, making them more efficient, safer, and profitable. Lately, somemajor technologies like ERTMS for high-speed railways and CBTC for subways havemade possible a reduction of headway and increased safety never before seen inthis field. The railway industry is now looking at wireless communications withgreat interest, and this can be seen in many projects around the world. Thus,railway radio communications is again a flourishing field, with a lot ofresearch and many things to be done. This survey article explains bothopportunities and challenges to be addressed by the railway sector in order toobtain all the possible benefits of the latest radio technologies.","Juan Moreno Garcia-Loygorri, Jose Manuel Riera, Leandro de Haro, and
  Carlos Rodriguez",,,11,
NV-Fogstore : Device-aware hybrid caching in fog computing environments,"  Edge caching via the placement of distributed storages throughout the networkis a promising solution to reduce latency and network costs of contentdelivery. With the advent of the upcoming 5G future, billions of F-RAN(Fog-Radio Access Network) nodes will created and used for for the purpose ofEdge Caching. Hence, the total amount of memory deployed at the edge isexpected to increase 100 times.  Currently, used DRAM-based caches in CDN (Content Delivery Networks) areextremely power-hungry and costly. Our purpose is to reduce the cost ofownership and recurring costs (of power consumption) in an F-RAN node whilemaintaining Quality of Service.  For our purpose, we propose NV-FogStore, a scalable hybrid key-value storagearchitecture for the utilization of Non-Volatile Memories (such as RRAM, MRAM,Intel Optane) in Edge Cache.  We further describe in detail a novel, hierarchical, write-damage, size andfrequency aware content caching policy H-GREEDY for our architecture.  We show that our policy can be tuned as per performance objectives, to lowerthe power, energy consumption and total cost over an only DRAM-based system foronly a relatively smaller trade-off in the average access latency.","Khushal Sethi, Manan Suri",,,11,
"Analytical Study of Incremental Approach for Information Dissemination
  in Wireless Networks","  In many scenarios, control information dissemination becomes a bottleneck,which limits the scalability and the performance of wireless networks. Such aproblem is especially crucial in mobile ad hoc networks, dense networks,networks of vehicles and drones, sensor networks. In other words, this problemoccurs in any scenario with frequent changes in topology or interference levelon one side and with strong requirements on delay, reliability, powerconsumption, or capacity on the other side. If the control information changespartially, it may be worth sending only differential updates instead ofmessages containing full information to reduce overhead. However, such anapproach needs accurate tuning of dissemination parameters, since it isnecessary to guarantee information relevance in error-prone wireless networks.In the paper, we provide a deep study of two approaches for generatingdifferential updates - namely, incremental and cumulative - and compare theirefficiency. We show that the incremental approach allows significantly reducingthe amount of generated control information compared to the cumulative one,while providing the same level of information relevance. We develop ananalytical model for the incremental approach and propose an algorithm whichallows tuning its parameters, depending on the number of nodes in the network,their mobility, and wireless channel quality. Using the developed analyticalmodel, we show that the incremental approach is very useful for static densenetwork deployments and networks with low and medium mobility, since it allowsus to significantly reduce the amount of control information compared to theclassical full dump approach.","Andrey Belogaev, Evgeny Khorov, Artem Krasilon, Andrey Lyakhov",,,11,
Optimal and Quantized Mechanism Design for Fresh Data Acquisition,"  The proliferation of real-time applications has spurred much interest in datafreshness, captured by the {\it age-of-information} (AoI) metric. Whenstrategic data sources have private market information, a fundamental economicchallenge is how to incentivize them to acquire fresh data and optimize theage-related performance. In this work, we consider an information update systemin which a destination acquires, and pays for, fresh data updates from multiplesources. The destination incurs an age-related cost, modeled as a generalincreasing function of the AoI. Each source is strategic and incurs a samplingcost, which is its private information and may not be truthfully reported tothe destination. The destination decides on the price of updates, when to getthem, and who should generate them, based on the sources' reported samplingcosts. We show that a benchmark that naively trusts the sources' reports canlead to an arbitrarily bad outcome compared to the case where sourcestruthfully report. To tackle this issue, we design an optimal (economic)mechanism for timely information acquisition following Myerson's seminal work.To this end, our proposed optimal mechanism minimizes the sum of thedestination's age-related cost and its payment to the sources, while ensuringthat the sources truthfully report their private information and willvoluntarily participate in the mechanism. However, finding the optimalmechanisms may suffer from \textit{prohibitively expensive computationaloverheads} as it involves solving a nonlinear infinite-dimensional optimizationproblem. We further propose a quantized version of the optimal mechanism thatachieves asymptotic optimality, maintains the other economic properties, andenables one to tradeoff between optimality and computational overheads.","Meng Zhang, Ahmed Arafa, Ermin Wei, and Randall A. Berry",,,11,
UAV Trajectory Optimization for Time Constrained Applications,"  Unmanned Aerial Vehicles (UAVs) are poised to revolutionize communications.Utilizing their flexibility and fast deployment, we can deliver content incongested areas or provide services in areas without infrastructure. In thispaper, we consider a UAV that flies over multiple locations and serves as manyusers as possible within a given time duration. We study the problem of optimaltrajectory design, which we formulate as a mixed-integer linear program. Forlarge instances of the problem where the options for trajectories becomeprohibitively many, we establish a connection to the orienteering problem, andpropose a corresponding greedy algorithm. Simulation results show that theproposed algorithm is fast and yields solutions close to the optimal ones. Theproposed algorithm can be used for trajectory planning in strategic contentcaching or tactical field operations.","Emmanouil Fountoulakis, Georgios S. Paschos, and Nikolaos Pappas",,,11,
RetroRenting: An Online Policy for Service Caching at the Edge,"  The rapid proliferation of shared edge computing platforms has enabledapplication service providers to deploy a wide variety of services withstringent latency and high bandwidth requirements. A key advantage of theseplatforms is that they provide pay-as-you-go flexibility by charging clients inproportion to their resource usage through short-term contracts. This affordsthe client significant cost-saving opportunities, by dynamically deciding whento host (cache) its service on the platform, depending on the changingintensity of requests. A natural caching policy for our setting is theTime-To-Live (TTL) policy. We show that TTL performs poorly both in theadversarial arrival setting, i.e., in terms of the competitive ratio, and fori.i.d. stochastic arrivals with low arrival rates, irrespective of the value ofthe TTL timer. We propose an online caching policy called RetroRenting (RR) andshow that in the class of deterministic online policies, RR is order-optimalwith respect to the competitive ratio. In addition, we provide performanceguarantees for RR for i.i.d. stochastic arrival processes and prove that itcompares well with the optimal online policy. Further, we conduct simulationsusing both synthetic and real world traces to compare the performance of RRwith the optimal offline and online policies. The simulations show that theperformance of RR is near optimal for all settings considered. Our resultsillustrate the universality of RR.","Lakshmi Narayana, Sharayu Moharir, Nikhil Karamchandani",,,11,
Wireless AI: Enabling an AI-Governed Data Life Cycle,"  Recent years have seen rapid deployment of mobile computing and Internet ofThings (IoT) networks, which can be mostly attributed to the increasingcommunication and sensing capabilities of wireless systems. Big data analysis,pervasive computing , and eventually artificial intelligence (AI) are envisagedto be deployed on top of IoT and create a new world featured by data-driven AI.In this context, a novel paradigm of merging AI and wireless communications,called Wireless AI that pushes AI frontiers to the network edge, is widelyregarded as a key enabler for future intelligent network evolution. To thisend, we present a comprehensive survey of the latest studies in wireless AIfrom the data-driven perspective. Specifically, we first propose a novelWireless AI architecture that covers five key data-driven AI themes in wirelessnetworks, including Sensing AI, Network Device AI, Access AI, User Device AIand Data-provenance AI. Then, for each data-driven AI theme, we present anoverview on the use of AI approaches to solve the emerging data-relatedproblems and show how AI can empower wireless network functionalities.Particularly, compared to the other related survey papers, we provide anin-depth discussion on the Wireless AI applications in various data-drivendomains wherein AI proves extremely useful for wireless network design andoptimization. Finally, research challenges and future visions are alsodiscussed to spur further research in this promising area.","Dinh C. Nguyen, Peng Cheng, Ming Ding, David Lopez-Perez, Pubudu N.
  Pathirana, Jun Li, Aruna Seneviratne, Yonghui Li, H. Vincent Poor",,,11,
Real-time QoS Routing Scheme in SDN-based Robotic Cyber-Physical Systems,"  Industrial cyber-physical systems (CPS) have gained enormous attention ofmanufacturers in recent years due to their automation and cost reductioncapabilities in the fourth industrial revolution (Industry 4.0). Such anindustrial network of connected cyber and physical components may consist ofhighly expensive components such as robots. In order to provide efficientcommunication in such a network, it is imperative to improve theQuality-of-Service (QoS). Software Defined Networking (SDN) has become a keytechnology in realizing QoS concepts in a dynamic fashion by allowing acentralized controller to program each flow with a unified interface. However,state-of-the-art solutions do not effectively use the centralized visibility ofSDN to fulfill QoS requirements of such industrial networks. In this paper, wepropose an SDN-based routing mechanism which attempts to improve QoS in roboticcyber-physical systems which have hard real-time requirements. We exploit theSDN capabilities to dynamically select paths based on current link parametersin order to improve the QoS in such delay-constrained networks. We verify theefficiency of the proposed approach on a realistic industrial OpenFlowtopology. Our experiments reveal that the proposed approach significantlyoutperforms an existing delay-based routing mechanism in terms of averagethroughput, end-to-end delay and jitter. The proposed solution would prove tobe significant for the industrial applications in robotic cyber-physicalsystems.","Rutvij H. Jhaveri, Rui Tan, Sagar V. Ramani",,,11,
"Hover or Perch: Comparing Capacity of Airborne and Landed
  Millimeter-Wave UAV Cells","  On-demand deployments of millimeter-wave (mmWave) access points (APs) carriedby unmanned aerial vehicles (UAVs) are considered today as a potential solutionto enhance the performance of 5G+ networks. The battery lifetime of modernUAVs, though, limits the flight times in such systems. In this letter, weevaluate a feasible deployment alternative for temporary capacity boost in theareas with highly fluctuating user demands. The approach is to land UAV-basedmmWave APs on the nearby buildings instead of hovering over the area. Withinthe developed mathematical framework, we compare the system-level performanceof airborne and landed deployments by taking into account the full operationcycle of the employed drones. Our numerical results demonstrate that the choiceof the UAV deployment option is determined by an interplay of the separationdistance between the service area and the UAV charging station, drone batterylifetime, and the number of aerial APs in use. The presented methodology andresults can support efficient on-demand deployments of UAV-based mmWave APs inprospective 5G+ networks.","Vitaly Petrov and Margarita Gapeyenko and Dmitri Moltchanov and Sergey
  Andreev and Robert W. Heath Jr",,,11,
Debunking Wireless Sensor Networks Myths,"  In this article we revisit Wireless Sensor Networks from a contemporaryperspective, after the surge of the Internet of Things. First, we analyze theevolution of distributed monitoring applications, which we consider inheritedfrom the early idea of collaborative sensor networks. Second, we evaluate,within the current context of networked objects, the level of adoption oflow-power multi-hop wireless, a technology pivotal to the Wireless SensorNetwork paradigm. This article assesses the transformation of this technologyin its integration into the Internet of Things, identifying outdatedrequirements and providing a critical view on future research directions.","Borja Martinez, Cristina Cano, Xavier Vilajosana",,,11,
"Contention-Driven Feature Extraction for Low-Regret Contextual
  Bandit-Based Channel Selection Dedicated to Wireless LANs","  To achieve low-regret learning in a radio channel selection for wirelesslocal area networks (WLANs), we propose a contention-driven feature extraction(FE) scheme for a contextual multi-armed bandit (CMAB) algorithm. This studyaims to learn the optimal WLAN channel online particularly in a scalable mannerwith respect to the number of APs and channels, which is accomplished byleveraging the context, i.e., channel allocation information. The proposed FEis designed by focusing on contention with neighboring and same-channel APswhere the key idea is to consolidate contexts ignoring APs that are notconnected to the target AP on the contention graph. The simulation resultsconfirm that contention-driven FE enables a target AP to learn the optimalchannel in a scalable manner for the number of APs and available channels andto have low regret using the CMAB algorithm.","Kota Yamashita, Shotaro Kamiya, Koji Yamamoto, Takayuki Nishio,
  Masahiro Morikura",,,11,
Interfaz Grafica para la Gestion SDWN de un Entorno WLAN,"  This paper presents a graphical interface for the management of a wirelesslocal area network that integrates a set of coordinated Wi-Fi access points.The interface interacts with a network application that is responsible for loadbalancing and mobility management. The graphical application is able to obtainand display the information stored in a system that includes a number of accesspoints, and displays it in a friendly way for the network manager. Finally, theapplication allows remote management of the system, adjusting its parameters ormaking transfers between access points.","Pedro Forton, Jose M Saldana, Julian Fernandez-Navajas, Jose Ruiz-Mas",,,11,
Cost-optimal V2X Service Placement in Distributed Cloud/Edge Environment,"  Deploying V2X services has become a challenging task. This is mainly due tothe fact that such services have strict latency requirements. To meet theserequirements, one potential solution is adopting mobile edge computing (MEC).However, this presents new challenges including how to find a cost efficientplacement that meets other requirements such as latency. In this work, theproblem of cost-optimal V2X service placement (CO-VSP) in a distributedcloud/edge environment is formulated. Additionally, a cost-focused delay-awareV2X service placement (DA-VSP) heuristic algorithm is proposed. Simulationresults show that both CO-VSP model and DA-VSP algorithm guarantee the QoSrequirements of all such services and illustrates the trade-off between latencyand deployment cost.","Abdallah Moubayed, Abdallah Shami, Parisa Heidari, Adel Larabi,
  Richard Brunner",,,11,
"Configured Grant for Semi-Deterministic Traffic for Ultra-Reliable and
  Low Latency Communications","  Configured Grant-based allocation has been adopted in New Radio 3rdGeneration Partnership Project Release 16. This scheme is beneficial insupporting Ultra-Reliable and Low Latency Communication for industrialcommunication, a key Fifth Generation mobile communication usage scenario. Thisscheduling mechanism enables a user with a periodic traffic to transmit itsdata readily and bypasses the signaling entailed to scheduling requests andscheduling grants; and provides low latency access. To facilitateultra-reliable communication, the scheduling mechanism can allow the user totransmit redundant transmissions at consecutive repetition occasions in apre-defined period. However, for the user with semi-deterministic traffic, thereliability and latency performance with Configured Grant-based allocationdeteriorates. This can be due to, e.g., late data arrival in the buffer, andthe user unable to transmit its repetitions, which leads to reliabilitydegradation. To improve the Configured Grant reliability performance withsemi-deterministic traffic, we consider various allocation designs utilizing,e.g., additional unlicensed spectrum, or flexible transmission in a ConfiguredGrant period, or allowing time-gaps between the repetitions, etc. Theseenhancements could be a stepping-stone for Sixth Generation Configured Grantmodels.",Bikramjit Singh and Majid Gerami,,,11,
"Data-Driven Path Selection for Real-Time Video Streaming at the Network
  Edge","  In this paper, we present a framework for the dynamic selection of thewireless channels used to deliver information-rich data streams to edgeservers. The approach we propose is data-driven, where a predictor, whoseoutput informs the decision making of the channel selector, is built fromavailable data on the transformation imposed by the network on previouslytransmitted packets. The proposed technique is contextualized to real-timevideo streaming for immediate processing. The core of our framework is thenotion of probes, that is, short bursts of packets transmitted over unusedchannels to acquire information while generating a controlled impact on otheractive links. Results indicate a high accuracy of the prediction output and asignificant improvement in terms of received video quality when the predictionoutput is used to dynamically select the used channel for transmission.","Sabur Baidya, Peyman Tehrani and Marco Levorato",,,11,
"Impact of Traffic Characteristics on Request Aggregation in an NDN
  Router","  The paper revisits the performance evaluation of caching in a Named DataNetworking (NDN) router where the content store (CS) is supplemented by apending interest table (PIT). The PIT aggregates requests for a given contentthat arrive within the download delay and thus brings an additional reductionin upstream bandwidth usage beyond that due to CS hits. We extend prior work oncaching with non-zero download delay (non-ZDD) by proposing a novelmathematical framework that is more easily applicable to general traffic modelsand by considering alternative cache insertion policies. Specifically weevaluate the use of an LRU filter to improve CS hit rate performance in thisnon-ZDD context. We also consider the impact of time locality in demand due tofinite content lifetimes. The models are used to quantify the impact of the PITon upstream bandwidth reduction, demonstrating notably that this is significantonly for relatively small content catalogues or high average request rate percontent. We further explore how the effectiveness of the filter with finitecontent lifetimes depends on catalogue size and traffic intensity.","Mahdieh Ahmadi, James Roberts, Emilio Leonardi, Ali Movaghar",,,11,
"Predicting Memory Compiler Performance Outputs using Feed-Forward Neural
  Networks","  Typical semiconductor chips include thousands of mostly small memories. Asmemories contribute an estimated 25% to 40% to the overall power, performance,and area (PPA) of a chip, memories must be designed carefully to meet thesystem's requirements. Memory arrays are highly uniform and can be described byapproximately 10 parameters depending mostly on the complexity of theperiphery. Thus, to improve PPA utilization, memories are typically generatedby memory compilers. A key task in the design flow of a chip is to find optimalmemory compiler parametrizations which on the one hand fulfill systemrequirements while on the other hand optimize PPA. Although most compilervendors also provide optimizers for this task, these are often slow orinaccurate. To enable efficient optimization in spite of long compiler runtimes, we propose training fully connected feed-forward neural networks topredict PPA outputs given a memory compiler parametrization. Using anexhaustive search-based optimizer framework which obtains neural networkpredictions, PPA-optimal parametrizations are found within seconds after chipdesigners have specified their requirements. Average model prediction errors ofless than 3%, a decision reliability of over 99% and productive usage of theoptimizer for successful, large volume chip design projects illustrate theeffectiveness of the approach.","Felix Last, Max Haeberlein, Ulf Schlichtmann",,,11,
"To Lane or Not to Lane? Comparing On-Road Experiences in Developing and
  Developed Countries using a New Simulator ""RoadBird""","  Even though the traffic systems in developed countries have been analyzedwith rigor and operated efficiently, the same does not generally hold fordeveloping countries due to inadequate planning, design, and operations oftheir transportation systems. Because of inherent differences between internalinfrastructures, the systems deployed in developed countries may not beamenable to developing ones. Besides, the traffic systems of developingcountries are not well-studied in the literature to the best of our knowledge.For example, it is yet to explore how a developed country's lane-based trafficflow would perform in the context of a developing country, which generallyexperiences non-lane-based traffic. As such, by using our newly developedtraffic simulator 'RoadBird', we investigate outcomes of both lane-based andnon-lane-based traffic from the contexts of both developing and developedcountries. To do so, we run simulations over real road topologies (extractedfrom the GIS maps of major cities such as Dhaka, Miami, and Riyadh) consideringdifferent scenarios such as lane-based or non-lane-based flows, homogeneous orheterogeneous traffic, with or without pedestrians, etc. We also incorporatedifferent car-following and lane-changing models to mimic traffic behaviors andinvestigate their performances. While the lane changing dilemma remains an openresearch question, our experimental evidences indicate: (i) lane-basedapproaches will not necessarily perform better in the case of currently-adoptednon-lane-based scenarios; and (ii) non-lane-based strategies may benefit systemperformance in lane-based scenarios while having heavy mixed traffic.Nonetheless, we reveal several new insights for on-road experiences both indeveloping and developed countries.","Md. Masum Mushfiq, Tarik Reza Toha, Saiful Islam Salim, Aaiyeesha
  Mostak, Masfiqur Rahaman, Najla Abdulrahman Al-Nabhan, Arif Mohamin Sadri and
  A. B. M. Alim Al Islam",,,11,
Quality Classification of Defective Parts from Injection Moulding,"  This report examines machine learning algorithms for detecting short formingand weaving in plastic parts produced by injection moulding. Transfer learningwas implemented by using pretrained models and finetuning them on our datasetof 494 samples of 150 by 150 pixels images. The models tested were Xception,InceptionV3 and Resnet-50. Xception showed the highest overall accuracy(86.66%), followed by InceptionV3 (82.47%) and Resnet-50 (80.41%). Shortforming was the easiest fault to identify, with the highest F1 score for eachmodel.",Adithya Venkatadri Hulagadri,,,11,
Quantum Approximation for Wireless Scheduling,"  This paper proposes a quantum approximate optimization algorithm (QAOA)method for wireless scheduling problems. The QAOA is one of the promisinghybrid quantum-classical algorithms for many applications and it provideshighly accurate optimization solutions in NP-hard problems. QAOA maps the givenproblems into Hilbert spaces, and then it generates Hamiltonian for the givenobjectives and constraints. Then, QAOA finds proper parameters from classicaloptimization approaches in order to optimize the expectation value of generatedHamiltonian. Based on the parameters, the optimal solution to the given problemcan be obtained from the optimum of the expectation value of Hamiltonian.Inspired by QAOA, a quantum approximate optimization for scheduling (QAOS)algorithm is proposed. First of all, this paper formulates a wirelessscheduling problem using maximum weight independent set (MWIS). Then, for thegiven MWIS, the proposed QAOS designs the Hamiltonian of the problem. Afterthat, the iterative QAOS sequence solves the wireless scheduling problem. Thispaper verifies the novelty of the proposed QAOS via simulations implemented byCirq and TensorFlow-Quantum.","Jaeho Choi, Seunghyeok Oh, Joongheon Kim",,,11,
Value driven Analysis Framework of Service Ecosystem Evolution Mechanism,"  With the development of cloud computing, service computing, IoT(Internet ofThings) and mobile Internet, the diversity and sociality of services areincreasingly apparent. To meet the customized user demands, Service Ecosystemis emerging as a complex social-technology system, which is formed with variousIT services through cross-border integration. However, how to analyze andpromote the evolution mechanism of service ecosystem is still a seriouschallenge in the field, which is of great significance to achieve the expectedsystem evolution trends. Based on this, this paper proposes a value-drivenanalysis framework of service ecosystem, including value creation, valueoperation, value realization and value distribution. In addition, acomputational experiment system is established to verify the effectiveness ofthe analysis framework, which stimulates the effect of different operationstrategies on the value network in the service ecosystem. The result shows thatour analysis framework can provide new means and ideas for the analysis ofservice ecosystem evolution, and can also support the design of operationstrategies. Index","Xiao Xue, Deyu Zhou, Yaodan Guo, Zhiyong Feng, Lejun Zhang, Lin Meng",,,11,
Efficient Metastability Characterization for Schmitt-Triggers,"  Despite their attractiveness as metastability filters, Schmitt-Triggers cansuffer from metastability themselves. Therefore, in the selection orconstruction of a suitable Schmitt-Trigger implementation, it is a necessity toaccurately determine the metastable behavior. Only then one is able to comparedifferent designs and thus guide proper optimizations, and only then one canassess the potential for residual metastable upsets. However, while the stateof the art provides a lot of research and practical characterization approachesfor flip-flops, comparatively little is known about Schmitt-Triggercharacterization. Unlike the flip-flop with its single metastable point, theSchmitt-Trigger exhibits a whole range of metastable points depending on theinput voltage. Thus the task of characterization gets much more challenging.  In this paper we present different approaches to determine the metastablebehavior of Schmitt-Triggers using novel methods and mechanisms. We comparetheir accuracy and runtime by applying them to three common circuitimplementations. The achieved results are then used to reason about themetastable behavior of the chosen designs which turns out to be problematic insome cases. Overall the approaches proposed in this paper are generic and canbe extended beyond the Schmitt-Trigger, i.e., to efficiently characterizemetastable states in other circuits as well.","J\""urgen Maier and Andreas Steininger",,,11,
Algorithmic measurement procedures,"  Measurements are shown to be processes designed to return figures: they areeffective. This effectivity allows for a formalization as Turing machines,which can be described employing computation theory. Inspired in the haltingproblem we draw some limitations for measurement procedures: procedures thatverify if a quantity is measured cannot work in every case.",Aldo F. G. Solis-Labastida and Jorge G. Hirsch,,,11,
"Proceedings of the X International Workshop on Locational Analysis and
  Related Problems","  The International Workshop on Locational Analysis and Related Problems willtake place during January 23-24, 2020 in Seville (Spain). It is organized bythe Spanish Location Network and the Location Group GELOCA from the SpanishSociety of Statistics and Operations Research(SEIO). The Spanish LocationNetwork is a group of more than 140 researchers from several Spanishuniversities organized into 7 thematic groups. The Network has been funded bythe Spanish Government since 2003.  One of the main activities of the Network is a yearly meeting aimed atpromoting the communication among its members and between them and otherresearchers, and to contribute to the development of the location field andrelated problems. The last meetings have taken place in C\'adiz (January20-February 1, 2019), Segovia (September 27-29, 2017), M\'alaga (September14-16, 2016), Barcelona (November 25-28, 2015), Sevilla (October 1-3, 2014),Torremolinos (M\'alaga, June 19-21, 2013), Granada (May 10-12, 2012), LasPalmas de Gran Canaria (February 2-5, 2011) and Sevilla (February 1-3, 2010).  The topics of interest are location analysis and related problems. Thisincludes location models, networks, transportation, logistics, exact andheuristic solution methods, and computational geometry, among others.","Maria Albareda-Sambola, Marta Baldomero-Naranjo, Luisa I.
  Mart\'inez-Merino, Diego Ponce, Miguel A. Pozo, Justo Puerto, Victoria
  Rebillas-Loredo. (Editors)",,,11,
"System of Computer Modeling and Features of their use in the Educational
  Process of General Secondary Eeducation","  The article analyzes the historical aspect of the formation of computermodeling as one of the perspective directions of educational processdevelopment. The notion of ""system of computer modeling"", conceptual model ofsystem of computer modeling (SCMod), its components (mathematical, animation,graphic, strategic), functions, principles and purposes of use are grounded.The features of the organization of students work using SCMod, individual andgroup work, the formation of subject competencies are described; the aspect ofstudents' motivation to learning is considered. It is established thateducational institutions can use SCMod at different levels and stages oftraining and in different contexts, which consist of interrelated physical,social, cultural and technological aspects. It is determined that the use ofSCMod in general secondary school would increase the capacity of teachers toimprove the training of students in natural and mathematical subjects andcontribute to the individualization of the learning process, in order to meetthe pace, educational interests and capabilities of each particular student. Itis substantiated that the use of SCMod in the study of natural-mathematicalsubjects contributes to the formation of subject competencies, develops theskills of analysis and decision-making, increases the level of digitalcommunication, develops vigilance, raises the level of knowledge, increases theduration of attention of students. Further research requires the justificationof the process of forming students' competencies in natural-mathematicalsubjects and designing cognitive tasks using SCMod.",Svitlana H. Lytvynova,,,11,
"AGI and the Knight-Darwin Law: why idealized AGI reproduction requires
  collaboration","  Can an AGI create a more intelligent AGI? Under idealized assumptions, for acertain theoretical type of intelligence, our answer is: ""Not without outsidehelp"". This is a paper on the mathematical structure of AGI populations whenparent AGIs create child AGIs. We argue that such populations satisfy a certainbiological law. Motivated by observations of sexual reproduction inseemingly-asexual species, the Knight-Darwin Law states that it is impossiblefor one organism to asexually produce another, which asexually producesanother, and so on forever: that any sequence of organisms (each one a child ofthe previous) must contain occasional multi-parent organisms, or mustterminate. By proving that a certain measure (arguably an intelligence measure)decreases when an idealized parent AGI single-handedly creates a child AGI, weargue that a similar Law holds for AGIs.",Samuel Allen Alexander,,,11,
"A Conceptual Approach to Complex Model Management with Generalized
  Modelling Patterns and Evolutionary Identification","  Complex systems' modeling and simulation are powerful ways to investigate amultitude of natural phenomena providing extended knowledge on their structureand behavior. However, enhanced modeling and simulation require integration ofvarious data and knowledge sources, models of various kinds (data-drivenmodels, numerical models, simulation models, etc.), intelligent components inone composite solution. Growing complexity of such composite model leads to theneed of specific approaches for management of such model. This need extendswhere the model itself becomes a complex system. One of the important aspectsof complex model management is dealing with the uncertainty of various kinds(context, parametric, structural, input/output) to control the model. In thesituation where a system being modeled, or modeling requirements change overtime, specific methods and tools are needed to make modeling and applicationprocedures (meta-modeling operations) in an automatic manner. To supportautomatic building and management of complex models we propose a generalevolutionary computation approach which enables managing of complexity anduncertainty of various kinds. The approach is based on an evolutionaryinvestigation of model phase space to identify the best model's structure andparameters. Examples of different areas (healthcare, hydrometeorology, socialnetwork analysis) were elaborated with the proposed approach and solutions.","Sergey V. Kovalchuk, Oleg G. Metsker, Anastasia A. Funkner, Ilia O.
  Kisliakovskii, Nikolay O. Nikitin, Anna V. Kalyuzhnaya, Danila A. Vaganov,
  Klavdiya O. Bochenina",,,11,
"The Separator, a Two-Phase Oil and Water Gravity CPS Separator Testbed","  Industrial Control Systems (ICS) are evolving with advances in newtechnology. The addition of wireless sensors and actuators and new controltechniques means that engineering practices from communication systems arebeing integrated into those used for control systems. The two are engineered invery different ways. Neither engineering approach is capable of accounting forthe subtle interactions and interdependence that occur when the two arecombined. This paper describes our first steps to bridge this gap, and push theboundaries of both computer communication system and control system design. Wepresent The Separator testbed, a Cyber-Physical testbed enabling our search fora suitable way to engineer systems that combine both computer networks andcontrol systems.","Michael Breza, Laksh Bhatia, Ivana Tomic, Anqi Fu, Waqas Ikram,
  Valentinos Kongezos, Julie A. McCann",,,11,
"Environmental Impact of Bundling Transport Deliveries Using SUMO:
  Analysis of a cooperative approach in Austria","  Urban Traffic is recognized as one of the major CO2 contributors that puts ahigh burden on the environment. Different attempts have been made for reducingthe impacts ranging from traffic management actions to shared-vehicle conceptsto simply reducing the number of vehicles on the streets. By relying oncooperative approaches between different logistics companies, such as sharingand pooling resources for bundling deliveries in the same zone, an increasedenvironmental benefit can be attained. To quantify this benefit we compare theCO2 emissions, fuel consumption and total delivery time resulting fromdeliveries performed by one cargo truck with two trailers versus by twosingle-trailer cargo trucks under real conditions in a simulation scenario inthe city of Linz in Austria. Results showed a fuel consumption and CO2emissions reduction of 28% and 34% respectively in the scenario in whichresources were bundled in one single truck.","Aso Validi, Nicole Polasek, Leonie Alabi, Michael Leitner, Cristina
  Olaverri-Monreal",,,11,
Dyslexia and Dysgraphia prediction: A new machine learning approach,"  Learning disabilities like dysgraphia, dyslexia, dyspraxia, etc. interferewith academic achievements but have also long terms consequences beyond theacademic time. It is widely admitted that between 5% to 10% of the worldpopulation is subject to this kind of disabilities. For assessing suchdisabilities in early childhood, children have to solve a battery of tests.Human experts score these tests, and decide whether the children requirespecific education strategy on the basis of their marks. The assessment can belengthy, costly and emotionally painful. In this paper, we investigate howArtificial Intelligence can help in automating this assessment. Gathering adataset of handwritten text pictures and audio recordings, both from standardchildren and from dyslexic and/or dysgraphic children, we apply machinelearning techniques for classification in order to analyze the differencesbetween dyslexic/dysgraphic and standard readers/writers and to build a model.The model is trained on simple features obtained by analysing the pictures andthe audio files. Our preliminary implementation shows relatively highperformances on the dataset we have used. This suggests the possibility toscreen dyslexia and dysgraphia via non-invasive methods in an accurate way assoon as enough data are available.",Gilles Richard and Mathieu Serrurier,,,11,
"BiEntropy, TriEntropy and Primality","  The order and disorder of binary representations of the natural numbers < 2^8is measured using the BiEntropy function. Significant differences are detectedbetween the primes and the non primes. The BiEntropic prime density is shown tobe quadratic with a very small Gaussian distributed error. The work is repeatedin binary using a monte carlo simulation for a sample of the natural numbers <2^32 and in trinary for all natural numbers < 3^9 with similar but cubicresults. We find a significant relationship between BiEntropy and TriEntropysuch that we can discriminate between the primes and numbers divisible by six.We discuss the theoretical underpinnings of these results and show how theygeneralise to give a tight bound on the variance of Pi(x) - Li(x) for all x.This bound is much tighter than the bound given by Von Koch in 1901 as anequivalence for proof of the Riemann Hypothesis. Since the primes are Gaussiandue to a simple induction on the binary derivative, this implies that the twinprimes conjecture is true. We also provide absolutely convergent asymptotes forthe numbers of Fermat and Mersenne primes in the appendices.",Grenville J. Croll,,,11,
Duty to Delete on Non-Volatile Memory,We firstly suggest new cache policy applying the duty to delete invalid cachedata on Non-volatile Memory (NVM). This cache policy includes generating randomdata and overwriting the random data into invalid cache data. Proposed cachepolicy is more economical and effective regarding perfect deletion of data. Itis ensure that the invalid cache data in NVM is secure against malicioushackers.,Na-Young Ahn and Dong Hoon Lee,,,11,
Neighbourhood Evaluation Criteria for Vertex Cover Problem,"  Neighbourhood Evaluation Criteria is a heuristical approximate algorithm thatattempts to solve the Minimum Vertex Cover. degree count is kept in check foreach vertex and the highest count based vertex is included in our cover set. Inthe case of multiple equivalent vertices, the one with the lowest neighbourhoodinfluence is selected. In the case of still existing multiple equivalentvertices, the one with the lowest remaining active vertex count (the highestIndependent Set enabling count) is selected as a tie-breaker.",Kaustubh K Joshi,,,11,
"Run-Time Power Modelling in Embedded GPUs with Dynamic Voltage and
  Frequency Scaling","  This paper investigates the application of a robust CPU-based power modellingmethodology that performs an automatic search of explanatory events derivedfrom performance counters to embedded GPUs. A 64-bit Tegra TX1 SoC isconfigured with DVFS enabled and multiple CUDA benchmarks are used to train andtest models optimized for each frequency and voltage point. These optimizedmodels are then compared with a simpler unified model that uses a single set ofmodel coefficients for all frequency and voltage points of interest. To obtainthis unified model, a number of experiments are conducted to extractinformation on idle, clock and static power to derive power usage from a singlereference equation. The results show that the unified model offers competitiveaccuracy with an average 5\% error with four explanatory variables on the testdata set and it is capable to correctly predict the impact of voltage,frequency and temperature on power consumption. This model could be used toreplace direct power measurements when these are not available due to hardwarelimitations or worst-case analysis in emulation platforms.","Jose Nunez-Yanez, Kris Nikov, Kerstin Eder and Mohammad Hosseinabady",,,11,
"Current Practices in the Information Collection for Enterprise
  Architecture Management","  The digital transformation influences business models, processes, andenterprise IT landscape as a whole. Therefore, business-IT alignment isbecoming more important than ever before. Enterprise architecture management(EAM) is designed to support and improve this business-IT alignment. Thesuccess of EAM crucially depends on the information available about a company'senterprise architecture, such as infrastructure components, applications, andbusiness processes. This paper discusses the results of a qualitative expertsurvey with 26 experts in the field of EAM. The goal of this survey was tohighlight current practices in the information collection for EAM and identifyrelevant information from enterprise-external data sources. The results providea comprehensive overview of collected and utilized information in the industry,including an assessment of the relevance of such information. Furthermore, theresults highlight challenges in practice and point out investments thatorganizations plan in the field of EAM.","Robert Ehrensperger, Clemens Sauerwein and Ruth Breu",,,11,
C-ITS bundling for integrated traffic management,"  Cooperative Intelligent Transportation Systems (C-ITS) enable vehiclescommunication with each other (Vehicle-to-Vehicle, V2V) and with roadsideinfrastructure (Vehicle-to-Infrastructure, V2I). In the context of trafficefficiency, C-ITS technologies could assist in road network statusvisualization and monitoring, through data exchange, improving this way trafficcontrol organization and traffic management implementation. Bundling is theprovision of several C-ITS services as one combined service. The purpose ofbundling is to harvest the usability of C-ITS services by developing a strategyfor the operation and exploitation of services in real-time and within varyinggeographical areas. Two different dimensions of bundling have been recognizedcovering: 1) end-users, and 2) operators-managers. The objective of theoperators-managers dimension is the integration of C-ITS services inoperational traffic management. This work spotlights the operators-managersbundling dimension, presenting a framework based on a step-by-step approach forintegrating C-ITS services in traditional traffic management.","Evangelos Mitsakis, Areti Kotsi, Vasileios Psonis",,,11,
A Parameterized Approximation Scheme for Min $k$-Cut,"  In the Min $k$-Cut problem, input is an edge weighted graph $G$ and aninteger $k$, and the task is to partition the vertex set into $k$ non-emptysets, such that the total weight of the edges with endpoints in different partsis minimized. When $k$ is part of the input, the problem is NP-complete andhard to approximate within any factor less than $2$. Recently, the problem hasreceived significant attention from the perspective of parameterizedapproximation. Gupta et al.~[SODA 2018] initiated the study ofFPT-approximation for the Min $k$-Cut problem and gave an$1.9997$-approximation algorithm running in time$2^{\mathcal{O}(k^6)}n^{\mathcal{O}(1)}$. Later, the same set of authors~[FOCS2018] designed an $(1 +\epsilon)$-approximation algorithm that runs in time$(k/\epsilon)^{\mathcal{O}(k)}n^{k+\mathcal{O}(1)}$, and a $1.81$-approximationalgorithm running in time $2^{\mathcal{O}(k^2)}n^{\mathcal{O}(1)}$. More,recently, Kawarabayashi and Lin~[SODA 2020] gave a $(5/3 +\epsilon)$-approximation for Min $k$-Cut running in time $2^{\mathcal{O}(k^2\log k)}n^{\mathcal{O}(1)}$.  In this paper we give a parameterized approximation algorithm with bestpossible approximation guarantee, and best possible running time dependence onsaid guarantee (up to Exponential Time Hypothesis (ETH) and constants in theexponent). In particular, for every $\epsilon > 0$, the algorithm obtains a $(1+\epsilon)$-approximate solution in time$(k/\epsilon)^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$. The main ingredients of ouralgorithm are: a simple sparsification procedure, a new polynomial timealgorithm for decomposing a graph into highly connected parts, and a new exactalgorithm with running time $s^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ onunweighted (multi-) graphs. Here, $s$ denotes the number of edges in a minimum$k$-cut. The latter two are of independent interest.","Daniel Lokshtanov, Saket Saurabh, Vaishali Surianarayanan",,,11,
Note on Generalized Cuckoo Hashing with a Stash,"  Cuckoo hashing is a common hashing technique, guaranteeing constant-timelookups in the worst case. Adding a stash was proposed by Kirsch, Mitzenmacher,and Wieder at SICOMP 2010, as a way to reduce the probability of rehash. It hassince become a standard technique in areas such as cryptography, where asuperpolynomially low probability of rehash is often required. Anotherextension of cuckoo hashing is to allow multiple items per bucket, improvingthe load factor. That extension was also analyzed by Kirsch et al. in thepresence of a stash. The purpose of this note is to repair a bug in thatanalysis. Letting $d$ be the number of items per bucket, and $s$ be the stashsize, the original claim was that the probability that a valid cuckooassignment fails to exist is $O(n^{(1-d)(s+1)})$. We point to an error in theargument, and show that it is $\Theta(n^{-d-s})$.","Brice Minaud, Charalampos Papamanthou",,,11,
The Number of Repetitions in 2D-Strings,"  The notions of periodicity and repetitions in strings, and hence these ofruns and squares, naturally extend to two-dimensional strings. We consider twotypes of repetitions in 2D-strings: 2D-runs and quartics (quartics are a2D-version of squares in standard strings). Amir et al. introduced 2D-runs,showed that there are $O(n^3)$ of them in an $n \times n$ 2D-string andpresented a simple construction giving a lower bound of $\Omega(n^2)$ for theirnumber (TCS 2020). We make a significant step towards closing the gap betweenthese bounds by showing that the number of 2D-runs in an $n \times n$ 2D-stringis $O(n^2 \log^2 n)$. In particular, our bound implies that the $O(n^2\log n +\textsf{output})$ run-time of the algorithm of Amir et al. for computing2D-runs is also $O(n^2 \log^2 n)$. We expect this result to allow forexploiting 2D-runs algorithmically in the area of 2D pattern matching.  A quartic is a 2D-string composed of $2 \times 2$ identical blocks(2D-strings) that was introduced by Apostolico and Brimkov (TCS 2000), where byquartics they meant only primitively rooted quartics, i.e. built of a primitiveblock. Here our notion of quartics is more general and analogous to that ofsquares in 1D-strings. Apostolico and Brimkov showed that there are $O(n^2\log^2 n)$ occurrences of primitively rooted quartics in an $n \times n$2D-string and that this bound is attainable. Consequently the number ofdistinct primitively rooted quartics is $O(n^2 \log^2 n)$. Here, we prove thatthe number of distinct general quartics is also $O(n^2 \log^2 n)$. This extendsthe rich combinatorial study of the number of distinct squares in a 1D-string,that was initiated by Fraenkel and Simpson (J. Comb. Theory A 1998), to twodimensions.  Finally, we show some algorithmic applications of 2D-runs. (Abstractshortened due to arXiv requirements.)","Panagiotis Charalampopoulos, Jakub Radoszewski, Wojciech Rytter,
  Tomasz Wale\'n, Wiktor Zuba",,,11,
Target set selection with maximum activation time,"  A target set selection model is a graph $G$ with a threshold function$\tau:V\to \mathbb{N}$ upper-bounded by the vertex degree. For a given model, aset $S_0\subseteq V(G)$ is a target set if $V(G)$ can be partitioned intonon-empty subsets $S_0,S_1,\dotsc,S_t$ such that, for $i \in \{1, \ldots, t\}$,$S_i$ contains exactly every vertex $v$ having at least $\tau(v)$ neighbors in$S_0\cup\dots\cup S_{i-1}$. We say that $t$ is the activation time$t_{\tau}(S_0)$ of the target set $S_0$. The problem of, given such a model,finding a target set of minimum size has been extensively studied in theliterature. In this article, we investigate its variant, which we callTSS-time, in which the goal is to find a target set $S_0$ that maximizes$t_{\tau}(S_0)$. That is, given a graph $G$, a threshold function $\tau$ in$G$, and an integer $k$, the objective of the TSS-time problem is to decidewhether $G$ contains a target set $S_0$ such that $t_{\tau}(S_0)\geq k$. Let$\tau^* = \max_{v \in V(G)} \tau(v)$. Our main result is the followingdichotomy about the complexity of TSS-time when $G$ belongs to a minor-closedgraph class ${\cal C}$: if ${\cal C}$ has bounded local treewidth, the problemis FPT parameterized by $k$ and $\tau^{\star}$; otherwise, it is NP-completeeven for fixed $k=4$ and $\tau^{\star}=2$. We also prove that, with $\tau^*=2$,the problem is NP-hard in bipartite graphs for fixed $k=5$, and from previousresults we observe that TSS-time is NP-hard in planar graphs and W[1]-hardparameterized by treewidth. Finally, we present a linear-time algorithm to finda target set $S_0$ in a given tree maximizing $t_{\tau}(S_0)$.","Lucas Keiler, Carlos Vinicius G. C. Lima, Ana Karolinna Maia, Rudini
  Sampaio, Ignasi Sau",,,11,
"High-Performance Parallel Graph Coloring with Strong Guarantees on Work,
  Depth, and Quality","  We develop the first parallel graph coloring heuristics with strongtheoretical guarantees on work and depth and coloring quality. The key idea isto design a relaxation of the vertex degeneracy order, a well-known graphtheory concept, and to color vertices in the order dictated by this relaxation.This introduces a tunable amount of parallelism into the degeneracy orderingthat is otherwise hard to parallelize. This simple idea enables significantbenefits in several key aspects of graph coloring. For example, one of ouralgorithms ensures polylogarithmic depth and a bound on the number of usedcolors that is superior to all other parallelizable schemes, while maintainingwork-efficiency. In addition to provable guarantees, the developed algorithmshave competitive run-times for several real-world graphs, while almost alwaysproviding superior coloring quality. Our degeneracy ordering relaxation is ofseparate interest for algorithms outside the context of coloring.","Maciej Besta, Armon Carigiet, Zur Vonarburg-Shmaria, Kacper Janda,
  Lukas Gianinazzi, Torsten Hoefler",,,11,
Exploitation of Multiple Replenishing Resources with Uncertainty,"  We consider an optimization problem in which a (single) bat aims to exploitthe nectar in a set of $n$ cacti with the objective of maximizing the expectedtotal amount of nectar it drinks. Each cactus $i \in [n]$ is characterized by aparameter $r_{i} > 0$ that determines the rate in which nectar accumulates in$i$. In every round, the bat can visit one cactus and drink all the nectaraccumulated there since its previous visit. Furthermore, competition with otherbats, that may also visit some cacti and drink their nectar, is modeled bymeans of a stochastic process in which cactus $i$ is emptied in each round(independently) with probability $0 < s_i < 1$. Our attention is restricted topurely-stochastic strategies that are characterized by a probability vector$(p_1, \ldots, p_n)$ determining the probability $p_i$ that the bat visitscactus $i$ in each round. We prove that for every $\epsilon > 0$, there existsa purely-stochastic strategy that approximates the optimal purely-stochasticstrategy to within a multiplicative factor of $1 + \epsilon$, while exploitingonly a small core of cacti. Specifically, we show that it suffices to includeat most $\frac{2 (1 - \sigma)}{\epsilon \cdot \sigma}$ cacti in the core, where$\sigma = \min_{i \in [n]} s_{i}$. We also show that this upper bound on coresize is asymptotically optimal as a core of a significantly smaller size cannotprovide a $(1 + \epsilon)$-approximation of the optimal purely-stochasticstrategy. This means that when the competition is more intense (i.e., $\sigma$is larger), a strategy based on exploiting smaller cores will be favorable.","Amos Korman, Yuval Emek, Simon Collet, Aya Goldshtein, Yossi Yovel",,,11,
Optimal algebraic Breadth-First Search for sparse graphs,"  There has been a rise in the popularity of algebraic methods for graphalgorithms given the development of the GraphBLAS library and other sparsematrix methods. An exemplar for these approaches is Breadth-First Search (BFS).The algebraic BFS algorithm is simply a recursion of matrix-vectormultiplications with the $n \times n$ adjacency matrix, but the many redundantoperations over nonzeros ultimately lead to suboptimal performance. Thereforean optimal algebraic BFS should be of keen interest especially if it is easilyintegrated with existing matrix methods.  Current methods, notably in the GraphBLAS, use a Sparse Matrix masked-SparseVector (SpMmSpV) multiplication in which the input vector is kept in a sparserepresentation in each step of the BFS, and nonzeros in the vector are maskedin subsequent steps. This has been an area of recent research in GraphBLAS andother libraries. While in theory these masking methods are asymptoticallyoptimal on sparse graphs, many add work that leads to suboptimal runtime. Wegive a new optimal, algebraic BFS for sparse graphs, thus closing a gap in theliterature.  Our method multiplies progressively smaller submatrices of the adjacencymatrix at each step, taking $O(n)$ algebraic operations on a sparse graph of$O(m)$ edges as opposed to $O(m)$ operations needed by theoretically optimalsparse matrix approaches. Thus for sparse graphs it matches the bounds of thebest-known sequential algorithm and on a Parallel Random Access Machine (PRAM)it is work-optimal. Our result holds for both directed and undirected graphs.Compared to a leading GraphBLAS library our method achieves up to 24x fastersequential time and for parallel computation it can be 17x faster on largegraphs and 12x faster on large-diameter graphs.",Paul Burkhardt,,,11,
Streaming Submodular Matching Meets the Primal-Dual Method,"  We study streaming submodular maximization subject to matching/$b$-matchingconstraints (MSM/MSbM), and present improved upper and lower bounds for theseproblems. On the upper bounds front, we give primal-dual algorithms achievingthe following approximation ratios.  $\bullet$ $3+2\sqrt{2}\approx 5.828$ for monotone MSM, improving the previousbest ratio of $7.75$.  $\bullet$ $4+3\sqrt{2}\approx 7.464$ for non-monotone MSM, improving theprevious best ratio of $9.899$.  $\bullet$ $3+\epsilon$ for maximum weight b-matching, improving the previousbest ratio of $4+\epsilon$.  On the lower bounds front, we improve on the previous best lower bound of$\frac{e}{e-1}\approx 1.582$ for MSM, and show ETH-based lower bounds of$\approx 1.914$ for polytime monotone MSM streaming algorithms.  Our most substantial contributions are our algorithmic techniques. We showthat the (randomized) primal-dual method, which originated in the study ofmaximum weight matching (MWM), is also useful in the context of MSM. To ourknowledge, this is the first use of primal-dual based analysis for streamingsubmodular optimization. We also show how to reinterpret previous algorithmsfor MSM in our framework; hence, we hope our work is a step towards unifyingold and new techniques for streaming submodular maximization, and that it pavesthe way for further new results.",Roie Levin and David Wajc,,,11,
"On the Finite Optimal Convergence of Logic-Based Benders' Decomposition
  in Solving 0-1 Min-max Regret Optimization Problems with Interval Costs","  This paper addresses a class of problems under interval data uncertaintycomposed of min-max regret versions of classical 0-1 optimization problems withinterval costs. We refer to them as interval 0-1 min-max regret problems. Thestate-of-the-art exact algorithms for this class of problems work by solving acorresponding mixed integer linear programming formulation in a Benders'decomposition fashion. Each of the possibly exponentially many Benders' cuts isseparated on the fly through the resolution of an instance of the classical 0-1optimization problem counterpart. Since these separation subproblems may beNP-hard, not all of them can be modeled by means of linear programming, unlessP = NP. In these cases, the convergence of the aforementioned algorithms arenot guaranteed in a straightforward manner. In fact, to the best of ourknowledge, their finite convergence has not been explicitly proved for anyinterval 0-1 min-max regret problem. In this work, we formally describe thesealgorithms through the definition of a logic-based Benders' decompositionframework and prove their convergence to an optimal solution in a finite numberof iterations. As this framework is applicable to any interval 0-1 min-maxregret problem, its finite optimal convergence also holds in the cases wherethe separation subproblems are NP-hard.","Lucas Assun\c{c}\~ao, Andr\'ea Cynthia Santos, Thiago F. Noronha and
  Rafael Andrade",,,11,
"Optimal Sensor Placement in Power Grids: Power Domination, Set Covering,
  and the Neighborhoods of Zero Forcing Forts","  To monitor electrical activity throughout the power grid and mitigateoutages, sensors known as phasor measurement units can installed. Due toimplementation costs, it is desirable to minimize the number of sensorsdeployed while ensuring that the grid can be effectively monitored. Thisoptimization problem motivates the graph theoretic power dominating setproblem. In this paper, we propose a novel integer program for identifyingminimum power dominating sets by formulating a set cover problem. Thisproblem's constraints correspond to neighborhoods of zero forcing forts; westudy their structural properties and show they can be separated, allowing theproposed model to be solved via row generation. The proposed and existingmethods are compared in several computational experiments in which the proposedmethod consistently exhibits an order of magnitude improvement in runtimeperformance.",Logan A. Smith and Illya V. Hicks,,,11,
"Graph Spanners by Sketching in Dynamic Streams and the Simultaneous
  Communication Model","  Graph sketching is a powerful technique introduced by the seminal work ofAhn, Guha and McGregor'12 on connectivity in dynamic graph streams that hasenjoyed considerable attention in the literature since then, and has led tonear optimal dynamic streaming algorithms for many fundamental problems such asconnectivity, cut and spectral sparsifiers and matchings. Interestingly,however, the sketching and dynamic streaming complexity of approximating theshortest path metric of a graph is still far from well-understood. Besides adirect $k$-pass implementation of classical spanner constructions (recentlyimproved to $\lfloor\frac k2\rfloor+1$-passes by Fernandez, Woodruff andYasuda'20) the state of the art amounts to a $O(\log k)$-pass algorithm of Ahn,Guha and McGregor'12, and a $2$-pass algorithm of Kapralov and Woodruff'14. Inparticular, no single pass algorithm is known, and the optimal tradeoff betweenthe number of passes, stretch and space complexity is open.  In this paper we introduce several new graph sketching techniques forapproximating the shortest path metric of the input graph. We give the first{\em single pass} sketching algorithm for constructing graph spanners: we showhow to obtain a $\widetilde{O}(n^{\frac23})$-spanner using $\widetilde{O}(n)$space, and in general a $\widetilde{O}(n^{\frac23(1-\alpha)})$-spanner using$\widetilde{O}(n^{1+\alpha})$ space for every $\alpha\in [0, 1]$, a tradeoffthat we think may be close optimal. We also give new spanner constructionalgorithms for any number of passes, simultaneously improving upon all priorwork on this problem. Finally, we study the simultaneous communication modeland propose the first protocols with low per player information.","Arnold Filtser, Michael Kapralov, Navid Nouri",,,11,
Improved Bounds for Perfect Sampling of $k$-Colorings in Graphs,"  We present a randomized algorithm that takes as input an undirected$n$-vertex graph $G$ with maximum degree $\Delta$ and an integer $k > 3\Delta$,and returns a random proper $k$-coloring of $G$. The distribution of thecoloring is \emph{perfectly} uniform over the set of all proper $k$-colorings;the expected running time of the algorithm is$\mathrm{poly}(k,n)=\widetilde{O}(n\Delta^2\cdot \log(k))$. This improves upona result of Huber~(STOC 1998) who obtained a polynomial time perfect samplingalgorithm for $k>\Delta^2+2\Delta$. Prior to our work, no algorithm withexpected running time $\mathrm{poly}(k,n)$ was known to guarantee perfectlysampling with sub-quadratic number of colors in general. Our algorithm (likeseveral other perfect sampling algorithms including Huber's) is based on theCoupling from the Past method. Inspired by the \emph{bounding chain} approach,pioneered independently by Huber~(STOC 1998) and H\""aggstr\""om \&Nelander~(Scand.{} J.{} Statist., 1999), we employ a novel bounding chain toderive our result for the graph coloring problem.","Siddharth Bhandari, Sayantan Chakraborty",,,11,
Random Sampling using k-vector,This work introduces two new techniques for random number generation with anyprescribed nonlinear distribution based on the k-vector methodology. The firstapproach is based on inverse transform sampling using the optimal k-vector togenerate the samples by inverting the cumulative distribution. The secondapproach generates samples by performing random searches in a pre-generatedlarge database previously built by massive inversion of the prescribednonlinear distribution using the k-vector. Both methods are shown suitable formassive generation of random samples. Examples are provided to clarify thesemethodologies.,David Arnas and Carl Leake and Daniele Mortari,,,11,
"Scalable Distributed Approximation of Internal Measures for Clustering
  Evaluation","  The most widely used internal measure for clustering evaluation is thesilhouette coefficient, whose naive computation requires a quadratic number ofdistance calculations, which is clearly unfeasible for massive datasets.Surprisingly, there are no known general methods to efficiently approximate thesilhouette coefficient of a clustering with rigorously provable high accuracy.In this paper, we present the first scalable algorithm to compute such arigorous approximation for the evaluation of clusterings based on any metricdistances. Our algorithm hinges on a Probability Proportional to Size (PPS)sampling scheme, and, for any fixed $\varepsilon, \delta \in (0,1)$, itapproximates the silhouette coefficient within a mere additive error$O(\varepsilon)$ with probability $1-\delta$, using a very small number ofdistance calculations. We also prove that the algorithm can be adapted toobtain rigorous approximations of other internal measures of clusteringquality, such as cohesion and separation. Importantly, we provide a distributedimplementation of the algorithm using the MapReduce model, which runs inconstant rounds and requires only sublinear local space at each worker, whichmakes our estimation approach applicable to big data scenarios. We perform anextensive experimental evaluation of our silhouette approximation algorithm,comparing its performance to a number of baseline heuristics on real andsynthetic datasets. The experiments provide evidence that, unlike otherheuristics, our estimation strategy not only provides tight theoreticalguarantees but is also able to return highly accurate estimations while runningin a fraction of the time required by the exact computation, and that itsdistributed implementation is highly scalable, thus enabling the computation ofinternal measures for very large datasets for which the exact computation isprohibitive.","Federico Altieri, Andrea Pietracaprina, Geppino Pucci, Fabio Vandin",,,11,
Efficient Tensor Decomposition,"  This chapter studies the problem of decomposing a tensor into a sum ofconstituent rank one tensors. While tensor decompositions are very useful indesigning learning algorithms and data analysis, they are NP-hard in theworst-case. We will see how to design efficient algorithms with provableguarantees under mild assumptions, and using beyond worst-case frameworks likesmoothed analysis.",Aravindan Vijayaraghavan,,,11,
A cost-scaling algorithm for computing the degree of determinants,"  In this paper, we address computation of the degree $\mathop{\rm deg Det} A$of Dieudonn\'e determinant $\mathop{\rm Det} A$ of \[ A = \sum_{k=1}^m A_k x_kt^{c_k}, \] where $A_k$ are $n \times n$ matrices over a field $\mathbb{K}$,$x_k$ are noncommutative variables, $t$ is a variable commuting with $x_k$,$c_k$ are integers, and the degree is considered for $t$. This problemgeneralizes noncommutative Edmonds' problem and fundamental combinatorialoptimization problems including the weighted linear matroid intersectionproblem. It was shown that $\mathop{\rm deg Det} A$ is obtained by a discreteconvex optimization on a Euclidean building. We extend this framework byincorporating a cost scaling technique, and show that $\mathop{\rm deg Det} A$can be computed in time polynomial of $n,m,\log_2 C$, where $C:= \max_k |c_k|$.We give a polyhedral interpretation of $\mathop{\rm deg Det}$, which says that$\mathop{\rm deg Det} A$ is given by linear optimization over an integralpolytope with respect to objective vector $c = (c_k)$. Based on it, we showthat our algorithm becomes a strongly polynomial one. We apply this result toan algebraic combinatorial optimization problem arising from a symbolic matrixhaving $2 \times 2$-submatrix structure.",Hiroshi Hirai and Motoki Ikeda,,,11,
"Estimating Normalizing Constants for Log-Concave Distributions:
  Algorithms and Lower Bounds","  Estimating the normalizing constant of an unnormalized probabilitydistribution has important applications in computer science, statisticalphysics, machine learning, and statistics. In this work, we consider theproblem of estimating the normalizing constant $Z=\int_{\mathbb{R}^d}e^{-f(x)}\,\mathrm{d}x$ to within a multiplication factor of $1 \pm\varepsilon$ for a $\mu$-strongly convex and $L$-smooth function $f$, givenquery access to $f(x)$ and $\nabla f(x)$. We give both algorithms andlowerbounds for this problem. Using an annealing algorithm combined with amultilevel Monte Carlo method based on underdamped Langevin dynamics, we showthat $\widetilde{\mathcal{O}}\Bigl(\frac{d^{4/3}\kappa +d^{7/6}\kappa^{7/6}}{\varepsilon^2}\Bigr)$ queries to $\nabla f$ aresufficient, where $\kappa= L / \mu$ is the condition number. Moreover, weprovide an information theoretic lowerbound, showing that at least$\frac{d^{1-o(1)}}{\varepsilon^{2-o(1)}}$ queries are necessary. This providesa first nontrivial lowerbound for the problem.","Rong Ge, Holden Lee, Jianfeng Lu",,,11,
Many visits TSP revisited,"  We study the Many Visits TSP problem, where given a number $k(v)$ for each of$n$ cities and pairwise (possibly asymmetric) integer distances, one has tofind an optimal tour that visits each city $v$ exactly $k(v)$ times. Thecurrently fastest algorithm is due to Berger, Kozma, Mnich and Vincze [SODA2019, TALG 2020] and runs in time and space $\mathcal{O}^*(5^n)$. They alsoshow a polynomial space algorithm running in time $\mathcal{O}^*(16^{n+o(n)})$.  In this work, we show three main results: (i) A randomized polynomial spacealgorithm in time $\mathcal{O}^*(2^nD)$, where $D$ is the maximum distancebetween two cities. By using standard methods, this results in$(1+\epsilon)$-approximation in time $\mathcal{O}^*(2^n\epsilon^{-1})$.Improving the constant $2$ in these results would be a major breakthrough, asit would result in improving the $\mathcal{O}^*(2^n)$-time algorithm forDirected Hamiltonian Cycle, which is a 50 years old open problem. (ii) A tightanalysis of Berger et al.'s exponential space algorithm, resulting in$\mathcal{O}^*(4^n)$ running time bound. (iii) A new polynomial spacealgorithm, running in time $\mathcal{O}(7.88^n)$.","{\L}ukasz Kowalik, Shaohua Li, Wojciech Nadara, Marcin Smulewicz,
  Magnus Wahlstr\""om",,,11,
Computing the Largest Bond and the Maximum Connected Cut of a Graph,"  The cut-set $\partial(S)$ of a graph $G=(V,E)$ is the set of edges that haveone endpoint in $S\subset V$ and the other endpoint in $V\setminus S$, andwhenever $G[S]$ is connected, the cut $[S,V\setminus S]$ of $G$ is called aconnected cut. A bond of a graph $G$ is an inclusion-wise minimal disconnectingset of $G$, i.e., bonds are cut-sets that determine cuts $[S,V\setminus S]$ of$G$ such that $G[S]$ and $G[V\setminus S]$ are both connected. Contrasting witha large number of studies related to maximum cuts, there exist very few resultsregarding the largest bond of general graphs. In this paper, we aim to reducethis gap on the complexity of computing the largest bond, and the maximumconnected cut of a graph. Although cuts and bonds are similar, we remark thatcomputing the largest bond and the maximum connected cut of a graph tends to beharder than computing its maximum cut. We show that it does not exist aconstant-factor approximation algorithm to compute the largest bond, unless P =NP. Also, we show that {\sc Largest Bond} and {\sc Maximum Connected Cut} areNP-hard even for planar bipartite graphs, whereas \textsc{Maximum Cut} istrivial on bipartite graphs and polynomial-time solvable on planar graphs. Inaddition, we show that {\sc Largest Bond} and {\sc Maximum Connected Cut} areNP-hard on split graphs, and restricted to graphs of clique-width $w$ they cannot be solved in time $f(w)\times n^{o(w)}$ unless the Exponential TimeHypothesis fails, but they can be solved in time $f(w)\times n^{O(w)}$.Finally, we show that both problems are fixed-parameter tractable whenparameterized by the size of the solution, the treewidth, and the twin-covernumber.","Gabriel L. Duarte, Hiroshi Eto, Tesshu Hanaka, Yasuaki Kobayashi,
  Yusuke Kobayashi, Daniel Lokshtanov, Lehilton L. C. Pedrosa, Rafael C. S.
  Schouery, and U\'everton S. Souza",,,11,
Circular Trace Reconstruction,"  Trace Reconstruction is the problem of learning an unknown string $x$ fromindependent traces of $x$, where traces are generated by independently deletingeach bit of $x$ with some deletion probability $q$. In this paper, we initiatethe study of Circular Trace Reconstruction, where the unknown string $x$ iscircular and traces are now rotated by a random cyclic shift. Tracereconstruction is related to many computational biology problems studying DNA,which is a primary motivation for this problem as well, as many types of DNAare known to be circular.  Our main results are as follows. First, we prove that we can reconstructarbitrary circular strings of length $n$ using$\exp\big(\tilde{O}(n^{1/3})\big)$ traces for any constant deletion probability$q$, as long as $n$ is prime or the product of two primes. For $n$ of thisform, this nearly matches the best known bound of $\exp\big(O(n^{1/3})\big)$for standard trace reconstruction. Next, we prove that we can reconstructrandom circular strings with high probability using $n^{O(1)}$ traces for anyconstant deletion probability $q$. Finally, we prove a lower bound of$\tilde{\Omega}(n^3)$ traces for arbitrary circular strings, which is greaterthan the best known lower bound of $\tilde{\Omega}(n^{3/2})$ in standard tracereconstruction.","Shyam Narayanan, Michael Ren",,,11,
Investigating the discrepancy property of de Bruijn sequences,"  The discrepancy of a binary string refers to the maximum (absolute)difference between the number of ones and the number of zeroes over allpossible substrings of the given binary string. We provide an investigation ofthe discrepancy of known simple constructions of de Bruijn sequences.Furthermore, we demonstrate constructions that attain the lower bound of$\Theta(n)$ and a new construction that attains the previously known upperbound of $\Theta(\frac{2^n}{\sqrt{n}})$. This extends the work of Cooper andHeitsch~[\emph{Discrete Mathematics}, 310 (2010)].",Daniel Gabric and Joe Sawada,,,11,
Generating random bigraphs with preferential attachment,"  The bigraph theory is a relatively young, yet formally rigorous, mathematicalframework encompassing Robin Milner's previous work on process calculi, on theone hand, and provides a generic meta-model for complex systems such asmulti-agent systems, on the other. A bigraph $F = \langle F^P, F^L\rangle$ is asuperposition of two independent graph structures comprising a place graph$F^P$ (i.e., a forest) and a link graph $F^L$ (i.e., a hypergraph), sharing thesame node set, to express locality and communication of processes independentlyfrom each other.  In this paper, we take some preparatory steps towards an algorithm forgenerating random bigraphs with preferential attachment feature w.r.t. $F^P$and assortative (disassortative) linkage pattern w.r.t. $F^L$. We employparameters allowing one to fine-tune the characteristics of the generatedbigraph structures. To study the pattern formation properties of ouralgorithmic model, we analyze several metrics from graph theory based onartificially created bigraphs under different configurations.  Bigraphs provide a quite useful and expressive semantic for process calculifor mobile and global ubiquitous computing. So far, this subject has notreceived attention in the bigraph-related scientific literature. However,artificial models may be particularly useful for simulation and evaluation ofreal-world applications in ubiquitous systems necessitating random structures.","Dominik Grzelak (1 and 2), Barbara Priwitzer (3) and Uwe A{\ss}mann (1
  and 2) ((1) Software Technology Group at Technische Universit\""at Dresden,
  (2) Centre for Tactile Internet with Human-in-the-Loop (CeTI) at Technische
  Universit\""at Dresden, (3) Fakult\""at Technik at Hochschule Reutlingen)",,,11,
Algorithms for the rainbow vertex coloring problem on graph classes,"  Given a vertex-colored graph, we say a path is a rainbow vertex path if allits internal vertices have distinct colors. The graph is rainbowvertex-connected if there is a rainbow vertex path between every pair of itsvertices. In the Rainbow Vertex Coloring (RVC) problem we want to decidewhether the vertices of a given graph can be colored with at most $k$ colors sothat the graph becomes rainbow vertex-connected. This problem is known to beNP-complete even in very restricted scenarios, and very few efficientalgorithms are known for it. In this work, we give polynomial-time algorithmsfor RVC on permutation graphs, powers of trees and split strongly chordalgraphs. The algorithm for the latter class also works for the strong variant ofthe problem, where the rainbow vertex paths between each vertex pair must beshortest paths. We complement the polynomial-time solvability results for splitstrongly chordal graphs by showing that, for any fixed $p\geq 3$ both variantsof the problem become NP-complete when restricted to split$(S_3,\ldots,S_p)$-free graphs, where $S_q$ denotes the $q$-sun graph.","Paloma T. Lima, Erik Jan van Leeuwen, and Marieke van der Wegen",,,11,
Precedence thinness in graphs,"  Interval and proper interval graphs are very well-known graph classes, forwhich there is a wide literature. As a consequence, some generalizations ofinterval graphs have been proposed, in which graphs in general are expressed interms of $k$ interval graphs, by splitting the graph in some special way.  As a recent example of such an approach, the classes of $k$-thin and proper$k$-thin graphs have been introduced generalizing interval and proper intervalgraphs, respectively. The complexity of the recognition of each of theseclasses is still open, even for fixed $k \geq 2$.  In this work, we introduce a subclass of $k$-thin graphs (resp. proper$k$-thin graphs), called precedence $k$-thin graphs (resp. precedence proper$k$-thin graphs). Concerning partitioned precedence $k$-thin graphs, we presenta polynomial time recognition algorithm based on $PQ$-trees. With respect topartitioned precedence proper $k$-thin graphs, we prove that the relatedrecognition problem is \NP-complete for an arbitrary $k$ and polynomial-timesolvable when $k$ is fixed. Moreover, we present a characterization for theseclasses based on threshold graphs.","Flavia Bonomo-Braberman, Fabiano S. Oliveira, Moys\'es S. Sampaio Jr.,
  Jayme L. Szwarcfiter",,,11,
The Dual Polynomial of Bipartite Perfect Matching,"  We obtain a description of the Boolean dual function of the Bipartite PerfectMatching decision problem, as a multilinear polynomial over the Reals. We showthat in this polynomial, both the number of monomials and the magnitude oftheir coefficients are at most exponential in $\mathcal{O}(n \log n)$. As anapplication, we obtain a new upper bound of $\mathcal{O}(n^{1.5} \sqrt{\logn})$ on the approximate degree of the bipartite perfect matching function,improving the previous best known bound of $\mathcal{O}(n^{1.75})$. We deducethat, beyond a $\mathcal{O}(\sqrt{\log n})$ factor, the polynomial methodcannot be used to improve the lower bound on the bounded-error quantum querycomplexity of bipartite perfect matching.",Gal Beniamini,,,11,
Algorithmic Complexity of Isolate Secure Domination in Graphs,"  A dominating set $S$ is an Isolate Dominating Set (IDS) if the inducedsubgraph $G[S]$ has at least one isolated vertex. In this paper, we initiatethe study of new domination parameter called, isolate secure domination. Anisolate dominating set $S\subseteq V$ is an isolate secure dominating set(ISDS), if for each vertex $u \in V \setminus S$, there exists a neighboringvertex $v$ of $u$ in $S$ such that $(S \setminus \{v\}) \cup \{u\}$ is an IDSof $G$. The minimum cardinality of an ISDS of $G$ is called as an isolatesecure domination number, and is denoted by $\gamma_{0s}(G)$. Given a graph $G=(V,E)$ and a positive integer $ k,$ the ISDM problem is to check whether $ G$ has an isolate secure dominating set of size at most $ k.$ We prove that ISDMis NP-complete even when restricted to bipartite graphs and split graphs. Wealso show that ISDM can be solved in linear time for graphs of boundedtree-width.",Jakkepalli Pavan Kumar and P. Venkata Subba Reddy,,,11,
Binary expression of ancestors in the Collatz graph,"  The Collatz graph is a directed graph with natural number nodes and wherethere is an edge from node $x$ to node $T(x)=T_0(x)=x/2$ if $x$ is even, or tonode $T(x)=T_1(x)=\frac{3x+1}{2}$ if $x$ is odd. Studying the Collatz graph inbinary reveals complex message passing behaviors based on carry propagationwhich seem to capture the essential dynamics and complexity of the Collatzprocess. We study the set $\mathcal{E} \text{Pred}_k(x)$ that contains thebinary expression of any ancestor $y$ that reaches $x$ with a limited budget of$k$ applications of $T_1$. The set $\mathcal{E} \text{Pred}_k(x)$ is known tobe regular, Shallit and Wilson [EATCS 1992].  In this paper, we find that the geometry of the Collatz graph naturally leadsto the construction of a regular expression, $\texttt{reg}_k(x)$, which defines$\mathcal{E} \text{Pred}_k(x)$. Our construction, is exponential in $k$ whichimproves upon the doubly exponentially construction of Shallit and Wilson.Furthermore, our result generalises Colussi's work on the $x = 1$ case [TCS2011] to any natural number $x$, and gives mathematical and algorithmic toolsfor further exploration of the Collatz graph in binary.",Tristan St\'erin,,,11,
Improving on Best-of-Many-Christofides for $T$-tours,"  The $T$-tour problem is a natural generalization of TSP and Path TSP. Given agraph $G=(V,E)$, edge cost $c: E \to \mathbb{R}_{\ge 0}$, and an evencardinality set $T\subseteq V$, we want to compute a minimum-cost $T$-joinconnecting all vertices of $G$ (and possibly containing parallel edges).  In this paper we give an $\frac{11}{7}$-approximation for the $T$-tourproblem and show that the integrality ratio of the standard LP relaxation is atmost $\frac{11}{7}$. Despite much progress for the special case Path TSP, forgeneral $T$-tours this is the first improvement on Seb\H{o}'s analysis of theBest-of-Many-Christofides algorithm (Seb\H{o} [2013]).",Vera Traub,,,11,
Steepest ascent can be exponential in bounded treewidth problems,"  We investigate the complexity of local search based on steepest ascent. Weshow that even when all variables have domains of size two and the underlyingconstraint graph of variable interactions has bounded treewidth (in ourconstruction, treewidth 7), there are fitness landscapes for which anexponential number of steps may be required to reach a local optimum. This isan improvement on prior recursive constructions of long steepest ascents, whichwe prove to need constraint graphs of unbounded treewidth.","David A. Cohen, Martin C. Cooper, Artem Kaznatcheev, Mark Wallace",,,11,
Algorithmic Aspects of Some Variants of Domination in Graphs,"  A set $S \subseteq V$ is a dominating set in G if for every u \in V \ S,there exists $v \in S$ such that $(u,v) \in E$, i.e., $N[S] = V$. A dominatingset $S$ is an Isolate Dominating Set} (IDS) if the induced subgraph $G[S]$ hasat least one isolated vertex. It is known that Isolate Domination Decisionproblem (IDOM) is NP-complete for bipartite graphs. In this paper, we extendthis by showing that the IDOM is NP-complete for split graphs and perfectelimination bipartite graphs, a subclass of bipartite graphs. A set $S\subseteq V$ is an independent set if G[S] has no edge. A set S \subseteq V isa secure dominating set of $G$ if, for each vertex $u \in V \setminus S$, thereexists a vertex $v \in S$ such that $ (u,v) \in E $ and $(S \ \{v\}) \cup\{u\}$ is a dominating set of $G$. In addition, we initiate the study of a newdomination parameter called, independent secure domination. A set $S\subseteqV$ is an Independent Secure Dominating Set (InSDS) if $S$ is an independent setand a secure dominating set of $G$. The minimum size of an InSDS in $G$ iscalled the independent secure domination number of $G$ and is denoted by$\gamma_{is}(G)$. Given a graph $ G$ and a positive integer $ k,$ the InSDMproblem is to check whether $ G $ has an independent secure dominating set ofsize at most $ k.$ We prove that InSDM is NP-complete for bipartite graphs andlinear time solvable for bounded tree-width graphs and threshold graphs, asubclass of split graphs. The MInSDS problem is to find an independent securedominating set of minimum size, in the input graph. Finally, we prove that theMInSDS problem is APX-hard for graphs with maximum degree $5.$","Jakkepalli Pavan Kumar, P. Venkata Subba Reddy",,,11,
Balancing Gaussian vectors in high dimension,"  Motivated by problems in controlled experiments, we study the discrepancy ofrandom matrices with continuous entries where the number of columns $n$ is muchlarger than the number of rows $m$. Our first result shows that if $\omega(1) =m = o(n)$, a matrix with i.i.d. standard Gaussian entries has discrepancy$\Theta(\sqrt{n} \, 2^{-n/m})$ with high probability. This provides sharpguarantees for Gaussian discrepancy in a regime that had not been consideredbefore in the existing literature. Our results also apply to a more generalfamily of random matrices with continuous i.i.d entries, assuming that $m =O(n/\log{n})$. The proof is non-constructive and is an application of thesecond moment method. Our second result is algorithmic and applies to randommatrices whose entries are i.i.d. and have a Lipschitz density. We present arandomized polynomial-time algorithm that achieves discrepancy$e^{-\Omega(\log^2(n)/m)}$ with high probability, provided that $m =O(\sqrt{\log{n}})$. In the one-dimensional case, this matches the best knownalgorithmic guarantees due to Karmarkar--Karp. For higher dimensions $2 \leq m= O(\sqrt{\log{n}})$, this establishes the first efficient algorithm achievingdiscrepancy smaller than $O( \sqrt{m} )$.","Paxton Turner, Raghu Meka, Philippe Rigollet",,,11,
"$2$-Layer $k$-Planar Graphs: Density, Crossing Lemma, Relationships, and
  Pathwidth","  The $2$-layer drawing model is a well-established paradigm to visualizebipartite graphs. Several beyond-planar graph classes have been studied underthis model. Surprisingly, however, the fundamental class of $k$-planar graphshas been considered only for $k=1$ in this context. We provide severalcontributions that address this gap in the literature. First, we show tightdensity bounds for the classes of $2$-layer $k$-planar graphs with$k\in\{2,3,4,5\}$. Based on these results, we provide a Crossing Lemma for$2$-layer $k$-planar graphs, which then implies a general density bound for$2$-layer $k$-planar graphs. We prove this bound to be almost optimal with acorresponding lower bound construction. Finally, we study relationships between$k$-planarity and $h$-quasiplanarity in the $2$-layer model and show that$2$-layer $k$-planar graphs have pathwidth at most $k+1$.","Patrizio Angelini, Giordano Da Lozzo, Henry F\""orster and Thomas
  Schneck",,,11,
Guarding Quadrangulations and Stacked Triangulations with Edges,"  Let $G = (V,E)$ be a plane graph. A face $f$ of $G$ is guarded by an edge $vw\in E$ if at least one vertex from $\{v,w\}$ is on the boundary of $f$. For aplanar graph class $\mathcal{G}$ we ask for the minimal number of edges neededto guard all faces of any $n$-vertex graph in $\mathcal{G}$. We prove that$\lfloor n/3 \rfloor$ edges are always sufficient for quadrangulations and givea construction where $\lfloor (n-2)/4 \rfloor$ edges are necessary. For$2$-degenerate quadrangulations we improve this to a tight upper bound of$\lfloor n/4 \rfloor$ edges. We further prove that $\lfloor 2n/7 \rfloor$ edgesare always sufficient for stacked triangulations (that are the $3$-degeneratetriangulations) and show that this is best possible up to a small additiveconstant.","Paul Jungeblut, Torsten Ueckerdt",,,11,
On the Convexity of Independent Set Games,"  Independent set games are cooperative games defined on graphs, where playersare edges and the value of a coalition is the maximum cardinality ofindependent sets in the subgraph defined by the coalition. In this paper, weinvestigate the convexity of independent set games, as convex games possessmany nice properties both economically and computationally. For independent setgames introduced by Deng et al. (Math. Oper. Res., 24:751-766, 1999), weprovide a necessary and sufficient characterization for the convexity, i.e.,every non-pendant edge is incident to a pendant edge in the underlying graph.Our characterization immediately yields a polynomial time algorithm forrecognizing convex instances of independent set games. Besides, we introduce anew class of independent set games and provide an efficient characterizationfor the convexity.","Han Xiao, Yuanxi Wang, Qizhi Fang",,,11,
Ultimate periodicity problem for linear numeration systems,"  We address the following decision problem. Given a numeration system $U$ anda $U$-recognizable set $X\subseteq\mathbb{N}$, i.e. the set of its greedy$U$-representations is recognized by a finite automaton, decide whether or not$X$ is ultimately periodic. We prove that this problem is decidable for a largeclass of numeration systems built on linearly recurrent sequences. Based onarithmetical considerations about the recurrence equation and on $p$-adicmethods, the DFA given as input provides a bound on the admissible periods totest.","E. Charlier, A. Massuir, M. Rigo, E. Rowland",,,11,
Extending Partial Orthogonal Drawings,"  We study the planar orthogonal drawing style within the framework of partialrepresentation extension. Let $(G,H,{\Gamma}_H )$ be a partial orthogonaldrawing, i.e., G is a graph, $H\subseteq G$ is a subgraph and ${\Gamma}_H$ is aplanar orthogonal drawing of H. We show that the existence of an orthogonaldrawing ${\Gamma}_G$ of $G$ that extends ${\Gamma}_H$ can be tested in lineartime. If such a drawing exists, then there also is one that uses $O(|V(H)|)$bends per edge. On the other hand, we show that it is NP-complete to find anextension that minimizes the number of bends or has a fixed number of bends peredge.","Patrizio Angelini (John Cabot University, Rome, Italy), Ignaz Rutter
  (Universit\""at Passau, Germany), Sandhya T P (Universit\""at Passau, Germany)",,,11,
Fan-Crossing Free Graphs,"  A graph is fan-crossing free if it admits a drawing in the plane so that eachedge can be crossed by independent edges. Then the crossing edges have distinctvertices. In complement, a graph is fan-crossing if each edge can be crossed byedges of a fan. Then the crossing edges are incident to a common vertex. Graphsare k-planar if each edge is crossed by at most k edges, and k-gap-planar ifeach crossing is assigned to an edge involved in the crossing, so that at mostk crossings are assigned to each edge. We use the s-subdivision, path-addition,and node-to-circle expansion operations to show that there are fan-crossingfree graphs that are not fan-crossing, k-planar, and k-gap-planar for k >= 1,respectively. A path-addition adds a long path between any two vertices to agraph. An s-subdivision replaces an edge by a path of length s, and anode-to-circle expansion substitutes a vertex by a 3-regular circle, so thateach vertex of the circle inherits an edge incident to the original vertex. Weintroduce universality for an operation and a graph class, so the every graphhas an image in the graph class. In particular, we show the fan22 crossing freegraphs are universal for 2-subdivision and for node-to-circle 3 expansion.Finally, we show that some graphs have a unique fan-crossing free embedding,that there are maximal fan-crossing free graphs with less edges than thedensity, and that the recognition problem for fan-crossing free graphs isNP-complete.",Franz J. Brandenburg,,,11,
A General Stabilization Bound for Influence Propagation in Graphs,"  We study the stabilization time of a wide class of processes on graphs, inwhich each node can only switch its state if it is motivated to do so by atleast a $\frac{1+\lambda}{2}$ fraction of its neighbors, for some $0 < \lambda< 1$. Two examples of such processes are well-studied dynamically changingcolorings in graphs: in majority processes, nodes switch to the most frequentcolor in their neighborhood, while in minority processes, nodes switch to theleast frequent color in their neighborhood. We describe a non-elementaryfunction $f(\lambda)$, and we show that in the sequential model, the worst-casestabilization time of these processes can completely be characterized by$f(\lambda)$. More precisely, we prove that for any $\epsilon>0$,$O(n^{1+f(\lambda)+\epsilon})$ is an upper bound on the stabilization time ofany proportional majority/minority process, and we also show that there aregraph constructions where stabilization indeed takes$\Omega(n^{1+f(\lambda)-\epsilon})$ steps.","P\'al Andr\'as Papp, Roger Wattenhofer",,,11,
Inference and mutual information on random factor graphs,"  Random factor graphs provide a powerful framework for the study of inferenceproblems such as decoding problems or the stochastic block model.Information-theoretically the key quantity of interest is the mutualinformation between the observed factor graph and the underlying ground trutharound which the factor graph was created; in the stochastic block model, thiswould be the planted partition. The mutual information gauges whether and howwell the ground truth can be inferred from the observable data. For a verygeneral model of random factor graphs we verify a formula for the mutualinformation predicted by physics techniques. As an application we prove aconjecture about low-density generator matrix codes from [Montanari: IEEETransactions on Information Theory 2005]. Further applications include phasetransitions of the stochastic block model and the mixed $k$-spin model fromphysics.","Amin Coja-Oghlan, Max Hahn-Klimroth, Philipp Loick, Noela M\""uller,
  Konstantinos Panagiotou, Matija Pasch",,,11,
Towards a Proof of the Fourier--Entropy Conjecture?,"  The total influence of a function is a central notion in analysis of Booleanfunctions, and characterizing functions that have small total influence is oneof the most fundamental questions associated with it. The KKL theorem and theFriedgut junta theorem give a strong characterization of such functionswhenever the bound on the total influence is $o(\log n)$. However, both resultsbecome useless when the total influence of the function is $\omega(\log n)$.The only case in which this logarithmic barrier has been broken for aninteresting class of functions was proved by Bourgain and Kalai, who focused onfunctions that are symmetric under large enough subgroups of $S_n$.  In this paper, we build and improve on the techniques of the Bourgain-Kalaipaper and establish new concentration results on the Fourier spectrum ofBoolean functions with small total influence. Our results include:  1. A quantitative improvement of the Bourgain--Kalai result regarding thetotal influence of functions that are transitively symmetric.  2. A slightly weaker version of the Fourier--Entropy Conjecture of Friedgutand Kalai. This weaker version implies in particular that the Fourier spectrumof a constant variance, Boolean function $f$ is concentrated on $2^{O(I[f]\logI[f])}$ characters, improving an earlier result of Friedgut. Removing the $\logI[f]$ factor would essentially resolve the Fourier--Entropy Conjecture, as wellas settle a conjecture of Mansour regarding the Fourier spectrum of polynomialsize DNF formulas.  Our concentration result has new implications in learning theory: it impliesthat the class of functions whose total influence is at most $K$ isagnostically learnable in time $2^{O(K\log K)}$, using membership queries.","Esty Kelman, Guy Kindler, Noam Lifshitz, Dor Minzer, Muli Safra",,,11,
Extending the Reach of the Point-to-Set Principle,"  The point-to-set principle of J. Lutz and N. Lutz (2018) has recently enabledthe theory of computing to be used to answer open questions about fractalgeometry in Euclidean spaces $\mathbb{R}^n$. These are classical questionswhose statements do not involve computation or related aspects of logic.  In this paper we extend the reach of the point-to-set principle fromEuclidean spaces to arbitrary separable metric spaces $X$. We first extend twofractal dimensions--computability-theoretic versions of classical Hausdorff andpacking dimensions that assign dimensions $\dim(x)$ and $\mathrm{Dim}(x)$ toindividual points $x\in X$--to arbitrary separable metric spaces and toarbitrary gauge families. Our first two main results then extend thepoint-to-set principle to arbitrary separable metric spaces and to a largeclass of gauge families.  We demonstrate the power of our extended point-to-set principle by using itto prove new theorems about classical fractal dimensions in hyperspaces. (For aconcrete computational example, the stages $E_0, E_1, E_2, \ldots$ used toconstruct a self-similar fractal $E$ in the plane are elements of thehyperspace of the plane, and they converge to $E$ in the hyperspace.) Our thirdmain result, proven via our extended point-to-set principle, states that, undera wide variety of gauge functions, the classical packing dimension agrees withthe classical upper Minkowski dimension on all hyperspaces of compact sets. Weuse this theorem to give, for all sets $E$ that are analytic, i.e.,$\mathbf{\Sigma}^1_1$, a tight bound on the packing dimension of the hyperspaceof $E$ in terms of the packing dimension of $E$ itself.","Jack H. Lutz, Neil Lutz, Elvira Mayordomo",,,11,
Complexity of Computing the Anti-Ramsey Numbers for Paths,"  The anti-Ramsey numbers are a fundamental notion in graph theory, introducedin 1978, by Erd\"" os, Simonovits and S\' os. For given graphs $G$ and $H$ the\emph{anti-Ramsey number} $\textrm{ar}(G,H)$ is defined to be the maximumnumber $k$ such that there exists an assignment of $k$ colors to the edges of$G$ in which every copy of $H$ in $G$ has at least two edges with the samecolor.  There are works on the computational complexity of the problem when $H$ is astar. Along this line of research, we study the complexity of computing theanti-Ramsey number $\textrm{ar}(G,P_k)$, where $P_k$ is a path of length $k$.First, we observe that when $k = \Omega(n)$, the problem is hard; hence, thechallenging part is the computational complexity of the problem when $k$ is afixed constant.  We provide a characterization of the problem for paths of constant length.Our first main contribution is to prove that computing $\textrm{ar}(G,P_k)$ forevery integer $k>2$ is NP-hard. We obtain this by providing several structuralproperties of such coloring in graphs. We investigate further and show thatapproximating $\textrm{ar}(G,P_3)$ to a factor of $n^{-1/2 - \epsilon}$ is hardalready in $3$-partite graphs, unless P=NP. We also study the exact complexityof the precolored version and show that there is no subexponential algorithmfor the problem unless ETH fails for any fixed constant $k$.  Given the hardness of approximation and parametrization of the problem, it isnatural to study the problem on restricted graph families. We introduce thenotion of color connected coloring and employing this structural property. Weobtain a linear time algorithm to compute $\textrm{ar}(G,P_k)$, for everyinteger $k$, when the host graph, $G$, is a tree.","Saeed Akhoondian Amiri, Alexandru Popa, Mohammad Roghani, Golnoosh
  Shahkarami, Reza Soltani, Hossein Vahidi",,,11,
Lower bounds for prams over Z,"  This paper presents a new abstract method for proving lower bounds incomputational complexity. Based on the notion of topological entropy fordynamical systems, the method captures four previous lower bounds results fromthe literature in algebraic complexity. Among these results lies Mulmuley'sproof that ""prams without bit operations"" do not compute the maxflow problem inpolylogarithmic time, which was the best known lower bounds in the quest for aproof that NC = Ptime. Inspired from a refinement of Steele and Yao's lowerbounds, due to Ben-Or, we strengthen Mulmuley's result to a larger class ofmachines, showing that prams over integer do not compute maxflow inpolylogarithmic time.","Luc Pellissier (LIX, X, Inria), Thomas Seiller (LIPN, CNRS)",,,11,
"A dichotomy for bounded degree graph homomorphisms with nonnegative
  weights","  We consider the complexity of counting weighted graph homomorphisms definedby a symmetric matrix $A$. Each symmetric matrix $A$ defines a graphhomomorphism function $Z_A(\cdot)$, also known as the partition function. Dyerand Greenhill [10] established a complexity dichotomy of $Z_A(\cdot)$ forsymmetric $\{0, 1\}$-matrices $A$, and they further proved that its #P-hardnesspart also holds for bounded degree graphs. Bulatov and Grohe [4] extended theDyer-Greenhill dichotomy to nonnegative symmetric matrices $A$. However, theirhardness proof requires graphs of arbitrarily large degree, and whether thebounded degree part of the Dyer-Greenhill dichotomy can be extended has been anopen problem for 15 years. We resolve this open problem and prove that fornonnegative symmetric $A$, either $Z_A(G)$ is in polynomial time for all graphs$G$, or it is #P-hard for bounded degree (and simple) graphs $G$. We furtherextend the complexity dichotomy to include nonnegative vertex weights.Additionally, we prove that the #P-hardness part of the dichotomy by Goldberget al. [12] for $Z_A(\cdot)$ also holds for simple graphs, where $A$ is anyreal symmetric matrix.","Artem Govorov, Jin-Yi Cai, Martin Dyer",,,11,
Efficient List-Decoding with Constant Alphabet and List Sizes,"  We present an explicit and efficient algebraic construction ofcapacity-achieving list decodable codes with both constant alphabet andconstant list sizes. More specifically, for any $R \in (0,1)$ and $\epsilon>0$,we give an algebraic construction of an infinite family of error-correctingcodes of rate $R$, over an alphabet of size $(1/\epsilon)^{O(1/\epsilon^2)}$,that can be list decoded from a $(1-R-\epsilon)$-fraction of errors with listsize at most $\exp(\mathrm{poly}(1/\epsilon))$. Moreover, the codes can beencoded in time $\mathrm{poly}(1/\epsilon, n)$, the output list is contained ina linear subspace of dimension at most $\mathrm{poly}(1/\epsilon)$, and a basisfor this subspace can be found in time $\mathrm{poly}(1/\epsilon, n)$. Thus,both encoding and list decoding can be performed in fully polynomial-time$\mathrm{poly}(1/\epsilon, n)$, except for pruning the subspace and outputtingthe final list which takes time$\exp(\mathrm{poly}(1/\epsilon))\cdot\mathrm{poly}(n)$.  Our codes are quite natural and structured. Specifically, we usealgebraic-geometric (AG) codes with evaluation points restricted to a subfield,and with the message space restricted to a (carefully chosen) linear subspace.Our main observation is that the output list of AG codes with subfieldevaluation points is contained in an affine shift of the image of ablock-triangular-Toeplitz (BTT) matrix, and that the list size can potentiallybe reduced to a constant by restricting the message space to a BTT evasivesubspace, which is a large subspace that intersects the image of any BTT matrixin a constant number of points. We further show how to explicitly constructsuch BTT evasive subspaces, based on the explicit subspace designs of Guruswamiand Kopparty (Combinatorica, 2016), and composition.","Zeyu Guo, Noga Ron-Zewi",,,11,
A note on weak near unanimity polymorphisms,We show that deciding whether a given relational structure $\mathcal{R}$admits a weak near unanimity polymorphism is polynomial time solvable.,Arash Rafiey,,,11,
"Counting Homomorphisms to $K_4$-minor-free Graphs, modulo 2","  We study the problem of computing the parity of the number of homomorphismsfrom an input graph $G$ to a fixed graph $H$. Faben and Jerrum [ToC'15]introduced an explicit criterion on the graph $H$ and conjectured that, ifsatisfied, the problem is solvable in polynomial time and, otherwise, theproblem is complete for the complexity class $\oplus\mathrm{P}$ of parityproblems. We verify their conjecture for all graphs $H$ that exclude thecomplete graph on $4$ vertices as a minor. Further, we rule out the existenceof a subexponential-time algorithm for the $\oplus\mathrm{P}$-complete cases,assuming the randomised Exponential Time Hypothesis. Our proofs introduce anovel method of deriving hardness from globally defined substructures of thefixed graph $H$. Using this, we subsume all prior progress towards resolvingthe conjecture (Faben and Jerrum [ToC'15]; G\""obel, Goldberg and Richerby[ToCT'14,'16]). As special cases, our machinery also yields a proof of theconjecture for graphs with maximum degree at most $3$, as well as a fullclassification for the problem of counting list homomorphisms, modulo $2$.","Jacob Focke, Leslie Ann Goldberg, Marc Roth and Stanislav \v{Z}ivn\'y",,,11,
"Parameterized inapproximability for Steiner Orientation by Gap
  Amplification","  In the $k$-Steiner Orientation problem, we are given a mixed graph, that is,with both directed and undirected edges, and a set of $k$ terminal pairs. Thegoal is to find an orientation of the undirected edges that maximizes thenumber of terminal pairs for which there is a path from the source to the sink.The problem is known to be W[1]-hard when parameterized by k and hard toapproximate up to some constant for FPT algorithms assuming Gap-ETH. On theother hand, no approximation factor better than $O(k)$ is known.  We show that $k$-Steiner Orientation is unlikely to admit an approximationalgorithm with any constant factor, even within FPT running time. To obtainthis result, we construct a self-reduction via a hashing-based gapamplification technique, which turns out useful even outside of the FPTparadigm. Precisely, we rule out any approximation factor of the form $(\logk)^{o(1)}$ for FPT algorithms (assuming FPT $\ne$ W[1]) and $(\log n)^{o(1)}$for~purely polynomial-time algorithms (assuming that the class W[1] does notadmit randomized FPT algorithms). Moreover, we prove $k$-Steiner Orientation tobelong to W[1], which entails W[1]-completeness of $(\logk)^{o(1)}$-approximation for $k$-Steiner Orientation This provides an exampleof a natural approximation task that is complete in a parameterized complexityclass.  Finally, we apply our technique to the maximization version of directedmulticut - Max $(k,p)$-Directed Multicut - where we are given a directed graph,$k$ terminals pairs, and a budget $p$. The goal is to maximize the number ofseparated terminal pairs by removing $p$ edges. We present a simple proof thatthe problem admits no FPT approximation with factor $O(k^{\frac 1 2 -\epsilon})$ (assuming FPT $\ne$ W[1]) and no polynomial-time approximation withratio $O(|E(G)|^{\frac 1 2 - \epsilon})$ (assuming NP $\not\subseteq$ co-RP).",Micha{\l} W{\l}odarczyk,,,11,
"Trains, Games, and Complexity: 0/1/2-Player Motion Planning through
  Input/Output Gadgets","  We analyze the computational complexity of motion planning through local""input/output"" gadgets with separate entrances and exits, and a subset ofallowed traversals from entrances to exits, each of which changes the state ofthe gadget and thereby the allowed traversals. We study such gadgets in the 0-,1-, and 2-player settings, in particular extending pastmotion-planning-through-gadgets work to 0-player games for the first time, byconsidering ""branchless"" connections between gadgets that route every gadget'sexit to a unique gadget's entrance. Our complexity results include containmentin L, NL, P, NP, and PSPACE; as well as hardness for NL, P, NP, and PSPACE. Weapply these results to show PSPACE-completeness for certain mechanics inFactorio, [the Sequence], and a restricted version of Trainyard, improvingprior results. This work strengthens prior results on switching graphs andreachability switching games.","Joshua Ani, Erik D. Demaine, Dylan H. Hendrickson, Jayson Lynch",,,11,
On the complexity of detecting hazards,"  Detecting and eliminating logic hazards in Boolean circuits is a fundamentalproblem in logic circuit design. We show that there is no $O(3^{(1-\epsilon)n}\text{poly}(s))$ time algorithm, for any $\epsilon > 0$, that detects logichazards in Boolean circuits of size $s$ on $n$ variables under the assumptionthat the strong exponential time hypothesis is true. This lower bound holdseven when the input circuits are restricted to be formulas of depth four. Wealso present a polynomial time algorithm for detecting $1$-hazards in DNF (or,$0$-hazards in CNF) formulas. Since $0$-hazards in DNF (or, $1$-hazards in CNF)formulas are easy to eliminate, this algorithm can be used to detect whether agiven DNF or CNF formula has a hazard in practice.",Balagopal Komarath and Nitin Saurabh,,,11,
Computational Aspects of Optimal Strategic Network Diffusion,"  Diffusion on complex networks is often modeled as a stochastic process. Yet,recent work on strategic diffusion emphasizes the decision power of agents andtreats diffusion as a strategic problem. Here we study the computationalaspects of strategic diffusion, i.e., finding the optimal sequence of nodes toactivate a network in the minimum time. We prove that finding an optimalsolution to this problem is NP-complete in a general case. To overcome thiscomputational difficulty, we present an algorithm to compute an optimalsolution based on a dynamic programming technique. We also show that theproblem is fixed parameter-tractable when parametrized by the product of thetreewidth and maximum degree. We analyze the possibility of developing anefficient approximation algorithm and show that two heuristic algorithmsproposed so far cannot have better than a logarithmic approximation guarantee.Finally, we prove that the problem does not admit better than a logarithmicapproximation, unless P=NP.","Marcin Waniek, Khaled Elbassioni, Flavio L. Pinheiro, Cesar A. Hidalgo
  and Aamena Alshamsi",,,11,
On simulation in automata networks,"  An automata network is a finite graph where each node holds a state from somefinite alphabet and is equipped with an update function that changes its stateaccording to the configuration of neighboring states. More concisely, it isgiven by a finite map $f:Q^n\rightarrow Q^n$. In this paper we study how some(sets of) automata networks can be simulated by some other (set of) automatanetworks with prescribed update mode or interaction graph. Our contributionsare the following. For non-Boolean alphabets and for any network size, thereare intrinsically non-sequential transformations (i.e. that can not be obtainedas composition of sequential updates of some network). Moreover there is nouniversal automaton network that can produce all non-bijective functions viacompositions of asynchronous updates. On the other hand, we show that there areuniversal automata networks for sequential updates if one is allowed to use alarger alphabet and then use either projection onto or restriction to theoriginal alphabet. We also characterize the set of functions that are generatedby non-bijective sequential updates. Following Tchuente, we characterize theinteraction graphs $D$ whose semigroup of transformations is the full semigroupof transformations on $Q^n$, and we show that they are the same if we forceeither sequential updates only, or all asynchronous updates.",Florian Bridoux and Maximilien Gadouleau and Guillaume Theyssier,,,11,
A Simpler NP-Hardness Proof for Familial Graph Compression,This document presents a simpler proof showcasing the NP-hardness of FamilialGraph Compression.,"Ammar Ahmed, Zohair Raza Hassan, Mudassir Shabbir",,,11,
"A faster algorithm for the FSSP in one-dimensional CA with multiple
  speeds",In cellular automata with multiple speeds for each cell $i$ there is apositive integer $p_i$ such that this cell updates its state still periodicallybut only at times which are a multiple of $p_i$. Additionally there is a finiteupper bound on all $p_i$. Manzoni and Umeo have described an algorithm forthese (one-dimensional) cellular automata which solves the Firing SquadSynchronization Problem. This algorithm needs linear time (in the number ofcells to be synchronized) but for many problem instances it is slower than theoptimum time by some positive constant factor. In the present paper we derivelower bounds on possible synchronization times and describe an algorithm whichis never slower and in some cases faster than the one by Manzoni and Umeo andwhich is close to a lower bound (up to a constant summand) in more cases.,Thomas Worsch (Karlsruhe Institute of Technology),,,11,
NP-complete variants of some classical graph problems,"  Some classical graph problems such as finding minimal spanning tree, shortestpath or maximal flow can be done efficiently. We describe slight variations ofsuch problems which are shown to be NP-complete. Our proofs use straightforwardreduction from $3$-SAT.",Per Alexandersson,,,11,
"Time-Space Tradeoffs for Distinguishing Distributions and Applications
  to Security of Goldreich's PRG","  In this work, we establish lower-bounds against memory bounded algorithms fordistinguishing between natural pairs of related distributions from samples thatarrive in a streaming setting.  In our first result, we show that any algorithm that distinguishes betweenuniform distribution on $\{0,1\}^n$ and uniform distribution on an$n/2$-dimensional linear subspace of $\{0,1\}^n$ with non-negligible advantageneeds $2^{\Omega(n)}$ samples or $\Omega(n^2)$ memory.  Our second result applies to distinguishing outputs of Goldreich's localpseudorandom generator from the uniform distribution on the output domain.Specifically, Goldreich's pseudorandom generator $G$ fixes a predicate$P:\{0,1\}^k \rightarrow \{0,1\}$ and a collection of subsets $S_1, S_2,\ldots, S_m \subseteq [n]$ of size $k$. For any seed $x \in \{0,1\}^n$, itoutputs $P(x_{S_1}), P(x_{S_2}), \ldots, P(x_{S_m})$ where $x_{S_i}$ is theprojection of $x$ to the coordinates in $S_i$. We prove that whenever $P$ is$t$-resilient (all non-zero Fourier coefficients of $(-1)^P$ are of degree $t$or higher), then no algorithm, with $<n^\epsilon$ memory, can distinguish theoutput of $G$ from the uniform distribution on $\{0,1\}^m$ with a large inversepolynomial advantage, for stretch $m \le\left(\frac{n}{t}\right)^{\frac{(1-\epsilon)}{36}\cdot t}$ (barring somerestrictions on $k$). The lower bound holds in the streaming model where ateach time step $i$, $S_i\subseteq [n]$ is a randomly chosen (ordered) subset ofsize $k$ and the distinguisher sees either $P(x_{S_i})$ or a uniformly randombit along with $S_i$.  Our proof builds on the recently developed machinery for proving time-spacetrade-offs (Raz 2016 and follow-ups) for search/learning problems.","Sumegha Garg, Pravesh K. Kothari, Ran Raz",,,11,
"Learning sums of powers of low-degree polynomials in the non-degenerate
  case","  We develop algorithms for writing a polynomial as sums of powers of lowdegree polynomials. Consider an $n$-variate degree-$d$ polynomial $f$ which canbe written as $$f = c_1Q_1^{m} + \ldots + c_s Q_s^{m},$$ where each $c_i\in\mathbb{F}^{\times}$, $Q_i$ is a homogeneous polynomial of degree $t$, and $t m= d$. In this paper, we give a $\text{poly}((ns)^t)$-time learning algorithmfor finding the $Q_i$'s given (black-box access to) $f$, if the $Q_i's$ satisfycertain non-degeneracy conditions and $n$ is larger than $d^2$. The set ofdegenerate $Q_i$'s (i.e., inputs for which the algorithm does not work) form anon-trivial variety and hence if the $Q_i$'s are chosen according to anyreasonable (full-dimensional) distribution, then they are non-degenerate withhigh probability (if $s$ is not too large).  Our algorithm is based on a scheme for obtaining a learning algorithm for anarithmetic circuit model from a lower bound for the same model, providedcertain non-degeneracy conditions hold. The scheme reduces the learning problemto the problem of decomposing two vector spaces under the action of a set oflinear operators, where the spaces and the operators are derived from the inputcircuit and the complexity measure used in a typical lower bound proof. Thenon-degeneracy conditions are certain restrictions on how the spaces decompose.","Ankit Garg, Neeraj Kayal, and Chandan Saha",,,11,
In-memory eigenvector computation in time O(1),"  In-memory computing with crosspoint resistive memory arrays has gainedenormous attention to accelerate the matrix-vector multiplication in thecomputation of data-centric applications. By combining a crosspoint array andfeedback amplifiers, it is possible to compute matrix eigenvectors in one stepwithout algorithmic iterations. In this work, time complexity of theeigenvector computation is investigated, based on the feedback analysis of thecrosspoint circuit. The results show that the computing time of the circuit isdetermined by the mismatch degree of the eigenvalues implemented in thecircuit, which controls the rising speed of output voltages. For a dataset ofrandom matrices, the time for computing the dominant eigenvector in the circuitis constant for various matrix sizes, namely the time complexity is O(1). TheO(1) time complexity is also supported by simulations of PageRank of real-worlddatasets. This work paves the way for fast, energy-efficient accelerators foreigenvector computation in a wide range of practical applications.","Zhong Sun, Giacomo Pedretti, Elia Ambrosi, Alessandro Bricalli,
  Daniele Ielmini",,,11,
Lower Bounds for QBFs of Bounded Treewidth,"  The problem of deciding the validity (QSAT) of quantified Boolean formulas(QBF) is a vivid research area in both theory and practice. In the field ofparameterized algorithmics, the well-studied graph measure treewidth turned outto be a successful parameter. A well-known result by Chen in parameterizedcomplexity is that QSAT when parameterized by the treewidth of the primal graphof the input formula together with the quantifier depth of the formula isfixed-parameter tractable. More precisely, the runtime of such an algorithm ispolynomial in the formula size and exponential in the treewidth, where theexponential function in the treewidth is a tower, whose height is thequantifier depth. A natural question is whether one can significantly improvethese results and decrease the tower while assuming the Exponential TimeHypothesis (ETH). In the last years, there has been a growing interest in thequest of establishing lower bounds under ETH, showing mostly problem-specificlower bounds up to the third level of the polynomial hierarchy. Still, animportant question is to settle this as general as possible and to cover thewhole polynomial hierarchy. In this work, we show lower bounds based on the ETHfor arbitrary QBFs parameterized by treewidth (and quantifier depth). Moreformally, we establish lower bounds for QSAT and treewidth, namely, that underETH there cannot be an algorithm that solves QSAT of quantifier depth i inruntime significantly better than i-fold exponential in the treewidth andpolynomial in the input size. In doing so, we provide a versatile reductiontechnique to compress treewidth that encodes the essence of dynamic programmingon arbitrary tree decompositions. Further, we describe a general methodologyfor a more fine-grained analysis of problems parameterized by treewidth thatare at higher levels of the polynomial hierarchy.","Johannes Klaus Fichte, Markus Hecher, Andreas Pfandler",,,11,
Multistage Graph Problems on a Global Budget,"  Time-evolving or temporal graphs gain more and more popularity when studyingthe behavior of complex networks. In this context, the multistage view oncomputational problems is among the most natural frameworks. Roughly speaking,herein one studies the different (time) layers of a temporal graph (effectivelymeaning that the edge set may change over time, but the vertex set remainsunchanged), and one searches for a solution of a given graph problem for eachlayer. The twist in the multistage setting is that the solutions found must notdiffer too much between subsequent layers. We relax on this already establishednotion by introducing a global instead of the local budget view studied so far.More specifically, we allow for few disruptive changes between subsequentlayers but request that overall, that is, summing over all layers, the degreeof change is moderate. Studying several classical graph problems (both NP-hardand polynomial-time solvable ones) from a parameterized complexity angle, weencounter both fixed-parameter tractability and parameterized hardness results.Somewhat surprisingly, we find that sometimes the global multistage versions ofNP-hard problems such as Vertex Cover turn out to be computationally moretractable than the ones of polynomial-time solvable problems such as Matching.","Klaus Heeger, Anne-Sophie Himmel, Frank Kammer, Rolf Niedermeier,
  Malte Renken, Andrej Sajenko",,,11,
Expected Outcomes and Manipulations in Online Fair Division,"  Two simple and attractive mechanisms for the fair division of indivisiblegoods in an online setting are LIKE and BALANCED LIKE. We study somefundamental computational problems concerning the outcomes of these mechanisms.In particular, we consider what expected outcomes are possible, what outcomesare necessary, and how to compute their exact outcomes. In general, we showthat such questions are more tractable to compute for LIKE than for BALANCEDLIKE. As LIKE is strategy-proof but BALANCED LIKE is not, we also consider thecomputational problem of how, with BALANCED LIKE, an agent can compute astrategic bid to improve their outcome. We prove that this problem isintractable in general.",Martin Aleksandrov and Toby Walsh,,,11,
"Chaos, Extremism and Optimism: Volume Analysis of Learning in Games","  We present volume analyses of Multiplicative Weights Updates (MWU) andOptimistic Multiplicative Weights Updates (OMWU) in zero-sum as well ascoordination games. Such analyses provide new insights into these gamedynamical systems, which seem hard to achieve via the classical techniqueswithin Computer Science and Machine Learning.  The first step is to examine these dynamics not in their original space(simplex of actions) but in a dual space (aggregate payoff space of actions).The second step is to explore how the volume of a set of initial conditionsevolves over time when it is pushed forward according to the algorithm. This isreminiscent of approaches in Evolutionary Game Theory where replicatordynamics, the continuous-time analogue of MWU, is known to always preservevolume in all games. Interestingly, when we examine discrete-time dynamics,both the choice of the game and the choice of the algorithm play a criticalrole. So whereas MWU expands volume in zero-sum games and is thus Lyapunovchaotic, we show that OMWU contracts volume, providing an alternativeunderstanding for its known convergent behavior. However, we also prove ano-free-lunch type of theorem, in the sense that when examining coordinationgames the roles are reversed: OMWU expands volume exponentially fast, whereasMWU contracts.  Using these tools, we prove two novel, rather negative properties of MWU inzero-sum games: (1) Extremism: even in games with unique fully mixed Nashequilibrium, the system recurrently gets stuck near pure-strategy profiles,despite them being clearly unstable from game theoretic perspective. (2)Unavoidability: given any set of good points (with your own interpretation of""good""), the system cannot avoid bad points indefinitely.",Yun Kuen Cheung and Georgios Piliouras,,,11,
Efficient Two-Sided Markets with Limited Information,"  Many important practical markets inherently involve the interaction ofstrategic buyers with strategic sellers. A fundamental impossibility result forsuch two-sided markets due to Myerson and Satterthwaite establishes that evenin the simplest such market, that of bilateral trade, it is impossible todesign a mechanism that is individually rational, truthful, (weakly) budgetbalanced, and efficient. Even worse, it is known that the ""second best""mechanism$-$the mechanism that maximizes social welfare subject to the otherconstraints$-$has to be carefully tailored to the Bayesian priors and isextremely complex.  In light of this impossibility result it is very natural to seek ""simple""mechanisms that are approximately optimal, and indeed a very active line ofrecent work has established a broad spectrum of constant-factor approximationguarantees, which apply to settings well beyond those for which (implicit)characterizations of the optimal (second best) mechanism are known.  In this work, we go one step further and show that for many fundamentaltwo-sided markets$-$e.g., bilateral trade, double auctions, and combinatorialdouble auctions$-$it is possible to design near-optimal mechanisms withprovable, constant-factor approximation guarantees with just a single samplefrom the priors! In fact, most of our results in addition to requiring lessinformation also improve upon the best known approximation guarantees for therespective setting.","Paul D\""utting, Federico Fusco, Philip Lazos, Stefano Leonardi,
  Rebecca Reiffenh\""auser",,,11,
On the Robustness of Winners: Counting Briberies in Elections,"  We study the parameterized complexity of counting variants of Swap- andShift-Bribery problems, focusing on the parameterizations by the number ofswaps and the number of voters. We show experimentally that Swap-Bribery offersa new approach to the robustness analysis of elections.","Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier",,,11,
Committee Selection with Attribute Level Preferences,"  We consider the problem of committee selection from a fixed set of candidateswhere each candidate has multiple quantifiable attributes. To select the bestpossible committee, instead of voting for a candidate, a voter is allowed toapprove the preferred attributes of a given candidate. Though attribute basedpreference is addressed in several contexts, committee selection problem withattribute approval of voters has not been attempted earlier. A committee formedon attribute preferences is more likely to be a better representative of thequalities desired by the voters and is less likely to be susceptible tocollusion or manipulation. In this work, we provide a formal study of thedifferent aspects of this problem and define properties of weak unanimity,strong unanimity, simple justified representations and compound justifiedrepresentation, that are required to be satisfied by the selected committee. Weshow that none of the existing vote/approval aggregation rules satisfy thesenew properties for attribute aggregation. We describe a greedy approach forattribute aggregation that satisfies the first three properties, but not thefourth, i.e., compound justified representation, which we prove to beNP-complete. Furthermore, we prove that finding a committee with justifiedrepresentation and the highest approval voting score is NP-complete.","Venkateswara Rao Kagita, Arun K Pujari, Vineet Padmanabhan, Vikas
  Kumar",,,11,
Search for an Immobile Hider on a Stochastic Network,"  Harry hides on an edge of a graph and does not move from there. Sally,starting from a known origin, tries to find him as soon as she can. Harry'sgoal is to be found as late as possible. At any given time, each edge of thegraph is either active or inactive, independently of the other edges, with aknown probability of being active. This situation can be modeled as a zero-sumtwo-person stochastic game. We show that the game has a value and we provideupper and lower bounds for this value. Finally, by generalizing optimalstrategies of the deterministic case, we provide more refined results for treesand Eulerian graphs.",Tristan Garrec and Marco Scarsini,,,11,
Maximin Fairness with Mixed Divisible and Indivisible Goods,"  We study fair resource allocation when the resources contain a mixture ofdivisible and indivisible goods, focusing on the well-studied fairness notionof maximin share fairness (MMS). With only indivisible goods, a full MMSallocation may not exist, but a constant multiplicative approximate allocationalways does. We analyze how the MMS approximation guarantee would be affectedwhen the resources to be allocated also contain divisible goods. In particular,we show that the worst-case MMS approximation guarantee with mixed goods is noworse than that with only indivisible goods. However, there exist probleminstances to which adding some divisible resources would strictly decrease theMMS approximation ratio of the instance. On the algorithmic front, we propose aconstructive algorithm that will always produce an $\alpha$-MMS allocation forany number of agents, where $\alpha$ takes values between $1/2$ and $1$ and isa monotone increasing function determined by how agents value the divisiblegoods relative to their MMS values.","Xiaohui Bei, Shengxin Liu, Xinhang Lu, Hongao Wang",,,11,
Flow-Based Network Creation Games,"  Network Creation Games(NCGs) model the creation of decentralizedcommunication networks like the Internet. In such games strategic agentscorresponding to network nodes selfishly decide with whom to connect tooptimize some objective function. Past research intensively analyzed modelswhere the agents strive for a central position in the network. This modelsagents optimizing the network for low-latency applications like VoIP. However,with today's abundance of streaming services it is important to ensure that thecreated network can satisfy the increased bandwidth demand. To the best of ourknowledge, this natural problem of the decentralized strategic creation ofnetworks with sufficient bandwidth has not yet been studied.  We introduce Flow-Based NCGs where the selfish agents focus on bandwidthinstead of latency. In essence, budget-constrained agents create network linksto maximize their minimum or average network flow value to all other networknodes. Equivalently, this can also be understood as agents who create links toincrease their connectivity and thus also the robustness of the network. Forthis novel type of NCG we prove that pure Nash equilibria exist, we give asimple algorithm for computing optimal networks, we show that the Price ofStability is 1 and we prove an (almost) tight bound of 2 on the Price ofAnarchy. Last but not least, we show that our models do not admit a potentialfunction.","Hagen Echzell, Tobias Friedrich, Pascal Lenzner, Anna Melnichenko",,,11,
"Game on Random Environment, Mean-field Langevin System and Neural
  Networks","  In this paper we study a type of games regularized by the relative entropy,where the players' strategies are coupled through a random environmentvariable. Besides the existence and the uniqueness of equilibria of such games,we prove that the marginal laws of the corresponding mean-field Langevinsystems can converge towards the games' equilibria in different settings. Asapplications, the dynamic games can be treated as games on a random environmentwhen one treats the time horizon as the environment. In practice, our resultscan be applied to analysing the stochastic gradient descent algorithm for deepneural networks in the context of supervised learning as well as for thegenerative adversarial networks.","Giovanni Conforti (CMAP), Anna Kazeykina (LMO), Zhenjie Ren (CEREMADE)",,,11,
The Price of Anarchy for Instantaneous Dynamic Equilibria,"  We consider flows over time within the deterministic queueing model and studythe solution concept of instantaneous dynamic equilibrium (IDE) in which flowparticles select at every decision point a currently shortest path. The lengthof such a path is measured by the physical travel time plus the time spent inqueues. Although IDE have been studied since the eighties, the efficiency ofthe solution concept is not well understood. We study the price of anarchy forthis model and show an upper bound of order $\mathcal{O}(U\cdot \tau)$ forsingle-sink instances, where $U$ denotes the total inflow volume and $\tau$ thesum of edge travel times. We complement this upper bound with a family of quitecomplex instances proving a lower bound of order $\Omega(U\cdot\log\tau)$.",Lukas Graf and Tobias Harks,,,11,
"An O(log log m) Prophet Inequality for Subadditive Combinatorial
  Auctions","  Prophet inequalities compare the expected performance of an online algorithmfor a stochastic optimization problem to the expected optimal solution inhindsight. They are a major alternative to classic worst-case competitiveanalysis, of particular importance in the design and analysis of simple(posted-price) incentive compatible mechanisms with provable approximationguarantees.  A central open problem in this area concerns subadditive combinatorialauctions. Here $n$ agents with subadditive valuation functions compete for theassignment of $m$ items. The goal is to find an allocation of the items thatmaximizes the total value of the assignment. The question is whether thereexists a prophet inequality for this problem that significantly beats the bestknown approximation factor of $O(\log m)$.  We make major progress on this question by providing an $O(\log \log m)$prophet inequality. Our proof goes through a novel primal-dual approach. It isalso constructive, resulting in an online policy that takes the form of staticand anonymous item prices that can be computed in polynomial time givenappropriate query access to the valuations. As an application of our approach,we construct a simple and incentive compatible mechanism based on posted pricesthat achieves an $O(\log \log m)$ approximation to the optimal revenue forsubadditive valuations under an item-independence assumption.","Paul D\""utting, Thomas Kesselheim, Brendan Lucier",,,11,
"Infinite-Dimensional Fisher Markets: Equilibrium, Duality and
  Optimization","  This paper considers a linear Fisher market with $n$ buyers and a continuumof items. In order to compute market equilibria, we introduce(infinite-dimensional) convex programs over Banach spaces, thereby generalizingthe Eisenberg-Gale convex program and its dual. Regarding the new convexprograms, we establish existence of optimal solutions, the existence ofKKT-type conditions, as well as strong duality. All these properties areestablished via non-standard arguments, which circumvent the limitations ofduality theory in optimization over infinite-dimensional vector spaces.Furthermore, we show that there exists a pure equilibrium allocation, i.e., adivision of the item space. Similar to the finite-dimensional case, a marketequilibrium under the infinite-dimensional Fisher market is Pareto optimal,envy-free and proportional. We also show how to obtain the (a.e. unique)equilibrium price vector and a pure equilibrium allocation from the (unique)$n$-dimensional equilibrium bang-per-buck vector. When the item space is theunit interval $[0,1]$ and buyers have piecewise linear utilities, we show that$\epsilon$-approximate equilibrium prices can be computed in time polynomial inthe market size and $\log \frac{1}{\epsilon}$. This is achieved by solving afinite-dimensional convex program using the ellipsoid method. To this end, wegive nontrivial and efficient subgradient and separation oracles. For generalbuyer valuations, we propose computing market equilibrium using stochastic dualaveraging, which finds an approximate equilibrium price vector with highprobability.","Yuan Gao, Christian Kroer",,,11,
Counteracting Inequality in Markets via Convex Pricing,"  We study market mechanisms for allocating divisible goods to competing agentswith quasilinear utilities. For \emph{linear} pricing (i.e., the cost of a goodis proportional to the quantity purchased), the First Welfare Theorem statesthat Walrasian equilibria maximize the sum of agent valuations. This ensuresefficiency, but can lead to extreme inequality across individuals. Manyreal-world markets -- especially for water -- use \emph{convex} pricinginstead, often known as increasing block tariffs (IBTs). IBTs are thought topromote equality, but there is a dearth of theoretical support for this claim.  In this paper, we study a simple convex pricing rule and show that theresulting equilibria are guaranteed to maximize a CES welfare function.Furthermore, a parameter of the pricing rule directly determines which CESwelfare function is implemented; by tweaking this parameter, the social plannercan precisely control the tradeoff between equality and efficiency. Our resultholds for any valuations that are homogeneous, differentiable, and concave. Wealso give an iterative algorithm for computing these pricing rules, derive atruthful mechanism for the case of a single good, and discuss Sybil attacks.",Ashish Goel and Benjamin Plaut,,,11,
"A Two-stage Game Framework to Secure Transmission in Two-Tier UAV
  Networks","  The multi-UAV network is promising to extend conventional networks byproviding broader coverage and better reliability. Nevertheless, the broadcastnature of wireless signals and the broader coverage expose multi-UAVcommunications to the threats of passive eavesdroppers. Recent studies mainlyfocus on securing a single legitimate link, or communications between a UAV andmultiple ground users in one/two-UAV-aided networks, while the physical layersecrecy analysis for hierarchical multi-UAV networks is underexplored. In thispaper, we investigate a general two-tier UAV network consisting of multiple UAVtransmitters (UTs) and multiple UAV receivers (URs) in the presence of multipleUAV eavesdroppers (UEs). To protect all legitimate UT-UR links against UEs atthe physical layer, we design a two-stage framework consisting of a UT-URassociation stage and a cooperative transmission stage. Specifically, weformulate the secure transmission problem into a many-to-one matching gamefollowed by an overlapping coalition formation (OCF) game, taking into accountthe limited capabilities and the throughput requirements of URs, as well as thetransmission power constraints of UTs. A matching algorithm and an OCFalgorithm are proposed to solve these two sequential games whose convergencesand stabilities are guaranteed theoretically. Simulation results show thesuperiority of our algorithms and the effectiveness of our two-stage gameframework in the terms of secrecy performance.","Mengnian Xu, Yanjiao Chen, Wei Wang",,,11,
Revenue Maximizing Markets for Zero-Day Exploits,"  Markets for zero-day exploits (software vulnerabilities unknown to thevendor) have a long history and a growing popularity. We study these marketsfrom a revenue-maximizing mechanism design perspective. We first propose atheoretical model for zero-day exploits markets. In our model, one exploit isbeing sold to multiple buyers. There are two kinds of buyers, which we call thedefenders and the offenders. The defenders are buyers who buy vulnerabilitiesin order to fix them (e.g., software vendors). The offenders, on the otherhand, are buyers who intend to utilize the exploits (e.g., national securityagencies and police). Our model is more than a single-item auction. First, anexploit is a piece of information, so one exploit can be sold to multiplebuyers. Second, buyers have externalities. If one defender wins, then theexploit becomes worthless to the offenders. Third, if we disclose the detailsof the exploit to the buyers before the auction, then they may leave with theinformation without paying. On the other hand, if we do not disclose thedetails, then it is difficult for the buyers to come up with their privatevaluations. Considering the above, our proposed mechanism discloses the detailsof the exploit to all offenders before the auction. The offenders then pay todelay the exploit being disclosed to the defenders.",Mingyu Guo and Hideaki Hata and Ali Babar,,,11,
A Network Formation Game for the Emergence of Hierarchies,"  We propose a novel network formation game that explains the emergence ofvarious hierarchical structures in groups where self-interested orutility-maximizing individuals decide to establish or severe relationships ofauthority or collaboration among themselves. We consider two settings: we firstconsider individuals who do not seek the other party's consent whenestablishing a relationship and then individuals who do. For both settings, weformally relate the emerged hierarchical structures with the novel inclusion ofwell-motivated hierarchy promoting terms in the individuals' utility functions.We first analyze the game via a static analysis and characterize all thehierarchical structures that can be formed as its solutions. We then considerthe game played dynamically under stochastic interactions among individualsimplementing better-response dynamics and analyze the nature of the convergednetworks.","Pedro Cisneros-Velarde, Francesco Bullo",,,11,
"Credible, Truthful, and Two-Round (Optimal) Auctions via Cryptographic
  Commitments","  We consider the sale of a single item to multiple buyers by arevenue-maximizing seller. Recent work of Akbarpour and Li formalizes\emph{credibility} as an auction desideratum, and prove that the only optimal,credible, strategyproof auction is the ascending price auction with reserves(Akbarpour and Li, 2019).  In contrast, when buyers' valuations are MHR, we show that the mildadditional assumption of a cryptographically secure commitment scheme sufficesfor a simple \emph{two-round} auction which is optimal, strategyproof, andcredible (even when the number of bidders is only known by the auctioneer).  We extend our analysis to the case when buyer valuations are$\alpha$-strongly regular for any $\alpha > 0$, up to arbitrary $\varepsilon$in credibility. Interestingly, we also prove that this construction cannot beextended to regular distributions, nor can the $\varepsilon$ be removed withmultiple bidders.",Matheus V. X. Ferreira and S. Matthew Weinberg,,,11,
On Rational Choice and the Representation of Decision Problems,"  In economic theory, an agent chooses from available alternatives -- modelledas a set. In decisions in the field or in the lab, however, agents do not haveaccess to the set of alternatives at once. Instead, alternatives arerepresented by the outside world in a structured way. Online search results arelists of items, wine menus are often lists of lists (grouped by type orcountry), and online shopping often involves filtering items which can beviewed as navigating a tree. Representations constrain how an agent can choose.At the same time, an agent can also leverage representations when choosing,simplifying his choice process. For instance, in the case of a list he can usethe order in which alternatives are represented to make his choice.  In this paper, we model representations and choice procedures operating onthem. We ask which properties procedures have to fulfill such that theirchoices are equivalent to maximizing a strict preference relation. We fullycharacterize such procedures for general representations including lists, listof lists, trees and other representations.",Paulo Oliva and Philipp Zahn,,,11,
On the Distortion Value of the Elections with Abstention,"  In Spatial Voting Theory, distortion is a measure of how good the winner is.It is proved that no deterministic voting mechanism can guarantee a distortionbetter than $3$, even for simple metrics such as a line. In this study, we wishto answer the following question: how does the distortion value change if weallow less motivated agents to abstain from the election?  We consider an election with two candidates and suggest an abstention model,which is a more general form of the abstention model proposed by Kirchgassner.We define the concepts of the expected winner and the expected distortion toevaluate the distortion of an election in our model. Our results fullycharacterize the distortion value and provide a rather complete picture of themodel.","Mohammad Ghodsi, Mohamad Latifian, Masoud Seddighin",,,11,
Approximating Stable Matchings with Ties of Bounded Size,"  Finding a stable matching is one of the central problems in algorithmic gametheory. If participants are allowed to have ties and incomplete preferences,computing a stable matching of maximum cardinality is known to be NP-hard. Inthis paper we present a $(3L-2)/(2L-1)$-approximation algorithm for the stablematching problem with ties of size at most $L$ and incomplete lists. Our resultmatches the known lower bound on the integrality gap for the associated LPformulation.","Jochen Koenemann, Kanstantsin Pashkovich, Natig Tofigzade",,,11,
On Euler's inequality and automated reasoning with dynamic geometry,"  Euler's inequality $R\geq 2r$ can be investigated in a novel way by usingimplicit loci in GeoGebra. Some unavoidable side effects of the implicit locuscomputation introduce unexpected algebraic curves. By using a mixture ofsymbolic and numerical methods a possible approach is sketched up toinvestigate the situation. By exploiting fast GPU computations, a webapplication written in CindyJS helps in understanding the situation evenbetter.","Zolt\'an Kov\'acs, R\'obert Vajda, Aaron Montag",,,11,
"On Exact Reznick, Hilbert-Artin and Putinar's Representations","  We consider the problem of computing exact sums of squares (SOS)decompositions for certain classes of non-negative multivariate polynomials,relying on semidefinite programming (SDP) solvers.  We provide a hybrid numeric-symbolic algorithm computing exact rational SOSdecompositions with rational coefficients for polynomials lying in the interiorof the SOS cone. The first step of this algorithm computes an approximate SOSdecomposition for a perturbation of the input polynomial with anarbitrary-precision SDP solver. Next, an exact SOS decomposition is obtainedthanks to the perturbation terms and a compensation phenomenon. We prove thatbit complexity estimates on output size and runtime are both polynomial in thedegree of the input polynomial and singly exponential in the number ofvariables. Next, we apply this algorithm to compute exact Reznick,Hilbert-Artin's representation and Putinar's representations respectively forpositive definite forms and positive polynomials over basic compactsemi-algebraic sets. We also report on practical experiments done with theimplementation of these algorithms and existing alternatives such as thecritical point method and cylindrical algebraic decomposition.",Victor Magron and Mohab Safey El Din,,,11,
A Condition for Multiplicity Structure of Univariate Polynomials,"  We consider the problem of finding a condition for a univariate polynomialhaving a given multiplicity structure when the number of distinct roots isgiven. It is well known that such conditions can be written as conjunctions ofseveral polynomial equations and one inequation in the coefficients, by usingrepeated parametric gcd's. In this paper, we give a novel condition which isnot based on repeated gcd's. Furthermore, it is shown that the number ofpolynomials in the condition is optimal and the degree of polynomials issmaller than that in the previous condition based on repeated gcd's.",Hoon Hong and Jing Yang,,,11,
"A Simple Method for Computing Some Pseudo-Elliptic Integrals in Terms of
  Elementary Functions","  We introduce a method for computing some pseudo-elliptic integrals in termsof elementary functions. The method is simple and fast in comparison to thealgebraic case of the Risch-Trager-Bronstein algorithm. This method can quicklysolve many pseudo-elliptic integrals, which other well-known computer algebrasystems either fail, return an answer in terms of special functions, or requiremore than 20 seconds of computing time. Randomised tests showed our methodsolved 73.4% of the integrals that could be solved with the best implementationof the Risch-Trager-Bronstein algorithm. Unlike the symbolic integrationalgorithms of Risch, Davenport, Trager, Bronstein and Miller; our method is nota decision process. The implementation of this method is less than 200 lines ofMathematica code and can be easily ported to other CAS that can solve systemsof polynomial equations.",Sam Blake,,,11,
"New Remarks on the Factorization and Equivalence Problems for a Class of
  Multivariate Polynomial Matrices","  This paper is concerned with the factorization and equivalence problems ofmultivariate polynomial matrices. We present some new criteria for theexistence of matrix factorizations for a class of multivariate polynomialmatrices, and obtain a necessary and sufficient condition for the equivalenceof a square polynomial matrix and a diagonal matrix. Based on the constructiveproof of the new criteria, we give a factorization algorithm and prove theuniqueness of the factorization. We implement the algorithm on Maple, and twoillustrative examples are given to show the effectiveness of the algorithm.","Dong Lu, Dingkang Wang, Fanghui Xiao",,,11,
"On Algorithmic Estimation of Analytic Complexity for Polynomial
  Solutions to Hypergeometric Systems",The paper deals with the analytic complexity of solutions to bivariateholonomic hypergeometric systems of the Horn type. We obtain estimates on theanalytic complexity of Puiseux polynomial solutions to the hypergeometricsystems defined by zonotopes. We also propose algorithms of the analyticcomplexity estimation for polynomials.,Vitaly A. Krasikov,,,11,
"Generic bivariate multi-point evaluation, interpolation and modular
  composition with precomputation","  Suppose $\mathbb{K}$ is a large enough field and $\mathcal{P} \subset\mathbb{K}^2$ is a fixed, generic set of points which is available forprecomputation. We introduce a technique called \emph{reshaping} which allowsus to design quasi-linear algorithms for both: computing the evaluations of aninput polynomial $f \in \mathbb{K}[x,y]$ at all points of $\mathcal{P}$; andcomputing an interpolant $f \in \mathbb{K}[x,y]$ which takes prescribed valueson $\mathcal{P}$ and satisfies an input $y$-degree bound. Our genericityassumption is explicit and we prove that it holds for most point sets over alarge enough field. If $\mathcal{P}$ violates the assumption, our algorithmsstill work and the performance degrades smoothly according to a distance frombeing generic. To show that the reshaping technique may have an impact on otherrelated problems, we apply it to modular composition: suppose genericpolynomials $M \in \mathbb{K}[x]$ and $A \in \mathbb{K}[x]$ are available forprecomputation, then given an input $f \in \mathbb{K}[x,y]$ we show how tocompute $f(x, A(x)) \operatorname{rem} M(x)$ in quasi-linear time.",Vincent Neiger and Johan Rosenkilde and Grigory Solomatov,,,11,
"A divide-and-conquer algorithm for computing Gr\""obner bases of syzygies
  in finite dimension","  Let $f_1,\ldots,f_m$ be elements in a quotient $R^n / N$ which has finitedimension as a $K$-vector space, where $R = K[X_1,\ldots,X_r]$ and $N$ is an$R$-submodule of $R^n$. We address the problem of computing a Gr\""obner basisof the module of syzygies of $(f_1,\ldots,f_m)$, that is, of vectors$(p_1,\ldots,p_m) \in R^m$ such that $p_1 f_1 + \cdots + p_m f_m = 0$.  An iterative algorithm for this problem was given by Marinari, M\""oller, andMora (1993) using a dual representation of $R^n / N$ as the kernel of acollection of linear functionals. Following this viewpoint, we design adivide-and-conquer algorithm, which can be interpreted as a generalization toseveral variables of Beckermann and Labahn's recursive approach for matrixPad\'e and rational interpolation problems. To highlight the interest of thismethod, we focus on the specific case of bivariate Pad\'e approximation andshow that it improves upon the best known complexity bounds.",Simone Naldi and Vincent Neiger,,,11,
"Algorithmic Averaging for Studying Periodic Orbits of Planar
  Differential Systems","  One of the main open problems in the qualitative theory of real planardifferential systems is the study of limit cycles. In this article, we presentan algorithmic approach for detecting how many limit cycles can bifurcate fromthe periodic orbits of a given polynomial differential center when it isperturbed inside a class of polynomial differential systems via the averagingmethod. We propose four symbolic algorithms to implement the averaging method.The first algorithm is based on the change of polar coordinates that allows oneto transform a considered differential system to the normal form of averaging.The second algorithm is used to derive the solutions of certain differentialsystems associated to the unperturbed term of the normal of averaging. Thethird algorithm exploits the partial Bell polynomials and allows one to computethe integral formula of the averaged functions at any order. The last algorithmis based on the aforementioned algorithms and determines the exact expressionsof the averaged functions for the considered differential systems. Theimplementation of our algorithms is discussed and evaluated using severalexamples. The experimental results have extended the existing relevant resultsfor certain classes of differential systems.",Bo Huang,,,11,
"Homotopy techniques for solving sparse column support determinantal
  polynomial systems","  Let $\mathbf{K}$ be a field of characteristic zero with$\overline{\mathbf{K}}$ its algebraic closure. Given a sequence of polynomials$\mathbf{g} = (g_1, \ldots, g_s) \in \mathbf{K}[x_1, \ldots , x_n]^s$ and apolynomial matrix $\mathbf{F} = [f_{i,j}] \in \mathbf{K}[x_1, \ldots, x_n]^{p\times q}$, with $p \leq q$, we are interested in determining the isolatedpoints of $V_p(\mathbf{F},\mathbf{g})$, the algebraic set of points in$\overline{\mathbf{K}}$ at which all polynomials in $\mathbf{g}$ and all$p$-minors of $\mathbf{F}$ vanish, under the assumption $n = q - p + s + 1$.Such polynomial systems arise in a variety of applications including forexample polynomial optimization and computational geometry. We design arandomized sparse homotopy algorithm for computing the isolated points in$V_p(\mathbf{F},\mathbf{g})$ which takes advantage of the determinantalstructure of the system defining $V_p(\mathbf{F}, \mathbf{g})$. Its complexityis polynomial in the maximum number of isolated solutions to such systemssharing the same sparsity pattern and in some combinatorial quantities attachedto the structure of such systems. It is the first algorithm which takesadvantage both on the determinantal structure and sparsity of inputpolynomials. We also derive complexity bounds for the particular but importantcase where $\mathbf{g}$ and the columns of $\mathbf{F}$ satisfy weighted degreeconstraints. Such systems arise naturally in the computation of critical pointsof maps restricted to algebraic sets when both are invariant by the action ofthe symmetric group.","George Labahn (SCG), Mohab Safey El Din (PolSys), \'Eric Schost (SCG),
  Thi Xuan Vu (PolSys, SCG)",,,11,
Exact algorithms for semidefinite programs with degenerate feasible set,"  Given symmetric matrices $A_0, A_1, \ldots, A_n$ of size $m$ with rationalentries, the set of real vectors $x = (x_1, \ldots, x_n)$ such that the matrix$A_0 + x_1 A_1 + \cdots + x_n A_n$ has non-negative eigenvalues is called aspectrahedron. Minimization of linear functions over spectrahedra is calledsemidefinite programming. Such problems appear frequently in control theory andreal algebra, especially in the context of nonnegativity certificates formultivariate polynomials based on sums of squares. Numerical software forsemidefinite programming are mostly based on interior point methods, assumingnon-degeneracy properties such as the existence of an interior point in thespectrahedron. In this paper, we design an exact algorithm based on symbolichomotopy for solving semidefinite programs without assumptions on the feasibleset, and we analyze its complexity. Because of the exactness of the output, itcannot compete with numerical routines in practice. However, we prove thatsolving such problems can be done in polynomial time if either $n$ or $m$ isfixed.","Didier Henrion, Simone Naldi and Mohab Safey El Din",,,11,
Separating Variables in Bivariate Polynomial Ideals,"  We present an algorithm which for any given ideal $I\subseteq\mathbb{K}[x,y]$ finds all elements of $I$ that have the form $f(x) - g(y)$, i.e., allelements in which no monomial is a multiple of $xy$.","Manfred Buchacher, Manuel Kauers, Gleb Pogudin",,,11,
A Low-Level Index for Distributed Logic Programming,"  A distributed logic programming language with support for meta-programmingand stream processing offers a variety of interesting research problems, suchas: How can a versatile and stable data structure for the indexing of a largenumber of expressions be implemented with simple low-level data structures? Canlow-level programming help to reduce the number of occur checks in Robinson'sunification algorithm? This article gives the answers.","Thomas Prokosch (Institute for Informatics, Ludwig-Maximilian
  University of Munich, Germany)",,,11,
"Kleene stars of the plane, polylogarithms and symmetries","  We extend the definition and construct several bases for polylogarithms Li T, where T are some series, recognizable by a finite state (multiplicity)automaton of alphabet 4 X = {x 0 , x 1 }. The kernel of this new""polylogarithmic map"" Li $\bullet$ is also characterized and provides arewriting process which terminates to a normal form. We concentrate onalgebraic and analytic aspects of this extension allowing index polylogarithmsat non positive multi-indices, by rational series and regularize polyzetas atnon positive multi-indices.","G\'erard Henry Edmond Duchamp (LIPN), Vincel Hoang Ngoc Minh, Ngo Quoc
  Hoan",,,11,
Algebraic number fields and the LLL algorithm,"  In this paper we analyze the computational costs of various operations andalgorithms in algebraic number fields using exact arithmetic. Let $K$ be analgebraic number field. In the first half of the paper, we calculate therunning time and the size of the output of many operations in $K$ in terms ofthe size of the input and the parameters of $K$. We include some earlierresults about these, but we go further than them, e.g. we also analyze some$\mathbb{R}$-specific operations in $K$ like less-than comparison. In thesecond half of the paper, we analyze two algorithms: the Bareiss algorithm,which is an integer-preserving version of the Gaussian elimination, and the LLLalgorithm, which is for lattice basis reduction. In both cases, we extend thealgorithm from $\mathbb{Z}^n$ to $K^n$, and give a polynomial upper bound onthe running time when the computations in $K$ are performed exactly (as opposedto floating-point approximations).",M. J. Uray,,,11,
Resultants over principal Artinian rings,"  The resultant of two univariate polynomials is an invariant of greatimportance in commutative algebra and vastly used in computer algebra systems.Here we present an algorithm to compute it over Artinian principal rings with amodified version of the Euclidean algorithm. Using the same strategy, we showhow the reduced resultant and a pair of B\'ezout coefficient can be computed.Particular attention is devoted to the special case of$\mathbf{Z}/n\mathbf{Z}$, where we perform a detailed analysis of theasymptotic cost of the algorithm. Finally, we illustrate how the algorithms canbe exploited to improve ideal arithmetic in number fields and polynomialarithmetic over $p$-adic fields.","Claus Fieker, Tommy Hofmann, Carlo Sircana",,,11,
Generalizing The Davenport-Mahler-Mignotte Bound -- The Weighted Case,"  Root separation bounds play an important role as a complexity measure inunderstanding the behaviour of various algorithms in computational algebra,e.g., root isolation algorithms. A classic result in the univariate setting isthe Davenport-Mahler-Mignotte (DMM) bound. One way to state the bound is toconsider a directed acyclic graph $(V,E)$ on a subset of roots of a degree $d$polynomial $f(z) \in \mathbb{C}[z]$, where the edges point from a root ofsmaller absolute value to one of larger absolute, and the in-degrees of allvertices is at most one. Then the DMM bound is an amortized lower bound on thefollowing product: $\prod_{(\alpha,\beta) \in E}|\alpha-\beta|$. However, thelower bound involves the discriminant of the polynomial $f$, and becomestrivial if the polynomial is not square-free. This was resolved by Eigenwillig,(2008), by using a suitable subdiscriminant instead of the discriminant.Escorcielo-Perrucci, 2016, further dropped the in-degree constraint on thegraph by using the theory of finite differences. Emiris et al., 2019, havegeneralized their result to handle the case where the exponent of the term$|\alpha-\beta|$ in the product is at most the multiplicity of either of theroots. In this paper, we generalize these results by allowing arbitrarypositive integer weights on the edges of the graph, i.e., for a weight function$w: E \rightarrow \mathbb{Z}_{>0}$, we derive an amortized lower bound on$\prod_{(\alpha,\beta) \in E}|\alpha-\beta|^{w(\alpha,\beta)}$. Such a productoccurs in the complexity estimates of some recent algorithms for rootclustering (e.g., Becker et al., 2016), where the weights are usually somefunction of the multiplicity of the roots. Because of its amortized nature, ourbound is arguably better than the bounds obtained by manipulating existingresults to accommodate the weights.",Vikram Sharma,,,11,
"Characterizing Triviality of the Exponent Lattice of A Polynomial
  through Galois and Galois-Like Groups","  The problem of computing \emph{the exponent lattice} which consists of allthe multiplicative relations between the roots of a univariate polynomial hasdrawn much attention in the field of computer algebra. As is known, almost allirreducible polynomials with integer coefficients have only trivial exponentlattices. However, the algorithms in the literature have difficulty in provingsuch triviality for a generic polynomial. In this paper, the relations betweenthe Galois group (respectively, \emph{the Galois-like groups}) and thetriviality of the exponent lattice of a polynomial are investigated. The$\bbbq$\emph{-trivial} pairs, which are at the heart of the relations betweenthe Galois group and the triviality of the exponent lattice of a polynomial,are characterized. An effective algorithm is developed to recognize thesepairs. Based on this, a new algorithm is designed to prove the triviality ofthe exponent lattice of a generic irreducible polynomial, which considerablyimproves a state-of-the-art algorithm of the same type when the polynomialdegree becomes larger. In addition, the concept of the Galois-like groups of apolynomial is introduced. Some properties of the Galois-like groups are provedand, more importantly, a sufficient and necessary condition is given for apolynomial (which is not necessarily irreducible) to have trivial exponentlattice.",Tao Zheng,,,11,
"Compatible rewriting of noncommutative polynomials for proving operator
  identities","  The goal of this paper is to prove operator identities using equalitiesbetween noncommutative polynomials. In general, a polynomial expression is notvalid in terms of operators, since it may not be compatible with domains andcodomains of the corresponding operators. Recently, some of the authorsintroduced a framework based on labelled quivers to rigorously translatepolynomial identities to operator identities. In the present paper, we extendand adapt the framework to the context of rewriting and polynomial reduction.We give a sufficient condition on the polynomials used for rewriting to ensurethat standard polynomial reduction automatically respects domains and codomainsof operators. Finally, we adapt the noncommutative Buchberger procedure tocompute additional compatible polynomials for rewriting. In the packageOperatorGB, we also provide an implementation of the concepts developed.","Cyrille Chenavier, Clemens Hofstadler, Clemens G. Raab, Georg
  Regensburger",,,11,
Groebner basis structure of ideal interpolation,"  We study the relationship between certain Groebner bases for zero dimensionalideals, and the interpolation condition functionals of ideal interpolation.Ideal interpolation is defined by a linear idempotent projector whose kernel isa polynomial ideal. In this paper, we propose the notion of ""reverse"" completereduced basis. Based on the notion, we present a fast algorithm to compute thereduced Groebner basis for the kernel of ideal projector under an arbitrarycompatible ordering. As an application, we show that knowing the affine varietymakes available information concerning the reduced Groebner basis.",Yihe Gong and Xue Jiang,,,11,
"SNS: A Solution-based Nonlinear Subspace method for time-dependent model
  order reduction","  Several reduced order models have been developed for nonlinear dynamicalsystems. To achieve a considerable speed-up, a hyper-reduction step is neededto reduce the computational complexity due to nonlinear terms. Manyhyper-reduction techniques require the construction of nonlinear term basis,which introduces a computationally expensive offline phase. A novel way ofconstructing nonlinear term basis within the hyper-reduction process isintroduced. In contrast to the traditional hyper-reduction techniques where thecollection of nonlinear term snapshots is required, the SNS method avoidscollecting the nonlinear term snapshots. Instead, it uses the solutionsnapshots that are used for building a solution basis, which enables avoidingan extra data compression of nonlinear term snapshots. As a result, the SNSmethod provides a more efficient offline strategy than the traditional modelorder reduction techniques, such as the DEIM, GNAT, and ST-GNAT methods. TheSNS method is theoretically justified by the conforming subspace condition andthe subspace inclusion relation. It is useful for model order reduction oflarge-scale nonlinear dynamical problems to reduce the offline cost. It isespecially useful for ST-GNAT that has shown promising results, such as a goodaccuracy with a considerable online speed-up for hyperbolic problems in arecent paper by Choi and Carlberg in SISC 2019, because ST-GNAT involves anexpensive offline cost related to collecting nonlinear term snapshots.Numerical results support that the accuracy of the solution from the SNS methodis comparable to the traditional methods and a considerable speed-up (i.e., afactor of two to a hundred) is achieved in the offline phase.",Youngsoo Choi and Deshawn Coombs and Robert Anderson,,,11,
"Algorithms and Comparisons of Non-negative Matrix Factorization with
  Volume Regularization for Hyperspectral Unmixing","  In this work, we consider nonnegative matrix factorization (NMF) with aregularization that promotes small volume of the convex hull spanned by thebasis matrix. We present highly efficient algorithms for three different volumeregularizers, and compare them on endmember recovery in hyperspectral unmixing.The NMF algorithms developed in this work are shown to outperform thestate-of-the-art volume-regularized NMF methods, and produce meaningfuldecompositions on real-world hyperspectral images in situations whereendmembers are highly mixed (no pure pixels). Furthermore, our extensivenumerical experiments show that when the data is highly separable, meaning thatthere are data points close to the true endmembers, and there are a fewendmembers, the regularizer based on the determinant of the Gramian producesthe best results in most cases. For data that is less separable and/or containsmore endmembers, the regularizer based on the logarithm of the determinant ofthe Gramian performs best in general.","M. S. Ang, Nicolas Gillis",,,11,
Block-Term Tensor Decomposition: Model Selection and Computation,"  The so-called block-term decomposition (BTD) tensor model has been recentlyreceiving increasing attention due to its enhanced ability of representingsystems and signals that are composed of \emph{blocks} of rank higher than one,a scenario encountered in numerous and diverse applications. Its uniqueness andapproximation have thus been thoroughly studied. Nevertheless, the challengingproblem of estimating the BTD model structure, namely the number of block termsand their individual ranks, has only recently started to attract significantattention. In this paper, a novel method of BTD model selection and computationis proposed, based on the idea of imposing column sparsity \emph{jointly} onthe factors and in a \emph{hierarchical} manner and estimating the ranks as thenumbers of factor columns of non-negligible magnitude. Following a blocksuccessive upper bound minimization (BSUM) approach for the proposedoptimization problem is shown to result in an alternating hierarchicaliteratively reweighted least squares (HIRLS) algorithm, which is fastconverging and enjoys high computational efficiency, as it relies in itsiterations on small-sized sub-problems with closed-form solutions. Simulationresults for both synthetic examples and a hyper-spectral image de-noisingapplication are reported, which demonstrate the superiority of the proposedscheme over the state-of-the-art in terms of success rate in rank estimation aswell as computation time and rate of convergence.","Athanasios A. Rontogiannis, Eleftherios Kofidis, Paris V. Giampouras",,,11,
"Accelerating Nonnegative Matrix Factorization Algorithms using
  Extrapolation","  In this paper, we propose a general framework to accelerate significantly thealgorithms for nonnegative matrix factorization (NMF). This framework isinspired from the extrapolation scheme used to accelerate gradient methods inconvex optimization and from the method of parallel tangents. However, the useof extrapolation in the context of the two-block exact coordinate descentalgorithms tackling the non-convex NMF problems is novel. We illustrate theperformance of this approach on two state-of-the-art NMF algorithms, namely,accelerated hierarchical alternating least squares (A-HALS) and alternatingnonnegative least squares (ANLS), using synthetic, image and document datasets.",Andersen Man Shun Ang and Nicolas Gillis,,,11,
"Data driven approximation of parametrized PDEs by Reduced Basis and
  Neural Networks","  We are interested in the approximation of partial differential equations witha data-driven approach based on the reduced basis method and machine learning.We suppose that the phenomenon of interest can be modeled by a parametrizedpartial differential equation, but that the value of the physical parameters isunknown or difficult to be directly measured. Our method allows to estimatefields of interest, for instance temperature of a sample of material orvelocity of a fluid, given data at a handful of points in the domain. Wepropose to accomplish this task with a neural network embedding a reduced basissolver as exotic activation function in the last layer. The reduced basissolver accounts for the underlying physical phenomenonon and it is constructedfrom snapshots obtained from randomly selected values of the physicalparameters during an expensive offline phase. The same full order solutions arethen employed for the training of the neural network. As a matter of fact, thechosen architecture resembles an asymmetric autoencoder in which the decoder isthe reduced basis solver and as such it does not contain trainable parameters.The resulting latent space of our autoencoder includes parameter-dependentquantities feeding the reduced basis solver, which -- depending on theconsidered partial differential equation -- are the values of the physicalparameters themselves or the affine decomposition coefficients of thedifferential operators.","Niccol\`o Dal Santo, Simone Deparis and Luca Pegolotti",,,11,
Approximate Newton Methods,"  Many machine learning models involve solving optimization problems. Thus, itis important to deal with a large-scale optimization problem in big dataapplications. Recently, subsampled Newton methods have emerged to attract muchattention due to their efficiency at each iteration, rectified a weakness inthe ordinary Newton method of suffering a high cost in each iteration whilecommanding a high convergence rate. Other efficient stochastic second ordermethods are also proposed. However, the convergence properties of these methodsare still not well understood. There are also several important gaps betweenthe current convergence theory and the performance in real applications. Inthis paper, we aim to fill these gaps. We propose a unifying framework toanalyze both local and global convergence properties of second order methods.Based on this framework, we present our theoretical results which match theperformance in real applications well.","Haishan Ye, Luo Luo, Zhihua Zhang",,,11,
Novel Modifications of Parallel Jacobi Algorithms,"  We describe two main classes of one-sided trigonometric and hyperbolicJacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitianmatrices. These types of algorithms exhibit significant advantages over manyother eigenvalue algorithms. If the matrices permit, both types of algorithmscompute the eigenvalues and eigenvectors with high relative accuracy.  We present novel parallelization techniques for both trigonometric andhyperbolic classes of algorithms, as well as some new ideas on how pivoting ineach cycle of the algorithm can improve the speed of the parallel one-sidedalgorithms. These parallelization approaches are applicable to bothdistributed-memory and shared-memory machines.  The numerical testing performed indicates that the hyperbolic algorithms maybe superior to the trigonometric ones, although, in theory, the latter seemmore natural.","Sanja Singer, Sasa Singer, Vedran Novakovic, Aleksandar Uscumlic and
  Vedran Dunjko",,,11,
"Approximation and Uncertainty Quantification of Systems with Arbitrary
  Parameter Distributions using Weighted Leja Interpolation","  Approximation and uncertainty quantification methods based on Lagrangeinterpolation are typically abandoned in cases where the probabilitydistributions of one or more {system} parameters are not normal, uniform, orclosely related {distributions}, due to the computational issues that arisewhen one wishes to define interpolation nodes for general distributions. Thispaper examines the use of the recently introduced weighted Leja nodes for thatpurpose. Weighted Leja interpolation rules are presented, along with adimension-adaptive sparse interpolation algorithm, to be employed in the caseof high-dimensional input uncertainty. The performance and reliability of thesuggested approach is verified by four numerical experiments, where therespective models feature extreme value and truncated normal parameterdistributions. Furthermore, the suggested approach is compared with awell-established polynomial chaos method and found to be either comparable orsuperior in terms of approximation and statistics estimation accuracy.",Dimitrios Loukrezis and Herbert De Gersem,,,11,
$\Psi$ec: A Local Spectral Exterior Calculus,"  We introduce $\Psi \mathrm{ec}$, a discretization of Cartan's exteriorcalculus of differential forms using wavelets. Our construction consists ofdifferential $r$-form wavelets with flexible directional localization thatprovide tight frames for the spaces $\Omega^r(\mathbb{R}^n)$ of forms in$\mathbb{R}^2$ and $\mathbb{R}^3$. By construction, the wavelets satisfy the deRahm co-chain complex, the Hodge decomposition, and that the $k$-dimensionalintegral of an $r$-form is an $(r-k)$-form. They also verify Stokes' theoremfor differential forms, with the most efficient finite dimensionalapproximation attained using directionally localized, curvelet- orridgelet-like forms. The construction of $\Psi \mathrm{ec}$ builds on thegeometric simplicity of the exterior calculus in the Fourier domain. Weestablish this structure by extending existing results on the Fourier transformof differential forms to a frequency description of the exterior calculus,including, for example, a Plancherel theorem for forms and a description of thesymbols of all important operators.",Christian Lessig,,,11,
"Varying-order NURBS discretization: An accurate and efficient method for
  isogeometric analysis of large deformation contact problems","  In this paper, a novel varying order NURBS discretization method is proposedto enhance the performance of isogeometric analysis within the framework ofcomputational contact mechanics. The method makes use of higher-order NURBS forcontact integral evaluations. Lower-orders NURBS capable of modelling complexgeometries exactly are utilized for the bulk discretization. This unexploredidea provides the possibility to refine the geometry through controllable orderelevation strategy for isogeometric analysis. To achieve this, a higher-orderNURBS layer is used as the contact boundary layer of the bodies. The NURBSlayer is constructed using the surface refinement strategies such that it isaccompanied by a large number of additional degrees of freedom and matches withthe bulk parametrization.  The validity of the presented isogeometric mortar contact formulation withvarying-order NURBS discretization is first examined through the contact patchtest. The capabilities and benefits of the proposed methodology are thendemonstrated in detail using two-dimensional frictionless and frictionalcontact problems, considering both small and large deformations. It is shownthat using the proposed method, accurate solutions can be achieved even with acoarse mesh. It is also shown that the current method requires a considerablylower computational cost compared to standard NURBS discretization whileretaining robustness and stability. The simplicity of the method lends itselfto be conveniently embedded in an existing isogeometric contact code after onlya few minor modifications.",Vishal Agrawal and Sachin S. Gautam,,,11,
Three-Level Parallel J-Jacobi Algorithms for Hermitian Matrices,"  The paper describes several efficient parallel implementations of theone-sided hyperbolic Jacobi-type algorithm for computing eigenvalues andeigenvectors of Hermitian matrices. By appropriate blocking of the algorithmsan almost ideal load balancing between all available processors/cores isobtained. A similar blocking technique can be used to exploit local cachememory of each processor to further speed up the process. Due to diversity ofmodern computer architectures, each of the algorithms described here may be themethod of choice for a particular hardware and a given matrix size. Allproposed block algorithms compute the eigenvalues with relative accuracysimilar to the original non-blocked Jacobi algorithm.","Sanja Singer, Sasa Singer, Vedran Novakovic, Davor Davidovic, Kresimir
  Bokulic and Aleksandar Uscumlic",,,11,
"Finite Volume Simulation Framework for Die Casting with Uncertainty
  Quantification","  The present paper describes the development of a novel and comprehensivecomputational framework to simulate solidification problems in materialsprocessing, specifically casting processes. Heat transfer, solidification andfluid flow due to natural convection are modeled. Empirical relations are usedto estimate the microstructure parameters and mechanical properties. Thefractional step algorithm is modified to deal with the numerical aspects ofsolidification by suitably altering the coefficients in the discretizedequation to simulate selectively only in the liquid and mushy zones. Thisbrings significant computational speed up as the simulation proceeds. Complexdomains are represented by unstructured hexahedral elements. The algebraicmultigrid method, blended with a Krylov subspace solver is used to accelerateconvergence. State of the art uncertainty quantification technique is includedin the framework to incorporate the effects of stochastic variations in theinput parameters. Rigorous validation is presented using published experimentalresults of a solidification problem.","Shantanu Shahane, Narayana Aluru, Placid Ferreira, Shiv G Kapoor,
  Surya Pratap Vanka",,,11,
"Memory footprint reduction for the FFT-based volume integral equation
  method via tensor decompositions","  We present a method of memory footprint reduction for FFT-based,electromagnetic (EM) volume integral equation (VIE) formulations. The arisingGreen's function tensors have low multilinear rank, which allows Tuckerdecomposition to be employed for their compression, thereby greatly reducingthe required memory storage for numerical simulations. Consequently, thecompressed components are able to fit inside a graphical processing unit (GPU)on which highly parallelized computations can vastly accelerate the iterativesolution of the arising linear system. In addition, the element-wise productsthroughout the iterative solver's process require additional flops, thus, weprovide a variety of novel and efficient methods that maintain the linearcomplexity of the classic element-wise product with an additionalmultiplicative small constant. We demonstrate the utility of our approach viaits application to VIE simulations for the Magnetic Resonance Imaging (MRI) ofa human head. For these simulations we report an order of magnitudeacceleration over standard techniques.","Ilias I. Giannakopoulos (1), Mikhail S. Litsarev (1), Athanasios G.
  Polimeridis (2) ((1) Skoltech Center for Computational Data-Intensive Science
  and Engineering, Skolkovo Institute of Science and Technology, Moscow,
  Russia, (2) Q Bio, CA, USA)",,,11,
"Multi-dimensional imaging data recovery via minimizing the partial sum
  of tubal nuclear norm","  In this paper, we investigate tensor recovery problems within the tensorsingular value decomposition (t-SVD) framework. We propose the partial sum ofthe tubal nuclear norm (PSTNN) of a tensor. The PSTNN is a surrogate of thetensor tubal multi-rank. We build two PSTNN-based minimization models for twotypical tensor recovery problems, i.e., the tensor completion and the tensorprincipal component analysis. We give two algorithms based on the alternatingdirection method of multipliers (ADMM) to solve proposed PSTNN-based tensorrecovery models. Experimental results on the synthetic data and real-world datareveal the superior of the proposed PSTNN.","Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, and Liang-Jian Deng",,,11,
"Data-driven quasi-interpolant spline surfaces for point cloud
  approximation","  In this paper we investigate a local surface approximation, the WeightedQuasi Interpolant Spline Approximation (wQISA), specifically designed for largeand noisy point clouds. We briefly describe the properties of the wQISArepresentation and introduce a novel data-driven implementation, which combinesprediction capability and complexity efficiency. We provide an extendedcomparative analysis with other continuous approximations on real data,including different types of surfaces and levels of noise, such as 3D models,terrain data and digital environmental data.",Andrea Raffo and Silvia Biasotti,,,11,
"Interpolation of scattered data in $\mathbb{R}^3$ using minimum
  $L_p$-norm networks, $1<p<\infty$",We consider the extremal problem of interpolation of scattered data in$\mathbb{R}^3$ by smooth curve networks with minimal $L_p$-norm of the secondderivative for $1<p<\infty$. The problem for $p=2$ was set and solved byNielson (1983). Andersson et al. (1995) gave a new proof of Nielson's result byusing a different approach. Partial results for the problem for $1<p<\infty$were announced without proof in (Vlachkova (1992)). Here we present a completecharacterization of the solution for $1<p<\infty$. Numerical experiments arevisualized and presented to illustrate and support our results.,Krassimira Vlachkova,,,11,
Predict-and-recompute conjugate gradient variants,"  The standard implementation of the conjugate gradient algorithm suffers fromcommunication bottlenecks on parallel architectures, due primarily to the twoglobal reductions required every iteration. In this paper, we study conjugategradient variants which decrease the runtime per iteration by overlappingglobal synchronizations, and in the case of pipelined variants, matrix-vectorproducts. Through the use of a predict-and-recompute scheme, wherebyrecursively-updated quantities are first used as a predictor for their truevalues and then recomputed exactly at a later point in the iteration, thesevariants are observed to have convergence behavior nearly as good as thestandard conjugate gradient implementation on a variety of test problems. Weprovide a rounding error analysis which provides insight into this observation.It is also verified experimentally that the variants studied do indeed reducethe runtime per iteration in practice and that they scale similarly topreviously-studied communication-hiding variants. Finally, because thesevariants achieve good convergence without the use of any additional inputparameters, they have the potential to be used in place of the standardconjugate gradient implementation in a range of applications.",Tyler Chen and Erin C. Carson,,,11,
"Foundation of Computer (Algebra) ANALYSIS Systems: Semantics, Logic,
  Programming, Verification","  We propose a semantics of operating on real numbers that is sound,Turing-complete, and practical. It modifies the intuitive but super-recursiveBlum-Shub-Smale model (formalizing Computer ALGEBRA Systems), to coincide inpower with the realistic but inconvenient Type-2 Turing machine underlyingComputable Analysis: reconciling both as foundation to a Computer ANALYSISSystem.  Several examples illustrate the elegance of rigorous numerical coding in thisframework, formalized as a simple imperative programming language ERC withdenotational semantics for REALIZING a real function $f$: arguments $x$ aregiven as exact real numbers, while values $y=f(x)$ suffice to be returnedapproximately up to absolute error $2^p$ with respect to an additionally giveninteger parameter $p\to-\infty$. Real comparison (necessarily) becomes partial,possibly 'returning' the lazy Kleenean value UNDEF (subtly different from$\bot$ for classically undefined expressions like 1/0). This asserts closureunder composition, and in fact 'Turing-completeness over the reals': All andonly functions computable in the sense of Computable Analysis can be realizedin ERC. Programs thus operate on a many-sorted structure involving real numbersand integers, connected via the 'error' embedding $Z\ni p\mapsto 2^p\in R$,whose first-order theory is proven decidable and model-complete. This logicserves for formally specifying and formally verifying correctness of ERCprograms. We finally expand ERC and its Turing-completeness from real functionsto functionALs.","Sewon Park, Franz Brau{\ss}e, Pieter Collins, SunYoung Kim, Michal
  Kone\v{c}n\'y, Gyesik Lee, Norbert M\""uller, Eike Neumann, Norbert Preining,
  Martin Ziegler",,,11,
"Escaping the Local Minima via Simulated Annealing: Optimization of
  Approximately Convex Functions","  We consider the problem of optimizing an approximately convex function over abounded convex set in $\mathbb{R}^n$ using only function evaluations. Theproblem is reduced to sampling from an \emph{approximately} log-concavedistribution using the Hit-and-Run method, which is shown to have the same$\mathcal{O}^*$ complexity as sampling from log-concave distributions. Inaddition to extend the analysis for log-concave distributions to approximatelog-concave distributions, the implementation of the 1-dimensional sampler ofthe Hit-and-Run walk requires new methods and analysis. The algorithm then isbased on simulated annealing which does not relies on first order conditionswhich makes it essentially immune to local minima.  We then apply the method to different motivating problems. In the context ofzeroth order stochastic convex optimization, the proposed method produces an$\epsilon$-minimizer after $\mathcal{O}^*(n^{7.5}\epsilon^{-2})$ noisy functionevaluations by inducing a $\mathcal{O}(\epsilon/n)$-approximately log concavedistribution. We also consider in detail the case when the ""amount ofnon-convexity"" decays towards the optimum of the function. Other applicationsof the method discussed in this work include private computation of empiricalrisk minimizers, two-stage stochastic programming, and approximate dynamicprogramming for online learning.","Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander
  Rakhlin",,,11,
Randomized CP Tensor Decomposition,"  The CANDECOMP/PARAFAC (CP) tensor decomposition is a populardimensionality-reduction method for multiway data. Dimensionality reduction isoften sought after since many high-dimensional tensors have low intrinsic rankrelative to the dimension of the ambient measurement space. However, theemergence of `big data' poses significant computational challenges forcomputing this fundamental tensor decomposition. By leveraging modernrandomized algorithms, we demonstrate that coherent structures can be learnedfrom a smaller representation of the tensor in a fraction of the time. Thus,this simple but powerful algorithm enables one to compute the approximate CPdecomposition even for massive tensors. The approximation error can thereby becontrolled via oversampling and the computation of power iterations. Inaddition to theoretical results, several empirical results demonstrate theperformance of the proposed algorithm.","N. Benjamin Erichson and Krithika Manohar and Steven L. Brunton and J.
  Nathan Kutz",,,11,
"From Checking to Inference: Actual Causality Computations as
  Optimization Problems","  Actual causality is increasingly well understood. Recent formal approaches,proposed by Halpern and Pearl, have made this concept mature enough to beamenable to automated reasoning. Actual causality is especially vital forbuilding accountable, explainable systems. Among other reasons, causalityreasoning is computationally hard due to the requirements of counterfactualityand the minimality of causes. Previous approaches presented either inefficientor restricted, and domain-specific, solutions to the problem of automatingcausality reasoning. In this paper, we present a novel approach to formulatedifferent notions of causal reasoning, over binary acyclic models, asoptimization problems, based on quantifiable notions within counterfactualcomputations. We contribute and compare two compact, non-trivial, and soundinteger linear programming (ILP) and Maximum Satisfiability (MaxSAT) encodingsto check causality. Given a candidate cause, both approaches identify what aminimal cause is. Also, we present an ILP encoding to infer causality withoutrequiring a candidate cause. We show that both notions are efficientlyautomated. Using models with more than $8000$ variables, checking is computedin a matter of seconds, with MaxSAT outperforming ILP in many cases. Incontrast, inference is computed in a matter of minutes.",Amjad Ibrahim and Alexander Pretschner,,,11,
"Minimizing Energy Use of Mixed-Fleet Public Transit for Fixed-Route
  Service","  Public transit can have significantly lower environmental impact thanpersonal vehicles; however, it still uses a substantial amount of energy,causing air pollution and greenhouse gas emission. While electric vehicles(EVs) can reduce energy use, most public transit agencies have to employ themin combination with conventional, internal-combustion engine vehicles due tothe high upfront costs of EVs. To make the best use of such a mixed fleet ofvehicles, transit agencies need to optimize route assignments and chargingschedules, which presents a challenging problem for large public transitnetworks. We introduce a novel problem formulation to minimize fuel andelectricity use by assigning vehicles to transit trips and scheduling them forcharging while serving an existing fixed-route transit schedule. We present aninteger program for optimal discrete-time scheduling, and we proposepolynomial-time heuristic algorithms and a genetic algorithm for findingsolutions for larger networks. We evaluate our algorithms on the transitservice of a mid-size U.S. city using operational data collected from publictransit vehicles. Our results show that the proposed algorithms are scalableand achieve near-minimum energy use.","Amutheezan Sivagnanam, Afiya Ayman, Michael Wilbur, Philip Pugliese,
  Abhishek Dubey, Aron Laszka",,,11,
"Human Robot Collaborative Assembly Planning: An Answer Set Programming
  Approach","  For planning an assembly of a product from a given set of parts, robotsnecessitate certain cognitive skills: high-level planning is needed to decidethe order of actuation actions, while geometric reasoning is needed to checkthe feasibility of these actions. For collaborative assembly tasks with humans,robots require further cognitive capabilities, such as commonsense reasoning,sensing, and communication skills, not only to cope with the uncertainty causedby incomplete knowledge about the humans' behaviors but also to ensure safercollaborations. We propose a novel method for collaborative assembly planningunder uncertainty, that utilizes hybrid conditional planning extended withcommonsense reasoning and a rich set of communication actions for collaborativetasks. Our method is based on answer set programming. We show the applicabilityof our approach in a real-world assembly domain, where a bi-manual Baxter robotcollaborates with a human teammate to assemble furniture. This manuscript isunder consideration for acceptance in TPLP.","Momina Rizwan, Volkan Patoglu, Esra Erdem",,,11,
Info Intervention,"  Causal diagrams based on do intervention are useful tools to formalize,process and understand causal relationship among variables. However, the dointervention has controversial interpretation of causal questions fornon-manipulable variables, and it also lacks the power to check the conditionsrelated to counterfactual variables. This paper introduces a new infointervention to tackle these two problems, and provides causal diagrams forcommunication and theoretical focus based on this info intervention. Our infointervention intervenes the input/output information of causal mechanisms,while the do intervention intervenes the causal mechanisms. Consequently, thecausality is viewed as information transfer in the info intervention framework.As an extension, the generalized info intervention is also proposed and studiedin this paper.",Gong Heyang and Zhu Ke,,,11,
"POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with
  Non-Asymptotic Analysis","  Monte-Carlo planning, as exemplified by Monte-Carlo Tree Search (MCTS), hasdemonstrated remarkable performance in applications with finite spaces. In thispaper, we consider Monte-Carlo planning in an environment with continuousstate-action spaces, a much less understood problem with important applicationsin control and robotics. We introduce POLY-HOOT, an algorithm that augmentsMCTS with a continuous armed bandit strategy named Hierarchical OptimisticOptimization (HOO) (Bubeck et al., 2011). Specifically, we enhance HOO by usingan appropriate polynomial, rather than logarithmic, bonus term in the upperconfidence bounds. Such a polynomial bonus is motivated by its empiricalsuccesses in AlphaGo Zero (Silver et al., 2017b), as well as its significantrole in achieving theoretical guarantees of finite space MCTS (Shah et al.,2019). We investigate, for the first time, the regret of the enhanced HOOalgorithm in non-stationary bandit problems. Using this result as a buildingblock, we establish non-asymptotic convergence guarantees for POLY-HOOT: thevalue estimate converges to an arbitrarily small neighborhood of the optimalvalue function at a polynomial rate. We further provide experimental resultsthat corroborate our theoretical findings.","Weichao Mao, Kaiqing Zhang, Qiaomin Xie, Tamer Ba\c{s}ar",,,11,
"Strategy for Boosting Pair Comparison and Improving Quality Assessment
  Accuracy","  The development of rigorous quality assessment model relies on the collectionof reliable subjective data, where the perceived quality of visual multimediais rated by the human observers. Different subjective assessment protocols canbe used according to the objectives, which determine the discriminability andaccuracy of the subjective data.  Single stimulus methodology, e.g., the Absolute Category Rating (ACR) hasbeen widely adopted due to its simplicity and efficiency. However, PairComparison (PC) is of significant advantage over ACR in terms ofdiscriminability. In addition, PC avoids the influence of observers' biasregarding their understanding of the quality scale. Nevertheless, full paircomparison is much more time-consuming. In this study, we therefore 1) employ ageneric model to bridge the pair comparison data and ACR data, where thevariance term could be recovered and the obtained information is more complete;2) propose a fusion strategy to boost pair comparisons by utilizing the ACRresults as initialization information; 3) develop a novel active batch samplingstrategy based on Minimum Spanning Tree (MST) for PC. In such a way, theproposed methodology could achieve the same accuracy of pair comparison butwith the compelxity as low as ACR. Extensive experimental results demonstratethe efficiency and accuracy of the proposed approach, which outperforms thestate of the art approaches.","Suiyi Ling, Jing Li, Anne Flore Perrin, Zhi Li, Luk\'a\v{s} Krasula,
  Patrick Le Callet",,,11,
"Connecting actuarial judgment to probabilistic learning techniques with
  graph theory",Graphical models have been widely used in applications ranging from medicalexpert systems to natural language processing. Their popularity partly arisessince they are intuitive representations of complex inter-dependencies amongvariables with efficient algorithms for performing computationally intensiveinference in high-dimensional models. It is argued that the formalism is veryuseful for applications in the modelling of non-life insurance claims data. Itis also shown that actuarial models in current practice can be expressedgraphically to exploit the advantages of the approach. More general models areproposed within the framework to demonstrate the potential use of graphicalmodels for probabilistic learning with telematics and other dynamic actuarialdata. The discussion also demonstrates throughout that the intuitive nature ofthe models allows the inclusion of qualitative knowledge or actuarial judgmentin analyses.,Roland R. Ramsahai,,,11,
T-Norms Driven Loss Functions for Machine Learning,"  Neural-symbolic approaches have recently gained popularity to inject priorknowledge into a learner without requiring it to induce this knowledge fromdata. These approaches can potentially learn competitive solutions with asignificant reduction of the amount of supervised data. A large class ofneural-symbolic approaches is based on First-Order Logic to represent priorknowledge, relaxed to a differentiable form using fuzzy logic. This paper showsthat the loss function expressing these neural-symbolic learning tasks can beunambiguously determined given the selection of a t-norm generator. Whenrestricted to supervised learning, the presented theoretical apparatus providesa clean justification to the popular cross-entropy loss, which has been shownto provide faster convergence and to reduce the vanishing gradient problem invery deep structures. However, the proposed learning formulation extends theadvantages of the cross-entropy loss to the general knowledge that can berepresented by a neural-symbolic method. Therefore, the methodology allows thedevelopment of a novel class of loss functions, which are shown in theexperimental results to lead to faster convergence rates than the approachespreviously proposed in the literature.","Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco
  Maggini and Marco Gori",,,11,
"An Autonomous Free Airspace En-route Controller using Deep Reinforcement
  Learning Techniques","  Air traffic control is becoming a more and more complex task due to theincreasing number of aircraft. Current air traffic control methods are notsuitable for managing this increased traffic. Autonomous air traffic control isdeemed a promising alternative. In this paper an air traffic control model ispresented that guides an arbitrary number of aircraft across athree-dimensional, unstructured airspace while avoiding conflicts andcollisions. This is done utilizing the power of graph based deep learningapproaches. These approaches offer significant advantages over currentapproaches to this task, such as invariance to the input ordering of aircraftand the ability to easily cope with a varying number of aircraft. Resultsacquired using these approaches show that the air traffic control modelperforms well on realistic traffic densities; it is capable of managing theairspace by avoiding 100% of potential collisions and preventing 89.8% ofpotential conflicts.","Joris Mollinga, Herke van Hoof",,,11,
Towards Contrastive Explanations for Comparing the Ethics of Plans,"  The development of robotics and AI agents has enabled their wider usage inhuman surroundings. AI agents are more trusted to make increasingly importantdecisions with potentially critical outcomes. It is essential to consider theethical consequences of the decisions made by these systems. In this paper, wepresent how contrastive explanations can be used for comparing the ethics ofplans. We build upon an existing ethical framework to allow users to makesuggestions to plans and receive contrastive explanations.","Benjamin Krarup, Senka Krivic, Felix Lindner, Derek Long",,,11,
"Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map
  Them to Actions","  We transform reinforcement learning (RL) into a form of supervised learning(SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL).Standard RL predicts rewards, while UDRL instead uses rewards as task-defininginputs, together with representations of time horizons and other computablefunctions of historic and desired future data. UDRL learns to interpret theseinput observations as commands, mapping them to actions (or actionprobabilities) through SL on past (possibly accidental) experience. UDRLgeneralizes to achieve high rewards or other goals, through input commands suchas: get lots of reward within at most so much time! A separate paper [63] onfirst experiments with UDRL shows that even a pilot version of UDRL canoutperform traditional baseline algorithms on certain challenging RL problems.We also also conceptually simplify an approach [60] for teaching a robot toimitate humans. First videotape humans imitating the robot's current behaviors,then let the robot learn through SL to map the videos (as input commands) tothese behaviors, then let it generalize and imitate videos of humans executingpreviously unknown behavior. This Imitate-Imitator concept may actually explainwhy biological evolution has resulted in parents who imitate the babbling oftheir babies.",Juergen Schmidhuber,,,11,
"The Efficiency of Human Cognition Reflects Planned Information
  Processing","  Planning is useful. It lets people take actions that have desirable long-termconsequences. But, planning is hard. It requires thinking about consequences,which consumes limited computational and cognitive resources. Thus, peopleshould plan their actions, but they should also be smart about how they deployresources used for planning their actions. Put another way, people should also""plan their plans"". Here, we formulate this aspect of planning as ameta-reasoning problem and formalize it in terms of a recursive Bellmanobjective that incorporates both task rewards and information-theoreticplanning costs. Our account makes quantitative predictions about how peopleshould plan and meta-plan as a function of the overall structure of a task,which we test in two experiments with human participants. We find that people'sreaction times reflect a planned use of information processing, consistent withour account. This formulation of planning to plan provides new insight into thefunction of hierarchical planning, state abstraction, and cognitive control inboth humans and machines.","Mark K. Ho, David Abel, Jonathan D. Cohen, Michael L. Littman, Thomas
  L. Griffiths",,,11,
"A Comparative Study on Parameter Estimation in Software Reliability
  Modeling using Swarm Intelligence","  This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.","Najla Akram AL-Saati, Marrwa Abd-AlKareem Alabajee",,,11,
"Managing caching strategies for stream reasoning with reinforcement
  learning","  Efficient decision-making over continuously changing data is essential formany application domains such as cyber-physical systems, industrydigitalization, etc. Modern stream reasoning frameworks allow one to model andsolve various real-world problems using incremental and continuous evaluationof programs as new data arrives in the stream. Applied techniques use, e.g.,Datalog-like materialization or truth maintenance algorithms to avoid costlyre-computations, thus ensuring low latency and high throughput of a streamreasoner. However, the expressiveness of existing approaches is quite limitedand, e.g., they cannot be used to encode problems with constraints, which oftenappear in practice. In this paper, we suggest a novel approach that uses theConflict-Driven Constraint Learning (CDCL) to efficiently update legacysolutions by using intelligent management of learned constraints. Inparticular, we study the applicability of reinforcement learning tocontinuously assess the utility of learned constraints computed in previousinvocations of the solving algorithm for the current one. Evaluations conductedon real-world reconfiguration problems show that providing a CDCL algorithmwith relevant learned constraints from previous iterations results insignificant performance improvements of the algorithm in stream reasoningscenarios.  Under consideration for acceptance in TPLP.","Carmine Dodaro, Thomas Eiter, Paul Ogris, Konstantin Schekotihin",,,11,
MAP Inference for Probabilistic Logic Programming,"  In Probabilistic Logic Programming (PLP) the most commonly studied inferencetask is to compute the marginal probability of a query given a program. In thispaper, we consider two other important tasks in the PLP setting: theMaximum-A-Posteriori (MAP) inference task, which determines the most likelyvalues for a subset of the random variables given evidence on other variables,and the Most Probable Explanation (MPE) task, the instance of MAP where thequery variables are the complement of the evidence variables. We present anovel algorithm, included in the PITA reasoner, which tackles these tasks byrepresenting each problem as a Binary Decision Diagram and applying a dynamicprogramming procedure on it. We compare our algorithm with the version ofProbLog that admits annotated disjunctions and can perform MAP and MPEinference. Experiments on several synthetic datasets show that PITA outperformsProbLog in many cases.","Elena Bellodi, Marco Alberti, Fabrizio Riguzzi, Riccardo Zese",,,11,
Landscape of Machine Implemented Ethics,"  This paper surveys the state-of-the-art in machine ethics, that is,considerations of how to implement ethical behaviour in robots, unmannedautonomous vehicles, or software systems. The emphasis is on covering thebreadth of ethical theories being considered by implementors, as well as theimplementation techniques being used. There is no consensus on which ethicaltheory is best suited for any particular domain, nor is there any agreement onwhich technique is best placed to implement a particular theory. Anotherunresolved problem in these implementations of ethical theories is how toobjectively validate the implementations. The paper discusses the dilemmasbeing used as validating 'whetstones' and whether any alternative validationmechanism exists. Finally, it speculates that an intermediate step of creatingdomain-specific ethics might be a possible stepping stone towards creatingmachines that exhibit ethical behaviour.",Vivek Nallur,,,11,
"A machine-learning software-systems approach to capture social,
  regulatory, governance, and climate problems","  This paper will discuss the role of an artificially-intelligent computersystem as critique-based, implicit-organizational, and an inherently necessarydevice, deployed in synchrony with parallel governmental policy, as a genuinemeans of capturing nation-population complexity in quantitative form, publiccontentment in societal-cooperative economic groups, regulatory proposition,and governance-effectiveness domains. It will discuss a solution involving awell-known algorithm and proffer an improved mechanism forknowledge-representation, thereby increasing range of utility, scope ofinfluence (in terms of differentiating class sectors) and operationalefficiency. It will finish with a discussion of these and other historicalimplications.",Christopher A. Tucker,,,11,
On the role of planning in model-based deep reinforcement learning,"  Model-based planning is often thought to be necessary for deep, carefulreasoning and generalization in artificial agents. While recent successes ofmodel-based reinforcement learning (MBRL) with deep function approximation havestrengthened this hypothesis, the resulting diversity of model-based methodshas also made it difficult to track which components drive success and why. Inthis paper, we seek to disentangle the contributions of recent methods byfocusing on three questions: (1) How does planning benefit MBRL agents? (2)Within planning, what choices drive performance? (3) To what extent doesplanning improve generalization? To answer these questions, we study theperformance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRLalgorithm, under a number of interventions and ablations and across a widerange of environments including control tasks, Atari, and 9x9 Go. Our resultssuggest the following: (1) The primary benefit of planning is in driving policylearning. (2) Using shallow trees with simple Monte-Carlo rollouts is asperformant as more complex methods, except in the most difficult reasoningtasks. (3) Planning alone is insufficient to drive strong generalization. Theseresults indicate where and how to utilize planning in reinforcement learningsettings, and highlight a number of open questions for future MBRL research.","Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez,
  Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar
  Veli\v{c}kovi\'c, Th\'eophane Weber",,,11,
The computerization of archaeology: survey on AI techniques,"  This paper analyses the application of artificial intelligence techniques tovarious areas of archaeology and more specifically: a) The use of softwaretools as a creative stimulus for the organization of exhibitions; the use ofhumanoid robots and holographic displays as guides that interact and involvemuseum visitors; b) The analysis of methods for the classification of fragmentsfound in archaeological excavations and for the reconstruction of ceramics,with the recomposition of the parts of text missing from historical documentsand epigraphs; c) The cataloguing and study of human remains to understand thesocial and historical context of belonging with the demonstration of theeffectiveness of the AI techniques used; d) The detection of particularlydifficult terrestrial archaeological sites with the analysis of thearchitectures of the Artificial Neural Networks most suitable for solving theproblems presented by the site; the design of a study for the exploration ofmarine archaeological sites, located at depths that cannot be reached by man,through the construction of a freely explorable 3D version.",Lorenzo Mantovan and Loris Nanni,,,11,
Norm Identification through Plan Recognition,"  Societal rules, as exemplified by norms, aim to provide a degree ofbehavioural stability to multi-agent societies. Norms regulate a society usingthe deontic concepts of permissions, obligations and prohibitions to specifywhat can, must and must not occur in a society. Many implementations ofnormative systems assume various combinations of the following assumptions:that the set of norms is static and defined at design time; that agents joininga society are instantly informed of the complete set of norms; that the set ofagents within a society does not change; and that all agents are aware of theexisting norms. When any one of these assumptions is dropped, agents need amechanism to identify the set of norms currently present within a society, orrisk unwittingly violating the norms. In this paper, we develop a normidentification mechanism that uses a combination of parsing-based planrecognition and Hierarchical Task Network (HTN) planning mechanisms, whichoperates by analysing the actions performed by other agents. While our basicmechanism cannot learn in situations where norm violations take place, wedescribe an extension which is able to operate in the presence of violations.",Nir Oren and Felipe Meneguzzi,,,11,
Benchmarking Blocking Algorithms for Web Entities,"  An increasing number of entities are described by interlinked data ratherthan documents on the Web. Entity Resolution (ER) aims to identify descriptionsof the same real-world entity within one or across knowledge bases in the Webof data. To reduce the required number of pairwise comparisons amongdescriptions, ER methods typically perform a pre-processing step, called\emph{blocking}, which places similar entity descriptions into blocks and thusonly compare descriptions within the same block. We experimentally evaluateseveral blocking methods proposed for the Web of data using real datasets,whose characteristics significantly impact their effectiveness and efficiency.The proposed experimental evaluation framework allows us to better understandthe characteristics of the missed matching entity descriptions and contrastthem with ground truth obtained from different kinds of relatedness links.","Vasilis Efthymiou, Kostas Stefanidis, Vassilis Christophides",,,11,
A metric Suite for Systematic Quality Assessment of Linked Open Data,"  Abstract- The vision of the Linked Open Data (LOD) initiative is to provide adistributed model for publishing and meaningfully interlinking open data. Therealization of this goal depends strongly on the quality of the data that ispublished as a part of the LOD. This paper focuses on the systematic qualityassessment of datasets prior to publication on the LOD cloud. To this end, weidentify important quality deficiencies that need to be avoided and/or resolvedprior to the publication of a dataset. We then propose a set of metrics tomeasure these quality deficiencies in a dataset. This way, we enable theassessment and identification of undesirable quality characteristics of adataset through our proposed metrics. This will help publishers to filter outlow-quality data based on the quality assessment results, which in turn enablesdata consumers to make better and more informed decisions when using the opendatasets.","Behshid Behkamal, Moshen Kahani, Ebrahim Bagheri, Majid Sazvar",,,11,
Constant-Delay Enumeration for Nondeterministic Document Spanners,"  We consider the information extraction framework known as document spanners,and study the problem of efficiently computing the results of the extractionfrom an input document, where the extraction task is described as a sequentialvariable-set automaton (VA). We pose this problem in the setting of enumerationalgorithms, where we can first run a preprocessing phase and must then producethe results with a small delay between any two consecutive results. Our goal isto have an algorithm which is tractable in combined complexity, i.e., in thesizes of the input document and the VA; while ensuring the best possible datacomplexity bounds in the input document size, i.e., constant delay in thedocument size. Several recent works at PODS'18 proposed such algorithms butwith linear delay in the document size or with an exponential dependency insize of the (generally nondeterministic) input VA. In particular, Florenzano etal. suggest that our desired runtime guarantees cannot be met for generalsequential VAs. We refute this and show that, given a nondeterministicsequential VA and an input document, we can enumerate the mappings of the VA onthe document with the following bounds: the preprocessing is linear in thedocument size and polynomial in the size of the VA, and the delay isindependent of the document and polynomial in the size of the VA. The resultingalgorithm thus achieves tractability in combined complexity and the bestpossible data complexity bounds. Moreover, it is rather easy to describe, inparticular for the restricted case of so-called extended VAs. Finally, weevaluate our algorithm empirically using a prototype implementation.","Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth",,,11,
"SoK: Chasing Accuracy and Privacy, and Catching Both in Differentially
  Private Histogram Publication","  Histograms and synthetic data are of key importance in data analysis.However, researchers have shown that even aggregated data such as histograms,containing no obvious sensitive attributes, can result in privacy leakage. Toenable data analysis, a strong notion of privacy is required to avoid riskingunintended privacy violations.  Such a strong notion of privacy is differential privacy, a statistical notionof privacy that makes privacy leakage quantifiable. The caveat regardingdifferential privacy is that while it has strong guarantees for privacy,privacy comes at a cost of accuracy. Despite this trade off being a central andimportant issue in the adoption of differential privacy, there exists a gap inthe literature regarding providing an understanding of the trade off and how toaddress it appropriately.  Through a systematic literature review (SLR), we investigate thestate-of-the-art within accuracy improving differentially private algorithmsfor histogram and synthetic data publishing. Our contribution is two-fold: 1)we identify trends and connections in the contributions to the field ofdifferential privacy for histograms and synthetic data and 2) we provide anunderstanding of the privacy/accuracy trade off challenge by crystallizingdifferent dimensions to accuracy improvement. Accordingly, we position andvisualize the ideas in relation to each other and external work, anddeconstruct each algorithm to examine the building blocks separately with theaim of pinpointing which dimension of accuracy improvement eachtechnique/approach is targeting. Hence, this systematization of knowledge (SoK)provides an understanding of in which dimensions and how accuracy improvementcan be pursued without sacrificing privacy.",Boel Nelson and Jenni Reuben,,,11,
Generative Datalog with Continuous Distributions,"  Arguing for the need to combine declarative and probabilistic programming,B\'ar\'any et al. (TODS 2017) recently introduced a probabilistic extension ofDatalog as a ""purely declarative probabilistic programming language."" Werevisit this language and propose a more principled approach towards definingits semantics. It is based on standard notions from probability theory known asstochastic kernels and Markov processes. This allows us to extend the semanticsto continuous probability distributions, thereby settling an open problem posedby B\'ar\'any et al. We show that our semantics is fairly robust, allowing bothparallel execution and arbitrary chase orders when evaluating a program. Wecast our semantics in the framework of infinite probabilistic databases (Groheand Lindner, ICDT 2020), and we show that the semantics remains meaningful evenwhen the input of a probabilistic Datalog program is an arbitrary probabilisticdatabase.","Martin Grohe, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Peter
  Lindner",,,11,
"RECOME: a New Density-Based Clustering Algorithm Using Relative KNN
  Kernel Density","  Discovering clusters from a dataset with different shapes, densities, andscales is a known challenging problem in data clustering. In this paper, wepropose the RElative COre MErge (RECOME) clustering algorithm. The core ofRECOME is a novel density measure, i.e., Relative $K$ nearest Neighbor KernelDensity (RNKD). RECOME identifies core objects with unit RNKD, and {partitions}non-core objects into atom clusters by successively following higher-densityneighbor relations toward core objects. Core objects and their correspondingatom clusters are then merged through $\alpha$-reachable paths on a KNN graph.We discover that the number of clusters computed by RECOME is a step functionof the $\alpha$ parameter with jump discontinuity on a small collection ofvalues. A fast jump discontinuity discovery (FJDD) method is proposed based ongraph theory. RECOME is evaluated on both synthetic datasets and real datasets.Experimental results indicate that RECOME is able to discover clusters withdifferent shapes, densities, and scales. It outperforms six baseline methods onboth synthetic datasets and real datasets. Moreover, FJDD is shown to beeffective to extract the jump discontinuity set of parameter $\alpha$ for alltested datasets, which can ease the task of data exploration and parametertuning.","Yangli-ao Geng, Qingyong Li, Rong Zheng, Fuzhen Zhuang, Ruisi He,
  Naixue Xiong",,,11,
"The Impact of Discretization Method on the Detection of Six Types of
  Anomalies in Datasets","  Anomaly detection is the process of identifying cases, or groups of cases,that are in some way unusual and do not fit the general patterns present in thedataset. Numerous algorithms use discretization of numerical data in theirdetection processes. This study investigates the effect of the discretizationmethod on the unsupervised detection of each of the six anomaly typesacknowledged in a recent typology of data anomalies. To this end, experimentsare conducted with various datasets and SECODA, a general-purpose algorithm forunsupervised non-parametric anomaly detection in datasets with numerical andcategorical attributes. This algorithm employs discretization of continuousattributes, exponentially increasing weights and discretization cut points, anda pruning heuristic to detect anomalies with an optimal number of iterations.The results demonstrate that standard SECODA can detect all six types, but thatdifferent discretization methods favor the discovery of certain anomaly types.The main findings also hold for other detection techniques usingdiscretization.",Ralph Foorthuis,,,11,
Revisiting compact RDF stores based on k2-trees,"  We present a new compact representation to efficiently store and query largeRDF datasets in main memory. Our proposal, called BMatrix, is based on thek2-tree, a data structure devised to represent binary matrices in a compressedway, and aims at improving the results of previous state-of-the-artalternatives, especially in datasets with a relatively large number ofpredicates. We introduce our technique, together with some improvements on thebasic k2-tree that can be applied to our solution in order to boostcompression. Experimental results in the flagship RDF dataset DBPedia show thatour proposal achieves better compression than existing alternatives, whileyielding competitive query times, particularly in the most frequent triplepatterns and in queries with unbound predicate, in which we outperform existingsolutions.","Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Antonio
  Fari\~na",,,11,
Uncovering Hidden Semantics of Set Information in Knowledge Bases,"  Knowledge Bases (KBs) contain a wealth of structured information aboutentities and predicates. This paper focuses on set-valued predicates, i.e., therelationship between an entity and a set of entities. In KBs, this informationis often represented in two formats: (i) via counting predicates such asnumberOfChildren and staffSize, that store aggregated integers, and (ii) viaenumerating predicates such as parentOf and worksFor, that store individual setmemberships. Both formats are typically complementary: unlike enumeratingpredicates, counting predicates do not give away individuals, but are morelikely informative towards the true set size, thus this coexistence couldenable interesting applications in question answering and KB curation.  In this paper we aim at uncovering this hidden knowledge. We proceed in twosteps. (i) We identify set-valued predicates from a given KB predicates viastatistical and embedding-based features. (ii) We link counting predicates andenumerating predicates by a combination of co-occurrence, correlation andtextual relatedness metrics. We analyze the prevalence of count information infour prominent knowledge bases, and show that our linking method achieves up to0.55 F1 score in set predicate identification versus 0.40 F1 score of a randomselection, and normalized discounted gains of up to 0.84 at position 1 and 0.75at position 3 in relevant predicate alignments. Our predicate alignments areshowcased in a demonstration system available athttps://counqer.mpi-inf.mpg.de/spo.","Shrestha Ghosh, Simon Razniewski, Gerhard Weikum",,,11,
"Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing
  (Technical Report)","  Errors are prevalent in time series data, such as GPS trajectories or sensorreadings. Existing methods focus more on anomaly detection but not on repairingthe detected anomalies. By simply filtering out the dirty data via anomalydetection, applications could still be unreliable over the incomplete timeseries. Instead of simply discarding anomalies, we propose to (iteratively)repair them in time series data, by creatively bonding the beauty of temporalnature in anomaly detection with the widely considered minimum change principlein data repairing. Our major contributions include: (1) a novel framework ofiterative minimum repairing (IMR) over time series data, (2) explicit analysison convergence of the proposed iterative minimum repairing, and (3) efficientestimation of parameters in each iteration. Remarkably, with incrementalcomputation, we reduce the complexity of parameter estimation from O(n) toO(1). Experiments on real datasets demonstrate the superiority of our proposalcompared to the state-of-the-art approaches. In particular, we show that (theproposed) repairing indeed improves the time series classification application.","Aoqian Zhang, Shaoxu Song, Jianmin Wang, Philip S. Yu",,,11,
Covering the Relational Join,"  In this paper, we initiate a theoretical study of what we call the joincovering problem. We are given a natural join query instance $Q$ on $n$attributes and $m$ relations $(R_i)_{i \in [m]}$. Let $J_{Q} = \ \Join_{i=1}^mR_i$ denote the join output of $Q$. In addition to $Q$, we are given aparameter $\Delta: 1\le \Delta\le n$ and our goal is to compute the smallestsubset $\mathcal{T}_{Q, \Delta} \subseteq J_{Q}$ such that every tuple in$J_{Q}$ is within Hamming distance $\Delta - 1$ from some tuple in$\mathcal{T}_{Q, \Delta}$. The join covering problem captures both computingthe natural join from database theory and constructing a covering code withcovering radius $\Delta - 1$ from coding theory, as special cases.  We consider the combinatorial version of the join covering problem, where ourgoal is to determine the worst-case $|\mathcal{T}_{Q, \Delta}|$ in terms of thestructure of $Q$ and value of $\Delta$. One obvious approach to upper bound$|\mathcal{T}_{Q, \Delta}|$ is to exploit a distance property (of Hammingdistance) from coding theory and combine it with the worst-case bounds onoutput size of natural joins (AGM bound hereon) due to Atserias, Grohe and Marx[SIAM J. of Computing'13]. Somewhat surprisingly, this approach is not tighteven for the case when the input relations have arity at most two. Instead, weshow that using the polymatroid degree-based bound of Abo Khamis, Ngo and Suciu[PODS'17] in place of the AGM bound gives us a tight bound (up to constantfactors) on the $|\mathcal{T}_{Q, \Delta}|$ for the arity two case. We provelower bounds for $|\mathcal{T}_{Q, \Delta}|$ using well-known classes oferror-correcting codes e.g, Reed-Solomon codes. We can extend our results forthe arity two case to general arity with a polynomial gap between our upper andlower bounds.",Shi Li and Sai Vikneshwar Mani Jayaraman and Atri Rudra,,,11,
A Novel Approach for Generating SPARQL Queries from RDF Graphs,"  This work is done as part of a research master's thesis project. The goal isto generate SPARQL queries based on user-supplied keywords to query RDF graphs.To do this, we first transformed the input ontology into an RDF graph thatreflects the semantics represented in the ontology. Subsequently, we storedthis RDF graph in the Neo4j graphical database to ensure efficient andpersistent management of RDF data. At the time of the interrogation, we studiedthe different possible and desired interpretations of the request originallymade by the user. We have also proposed to carry out a sort of transformationbetween the two query languages SPARQL and Cypher, which is specific to Neo4j.This allows us to implement the architecture of our system over a wide varietyof BD-RDFs providing their query languages, without changing any of the othercomponents of the system. Finally, we tested and evaluated our tool usingdifferent test bases, and it turned out that our tool is comprehensive,effective, and powerful enough.",Emna Jabri,,,11,
Maintaining Triangle Queries under Updates,"  We consider the problem of incrementally maintaining the triangle querieswith arbitrary free variables under single-tuple updates to the inputrelations. We introduce an approach called IVM$^\epsilon$ that exhibits atrade-off between the update time, the space, and the delay for the enumerationof the query result, such that the update time ranges from the square root tolinear in the database size while the delay ranges from constant to lineartime. IVM$^\epsilon$ achieves Pareto worst-case optimality in the update-delayspace conditioned on the Online Matrix-Vector Multiplication conjecture. It isstrongly Pareto optimal for the triangle queries with zero or three freevariables and weakly Pareto optimal for the triangle queries with one or twofree variables.","Ahmet Kara, Milos Nikolic, Hung Q. Ngo, Dan Olteanu, Haozhe Zhang",,,11,
ER model Partitioning: Towards Trustworthy Automated Systems Development,"  In database development, a conceptual model is created, in the form of anEntity-relationship(ER) model, and transformed to a relational database schema(RDS) to create the database. However, some important information representedon the ER model may not be transformed and represented on the RDS. Thissituation causes a loss of information during the transformation process. Witha view to preserving information, in our previous study, we standardized thetransformation process as a one-to-one and onto mapping from the ER model tothe RDS. For this purpose, we modified the ER model and the transformationalgorithm resolving some deficiencies existed in them. Since the mapping wasestablished using a few real-world cases as a basis and for verificationpurposes, a formal-proof is necessary to validate the work. Thus, the ongoingresearch aiming to create a proof will show how a given ER model can bepartitioned into a unique set of segments and use it to represent the ER modelitself. How the findings can be used to complete the proof in the future willalso be explained. Significance of the research on automating databasedevelopment, teaching conceptual modeling, and using formal methods will alsobe discussed.","Dhammika Pieris, M.C Wijegunesekera, N.G.J Dias",,,11,
Discovering Process Models from Uncertain Event Data,"  Modern information systems are able to collect event data in the form ofevent logs. Process mining techniques allow to discover a model from eventdata, to check the conformance of an event log against a reference model, andto perform further process-centric analyses. In this paper, we consideruncertain event logs, where data is recorded together with explicit uncertaintyinformation. We describe a technique to discover a directly-follows graph fromsuch event data which retains information about the uncertainty in the process.We then present experimental results of performing inductive mining over thedirectly-follows graph to obtain models representing the certain and uncertainpart of the process.","Marco Pegoraro, Merih Seran Uysal and Wil M.P. van der Aalst",,,11,
"FLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation","  Query optimizers rely on accurate cardinality estimation (CardEst) to producegood execution plans. The core problem of CardEst is how to model the richjoint distribution of attributes in an accurate and compact manner. Despitedecades of research, existing methods either over simplify the models onlyusing independent factorization which leads to inaccurate estimates and suboptimal query plans, or over-complicate them by lossless conditionalfactorization without any independent assumption which results in slowprobability computation. In this paper, we propose FLAT, a CardEst method thatis simultaneously fast in probability computation, lightweight in model sizeand accurate in estimation quality. The key idea of FLAT is a novelunsupervised graphical model, called FSPN. It utilizes both independent andconditional factorization to adaptively model different levels of attributescorrelations, and thus subsumes all existing CardEst models and dovetails theiradvantages. FLAT supports efficient online probability computation in nearliner time on the underlying FSPN model, and provides effective offline modelconstruction. It can estimate cardinality for both single table queries andmulti-table join queries. Extensive experimental study demonstrates thesuperiority of FLAT over existing CardEst methods on well-known benchmarks:FLAT achieves 1 to 5 orders of magnitude better accuracy, 1 to 3 orders ofmagnitude faster probability computation speed (around 0.2ms) and 1 to 2 ordersof magnitude lower storage cost (only tens of KB).","Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping
  Qian, Jingren Zhou, Bin Cui",,,11,
Effective Discovery of Meaningful Outlier Relationships,"  We propose PODS (Predictable Outliers in Data-trendS), a method that, given acollection of temporal data sets, derives data-driven explanations for outliersby identifying meaningful relationships between them. First, we formalize thenotion of meaningfulness, which so far has been informally framed in terms ofexplainability. Next, since outliers are rare and it is difficult to determinewhether their relationships are meaningful, we develop a new criterion thatdoes so by checking if these relationships could have been predicted fromnon-outliers, i.e., if we could see the outlier relationships coming. Finally,searching for meaningful outlier relationships between every pair of data setsin a large data collection is computationally infeasible. To address that, wepropose an indexing strategy that prunes irrelevant comparisons across datasets, making the approach scalable. We present the results of an experimentalevaluation using real data sets and different baselines, which demonstrates theeffectiveness, robustness, and scalability of our approach.","Aline Bessa, Juliana Freire, Divesh Srivastava, Tamraparni Dasu",,,11,
DySky: Dynamic Skyline Queries on Uncertain Graphs,"  Given a graph, and a set of query vertices (subset of the vertices), thedynamic skyline query problem returns a subset of data vertices (other thanquery vertices) which are not dominated by other data vertices based on certaindistance measure. In this paper, we study the dynamic skyline query problem onuncertain graphs (DySky). The input to this problem is an uncertain graph, asubset of its nodes as query vertices, and the goal here is to return all thedata vertices which are not dominated by others. We employ two distancemeasures in uncertain graphs, namely, \emph{Majority Distance}, and\emph{Expected Distance}. Our approach is broadly divided into three steps:\emph{Pruning}, \emph{Distance Computation}, and \emph{Skyline Vertex SetGeneration}. We implement the proposed methodology with three publiclyavailable datasets and observe that it can find out skyline vertex set withouttaking much time even for million sized graphs if expected distance isconcerned. Particularly, the pruning strategy reduces the computational timesignificantly.",Suman Banerjee and Bithika Pal,,,11,
Data Management for Context-Aware Computing,"  We envisage future context-aware applications will dynamically adapt theirbehaviors to various context data from sources in wide-area networks, such asthe Internet. Facing the changing context and the sheer number of contextsources, a data management system that supports effective source organizationand efficient data lookup becomes crucial to the easy development ofcontext-aware applications. In this paper, we propose the design of a newcontext data management system that is equipped with query processingcapabilities. We encapsulate the context sources into physical spaces belongingto different context spaces and organize them as peers in semantic overlaynetworks. Initial evaluation results of an experimental system prototypedemonstrate the effectiveness of our design","Wenwei Xue, Hungkeng Pung, Wenlong Ng, Tao Gu",,,11,
Data Warehouse and Decision Support on Integrated Crop Big Data,"  In recent years, precision agriculture is becoming very popular. Theintroduction of modern information and communication technologies forcollecting and processing Agricultural data revolutionise the agriculturepractises. This has started a while ago (early 20th century) and it is drivenby the low cost of collecting data about everything; from information on fieldssuch as seed, soil, fertiliser, pest, to weather data, drones and satellitesimages. Specially, the agricultural data mining today is considered as Big Dataapplication in terms of volume, variety, velocity and veracity. Hence it leadsto challenges in processing vast amounts of complex and diverse information toextract useful knowledge for the farmer, agronomist, and other businesses. Itis a key foundation to establishing a crop intelligence platform, which willenable efficient resource management and high quality agronomy decision makingand recommendations. In this paper, we designed and implemented a continentallevel agricultural data warehouse (ADW). ADW is characterised by its (1)flexible schema; (2) data integration from real agricultural multi datasets;(3) data science and business intelligent support; (4) high performance; (5)high storage; (6) security; (7) governance and monitoring; (8) consistency,availability and partition tolerant; (9) cloud compatibility. We also evaluatethe performance of ADW and present some complex queries to extract and returnnecessary knowledge about crop management.","V.M. Ngo, N.A. Le-Khac, and M.T. Kechadi",,,11,
"Security Wrappers for Information-Flow Control in Active Object
  Languages with Futures","  This paper introduces a run-time mechanism for preventing leakage of secureinformation in distributed systems. We consider a general concurrency languagemodel, where concurrent objects interact by asynchronous method calls andfutures. The aim is to prevent leakage of confidential information to low-levelviewers. The approach is based on the notion of a security wrapper, whichencloses an object or a component and controls its interactions with theenvironment. A wrapper is a mechanism added by the run-time system to provideprotection of an insecure component according to some security policies. Thesecurity policies of a wrapper are formalized based on a notion of securitylevels. At run-time, future components will be wrapped upon need, while onlyobjects of unsafe classes will be wrapped, using static checking to limit thenumber of unsafe classes and thereby reducing run-time overhead. We define anoperational semantics and prove that non-interference is satisfied. A serviceprovider may use wrappers to protect its services in an insecure environment,and vice-versa: a system platform may use wrappers to protect itself frominsecure service providers.","Farzane Karami, Olaf Owe, Gerardo Schneider",,,11,
Efficient global register allocation,"  In a compiler, an essential component is the register allocator. Two mainalgorithms have dominated implementations, graph coloring and linear scan,differing in how live values are modeled. Graph coloring uses an edge in an`interference graph' to show that two values cannot reside in the sameregister. Linear scan numbers all values, creates intervals between definitionand uses, and then intervals that do not overlap may be allocated to the sameregister. For both algorithms the liveness models are computed at considerableruntime and memory cost. Furthermore, these algorithms do little to improvecode quality, where the target architecture and register coalescing areimportant concerns.  We describe a new register allocation algorithm with lightweightimplementation characteristics. The algorithm introduces a `future-active' setfor values that will reside in a register later in the allocation. Registersare allocated and freed in the manner of linear scan, although other orderingheuristics could improve code quality or lower runtime cost. An advantageousproperty of the approach is an ability to make these trade-offs. A key resultis the `future-active' set can remove any liveness model for over 90% ofinstructions and 80% of methods. The major contribution is the allocationalgorithm that, for example, solves an inability of the similarly motivatedTreescan register allocator to look ahead of the instruction being allocated -allowing an unconstrained allocation order, and an ability to better handlefixed registers and loop carried values. The approach also is not reliant onproperties of SSA form, similar to the original linear scan work. An analysisis presented in a production compiler for Java code compiled through SSA formto Android dex files.",Ian Rogers,,,11,
"Proving Almost-Sure Termination of Probabilistic Programs via
  Incremental Pruning","  The extension of classical imperative programs with real-valued randomvariables and random branching gives rise to probabilistic programs. Thetermination problem is one of the most fundamental liveness properties for suchprograms. The qualitative (aka almost-sure) termination problem asks whether agiven program terminates with probability 1. Ranking functions provide a soundand complete approach for termination of non-probabilistic programs, and theirextension to probabilistic programs is achieved via ranking supermartingales(RSMs). RSMs have been extended to lexicographic RSMs to handle programs withinvolved control-flow structure, as well as for compositional approach. Thereare two key limitations of the existing RSM-based approaches: First, thelexicographic RSM-based approach requires a strong nonnegativity assumption,which need not always be satisfied. The second key limitation of the existingRSM-based algorithmic approaches is that they rely on pre-computed invariants.The main drawback of relying on pre-computed invariants is theinsufficiency-inefficiency trade-off: weak invariants might be insufficient forRSMs to prove termination, while using strong invariants leads to inefficiencyin computing them. Our contributions are twofold: First, we show how to relaxthe strong nonnegativity condition and still provide soundness guarantee foralmost-sure termination. Second, we present an incremental approach where theprocess of computing lexicographic RSMs proceeds by iterative pruning of partsof the program that were already shown to be terminating, in cooperation with asafety prover. In particular, our technique does not rely on strongpre-computed invariants. We present experimental results to show theapplicability of our approach to examples of probabilistic programs from theliterature.","Krishnendu Chatterjee (1), Ehsan Kafshdar Goharshady (2), Petr
  Novotn\'y (3), Ji\v{r}i Z\'arev\'ucky (3), {\DJ}or{\dj}e \v{Z}ikeli\'c (1)
  ((1) IST Austria, Klosterneuburg, Austria, (2) Ferdowsi University of
  Mashhad, Mashhad, Iran, (3) Masaryk University, Brno, Czech Republic)",,,11,
Employing Simulation to Facilitate the Design of Dynamic Code Generators,"  Dynamic Translation (DT) is a sophisticated technique that allows theimplementation of high-performance emulators and high-level-language virtualmachines. In this technique, the guest code is compiled dynamically at runtime.Consequently, achieving good performance depends on several design decisions,including the shape of the regions of code being translated. Researchers andengineers explore these decisions to bring the best performance possible.However, a real DT engine is a very sophisticated piece of software, andmodifying one is a hard and demanding task. Hence, we propose using simulationto evaluate the impact of design decisions on dynamic translators and presentRAIn, an open-source DT simulator that facilitates the test of DT's designdecisions, such as Region Formation Techniques (RFTs). RAIn outputs severalstatistics that support the analysis of how design decisions may affect thebehavior and the performance of a real DT. We validated RAIn running a set ofexperiments with six well known RFTs (NET, MRET2, LEI, NETPlus, NET-R, andNETPlus-e-r) and showed that it can reproduce well-known results from theliterature without the effort of implementing them on a real and complexdynamic translator engine.","Vanderson Martins do Rosario, Raphael Zinsly, Sandro Rigo, Edson Borin",,,11,
Probabilistic Programming Semantics for Name Generation,"  We make a formal analogy between random sampling and fresh name generation.We show that quasi-Borel spaces, a model for probabilistic programming, cansoundly interpret Stark's $\nu$-calculus, a calculus for name generation.Moreover, we prove that this semantics is fully abstract up to first-ordertypes. This is surprising for an 'off-the-shelf' model, and requires a novelanalysis of probability distributions on function spaces. Our tools are diverseand include descriptive set theory and normal forms for the $\nu$-calculus.","Marcin Sabok, Sam Staton, Dario Stein, Michael Wolman",,,11,
"Why You Cannot (Yet) Write an ""Interval Arithmetic"" Library in Common
  Lisp","  ""Interval Arithmetic"" (IA) appears to be a useful numerical tool to have athand in several applications. Alas, the current IA descriptions and proposedstandards are always formulated in terms of the IEEE-754 standard, and thestatus of IEEE-754 compliance of most Common Lisp implementations is not up topar. A solution would be for Common Lisp implementations to adhere to theLanguage Independent Arithmetic} (LIA) IEC standard, which includes IEEE/754.While the LIA standard provides a set of proposed bindings for Common Lisp, theformat and depth of the specification documents is not readily usable by aCommon Lisp programmer, should an implementation decide to comply with theprovisions. Moreover, much latitude is left to each implementation to providethe LIA ""environmental"" setup. It would be most beneficial if more precisionwere agreed upon by the Common Lisp community about how to provide LIAcompliance in the implementations. In that case, a new set of documentation ormanuals in the style of the HyperSpec could be provided, for the benefit of theCommon Lisp programmer.",Marco Antoniotti,,,11,
Advanced Graph-Based Deep Learning for Probabilistic Type Inference,"  Dynamically typed languages such as JavaScript and Python have emerged as themost popular programming languages in use. Important benefits can accrue fromincluding type annotations in dynamically typed programs. This approach togradual typing is exemplified by the TypeScript programming system which allowsprogrammers to specify partially typed programs, and then uses static analysisto infer the remaining types. However, in general, the effectiveness of statictype inference is limited and depends on the complexity of the program'sstructure and the initial type annotations. As a result, there is a strongmotivation for new approaches that can advance the state of the art instatically predicting types in dynamically typed programs, and that do so withacceptable performance for use in interactive programming environments.Previous work has demonstrated the promise of probabilistic type inferenceusing deep learning. In this paper, we advance past work by introducing a rangeof graph neural network (GNN) models that operate on a novel type flow graph(TFG) representation. The TFG represents an input program's elements as graphnodes connected with syntax edges and data flow edges, and our GNN models aretrained to predict the type labels in the TFG for a given input program. Westudy different design choices for our GNN models for the 100 most common typesin our evaluation dataset, and show that our best two GNN configurations foraccuracy achieve a top-1 accuracy of 87.76% and 86.89% respectively,outperforming the two most closely related deep learning type inferenceapproaches from past work -- DeepTyper with a top-1 accuracy of 84.62% andLambdaNet with a top-1 accuracy of 79.45%. Further, the average inferencethroughputs of those two configurations are 353.8 and 1,303.9 files/second,compared to 186.7 files/second for DeepTyper and 1,050.3 files/second forLambdaNet.","Fangke Ye, Jisheng Zhao, Vivek Sarkar",,,11,
A Diagrammatic Calculus for Algebraic Effects,"  We introduce a new diagrammatic notation for representing the result of(algebraic) effectful computations. Our notation explicitly separates theeffects produced during a computation from the possible values returned, thisway simplifying the extension of definitions and results on pure computationsto an effectful setting. Additionally, we show a number of algebraic andorder-theoretic laws on diagrams, this way laying the foundations for adiagrammatic calculus of algebraic effects. We give a formal foundation forsuch a calculus in terms of Lawvere theories and generic effects.",Ugo Dal Lago and Francesco Gavazzo,,,11,
Internalizing Representation Independence with Univalence,"  In their usual form, representation independence metatheorems provide anexternal guarantee that two implementations of an abstract interface areinterchangeable when they are related by an operation-preservingcorrespondence. If our programming language is dependently-typed, however, wewould like to appeal to such invariance results within the language itself, inorder to obtain correctness theorems for complex implementations bytransferring them from simpler, related implementations. Recent work in proofassistants has shown that Voevodsky's univalence principle allows transferringtheorems between isomorphic types, but many instances of representationindependence in programming involve non-isomorphic representations.  In this paper, we develop techniques for establishing internal relationalrepresentation independence results in dependent type theory, by using higherinductive types to simultaneously quotient two related implementation types bya heterogeneous correspondence between them. The correspondence becomes anisomorphism between the quotiented types, thereby allowing us to obtain anequality of implementations by univalence. We illustrate our techniques byconsidering applications to matrices, queues, and finite multisets. Our resultsare all formalized in Cubical Agda, a recent extension of Agda which supportsunivalence and higher inductive types in a computationally well-behaved way.","Carlo Angiuli, Evan Cavallo, Anders M\""ortberg, Max Zeuner",,,11,
A Term-Rewriting Semantics for Imperative Style Programming,"  Term rewriting systems have a simple syntax and semantics and facilitateproofs of correctness. However, they are not as popular in industry or academiaas imperative languages. We define a term rewriting based abstract programminglanguage with an imperative style and a precise semantics allowing programs tobe translatable into efficient imperative languages, to obtain proofs ofcorrectness together with efficient execution. This language is designed tofacilitate translations into correct programs in imperative languages withassignment statements, iteration, recursion, arrays, pointers, and sideeffects. It can also be used in place of a pseudo-programming language tospecify algorithms.",David Plaisted and Lee Barnett,,,11,
Compilation of Coordinated Choice,"  Recently, we have proposed coordinated choices, which are nondeterministicchoices equipped with names. The main characteristic of coordinated choices isthat they synchronize nondeterministic decision among choices of the same name.  The motivation of the synchronization mechanism is to solve a theoreticalproblem. So, as a practical programming language, we still want to usecoordinated choices like standard ones. In other words, we want to avoidsynchronization. Now, there are two problems: (i) practically, it is a bitcomplicated work to write a program using coordinated choices in whichexecution synchronization never happens; and (ii) theoretically, it is unknownwhether any programs using standard choices can be written by using onlycoordinated ones.  In this paper, we define two simply typed lambda calculi called$\lambda^\parallel$ equipped with standard choices and$\lambda^{\parallel\omega}$ equipped with coordinated choices, and givecompilation rules from the former into the latter. The challenge is to show thecorrectness of the compilation because behavioral correspondence betweenexpressions before and after compiling cannot be defined directly by thecompilation rules. For the challenge, we give an effect system for$\lambda^{\parallel\omega}$ that characterizes expressions in which executionsynchronization never happens. Then, we show that all compiled expressions canbe typed by the effect system. As a result, we can easily show the correctnessbecause the main concern of the correctness is whether synchronization happensor not.",Yuki Nishida and Atsushi Igarashi,,,11,
"Denotational Correctness of Foward-Mode Automatic Differentiation for
  Iteration and Recursion","  We present semantic correctness proofs of forward-mode AutomaticDifferentiation (AD) for languages with sources of partiality such as partialoperations, lazy conditionals on real parameters, iteration, and term and typerecursion. We first define an AD macro on a standard call-by-value languagewith some primitive operations for smooth partial functions and constructs forreal conditionals and iteration, as a unique structure preserving macrodetermined by its action on the primitive operations. We define a semantics forthe language in terms of diffeological spaces, where the key idea is to makeuse of a suitable partiality monad. A semantic logical relations argument,constructed through a subsconing construction over diffeological spaces, yieldsa correctness proof of the defined AD macro. A key insight is that, to reasonabout differentiation at sum types, we work with relations which form sheaves.Next, we extend our language with term and type recursion. To model this in oursemantics, we introduce a new notion of space, suitable for modeling bothrecursion and differentiation, by equipping a diffeological space with acompatible $\omega$cpo-structure. We demonstrate that our whole developmentextends to this setting. By making use of a semantic, rather than syntactic,logical relations argument, we circumvent the usual technicalities of logicalrelations techniques for type recursion.",Matthijs V\'ak\'ar,,,11,
Demand-Driven Incremental Object Queries,"  Object queries are essential in information seeking and decision making invast areas of applications. However, a query may involve complex conditions onobjects and sets, which can be arbitrarily nested and aliased. The objects andsets involved as well as the demand---the given parameter values ofinterest---can change arbitrarily. How to implement object queries efficientlyunder all possible updates, and furthermore to provide complexity guarantees?  This paper describes an automatic method. The method allows powerful queriesto be written completely declaratively. It transforms demand as well as allobjects and sets into relations. Most importantly, it defines invariants fornot only the query results, but also all auxiliary values about the objects andsets involved, including those for propagating demand, and incrementallymaintains all of them. Implementation and experiments with problems from avariety of application areas, including distributed algorithms andprobabilistic queries, confirm the analyzed complexities, trade-offs, andsignificant improvements over prior work.","Yanhong A. Liu, Jon Brandvein, Scott D. Stoller, Bo Lin",,,11,
A Categorical Programming Language,"  A theory of data types based on category theory is presented. We organizedata types under a new categorical notion of F,G-dialgebras which is anextension of the notion of adjunctions as well as that of T-algebras.T-algebras are also used in domain theory, but while domain theory needs someprimitive data types, like products, to start with, we do not need any.Products, coproducts and exponentiations (i.e. function spaces) are definedexactly like in category theory using adjunctions. F,G-dialgebras also enableus to define the natural number object, the object for finite lists and otherfamiliar data types in programming. Furthermore, their symmetry allows us tohave the dual of the natural number object and the object for infinite lists(or lazy lists). We also introduce a programming language in a categoricalstyle using F,G-dialgebras as its data type declaration mechanism. We definethe meaning of the language operationally and prove that any program terminatesusing Tait's computability method.",Tatsuya Hagino,,,11,
"Proceedings Eighth and Ninth International Workshop on Trends in
  Functional Programming in Education","  This volume contains five papers, accepted after post-reviewing, based onpresentations submitted to TFPIE 2019 and TFPIE 2020 that took places inVancouver, Canada and Krakow, Poland respectively. TFPIE stands for Trends inFunctional Programming in Education, where authors present research andexperiences in teaching concepts of functional programming at any level.",Jurriaan Hage (Utrecht University),,,11,
Foundations of a live data exploration environment,"  Context: A growing amount of code is written to explore and analyze data,often by data analysts who do not have a traditional background in programming,for example by journalists.  Inquiry: The way such data anlysts write code is different from the waysoftware engineers do so. They use few abstractions, work interactively andrely heavily on external libraries. We aim to capture this way of working andbuild a programming environment that makes data exploration easier by providinginstant live feedback.  Approach: We combine theoretical and applied approach. We present the\emph{data exploration calculus}, a formal language that captures the structureof code written by data analysts. We then implement a data explorationenvironment that evaluates code instantly during editing and shows previews ofthe results.  Knowledge: We formally describe an algorithm for providing instant previewsfor the data exploration calculus that allows the user to modify code in anunrestricted way in a text editor. Supporting interactive editing is tricky asany edit can change the structure of code and fully recomputing the outputwould be too expensive.  Grounding: We prove that our algorithm is correct and that it reuses previousresults when updating previews after a number of common code edit operations.We also illustrate the practicality of our approach with an empiricalevaluation and a case study.  Importance: As data analysis becomes an ever more important use ofprogramming, research on programming languages and tools needs to consider newkinds of programming workflows appropriate for those domains and conceive newkinds of tools that can support them. The present paper is one step in thisimportant direction.","Tomas Petricek (University of Kent, United Kingdom)",,,11,
"On the Termination Problem for Probabilistic Higher-Order Recursive
  Programs","  In the last two decades, there has been much progress on model checking ofboth probabilistic systems and higher-order programs. In spite of the emergenceof higher-order probabilistic programming languages, not much has been done tocombine those two approaches. In this paper, we initiate a study on theprobabilistic higher-order model checking problem, by giving some firsttheoretical and experimental results. As a first step towards our goal, weintroduce PHORS, a probabilistic extension of higher-order recursion schemes(HORS), as a model of probabilistic higher-order programs. The model of PHORSmay alternatively be viewed as a higher-order extension of recursive Markovchains. We then investigate the probabilistic termination problem -- or,equivalently, the probabilistic reachability problem. We prove that almost suretermination of order-2 PHORS is undecidable. We also provide a fixpointcharacterization of the termination probability of PHORS, and develop a sound(but possibly incomplete) procedure for approximately computing the terminationprobability. We have implemented the procedure for order-2 PHORSs, andconfirmed that the procedure works well through preliminary experiments thatare reported at the end of the article.","Naoki Kobayashi, Ugo Dal Lago, Charles Grellois",,,11,
WhylSon: Proving your Michelson Smart Contracts in Why3,"  This paper introduces WhylSon, a deductive verification tool for smartcontracts written in Michelson, which is the low-level language of the Tezosblockchain. WhylSon accepts a formally specified Michelson contract andautomatically translates it to an equivalent program written in WhyML, theprogramming and specification language of the Why3 framework. Smart contractinstructions are mapped into a corresponding WhyML shallow-embedding of thetheir axiomatic semantics, which we also developed in the context of this work.One major advantage of this approach is that it allows an out-of-the-boxintegration with the Why3 framework, namely its VCGen and the backend supportfor several automated theorem provers. We also discuss the use of WhylSon toautomatically prove the correctness of diverse annotated smart contracts.","Lu\'is Pedro Arrojado da Horta and Jo\~ao Santos Reis and M\'ario
  Pereira and Sim\~ao Melo de Sousa",,,11,
Automated Termination Analysis of Polynomial Probabilistic Programs,"  The termination behavior of probabilistic programs depends on the outcomes ofrandom assignments. Almost sure termination (AST) is concerned with thequestion whether a program terminates with probability one on all possibleinputs. Positive almost sure termination (PAST) focuses on termination in afinite expected number of steps. This paper presents a fully automated approachto the termination analysis of probabilistic while-programs whose guards andexpressions are polynomial expressions. As proving (positive) AST isundecidable in general, existing proof rules typically provide sufficientconditions. These conditions mostly involve constraints on supermartingales. Weconsider four proof rules from the literature and extend these withgeneralizations of existing proof rules for (P)AST. We automate the resultingset of proof rules by effectively computing asymptotic bounds on polynomialsover the program variables. These bounds are used to decide the sufficientconditions - including the constraints on supermartingales - of a proof rule.Our software tool Amber can thus check AST, PAST, as well as their negationsfor a large class of polynomial probabilistic programs, while carrying out thetermination reasoning fully with polynomial witnesses. Experimental resultsshow the merits of our generalized proof rules and demonstrate that Amber canhandle probabilistic programs that are out of reach for other state-of-the-arttools.","Marcel Moosbrugger, Ezio Bartocci, Joost-Pieter Katoen, Laura Kov\'acs",,,11,
Sums of Uncertainty: Refinements Go Gradual,"  A long-standing shortcoming of statically typed functional languages is thattype checking does not rule out pattern-matching failures (run-time matchexceptions). Refinement types distinguish different values of datatypes; if aprogram annotated with refinements passes type checking, pattern-matchingfailures become impossible. Unfortunately, refinement is a monolithic propertyof a type, exacerbating the difficulty of adding refinement types to nontrivialprograms.  Gradual typing has explored how to incrementally move between static typingand dynamic typing. We develop a type system of gradual sums that combinesrefinement with imprecision. Then, we develop a bidirectional version of thetype system, which rules out excessive imprecision, and give a type-directedtranslation to a target language with explicit casts. We prove that the staticsublanguage cannot have match failures, that a well-typed program remainswell-typed if its type annotations are made less precise, and that makingannotations less precise causes target programs to fail later. Several of theseresults correspond to criteria for gradual typing given by Siek et al. (2015).",Khurram A. Jafery and Jana Dunfield,,,11,
"Believing in BERT: Using expressive communication to enhance trust and
  counteract operational error in physical Human-Robot Interaction","  Strategies are necessary to mitigate the impact of unexpected behavior incollaborative robotics, and research to develop solutions is lacking. Our aimhere was to explore the benefits of an affective interaction, as opposed to amore efficient, less error prone but non-communicative one. The experiment tookthe form of an omelet-making task, with a wide range of participantsinteracting directly with BERT2, a humanoid robot assistant. Having significantimplications for design, results suggest that efficiency is not the mostimportant aspect of performance for users; a personable, expressive robot wasfound to be preferable over a more efficient one, despite a considerable tradeoff in time taken to perform the task. Our findings also suggest that a robotexhibiting human-like characteristics may make users reluctant to 'hurt itsfeelings'; they may even lie in order to avoid this.","Adriana Hamacher, Nadia Bianchi-Berthouze, Anthony G. Pipe and Kerstin
  Eder",,,11,
"Demonstrations of Cooperative Perception: Safety and Robustness in
  Connected and Automated Vehicle Operations","  Cooperative perception, or collective perception (CP) is an emerging andpromising technology for intelligent transportation systems (ITS). It enablesan ITS station (ITS-S) to share its local perception information with others bymeans of vehicle-to-X (V2X) communication, thereby achieving improvedefficiency and safety in road transportation. In this paper, we present ourrecent progress on the development of a connected and automated vehicle (CAV)and intelligent roadside unit (IRSU). We present three different experiments todemonstrate the use of CP service within intelligent infrastructure to improveawareness of vulnerable road users (VRU) and thus safety for CAVs in varioustraffic scenarios. We demonstrate in the experiments that a connected vehicle(CV) can ""see"" a pedestrian around the corners. More importantly, wedemonstrate how CAVs can autonomously and safely interact with walking andrunning pedestrians, relying only on the CP information from the IRSU throughvehicle-to-infrastructure (V2I) communication. This is one of the firstdemonstrations of urban vehicle automation using only CP information. We alsoaddress in the paper the handling of collective perception messages (CPMs)received from the IRSU, and passing them through a pipeline of CP informationcoordinate transformation with uncertainty, multiple road user tracking, andeventually path planning/decision making within the CAV. The experimentalresults were obtained with manually driven CV, fully autonomous CAV, and anIRSU retrofitted with vision and laser sensors and a road user tracking system.","Mao Shan, Karan Narula, Ricky Wong, Stewart Worrall, Malik Khan, Paul
  Alexander and Eduardo Nebot",,,11,
"Augmented Reality on the Large Scene Based on a Markerless Registration
  Framework","  In this paper, a mobile camera positioning method based on forward andinverse kinematics of robot is proposed, which can realize far pointpositioning of imaging position and attitude tracking in large sceneenhancement. Orbit precision motion through the framework overhead cameras andcombining with the ground system of sensor array object such as mobile robotplatform of various sensors, realize the good 3 d image registration, solve anyartifacts that is mobile robot in the large space position initializationproblem, effectively implement the large space no marks augmented reality,human-computer interaction, and information summary. Finally, the feasibilityand effectiveness of the method are verified by experiments.","Zhen Ma, He Xu, Yonghui Zhang, Junlong Chen, Dongbo Zhao, Siqing Chen",,,11,
"Self-Exploration in Complex Unknown Environments using Hybrid Map
  Representation","  A hybrid map representation, which consists of a modified generalized VoronoiDiagram (GVD)-based topological map and a grid-based metric map, is proposed tofacilitate a new frontier-driven exploration strategy. Exploration frontiersare the regions on the boundary between open space and unexplored space. Amobile robot is able to construct its map by adding new space and moving tounvisited frontiers until the entire environment has been explored. Theexisting exploration methods suffer from low exploration efficiency in complexenvironments due to the lack of a systematical way to determine and assignoptimal exploration command. Leveraging on the abstracted information from theGVD map (global) and the detected frontier in the local sliding window, aglobal-local exploration strategy is proposed to handle the exploration task ina hierarchical manner. The new exploration algorithm is able to create amodified tree structure to represent the environment while consolidating globalfrontier information during the self-exploration. The proposed method isverified in simulated environments, and then tested in real-world officeenvironments as well.","Wenchao Gao, Matthew Booker, Jiadong Wang",,,11,
Simulation Framework for Mobile Robots in Planetary-Like Environments,"  In this paper we present a simulation framework for the evaluation of thenavigation and localization metrological performances of a robotic platform.The simulator, based on ROS (Robot Operating System) Gazebo, is targeted to aplanetary-like research vehicle which allows to test various perception andnavigation approaches for specific environment conditions. The possibility ofsimulating arbitrary sensor setups comprising cameras, LiDARs (Light Detectionand Ranging) and IMUs makes Gazebo an excellent resource for rapid prototyping.In this work we evaluate a variety of open-source visual and LiDAR SLAM(Simultaneous Localization and Mapping) algorithms in a simulated Martianenvironment. Datasets are captured by driving the rover and recording sensorsoutputs as well as the ground truth for a precise performance evaluation.","Riccardo Giubilato and Andrea Masili and Sebastiano Chiodini and Marco
  Pertile and Stefano Debei",,,11,
Dynamic Cloth Manipulation with Deep Reinforcement Learning,"  In this paper we present a Deep Reinforcement Learning approach to solvedynamic cloth manipulation tasks. Differing from the case of rigid objects, westress that the followed trajectory (including speed and acceleration) has adecisive influence on the final state of cloth, which can greatly vary even ifthe positions reached by the grasped points are the same. We explore how goalpositions for non-grasped points can be attained through learning adequatetrajectories for the grasped points. Our approach uses few demonstrations toimprove control policy learning, and a sparse reward approach to avoidengineering complex reward functions. Since perception of textiles ischallenging, we also study different state representations to assess theminimum observation space required for learning to succeed. Finally, we comparedifferent combinations of control policy encodings, demonstrations, and sparsereward learning techniques, and show that our proposed approach can learndynamic cloth manipulation in an efficient way, i.e., using a reducedobservation space, a few demonstrations, and a sparse reward.","Rishabh Jangir, Guillem Alenya, Carme Torras",,,11,
"INS/Odometer Land Navigation by Accurate Measurement Modeling and
  Multiple-Model Adaptive Estimation","  Land vehicle navigation based on inertial navigation system (INS) andodometers is a classical autonomous navigation application and has beenextensively studied over the past several decades. In this work, we seriouslyanalyze the error characteristics of the odometer (OD) pulses and investigatethree types of odometer measurement models in the INS/OD integrated system.Specifically, in the pulse velocity model, a preliminary Kalman filter isdesigned to obtain accurate vehicle velocity from the accumulated pulses; thepulse increment model is accordingly obtained by integrating the pulsevelocity; a new pulse accumulation model is proposed by augmenting thetravelled distance into the system state. The three types of measurements,along with the nonhonolomic constraint (NHC), are implemented in the standardextended Kalman filter. In view of the motion-related pulse errorcharacteristics, the multiple model adaptive estimation (MMAE) approach isexploited to further enhance the performance. Simulations and long-distanceexperiments are conducted to verify the feasibility and effectiveness of theproposed methods. It is shown that the standard pulse velocity measurementachieves the superior performance, whereas the accumulated pulse measurement ismost favorable with the MMAE enhancement.","Wei Ouyang, Yuanxin Wu and Hongyue Chen",,,11,
"Multiagent Rollout and Policy Iteration for POMDP with Application to
  Multi-Robot Repair Problems","  In this paper we consider infinite horizon discounted dynamic programmingproblems with finite state and control spaces, partial state observations, anda multiagent structure. We discuss and compare algorithms that simultaneouslyor sequentially optimize the agents' controls by using multistep lookahead,truncated rollout with a known base policy, and a terminal cost functionapproximation. Our methods specifically address the computational challenges ofpartially observable multiagent problems. In particular: 1) We consider rolloutalgorithms that dramatically reduce required computation while preserving thekey cost improvement property of the standard rollout method. The per-stepcomputational requirements for our methods are on the order of $O(Cm)$ ascompared with $O(C^m)$ for standard rollout, where $C$ is the maximumcardinality of the constraint set for the control component of each agent, and$m$ is the number of agents. 2) We show that our methods can be applied tochallenging problems with a graph structure, including a class of robot repairproblems whereby multiple robots collaboratively inspect and repair a systemunder partial information. 3) We provide a simulation study that compares ourmethods with existing methods, and demonstrate that our methods can handlelarger and more complex partially observable multiagent problems (state spacesize $10^{37}$ and control space size $10^{7}$, respectively). Finally, weincorporate our multiagent rollout algorithms as building blocks in anapproximate policy iteration scheme, where successive rollout policies areapproximated by using neural network classifiers. While this scheme requires astrictly off-line implementation, it works well in our computationalexperiments and produces additional significant performance improvement overthe single online rollout iteration method.","Sushmita Bhattacharya, Siva Kailas, Sahil Badyal, Stephanie Gil,
  Dimitri Bertsekas",,,11,
ParkPredict: Motion and Intent Prediction of Vehicles in Parking Lots,"  We investigate the problem of predicting driver behavior in parking lots, anenvironment which is less structured than typical road networks and featurescomplex, interactive maneuvers in a compact space. Using the CARLA simulator,we develop a parking lot environment and collect a dataset of human parkingmaneuvers. We then study the impact of model complexity and feature informationby comparing a multi-modal Long Short-Term Memory (LSTM) prediction model and aConvolution Neural Network LSTM (CNN-LSTM) to a physics-based Extended KalmanFilter (EKF) baseline. Our results show that 1) intent can be estimated well(roughly 85% top-1 accuracy and nearly 100% top-3 accuracy with the LSTM andCNN-LSTM model); 2) knowledge of the human driver's intended parking spot has amajor impact on predicting parking trajectory; and 3) the semanticrepresentation of the environment improves long term predictions.","Xu Shen, Ivo Batkovic, Vijay Govindarajan, Paolo Falcone, Trevor
  Darrell, and Francesco Borrelli",,,11,
"Learning-based Bias Correction for Ultra-wideband Localization of
  Resource-constrained Mobile Robots","  Accurate indoor localization is a crucial enabling technology for manyrobotics applications, from warehouse management to monitoring tasks.Ultra-wideband (UWB) ranging is a promising solution which is low-cost,lightweight, and computationally inexpensive compared to alternativestate-of-the-art approaches such as simultaneous localization and mapping,making it especially suited for resource-constrained aerial robots. Manycommercially-available ultra-wideband radios, however, provide inaccurate,biased range measurements. In this article, we propose a bias correctionframework compatible with both two-way ranging and time difference of arrivalultra-wideband localization. Our method comprises of two steps: (i) statisticaloutlier rejection and (ii) a learning-based bias correction. This approach isscalable and frugal enough to be deployed on-board a nano-quadcopter'smicrocontroller. Previous research mostly focused on two-way ranging biascorrection and has not been implemented in closed-loop nor usingresource-constrained robots. Experimental results show that, using ourapproach, the localization error is reduced by ~18.5% and 48% (for TWR andTDoA, respectively), and a quadcopter can accurately track trajectories withposition information from UWB only.","Wenda Zhao, Abhishek Goudar, Jacopo Panerati, and Angela P. Schoellig
  (University of Toronto Institute for Aerospace Studies, Vector Institute for
  Artificial Intelligence)",,,11,
"Invariant Transform Experience Replay: Data Augmentation for Deep
  Reinforcement Learning","  Deep Reinforcement Learning (RL) is a promising approach for adaptive robotcontrol, but its current application to robotics is currently hindered by highsample requirements. To alleviate this issue, we propose to exploit thesymmetries present in robotic tasks. Intuitively, symmetries from observedtrajectories define transformations that leave the space of feasible RLtrajectories invariant and can be used to generate new feasible trajectories,which could be used for training. Based on this data augmentation idea, weformulate a general framework, called Invariant Transform Experience Replaythat we present with two techniques: (i) Kaleidoscope Experience Replayexploits reflectional symmetries and (ii) Goal-augmented Experience Replaywhich takes advantage of lax goal definitions. In the Fetch tasks from OpenAIGym, our experimental results show significant increases in learning rates andsuccess rates. Particularly, we attain a 13, 3, and 5 times speedup in thepushing, sliding, and pick-and-place tasks respectively in the multi-goalsetting. Performance gains are also observed in similar tasks with obstaclesand we successfully deployed a trained policy on a real Baxter robot. Our workdemonstrates that invariant transformations on RL trajectories are a promisingmethodology to speed up learning in deep RL.","Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan, Juan
  Rojas, Paul Weng",,,11,
Robust 2D Assembly Sequencing via Geometric Planning with Learned Scores,"  To compute robust 2D assembly plans, we present an approach that combinesgeometric planning with a deep neural network. We train the network using theBox2D physics simulator with added stochastic noise to yield robustnessscores--the success probabilities of planned assembly motions. As running asimulation for every assembly motion is impractical, we train a convolutionalneural network to map assembly operations, given as an image pair of thesubassemblies before and after they are mated, to a robustness score. Theneural network prediction is used within a planner to quickly prune out motionsthat are not robust. We demonstrate this approach on two-handed planarassemblies, where the motions are one-step translations. Results suggest thatthe neural network can learn robustness to plan robust sequences an order ofmagnitude faster than physics simulation.","Tzvika Geft, Aviv Tamar, Ken Goldberg, Dan Halperin",,,11,
Informative Path Planning for Mobile Sensing with Reinforcement Learning,"  Large-scale spatial data such as air quality, thermal conditions and locationsignatures play a vital role in a variety of applications. Collecting such datamanually can be tedious and labour intensive. With the advancement of robotictechnologies, it is feasible to automate such tasks using mobile robots withsensing and navigation capabilities. However, due to limited battery lifetimeand scarcity of charging stations, it is important to plan paths for the robotsthat maximize the utility of data collection, also known as the informativepath planning (IPP) problem. In this paper, we propose a novel IPP algorithmusing reinforcement learning (RL). A constrained exploration and exploitationstrategy is designed to address the unique challenges of IPP, and is shown tohave fast convergence and better optimality than a classical reinforcementlearning approach. Extensive experiments using real-world measurement datademonstrate that the proposed algorithm outperforms state-of-the-art algorithmsin most test cases. Interestingly, unlike existing solutions that have to bere-executed when any input parameter changes, our RL-based solution allows adegree of transferability across different problem instances.","Yongyong Wei, Rong Zheng",,,11,
"Efficient two step optimization for large embedded deformation graph
  based SLAM","  Embedded deformation nodes based formulation has been widely applied indeformable geometry and graphical problems. Though being promising in stereo(or RGBD) sensor based SLAM applications, it remains challenging to keepconstant speed in deformation nodes parameter estimation when model growslarger. In practice, the processing time grows rapidly in accordance with theexpansion of maps. In this paper, we propose an approach to decouple nodes ofdeformation graph in large scale dense deformable SLAM and keep the estimationtime to be constant. We observe that only partial deformable nodes in the graphare connected to visible points. Based on this fact, sparsity of originalHessian matrix is utilized to split parameter estimation in two independentsteps. With this new technique, we achieve faster parameter estimation withamortized computation complexity reduced from O(n^2) to closing O(1). As aresult, the computation cost barely increases as the map keeps growing. Basedon our strategy, computational bottleneck in large scale embedded deformationgraph based applications will be greatly mitigated. The effectiveness isvalidated by experiments, featuring large scale deformation scenarios.","Jingwei Song, Fang Bai, Liang Zhao, Shoudong Huang and Rong Xiong",,,11,
"PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line
  Features","  Leveraging line features to improve localization accuracy of point-basedvisual-inertial SLAM (VINS) is gaining interest as they provide additionalconstraints on scene structure. However, real-time performance whenincorporating line features in VINS has not been addressed. This paper presentsPL-VINS, a real-time optimization-based monocular VINS method with point andline features, developed based on the state-of-the-art point-based VINS-Mono\cite{vins}. We observe that current works use the LSD \cite{lsd} algorithm toextract line features; however, LSD is designed for scene shape representationinstead of the pose estimation problem, which becomes the bottleneck for thereal-time performance due to its high computational cost. In this paper, amodified LSD algorithm is presented by studying a hidden parameter tuning andlength rejection strategy. The modified LSD can run at least three times asfast as LSD. Further, by representing space lines with the Pl\""{u}ckercoordinates, the residual error in line estimation is modeled in terms of thepoint-to-line distance, which is then minimized by iteratively updating theminimum four-parameter orthonormal representation of the Pl\""{u}ckercoordinates. Experiments in a public benchmark dataset show that thelocalization error of our method is 12-16\% less than that of VINS-Mono at thesame pose update frequency. %For the benefit of the community, The source codeof our method is available at: https://github.com/cnqiangfu/PL-VINS.","Qiang Fu, Jialong Wang, Hongshan Yu, Islam Ali, Feng Guo, Yijia He,
  Hong Zhang",,,11,
Learning Human Search Behavior from Egocentric Visual Inputs,"  ""Looking for things"" is a mundane but critical task we repeatedly carry on inour daily life. We introduce a method to develop a human character capable ofsearching for a randomly located target object in a detailed 3D scene using itslocomotion capability and egocentric vision perception represented as RGBDimages. By depriving the privileged 3D information from the human character, itis forced to move and look around simultaneously to account for the restrictedsensing capability, resulting in natural navigation and search behaviors. Ourmethod consists of two components: 1) a search control policy based on anabstract character model, and 2) an online replanning control module forsynthesizing detailed kinematic motion based on the trajectories planned by thesearch policy. We demonstrate that the combined techniques enable the characterto effectively find often occluded household items in indoor environments. Thesame search policy can be applied to different full-body characters without theneed for retraining. We evaluate our method quantitatively by testing it onrandomly generated scenarios. Our work is a first step toward creatingintelligent virtual agents with humanlike behaviors driven by onboard sensors,paving the road toward future robotic applications.","Maks Sorokin, Wenhao Yu, Sehoon Ha, C. Karen Liu",,,11,
"Robust Planning and Control for Dynamic Quadrupedal Locomotion with
  Adaptive Feet","  In this paper, we aim to improve the robustness of dynamic quadrupedallocomotion through three aspects: 1) fast model predictive foothold planning,2) LQR control for robust motion tracking and 3) adaptive feet for terrainadaptation. In our proposed planning and control framework, foothold plans areupdated at 400 Hz considering the current robot state and an LQR controllergenerates optimal feedback gains for motion tracking. The LQR optimal gainmatrix with non-zero off-diagonal elements leverages the coupling of dynamicsto compensate for system underactuation, such as a quadruped robot with passiveankles. The specially designed foot with adaptive sole aims at improving thetraversability of rough terrains with rocks, loose gravel and rubble byenlarging the contact surfaces with ground. Experiments on the quadruped ANYmaldemonstrate the effectiveness of the proposed method for robust dynamiclocomotion given external disturbances and environmental uncertainties.","Guiyang Xin, Songyan Xin, Oguzhan Cebe, Mathew Jose Pollayil, Franco
  Angelini, Manolo Garabini, Sethu Vijayakumar, Michael Mistry",,,11,
"Efficient Multi-Agent Motion Planning in Continuous Workspaces Using
  Medial-Axis-Based Swap Graphs","  We present an algorithm for homogeneous, labeled, and disk-shaped multi-agentmotion planning in continuous workspaces with arbitrarily-shaped obstacles. Ourmethod consists of two steps. First, we convert the continuous free space intoa discrete graph where agents are placed on vertices and move along edges. Onthe graph, a set of swap operations are defined and we ensure that performingthese swap operations will not lead to collisions between agents or withobstacles. Second, we prove that it is possible for agents' locations to bearbitrarily permuted on graph vertices using our swap operations, as long asthese graph vertices are not fully occupied. In other words, a multi-agentmotion planning problem on our graph is always solvable. Finally, we show thatsuch continuous-to-discrete conversion can be performed efficiently with thehelp of a medial axis analysis and can be performed robustly for workspaceswith arbitrarily-shaped obstacles. Moreover, the resulting graph has manyvertices and can accommodate a large number of densely packed agents (up to$69\%$ of the volume of free space), and motion plans can be computed$10\times$ faster using our swap operations compared to state-of-the-artmethods.","Liang He, Zherong Pan, Biao Jia, and Dinesh Manocha",,,11,
"An Information-Theoretic Approach for Path Planning in Agents with
  Computational Constraints","  In this paper, we develop a framework for path-planning on abstractions thatare not provided to the system a-priori but instead emerge as a function of theagent's available computational resources. We show how a path-planning problemin an environment can be systematically approximated by solving a sequence ofeasier to solve problems on abstractions of the original space. The propertiesof the problem are analyzed, and supporting theoretical results presented anddiscussed. A numerical example is presented to show the utility of the approachand to corroborate the theoretical findings. We conclude by providing adiscussion of the results and their interpretation relating to anytimealgorithms and bounded rationality.",Daniel T. Larsson and Dipankar Maity and Panagiotis Tsiotras,,,11,
Dynamic Task Allocation for Robotic Network Cloud Systems,"  Every robotic network cloud system can be seen as a graph with nodes ashardware with independent computational processing powers and edges as datatransmissions between nodes. When assigning a task to a node we may changeseveral values corresponding to the node such as distance to other nodes, thetime to complete all of its tasks, the energy level of the node, energyconsumed while performing all of its tasks, geometrical position, communicationwith other nodes, and so on. These values can be seen as fingerprints for thecurrent state of the node which can be evaluated as a subspace of a hyperspace.We proposed a theoretical model describing how assigning tasks to a node willchange the subspace of the hyperspace, and from that, we show how to obtain theoptimal task allocation. We described the communication instability betweennodes and the capability of nodes as subspaces of a hyperspace. We translatetask scheduling to nodes as finding the maximum volume of the hyperspace.",Saeid Alirezazadeh and Lu\'is A. Alexandre,,,11,
MaskGAN: Towards Diverse and Interactive Facial Image Manipulation,"  Facial image manipulation has achieved great progress in recent years.However, previous methods either operate on a predefined set of face attributesor leave users little freedom to interactively manipulate images. To overcomethese drawbacks, we propose a novel framework termed MaskGAN, enabling diverseand interactive face manipulation. Our key insight is that semantic masks serveas a suitable intermediate representation for flexible face manipulation withfidelity preservation. MaskGAN has two main components: 1) Dense MappingNetwork (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically,DMN learns style mapping between a free-form user modified mask and a targetimage, enabling diverse generation results. EBST models the user editingbehavior on the source mask, making the overall framework more robust tovarious manipulated inputs. Specifically, it introduces dual-editingconsistency as the auxiliary supervision signal. To facilitate extensivestudies, we construct a large-scale high-resolution face dataset withfine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensivelyevaluated on two challenging tasks: attribute transfer and style copy,demonstrating superior performance over other state-of-the-art methods. Thecode, models, and dataset are available athttps://github.com/switchablenorms/CelebAMask-HQ.","Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo",,,11,
Generative Hierarchical Features from Synthesizing Images,"  Generative Adversarial Networks (GANs) have recently advanced image synthesisby learning the underlying distribution of observed data in an unsupervisedmanner. However, how the features trained from solving the task of imagesynthesis are applicable to visual tasks remains seldom explored. In this work,we show that learning to synthesize images is able to bring remarkablehierarchical visual features that are generalizable across a wide range ofvisual tasks. Specifically, we consider the pre-trained StyleGAN generator as alearned loss function and utilize its layer-wise disentangled representation totrain a novel hierarchical encoder. As a result, the visual feature produced byour encoder, termed as Generative Hierarchical Feature (GH-Feat), hascompelling discriminative and disentangled properties, facilitating a range ofboth discriminative and generative tasks. Extensive experiments on faceverification, landmark detection, layout prediction, transfer learning, stylemixing, and image editing show the appealing performance of the GH-Feat learnedfrom synthesizing images, outperforming existing unsupervised feature learningmethods.","Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou",,,11,
Facial Attribute Capsules for Noise Face Super Resolution,"  Existing face super-resolution (SR) methods mainly assume the input image tobe noise-free. Their performance degrades drastically when applied toreal-world scenarios where the input image is always contaminated by noise. Inthis paper, we propose a Facial Attribute Capsules Network (FACN) to deal withthe problem of high-scale super-resolution of noisy face image. Capsule is agroup of neurons whose activity vector models different properties of the sameentity. Inspired by the concept of capsule, we propose an integratedrepresentation model of facial information, which named Facial AttributeCapsule (FAC). In the SR processing, we first generated a group of FACs fromthe input LR face, and then reconstructed the HR face from this group of FACs.Aiming to effectively improve the robustness of FAC to noise, we generate FACin semantic, probabilistic and facial attributes manners by means of integratedlearning strategy. Each FAC can be divided into two sub-capsules: SemanticCapsule (SC) and Probabilistic Capsule (PC). Them describe an explicit facialattribute in detail from two aspects of semantic representation and probabilitydistribution. The group of FACs model an image as a combination of facialattribute information in the semantic space and probabilistic space by anattribute-disentangling way. The diverse FACs could better combine the faceprior information to generate the face images with fine-grained semanticattributes. Extensive benchmark experiments show that our method achievessuperior hallucination results and outperforms state-of-the-art for very lowresolution (LR) noise face image super resolution.","Jingwei Xin, Nannan Wang, Xinrui Jiang, Jie Li, Xinbo Gao, Zhifeng Li",,,11,
"Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion
  from 3D Geometry","  In autonomous driving, monocular sequences contain lots of information.Monocular depth estimation, camera ego-motion estimation and optical flowestimation in consecutive frames are high-profile concerns recently. Byanalyzing tasks above, pixels in the middle frame are modeled into three parts:the rigid region, the non-rigid region, and the occluded region. In jointunsupervised training of depth and pose, we can segment the occluded regionexplicitly. The occlusion information is used in unsupervised learning ofdepth, pose and optical flow, as the image reconstructed by depth-pose andoptical flow will be invalid in occluded regions. A less-than-mean mask isdesigned to further exclude the mismatched pixels interfered with by motion orillumination change in the training of depth and pose networks. This method isalso used to exclude some trivial mismatched pixels in the training of theoptical flow network. Maximum normalization is proposed for depth smoothnessterm to restrain depth degradation in textureless regions. In the occludedregion, as depth and camera motion can provide more reliable motion estimation,they can be used to instruct unsupervised learning of optical flow. Ourexperiments in KITTI dataset demonstrate that the model based on three regions,full and explicit segmentation of the occlusion region, the rigid region, andthe non-rigid region with corresponding unsupervised losses can improveperformance on three tasks significantly. The source code is available at:https://github.com/guangmingw/DOPlearning.","Guangming Wang, Chi Zhang, Hesheng Wang, Jingchuan Wang, Yong Wang,
  Xinlei Wang",,,11,
"Geometric Approaches to Increase the Expressivity of Deep Neural
  Networks for MR Reconstruction","  Recently, deep learning approaches have been extensively investigated toreconstruct images from accelerated magnetic resonance image (MRI) acquisition.Although these approaches provide significant performance gain compared tocompressed sensing MRI (CS-MRI), it is not clear how to choose a suitablenetwork architecture to balance the trade-off between network complexity andperformance. Recently, it was shown that an encoder-decoder convolutionalneural network (CNN) can be interpreted as a piecewise linear basis-likerepresentation, whose specific representation is determined by the ReLUactivation patterns for a given input image. Thus, the expressivity or therepresentation power is determined by the number of piecewise linear regions.As an extension of this geometric understanding, this paper proposes asystematic geometric approach using bootstrapping and subnetwork aggregationusing an attention module to increase the expressivity of the underlying neuralnetwork. Our method can be implemented in both k-space domain and image domainthat can be trained in an end-to-end manner. Experimental results show that theproposed schemes significantly improve reconstruction performance withnegligible complexity increases.","Eunju Cha, Gyutaek Oh, Jong Chul Ye",,,11,
"Performance Improvement of Path Planning algorithms with Deep Learning
  Encoder Model","  Currently, path planning algorithms are used in many daily tasks. They arerelevant to find the best route in traffic and make autonomous robots able tonavigate. The use of path planning presents some issues in large and dynamicenvironments. Large environments make these algorithms spend much time findingthe shortest path. On the other hand, dynamic environments request a newexecution of the algorithm each time a change occurs in the environment, and itincreases the execution time. The dimensionality reduction appears as asolution to this problem, which in this context means removing useless pathspresent in those environments. Most of the algorithms that reducedimensionality are limited to the linear correlation of the input data.Recently, a Convolutional Neural Network (CNN) Encoder was used to overcomethis situation since it can use both linear and non-linear information to datareduction. This paper analyzes in-depth the performance to eliminate theuseless paths using this CNN Encoder model. To measure the mentioned modelefficiency, we combined it with different path planning algorithms. Next, thefinal algorithms (combined and not combined) are checked in a database that iscomposed of five scenarios. Each scenario contains fixed and dynamic obstacles.Their proposed model, the CNN Encoder, associated to other existent pathplanning algorithms in the literature, was able to obtain a time decrease tofind the shortest path in comparison to all path planning algorithms analyzed.the average decreased time was 54.43 %.","Janderson Ferreira (1), Agostinho A. F. J\'unior (1), Yves M. Galv\~ao
  (1), Pablo Barros (2), Sergio Murilo Maciel Fernandes (1), Bruno J. T.
  Fernandes (1) ((1) Universidade de Pernambuco - Escola Polit\'ecnica de
  Pernambuco, (2) Cognitive Architecture for Collaborative Technologies Unit -
  Istituto Italiano di Tecnologia)",,,11,
Recursive Social Behavior Graph for Trajectory Prediction,"  Social interaction is an important topic in human trajectory prediction togenerate plausible paths. In this paper, we present a novel insight ofgroup-based social interaction model to explore relationships amongpedestrians. We recursively extract social representations supervised bygroup-based annotations and formulate them into a social behavior graph, calledRecursive Social Behavior Graph. Our recursive mechanism explores therepresentation power largely. Graph Convolutional Neural Network then is usedto propagate social interaction information in such a graph. With the guidanceof Recursive Social Behavior Graph, we surpass state-of-the-art method on ETHand UCY dataset for 11.1% in ADE and 10.8% in FDE in average, and successfullypredict complex social behaviors.","Jianhua Sun, Qinhong Jiang, Cewu Lu",,,11,
"Representing Point Clouds with Generative Conditional Invertible Flow
  Networks","  In this paper, we propose a simple yet effective method to represent pointclouds as sets of samples drawn from a cloud-specific probability distribution.This interpretation matches intrinsic characteristics of point clouds: thenumber of points and their ordering within a cloud is not important as allpoints are drawn from the proximity of the object boundary. We postulate torepresent each cloud as a parameterized probability distribution defined by agenerative neural network. Once trained, such a model provides a naturalframework for point cloud manipulation operations, such as aligning a new cloudinto a default spatial orientation. To exploit similarities between same-classobjects and to improve model performance, we turn to weight sharing: networksthat model densities of points belonging to objects in the same family shareall parameters with the exception of a small, object-specific embedding vector.We show that these embedding vectors capture semantic relationships betweenobjects. Our method leverages generative invertible flow networks to learnembeddings as well as to generate point clouds. Thanks to this formulation andcontrary to similar approaches, we are able to train our model in an end-to-endfashion. As a result, our model offers competitive or superior quantitativeresults on benchmark datasets, while enabling unprecedented capabilities toperform cloud manipulation tasks, such as point cloud registration andregeneration, by a generative network.","Micha{\l} Stypu{\l}kowski, Kacper Kania, Maciej Zamorski, Maciej
  Zi\k{e}ba, Tomasz Trzci\'nski, Jan Chorowski",,,11,
Binary Image Features Proposed to Empower Computer Vision,"  This literature has proposed three fast and easy computable image features toimprove computer vision by offering more human-like vision power. Thesefeatures are not based on image pixels absolute or relative intensity; neitherbased on shape or colour. So, no complex pixel by pixel calculation isrequired. For human eyes, pixel by pixel calculation is like seeing an imagewith maximum zoom which is done only when a higher level of details isrequired. Normally, first we look at an image to get an overall idea about itto know whether it deserves further investigation or not. This capacity ofgetting an idea at a glance is analysed and three basic features are proposedto empower computer vision. Potential of proposed features is tested andestablished through different medical dataset. Achieved accuracy inclassification demonstrates possibilities and potential of the use of theproposed features in image processing.","Soumi Ray, Vinod Kumar",,,11,
Improving Semantic Segmentation via Decoupled Body and Edge Supervision,"  Existing semantic segmentation approaches either aim to improve the object'sinner consistency by modeling the global context, or refine objects detailalong their boundaries by multi-scale feature fusion. In this paper, a newparadigm for semantic segmentation is proposed. Our insight is that appealingperformance of semantic segmentation requires \textit{explicitly} modeling theobject \textit{body} and \textit{edge}, which correspond to the high and lowfrequency of the image. To do so, we first warp the image feature by learning aflow field to make the object part more consistent. The resulting body featureand the residual edge feature are further optimized under decoupled supervisionby explicitly sampling different parts (body or edge) pixels. We show that theproposed framework with various baselines or backbone networks leads to betterobject inner consistency and object boundaries. Extensive experiments on fourmajor road scene semantic segmentation benchmarks including\textit{Cityscapes}, \textit{CamVid}, \textit{KIITI} and \textit{BDD} show thatour proposed approach establishes new state of the art while retaining highefficiency in inference. In particular, we achieve 83.7 mIoU \% on Cityscapewith only fine-annotated data. Code and models are made available to foster anyfurther research (\url{https://github.com/lxtGH/DecoupleSegNets}).","Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi,
  Zhouchen Lin, Shaohua Tan, Yunhai Tong",,,11,
"GAZED- Gaze-guided Cinematic Editing of Wide-Angle Monocular Video
  Recordings","  We present GAZED- eye GAZe-guided EDiting for videos captured by a solitary,static, wide-angle and high-resolution camera. Eye-gaze has been effectivelyemployed in computational applications as a cue to capture interesting scenecontent; we employ gaze as a proxy to select shots for inclusion in the editedvideo. Given the original video, scene content and user eye-gaze tracks arecombined to generate an edited video comprising cinematically valid actor shotsand shot transitions to generate an aesthetic and vivid representation of theoriginal narrative. We model cinematic video editing as an energy minimizationproblem over shot selection, whose constraints capture cinematographic editingconventions. Gazed scene locations primarily determine the shots constitutingthe edited video. Effectiveness of GAZED against multiple competing methods isdemonstrated via a psychophysical study involving 12 users and twelveperformance videos.","K L Bhanu Moorthy, Moneish Kumar, Ramanathan Subramaniam, Vineet
  Gandhi",,,11,
"Addressing the Cold-Start Problem in Outfit Recommendation Using Visual
  Preference Modelling","  With the global transformation of the fashion industry and a rise in thedemand for fashion items worldwide, the need for an effectual fashionrecommendation has never been more. Despite various cutting-edge solutionsproposed in the past for personalising fashion recommendation, the technologyis still limited by its poor performance on new entities, i.e. the cold-startproblem. In this paper, we attempt to address the cold-start problem for newusers, by leveraging a novel visual preference modelling approach on a smallset of input images. We demonstrate the use of our approach withfeature-weighted clustering to personalise occasion-oriented outfitrecommendation. Quantitatively, our results show that the proposed visualpreference modelling approach outperforms state of the art in terms of clothingattribute prediction. Qualitatively, through a pilot study, we demonstrate theefficacy of our system to provide diverse and personalised recommendations incold-start scenarios.","Dhruv Verma, Kshitij Gulati and Rajiv Ratn Shah",,,11,
"Face Quality Estimation and Its Correlation to Demographic and
  Non-Demographic Bias in Face Recognition","  Face quality assessment aims at estimating the utility of a face image forthe purpose of recognition. It is a key factor to achieve high face recognitionperformances. Currently, the high performance of these face recognition systemscome with the cost of a strong bias against demographic and non-demographicsub-groups. Recent work has shown that face quality assessment algorithmsshould adapt to the deployed face recognition system, in order to achievehighly accurate and robust quality estimations. However, this could lead to abias transfer towards the face quality assessment leading to discriminatoryeffects e.g. during enrolment. In this work, we present an in-depth analysis ofthe correlation between bias in face recognition and face quality assessment.Experiments were conducted on two publicly available datasets captured undercontrolled and uncontrolled circumstances with two popular face embeddings. Weevaluated four state-of-the-art solutions for face quality assessment towardsbiases to pose, ethnicity, and age. The experiments showed that the facequality assessment solutions assign significantly lower quality values towardssubgroups affected by the recognition bias demonstrating that these approachesare biased as well. This raises ethical questions towards fairness anddiscrimination which future works have to address.","Philipp Terh\""orst, Jan Niklas Kolf, Naser Damer, Florian
  Kirchbuchner, Arjan Kuijper",,,11,
"Learning to Detect Important People in Unlabelled Images for
  Semi-supervised Important People Detection","  Important people detection is to automatically detect the individuals whoplay the most important roles in a social event image, which requires thedesigned model to understand a high-level pattern. However, existing methodsrely heavily on supervised learning using large quantities of annotated imagesamples, which are more costly to collect for important people detection thanfor individual entity recognition (eg, object recognition). To overcome thisproblem, we propose learning important people detection on partially annotatedimages. Our approach iteratively learns to assign pseudo-labels to individualsin un-annotated images and learns to update the important people detectionmodel based on data with both labels and pseudo-labels. To alleviate thepseudo-labelling imbalance problem, we introduce a ranking strategy forpseudo-label estimation, and also introduce two weighting strategies: one forweighting the confidence that individuals are important people to strengthenthe learning on important people and the other for neglecting noisy unlabelledimages (ie, images without any important people). We have collected twolarge-scale datasets for evaluation. The extensive experimental results clearlyconfirm the efficacy of our method attained by leveraging unlabelled images forimproving the performance of important people detection.","Fa-Ting Hong, Wei-Hong Li, Wei-Shi Zheng",,,11,
Recurrent Feature Reasoning for Image Inpainting,"  Existing inpainting methods have achieved promising performance forrecovering regular or small image defects. However, filling in large continuousholes remains difficult due to the lack of constraints for the hole center. Inthis paper, we devise a Recurrent Feature Reasoning (RFR) network which ismainly constructed by a plug-and-play Recurrent Feature Reasoning module and aKnowledge Consistent Attention (KCA) module. Analogous to how humans solvepuzzles (i.e., first solve the easier parts and then use the results asadditional information to solve difficult parts), the RFR module recurrentlyinfers the hole boundaries of the convolutional feature maps and then uses themas clues for further inference. The module progressively strengthens theconstraints for the hole center and the results become explicit. To captureinformation from distant places in the feature map for RFR, we further developKCA and incorporate it in RFR. Empirically, we first compare the proposedRFR-Net with existing backbones, demonstrating that RFR-Net is more efficient(e.g., a 4\% SSIM improvement for the same model size). We then place thenetwork in the context of the current state-of-the-art, where it exhibitsimproved performance. The corresponding source code is available at:https://github.com/jingyuanli001/RFR-Inpainting","Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, Dacheng Tao",,,11,
AdversarialNAS: Adversarial Neural Architecture Search for GANs,"  Neural Architecture Search (NAS) that aims to automate the procedure ofarchitecture design has achieved promising results in many computer visionfields. In this paper, we propose an AdversarialNAS method specially tailoredfor Generative Adversarial Networks (GANs) to search for a superior generativemodel on the task of unconditional image generation. The AdversarialNAS is thefirst method that can search the architectures of generator and discriminatorsimultaneously in a differentiable manner. During searching, the designedadversarial search algorithm does not need to comput any extra metric toevaluate the performance of the searched architecture, and the search paradigmconsiders the relevance between the two network architectures and improvestheir mutual balance. Therefore, AdversarialNAS is very efficient and onlytakes 1 GPU day to search for a superior generative model in the proposed largesearch space ($10^{38}$). Experiments demonstrate the effectiveness andsuperiority of our method. The discovered generative model sets a newstate-of-the-art FID score of $10.87$ and highly competitive Inception Score of$8.74$ on CIFAR-10. Its transferability is also proven by setting newstate-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10.Code is at: \url{https://github.com/chengaopro/AdversarialNAS}.","Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan",,,11,
"From Image Collections to Point Clouds with Self-supervised Shape and
  Pose Networks","  Reconstructing 3D models from 2D images is one of the fundamental problems incomputer vision. In this work, we propose a deep learning technique for 3Dobject reconstruction from a single image. Contrary to recent works that eitheruse 3D supervision or multi-view supervision, we use only single view imageswith no pose information during training as well. This makes our approach morepractical requiring only an image collection of an object category and thecorresponding silhouettes. We learn both 3D point cloud reconstruction and poseestimation networks in a self-supervised manner, making use of differentiablepoint cloud renderer to train with 2D supervision. A key novelty of theproposed technique is to impose 3D geometric reasoning into predicted 3D pointclouds by rotating them with randomly sampled poses and then enforcing cycleconsistency on both 3D reconstructions and poses. In addition, usingsingle-view supervision allows us to do test-time optimization on a given testimage. Experiments on the synthetic ShapeNet and real-world Pix3D datasetsdemonstrate that our approach, despite using less supervision, can achievecompetitive performance compared to pose-supervised and multi-view supervisedapproaches.","K L Navaneet, Ansu Mathew, Shashank Kashyap, Wei-Chih Hung, Varun
  Jampani and R. Venkatesh Babu",,,11,
"MS$^2$L: Multi-Task Self-Supervised Learning for Skeleton Based Action
  Recognition","  In this paper, we address self-supervised representation learning from humanskeletons for action recognition. Previous methods, which usually learn featurepresentations from a single reconstruction task, may come across theoverfitting problem, and the features are not generalizable for actionrecognition. Instead, we propose to integrate multiple tasks to learn moregeneral representations in a self-supervised manner. To realize this goal, weintegrate motion prediction, jigsaw puzzle recognition, and contrastivelearning to learn skeleton features from different aspects. Skeleton dynamicscan be modeled through motion prediction by predicting the future sequence. Andtemporal patterns, which are critical for action recognition, are learnedthrough solving jigsaw puzzles. We further regularize the feature space bycontrastive learning. Besides, we explore different training strategies toutilize the knowledge from self-supervised tasks for action recognition. Weevaluate our multi-task self-supervised learning approach with actionclassifiers trained under different configurations, including unsupervised,semi-supervised and fully-supervised settings. Our experiments on the NW-UCLA,NTU RGB+D, and PKUMMD datasets show remarkable performance for actionrecognition, demonstrating the superiority of our method in learning morediscriminative and general features. Our project website is available athttps://langlandslin.github.io/projects/MSL/.","Lilang Lin, Sijie Song, Wenhan Yan and Jiaying Liu",,,11,
Explanation-Guided Training for Cross-Domain Few-Shot Classification,"  Cross-domain few-shot classification task (CD-FSC) combines few-shotclassification with the requirement to generalize across domains represented bydatasets. This setup faces challenges originating from the limited labeled datain each class and, additionally, from the domain shift between training andtest sets. In this paper, we introduce a novel training approach for existingFSC models. It leverages on the explanation scores, obtained from existingexplanation methods when applied to the predictions of FSC models, computed forintermediate feature maps of the models. Firstly, we tailor the layer-wiserelevance propagation (LRP) method to explain the prediction outcomes of FSCmodels. Secondly, we develop a model-agnostic explanation-guided trainingstrategy that dynamically finds and emphasizes the features which are importantfor the predictions. Our contribution does not target a novel explanationmethod but lies in a novel application of explanations for the training phase.We show that explanation-guided training effectively improves the modelgeneralization. We observe improved accuracy for three different FSC models:RelationNet, cross attention network, and a graph neural network-basedformulation, on five few-shot learning datasets: miniImagenet, CUB, Cars,Places, and Plantae.","Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao,
  Ngai-Man Cheung, Alexander Binder",,,11,
"Disentangling Multiple Features in Video Sequences using Gaussian
  Processes in Variational Autoencoders","  We introduce MGP-VAE (Multi-disentangled-features Gaussian ProcessesVariational AutoEncoder), a variational autoencoder which uses Gaussianprocesses (GP) to model the latent space for the unsupervised learning ofdisentangled representations in video sequences. We improve upon previous workby establishing a framework by which multiple features, static or dynamic, canbe disentangled. Specifically we use fractional Brownian motions (fBM) andBrownian bridges (BB) to enforce an inter-frame correlation structure in eachindependent channel, and show that varying this structure enables one tocapture different factors of variation in the data. We demonstrate the qualityof our representations with experiments on three publicly available datasets,and also quantify the improvement using a video prediction task. Moreover, weintroduce a novel geodesic loss function which takes into account the curvatureof the data manifold to improve learning. Our experiments show that thecombination of the improved representations with the novel loss function enableMGP-VAE to outperform the baselines in video prediction.","Sarthak Bhagat, Shagun Uppal, Zhuyun Yin and Nengli Lim",,,11,
"Fusing Keys for Secret Communications: Towards Information-Theoretic
  Security","  Modern cryptography is essential to communication and information securityfor performing all kinds of security actions, such as encryption,authentication, and signature. However, the exposure possibility of keys posesa great threat to almost all modern cryptography. This article proposes akey-fusing framework, which enables a high resilience to key exposure by fusingmultiple imperfect keys. The correctness of the scheme is strictly verifiedthrough a toy model that is general enough to abstract the physical-layer keygeneration (PLKG) mechanisms. Analysis and results demonstrate that theproposed scheme can dramatically reduce secret outage probability, so that keysources with even high exposure probability can be practically beneficial foractual secret communication. Our framework paves the way for achievinginformation-theoretic security by integrating various key sources, such asphysical layer key generation, lattice-based cryptography, and quantumcryptography.","Longjiang Li, Bingchuan Ma, Jianjun Yang, Yonggang Li, Yuming Mao",,,11,
"Adding Location and Global Context to the Google/Apple Exposure
  Notification Bluetooth API","  Contact tracing requires a strong understanding of the context of a user, andlocation with other sensory data could provide a context for any infectionencounter. Although Bluetooth technology gives a good insight into theproximity aspect of an encounter, it does not provide any location contextrelated to it which helps to make better decisions. Using the ideas presentedin this paper, one shall be able to obtain this valuable information that couldaddress the problem of false-positive and false-negative to a certain extent.All of this within the purview of Google/Apple Exposure Notification (GAEN)specification, while preserving complete user privacy. There are four ways ofpropagating context between any two users. Two such methods allow privatelocation logging, without revealing the location history within an app. Theother two are encryption-based methods. The first encryption method is avariant of Apple's FindMy protocol, that allows nearby Apple devices to capturethe GPS location of a lost Apple device. The second encryption is a minormodification of the existing GAEN protocol so that global context is availableto a healthy phone only when it is exposed - this is a better optioncomparatively. It will still be the role of Public Health smartphone app todecide, on how to use the location-time context, to build a full-fledgedcontact tracing and public health solution. Lastly, we highlight the benefitsand potential privacy issues with each of these context propagation methodsproposed here.","Ramesh Raskar, Abhishek Singh, Sam Zimmerman, Shrikant Kanaparti",,,11,
Profit lag and alternate network mining,"  For a mining strategy we define the notion of ""profit lag"" as the minimumtime it takes to be profitable after that moment. We compute closed forms forthe profit lag and the revenue ratio for the strategies ""selfish mining"" and""intermittent selfish mining"". This confirms some earlier numerical simulationsand clarifies misunderstandings on profitability in the literature. We alsostudy mining pairs of PoW cryptocurrencies, often coming from a fork, with thesame mining algorithm. This represents a vector of attack that can be exploitedusing the ""alternate network mining"" strategy that we define. We compute closedforms for the profit lag and the revenue ratiofor this strategy that is moreprofitable than selfish mining and intermittent selfish mining. It is alsoharder to counter since it does not rely on a flaw in the difficulty adjustmentformula that is the reason for profitability of the other strategies.","Cyril Grunspan, Ricardo P\'erez-Marco",,,11,
"Secure communication between UAVs using a method based on smart agents
  in unmanned aerial vehicles","  Unmanned aerial vehicles (UAVs) can be deployed to monitor very large areaswithout the need for network infrastructure. UAVs communicate with each otherduring flight and exchange information with each other. However, suchcommunication poses security challenges due to its dynamic topology. To solvethese challenges, the proposed method uses two phases to counter malicious UAVattacks. In the first phase, we applied a number of rules and principles todetect malicious UAVs. In this phase, we try to identify and remove maliciousUAVs according to the behavior of UAVs in the network in order to preventsending fake information to the investigating UAVs. In the second phase, amobile agent based on a three-step negotiation process is used to eliminatemalicious UAVs. In this way, we use mobile agents to inform our normal neighborUAVs so that they do not listen to the data generated by the malicious UAVs.Therefore, the mobile agent of each UAV uses reliable neighbors through athree-step negotiation process so that they do not listen to the trafficgenerated by the malicious UAVs. The NS-3 simulator was used to demonstrate theefficiency of the SAUAV method. The proposed method is more efficient thanCST-UAS, CS-AVN, HVCR, and BSUM-based methods in detection rate, false positiverate, false negative rate, packet delivery rate, and residual energy.",Maryam Faraji-Biregani and Reza Fotohi,,,11,
"Attributes affecting user decision to adopt a Virtual Private Network
  (VPN) app","  A Virtual Private Network (VPN) helps to mitigate security and privacy risksof data transmitting on unsecured network such as public Wi-Fi. However,despite awareness of public Wi-Fi risks becoming increasingly common, the useof VPN when using public Wi-Fi is low. To increase adoption, understandingfactors driving user decision to adopt a VPN app is an important first step.This study is the first to achieve this objective using discrete choiceexperiments (DCEs) to elicit individual preferences of specific attributes of aVPN app. The experiments were run in the United Kingdom (UK) and Japan (JP). Wefirst interviewed participants (15 UK, 17 JP) to identify common attributes ofa VPN app which they considered important. The results were used to design andrun a DCE in each country. Participants (149 UK, 94 JP) were shown a series oftwo hypothetical VPN apps, varying in features, and were asked to choose onewhich they preferred. Customer review rating, followed by price of a VPN app,significantly affected the decision to choose which VPN app to download andinstall. A change from a rating of 3 to 4-5 stars increased the probability ofchoosing an app by 33% in the UK and 14% in Japan. Unsurprisingly, price was adeterrent. Recommendations by friends, source of product reviews, and thepresence of in-app ads also played a role but to a lesser extent. To actuallyuse a VPN app, participants considered Internet speed, connection stability,battery level on mobile devices, and the presence of in-app ads as key drivers.Participants in the UK and in Japan prioritized these attributes differently,suggesting possible influences from cultural differences.","Nissy Sombatruang, Tan Omiya, Daisuke Miyamoto, M. Angela Sasse, Youki
  Kadobayashi, Michelle Baddeley",,,11,
"Database Intrusion Detection Systems (DIDs): Insider Threat Detection
  via Behavioural-based Anomaly Detection Systems -- A Brief Survey of Concepts
  and Approaches","  One of the data security and privacy concerns is of insider threats, wherelegitimate users of the system abuse the access privileges they hold. Theinsider threat to data security means that an insider steals or leaks sensitivepersonal information. Database Intrusion detection systems, specificallybehavioural-based database intrusion detection systems, have been showneffective in detecting insider attacks. This paper presents background conceptson database intrusion detection systems in the context of detecting insiderthreats and examines existing approaches in the literature on detectingmalicious accesses by an insider to Database Management Systems (DBMS).","Muhammad Imran Khan, Simon N. Foley, Barry O'Sullivan",,,11,
"Game-Theoretical Analysis of Mining Strategy for Bitcoin-NG Blockchain
  Protocol","  Bitcoin-NG, a scalable blockchain protocol, divides each block into a keyblock and many micro blocks to effectively improve the transaction processingcapacity. Bitcoin-NG has a special incentive mechanism (i.e. splittingtransaction fees to the current and the next leader) to maintain its security.However, this design of the incentive mechanism ignores the joint effect oftransaction fees, mint coins and mining duration lengths on the expected miningreward. In this paper, we identify the advanced mining attack that deliberatelyignores micro blocks to enlarge the mining duration length to increase thelikelihood of winning the mining race. We first show that an advanced miningattacker can maximize its expected reward by optimizing its mining durationlength. We then formulate a game-theoretical model in which multiple miningplayers perform advanced mining to compete with each other. We analyze the Nashequilibrium for the mining game. Our analytical and simulation results indicatethat all mining players in the mining game converge to having advanced miningat the equilibrium and have no incentives for deviating from the equilibrium;the transaction processing capability of the Bitcoin-NG network at theequilibrium is decreased by advanced mining. Therefore, we conclude that theBitcoin-NG blockchain protocol is vulnerable to advanced mining attack. Wediscuss how to reduce the negative impact of advanced mining for Bitcoin-NG.","Taotao Wang, Xiaoqian Bai, Hao Wang, Soung Chang Liew, and Shengli
  Zhang",,,11,
PACStack: an Authenticated Call Stack,"  A popular run-time attack technique is to compromise the control-flowintegrity of a program by modifying function return addresses on the stack. Sofar, shadow stacks have proven to be essential for comprehensively preventingreturn address manipulation. Shadow stacks record return addresses inintegrity-protected memory secured with hardware-assistance or software accesscontrol. Software shadow stacks incur high overheads or trade off security forefficiency. Hardware-assisted shadow stacks are efficient and secure, butrequire the deployment of special-purpose hardware.  We present authenticated call stack (ACS), an approach that uses chainedmessage authentication codes (MACs). Our prototype, PACStack, uses the ARMgeneral purpose hardware mechanism for pointer authentication (PA) to implementACS. Via a rigorous security analysis, we show that PACStack achieves securitycomparable to hardware-assisted shadow stacks without requiring dedicatedhardware. We demonstrate that PACStack's performance overhead is small (~3%).","Hans Liljestrand and Thomas Nyman and Lachlan J. Gunn and Jan-Erik
  Ekberg and N. Asokan",,,11,
"A Low-Power Dual-Factor Authentication Unit for Secure Implantable
  Devices","  This paper presents a dual-factor authentication protocol and its low-powerimplementation for security of implantable medical devices (IMDs). The protocolincorporates traditional cryptographic first-factor authentication usingDatagram Transport Layer Security - Pre-Shared Key (DTLS-PSK) followed by theuser's touch-based voluntary second-factor authentication for enhancedsecurity. With a low-power compact always-on wake-up timer and touch-basedwake-up circuitry, our test chip consumes only 735 pW idle state power at 20.15Hz and 2.5 V. The hardware accelerated dual-factor authentication unit consumes8 $\mu$W at 660 kHz and 0.87 V. Our test chip was coupled with commercialBluetooth Low Energy (BLE) transceiver, DC-DC converter, touch sensor and coincell battery to demonstrate standalone implantable operation and also testedusing in-vitro measurement setup.","Saurav Maji, Utsav Banerjee, Samuel H Fuller, Mohamed R Abdelhamid,
  Phillip M Nadeau, Rabia Tugce Yazicigil, Anantha P Chandrakasan",,,11,
Multi-Central Differential Privacy,"  Differential privacy is typically studied in the central model where atrusted ""aggregator"" holds the sensitive data of all the individuals and isresponsible for protecting their privacy. A popular alternative is the localmodel in which the aggregator is untrusted and instead each individual isresponsible for their own privacy. The decentralized privacy guarantee of thelocal model comes at a high price in statistical utility or computationalcomplexity. Thus intermediate models such as the shuffled model and pan privacyhave been studied in an attempt to attain the best of both worlds.  In this note, we propose an intermediate trust model for differentialprivacy, which we call the multi-central model. Here there are multipleaggregators and we only assume that they do not collude nefariously. This modelrelaxes the trust requirements of the central model while avoiding the price ofthe local model. We motivate this model and provide some simple and efficientalgorithms for it. We argue that this model is a promising direction forfurther research.",Thomas Steinke,,,11,
"Modeling and Assessment of IoT Supply Chain Security Risks: The Role of
  Structural and Parametric Uncertainties","  Supply chain security threats pose new challenges to security risk modelingtechniques for complex ICT systems such as the IoT. With established techniquesdrawn from attack trees and reliability analysis providing needed points ofreference, graph-based analysis can provide a framework for considering therole of suppliers in such systems. We present such a framework here whilehighlighting the need for a component-centered model. Given resourcelimitations when applying this model to existing systems, we study variousclasses of uncertainties in model development, including structuraluncertainties and uncertainties in the magnitude of estimated eventprobabilities. Using case studies, we find that structural uncertaintiesconstitute a greater challenge to model utility and as such should receiveparticular attention. Best practices in the face of these uncertainties areproposed.",Timothy Kieras and Muhammad Junaid Farooq and Quanyan Zhu,,,11,
The Athena Class of Risk-Limiting Ballot Polling Audits,"  The main risk-limiting ballot polling audit in use today, BRAVO, is designedfor use when single ballots are drawn at random and a decision regardingwhether to stop the audit or draw another ballot is taken after each ballotdraw ({\em ballot-by-ballot (B2)} audits). On the other hand, real ballotpolling audits draw many ballots in a single round before determining whetherto stop ({\em round-by-round (R2)} audits). We show that BRAVO results insignificant inefficiency when directly applied to real R2 audits. We presentthe ATHENA class of R2 risk-limiting stopping rules, and prove that each ruleis at least as efficient as the corresponding BRAVO stopping rule applied atthe end of the round. We have software libraries implementing most of ourresults in both python and MATLAB.  We show that ATHENA halves the number of ballots required, for all statemargins in the 2016 US Presidential election and a first round with 90%stopping probability, when compared to BRAVO (stopping rule applied at the endof the round). We present simulation results supporting the 90% stoppingprobability claims and our claims for the risk accrued in the first round.Further, ATHENA reduces the number of ballots by more than a quarter for lowmargins, when compared to the BRAVO stopping rule applied on ballots inselection order. Thus ATHENA is more efficient even without information onselection order. These results are significant because current approaches toreal ballot polling election audits use the B2 BRAVO rules, requiring abouttwice as much work on the part of election officials. Many states performingrisk-limiting audits for the first time plan to use ballot polling audits inNovember 2020 for the US Presidential election and could substantially benefitfrom improvements.","Filip Zag\'orski, Grant McClearn, Sarah Morin, Neal McBurnett, Poorvi
  L. Vora",,,11,
"Steganography GAN: Cracking Steganography with Cycle Generative
  Adversarial Networks","  For as long as humans have participated in the act of communication,concealing information in those communicative mediums has manifested into anart of its own. Crytographic messages, through written language or images, area means of concealment, usually reserved for highly sensitive or compromisinginformation. Specifically, the field of Cryptography is the construction andanalysis of protocols that prevent third parties from understanding privatemessages. Steganography is related to Cryptography in that the goal is toobscure information using some method or algorithm, but the most importantdifference is that the information and the method of concealing informationwithin Steganography both involve images--more precisely, the embedding of oneimage or piece of information into another image. Ever since the creation ofcovert communication methods, steps have been taken to crack cryptography andsteganography algorithms. The desire for this rises from both human curiosityand the need to counteract adverse uses, such as encoding harmful media ininconspicuous media (phishing attack). In this paper, we succeed in crackingthe Least Significant Bit (LSB) steganography algorithm using Cycle GenerativeAdversarial Networks (CycleGANs) and Bayesian Optimization and compare the useof CycleGANs against Convolutional Autoencoders. The results of our experimentshighlight the promising nature of CycleGANs in cracking steganography and openseveral possible avenues of research.","Nibraas Khan, Ruj Haan, George Boktor, Michael McComas, and Ramin
  Daneshi",,,11,
Share Withholding Attack in Blockchain Mining: Technical Report,"  Cryptocurrency achieves distributed consensus using proof of work (PoW).Prior research in blockchain security identified financially incentivizedattacks based on withholding blocks which have the attacker compromise a victimpool and pose as a PoW contributor by submitting the shares (earning credit formining) but withholding the blocks (no actual contributions to the pool). Weadvance such threats to generate greater reward advantage to the attackerswhile undermining the other miners and introduce the share withholding attack(SWH). SWH withholds shares to increase the attacker's reward payout within thepool, in contrast to the prior threats withholding blocks, and rather builds onthe block-withholding threats in order to exploit the information about theimpending block submission timing, challenging the popularly establishedassumption that the block submission time is completely random and unknown tominers. We analyze SWH's incentive compatibility and the vulnerability scope byidentifying the critical systems and environmental parameters which determinethe attack's impact. Our results show that SWH in conjunction with blockwithholding yield unfair reward advantage at the expense of theprotocol-complying victim miners and that a rational miner will selfishlylaunch SWH to maximize its reward profit. We inform the blockchain andcryptocurrency research of the novel SWH threat and include the potentialcountermeasure directions to facilitate such research and development.",Sang-Yoon Chang (University of Colorado Colorado Springs),,,11,
Advanced Persistent Threat: Detection and Defence,"  The critical assessment presented within this paper explores existingresearch pertaining to the Advanced Persistent Threat (APT) branch of cybersecurity, applying the knowledge extracted from this research to discuss,evaluate and opinionate upon the areas of discussion as well as involvingpersonal experiences and knowledge within this field. The synthesis of currentliterature delves into detection capabilities and techniques as well asdefensive solutions for organisations with respect to APTs. Higher-tierdetection and defensive strategies bear greater importance with largerorganisations; especially government departments or organisations whose workimpacts the public on a large scale. Successful APT attacks can result in theexfiltration of sensitive data, network down time and the infection of machineswhich allow for remote access from Command-and-control (C2) servers. This paperpresents a well-rounded analysis of the Advanced Persistent Threat problem andprovides well-reasoned conclusions of how to mitigate the security risk.",Mohammad Bilal Khan,,,11,
"Automated Retrieval of ATT&CK Tactics and Techniques for Cyber Threat
  Reports","  Over the last years, threat intelligence sharing has steadily grown, leadingcybersecurity professionals to access increasingly larger amounts ofheterogeneous data. Among those, cyber attacks' Tactics, Techniques andProcedures (TTPs) have proven to be particularly valuable to characterizethreat actors' behaviors and, thus, improve defensive countermeasures.Unfortunately, this information is often hidden within human-readable textualreports and must be extracted manually. In this paper, we evaluate severalclassification approaches to automatically retrieve TTPs from unstructuredtext. To implement these approaches, we take advantage of the MITRE ATT&CKframework, an open knowledge base of adversarial tactics and techniques, totrain classifiers and label results. Finally, we present rcATT, a tool built ontop of our findings and freely distributed to the security community to supportcyber threat report automated analysis.","Valentine Legoy, Marco Caselli, Christin Seifert, and Andreas Peter",,,11,
Exploring HTTPS Security Inconsistencies: A Cross-Regional Perspective,"  If two or more identical HTTPS clients, located at different geographiclocations (regions), make an HTTPS request to the same domain (e.g.example.com), on the same day, will they receive the same HTTPS securityguarantees in response? Our results give evidence that this is not always thecase. We conduct scans for the top 250,000 most visited domains on theInternet, from clients located at five different regions: Australia, Brazil,India, the UK, and the US. Our scans gather data from both application (URLsand HTTP headers) and transport (servers' selected TLS version, ciphersuite,and certificate) layers. Overall, we find that HTTPS inconsistencies at theapplication layer are higher than those at the transport layer. We also findthat HTTPS security inconsistencies are strongly related to URLs and IPsdiversity among regions, and to a lesser extent to the presence ofredirections. Further manual inspection shows that there are several reasonsbehind URLs diversity among regions such as downgrading to the plain-HTTPprotocol, using different subdomains, different TLDs, or different home pagedocuments. Furthermore, we find that downgrading to plain-HTTP is related towebsites' regional blocking. We also provide attack scenarios that show how anattacker can benefit from HTTPS security inconsistencies, and introduce a newattack scenario which we call the ""region confusion"" attack. Finally, based onour analysis and observations, we provide discussion, which include somerecommendations such as the need for testing tools for domain administratorsand users that help to mitigate and detect regional domains' inconsistencies,standardising regional domains format with the same-origin policy (of domains)in mind, standardising secure URL redirections, and avoid redirections wheneverpossible.",Eman Salem Alashwali and Pawel Szalachowski and Andrew Martin,,,11,
"MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical
  Fully-Associative Design","  Shared caches in modern processors are vulnerable to conflict-based attacks,whereby an attacker monitors the access pattern of a victim by engineeringcache-set conflicts. Recent mitigations propose a randomized mapping ofaddresses to cache locations to obfuscate addresses that can conflict with atarget address. Unfortunately, such designs continue to select evictioncandidates from a small subset of the resident cache lines, which makes suchdesigns vulnerable to algorithms that can quickly identify the conflictingaddresses.  This paper presents Mirage, a practical design for a fully associative cache,wherein eviction candidates are selected randomly from among all the linesresident in the cache, to be immune to set-conflicts. A key challenge innaively adopting such designs for large shared caches (containing tens ofthousands of lines) is the complexity of cache-lookup, as that can requiresearching through all the lines resident in the cache in such designs. Miragepractically enables a fully-associative design, while maintaining the accesslatency similar to a traditional set-associative cache using: (1) Pointer-basedindirection from the tag-store to the data-store, which allows a newlyinstalled address to evict data of any resident line, (2) Skewed-associativetag-store with extra invalid tags, wherein incoming addresses can be installedwithout set-conflicts, and (3) Load-aware placement that maximizes theavailability of sets with invalid tags, to eliminate set-conflicts. Ouranalysis shows Mirage provides the global-eviction property of afully-associative cache throughout the system lifetime (violations offull-associativity, i.e set-conflicts, occur less than once in 10^4 to 10^17years), offering a principled defense against set-conflict based attacks.Mirage incurs negligible slowdown (0.3%) and 12-15% extra storage compared tothe recently proposed Scatter-Cache.",Gururaj Saileshwar and Moinuddin Qureshi,,,11,
"The Dark (and Bright) Side of IoT: Attacks and Countermeasures for
  Identifying Smart Home Devices and Services","  We present a new machine learning-based attack that exploits network patternsto detect the presence of smart IoT devices and running services in the WiFiradio spectrum. We perform an extensive measurement campaign of datacollection, and we build up a model describing the traffic patternscharacterizing three popular IoT smart home devices, i.e., Google Nest Mini,Amazon Echo, and Amazon Echo Dot. We prove that it is possible to detect andidentify with overwhelming probability their presence and the services runningby the aforementioned devices in a crowded WiFi scenario. This work proves thatstandard encryption techniques alone are not sufficient to protect the privacyof the end-user, since the network traffic itself exposes the presence of boththe device and the associated service. While more work is required to preventnon-trusted third parties to detect and identify the user's devices, weintroduce Eclipse, a technique to mitigate these types of attacks, whichreshapes the traffic making the identification of the devices and theassociated services similar to the random classification baseline.","Ahmed Mohamed Hussain, Gabriele Oligeri, and Thiemo Voigt",,,11,
"Privacy-Preserving Claims Exchange Networks for Virtual Asset Service
  Providers","  In order for VASPs to fulfill the regulatory requirements from the FATF andthe Travel Rule, VASPs need access to truthful information regardingoriginators, beneficiaries and other VASPs involved in a virtual asset transferinstance. Additionally, in seeking data regarding subjects (individuals ororganizations) VASPs are faced with privacy regulations such as the GDPR andCCPA. In this paper we a propose privacy-preserving claims issuance model thatcarries indicators of the provenance of the data and the algorithms used toderive the claim or assertion. This allows VASPs to obtain originator andbeneficiary information without necessarily having access to the private dataabout these entities. Secondly we propose the use of a consortium trust networkarrangement for VASPs to exchange signed claims about subjects and theirpublic-key information or certificate.","Thomas Hardjono, Alexander Lipton, Alex Pentland",,,11,
A Quantum Vocal Theory of Sound,"  Concepts and formalism from acoustics are often used to exemplify quantummechanics. Conversely, quantum mechanics could be used to achieve a newperspective on acoustics, as shown by Gabor studies. Here, we focus inparticular on the study of human voice, considered as a probe to investigatethe world of sounds. We present a theoretical framework that is based onobservables of vocal production, and on some measurement apparati that can beused both for analysis and synthesis. In analogy to the description of spinstates of a particle, the quantum-mechanical formalism is used to describe therelations between the fundamental states associated with phonetic labels suchas phonation, turbulence, and supraglottal myoelastic vibrations. Theintermingling of these states, and their temporal evolution, can still beinterpreted in the Fourier/Gabor plane, and effective extractors can beimplemented. The bases for a Quantum Vocal Theory of Sound, with implicationsin sound analysis and design, are presented.",Davide Rocchesso and Maria Mannone,,,11,
"Using a Bi-directional LSTM Model with Attention Mechanism trained on
  MIDI Data for Generating Unique Music","  Generating music is an interesting and challenging problem in the field ofmachine learning. Mimicking human creativity has been popular in recent years,especially in the field of computer vision and image processing. With theadvent of GANs, it is possible to generate new similar images, based on traineddata. But this cannot be done for music similarly, as music has an extratemporal dimension. So it is necessary to understand how music is representedin digital form. When building models that perform this generative task, thelearning and generation part is done in some high-level representation such asMIDI (Musical Instrument Digital Interface) or scores. This paper proposes abi-directional LSTM (Long short-term memory) model with attention mechanismcapable of generating similar type of music based on MIDI data. The musicgenerated by the model follows the theme/style of the music the model istrained on. Also, due to the nature of MIDI, the tempo, instrument, and otherparameters can be defined, and changed, post generation.","Ashish Ranjan, Varun Nagesh Jolly Behera, Motahar Reza",,,11,
"Fully Learnable Front-End for Multi-Channel Acoustic Modeling using
  Semi-Supervised Learning","  In this work, we investigated the teacher-student training paradigm to traina fully learnable multi-channel acoustic model for far-field automatic speechrecognition (ASR). Using a large offline teacher model trained on beamformedaudio, we trained a simpler multi-channel student acoustic model used in thespeech recognition system. For the student, both multi-channel featureextraction layers and the higher classification layers were jointly trainedusing the logits from the teacher model. In our experiments, compared to abaseline model trained on about 600 hours of transcribed data, a relativeword-error rate (WER) reduction of about 27.3% was achieved when using anadditional 1800 hours of untranscribed data. We also investigated the benefitof pre-training the multi-channel front end to output the beamformed log-melfilter bank energies (LFBE) using L2 loss. We find that pre-training improvesthe word error rate by 10.7% when compared to a multi-channel model directlyinitialized with a beamformer and mel-filter bank coefficients for the frontend. Finally, combining pre-training and teacher-student training produces aWER reduction of 31% compared to our baseline.","Sanna Wager, Aparna Khare, Minhua Wu, Kenichi Kumatani, Shiva Sundaram",,,11,
"Assisted music creation with Flow Machines: towards new categories of
  new","  This chapter reflects on about 10 years of research in AI- assisted musiccomposition, in particular during the Flow Machines project. We reflect on themotivations for such a project, its background, its main results and impact,both technological and musical, several years after its completion. We concludewith a proposal for new categories of new, created by the many uses of AItechniques to generate novel material.",Fran\c{c}ois Pachet and Pierre Roy and Benoit Carr\'e,,,11,
Sound Event Recognition in a Smart City Surveillance Context,"  Due to the growing demand for improving surveillance capabilities in smartcities, systems need to be developed to provide better monitoring capabilitiesto competent authorities, agencies responsible for strategic resourcemanagement, and emergency call centers. This work assumes that, as acomplementary monitoring solution, the use of a system capable of detecting theoccurrence of sound events, performing the Sound Events Recognition (SER) task,is highly convenient. In order to contribute to the classification of suchevents, this paper explored several classifiers over the SESA dataset, composedof audios of three hazard classes (gunshots, explosions, and sirens) and aclass of casual sounds that could be misinterpreted as some of the othersounds. The best result was obtained by SGD, with an accuracy of 72.13% with6.81 ms classification time, reinforcing the viability of such an approach.","Tito Spadini, Dimitri Leandro de Oliveira Silva, Ricardo Suyama",,,11,
"Cascaded all-pass filters with randomized center frequencies and phase
  polarity for acoustic and speech measurement and data augmentation","  We introduce a new member of TSP (Time Stretched Pulse) for acoustic andspeech measurement infrastructure, based on a simple all-pass filter andsystematic randomization. This new infrastructure fundamentally upgrades ourprevious measurement procedure, which enables simultaneous measurement ofmultiple attributes, including non-linear ones without requiring extrafiltering nor post-processing. Our new proposal establishes a theoreticallysolid, flexible, and extensible foundation in acoustic measurement. Moreover,it is general enough to provide versatile research tools for other fields, suchas biological signal analysis. We illustrate using acoustic measurements anddata augmentation as representative examples among various prospectiveapplications. We open-sourced MATLAB implementation. It consists of aninteractive and real-time acoustic tool, MATLAB functions, and supportingmaterials.",Hideki Kawahara and Kohei Yatabe,,,11,
"PlugSonic: a web- and mobile-based platform for binaural audio and sonic
  narratives","  PlugSonic is a suite of web- and mobile-based applications for the curationand experience of binaural interactive soundscapes and sonic narratives. It wasdeveloped as part of the PLUGGY EU project (Pluggable Social Platform forHeritage Awareness and Participation) and consists of two main applications:PlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, tocreate and experience binaural soundscapes. The audio processing withinPlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while theexploration of soundscapes in a physical space is obtained using Apple's ARKit.In this paper we present the design choices, the user involvement processes andthe implementation details. The main goal of PlugSonic is technologydemocratisation; PlugSonic users - whether institutions or citizens - are allgiven the instruments needed to create, process and experience 3D soundscapesand sonic narrative; without the need for specific devices, external tools(software and/or hardware), specialised knowledge or custom development. Theevaluation, which was conducted with inexperienced users on three tasks -creation, curation and experience - demonstrates how PlugSonic is indeed asimple, effective, yet powerful tool.","Marco Comunit\`a, Andrea Gerino, Veranika Lim, Lorenzo Picinali",,,11,
"Time-Domain Audio Source Separation Based on Wave-U-Net Combined with
  Discrete Wavelet Transform","  We propose a time-domain audio source separation method using down-sampling(DS) and up-sampling (US) layers based on a discrete wavelet transform (DWT).The proposed method is based on one of the state-of-the-art deep neuralnetworks, Wave-U-Net, which successively down-samples and up-samples featuremaps. We find that this architecture resembles that of multiresolutionanalysis, and reveal that the DS layers of Wave-U-Net cause aliasing and maydiscard information useful for the separation. Although the effects of theseproblems may be reduced by training, to achieve a more reliable sourceseparation method, we should design DS layers capable of overcoming theproblems. With this belief, focusing on the fact that the DWT has ananti-aliasing filter and the perfect reconstruction property, we design theproposed layers. Experiments on music source separation show the efficacy ofthe proposed method and the importance of simultaneously considering theanti-aliasing filters and the perfect reconstruction property.",Tomohiko Nakamura and Hiroshi Saruwatari,,,11,
Adaptive music: Automated music composition and distribution,"  Creativity, or the ability to produce new useful ideas, is commonlyassociated to the human being, but there are many other examples in naturewhere this phenomenon can be observed. Inspired by this fact, in engineeringand particularly in computational sciences, many different models have beendeveloped to tackle a number of problems. Music, a form of art broadly presentalong the human history, is the main field addressed in this thesis, takingadvantage of the kind of ideas that bring diversity and creativity to natureand computation. We present Melomics, an algorithmic composition method basedon evolutionary search, with a genetic encoding of the solutions that areinterpreted in a complex developmental process that leads to music in standardformats. This bioinspired compositional system has exhibited a highly creativepower and versatility to produce music of different type, which in manyoccasions has proven to be indistinguishable from the music made by humancomposers. The system also has enabled the emergence of a set of completelynovel applications: from effective tools that help anyone to easily obtain theprecise music they need, to radically new uses like adaptive music for therapy,amusement or many other purposes. It is clear to us that there is yet muchresearch to be done in this field and that countless and new unimaginable useswill derive from it.",David Daniel Albarracin Molina,,,11,
A study on more realistic room simulation for far-field keyword spotting,"  We investigate the impact of more realistic room simulation for trainingfar-field keyword spotting systems without fine-tuning on in-domain data. Tothis end, we study the impact of incorporating the following factors in theroom impulse response (RIR) generation: air absorption, surface- andfrequency-dependent coefficients of real materials, and stochastic ray tracing.Through an ablation study, a wake word task is used to measure the impact ofthese factors in comparison with a ground-truth set of measured RIRs. On ahold-out set of re-recordings under clean and noisy far-field conditions, wedemonstrate up to $35.8\%$ relative improvement over the commonly-used (singleabsorption coefficient) image source method. Source code is made available inthe Pyroomacoustics package, allowing others to incorporate these techniques intheir work.","Eric Bezzam, Robin Scheibler, Cyril Cadoux, Thibault Gisselbrecht",,,11,
Melody Classification based on Performance Event Vector and BRNN,"  We proposed a model for the Conference of Music and Technology (CSMT2020)data challenge of melody classification. Our model used the Performance EventVector as the input sequence to build a Bidirectional RNN network forclassfication. The model achieved a satisfying performance on the developmentdataset and Wikifonia dataset. We also discussed the effect of severalhyper-parameters, and created multiple prediction outputs for the evaluationdataset.","Jinyue Guo, Aozhi Liu and Jing Xiao",,,11,
Reverberation Modeling for Source-Filter-based Neural Vocoder,"  This paper presents a reverberation module for source-filter-based neuralvocoders that improves the performance of reverberant effect modeling. Thismodule uses the output waveform of neural vocoders as an input and produces areverberant waveform by convolving the input with a room impulse response(RIR). We propose two approaches to parameterizing and estimating the RIR. Thefirst approach assumes a global time-invariant (GTI) RIR and directly learnsthe values of the RIR on a training dataset. The second approach assumes anutterance-level time-variant (UTV) RIR, which is invariant within one utterancebut varies across utterances, and uses another neural network to predict theRIR values. We add the proposed reverberation module to the phase spectrumpredictor (PSP) of a HiNet vocoder and jointly train the model. Experimentalresults demonstrate that the proposed module was helpful for modeling thereverberation effect and improving the perceived quality of generatedreverberant speech. The UTV-RIR was shown to be more robust than the GTI-RIR tounknown reverberation conditions and achieved a perceptually betterreverberation effect.","Yang Ai, Xin Wang, Junichi Yamagishi, Zhen-Hua Ling",,,11,
AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines,"  In this paper, we present AISHELL-3, a large-scale and high-fidelitymulti-speaker Mandarin speech corpus which could be used to train multi-speakerText-to-Speech (TTS) systems. The corpus contains roughly 85 hours ofemotion-neutral recordings spoken by 218 native Chinese mandarin speakers.Their auxiliary attributes such as gender, age group and native accents areexplicitly marked and provided in the corpus. Accordingly, transcripts inChinese character-level and pinyin-level are provided along with therecordings. We present a baseline system that uses AISHELL-3 for multi-speakerMadarin speech synthesis. The multi-speaker speech synthesis system is anextension on Tacotron-2 where a speaker verification model and a correspondingloss regarding voice similarity are incorporated as the feedback constraint. Weaim to use the presented corpus to build a robust synthesis model that is ableto achieve zero-shot voice cloning. The system trained on this dataset alsogeneralizes well on speakers that are never seen in the training process.Objective evaluation results from our experiments show that the proposedmulti-speaker synthesis system achieves high voice similarity concerning bothspeaker embedding similarity and equal error rate measurement. The dataset,baseline system code and generated samples are available online.","Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, Ming Li",,,11,
A Time-domain Monaural Speech Enhancement with Feedback Learning,"  In this paper, we propose a type of neural network with feedback learning inthe time domain called FTNet for monaural speech enhancement, where theproposed network consists of three principal components. The first part iscalled stage recurrent neural network, which is introduced to effectivelyaggregate the deep feature dependencies across different stages with a memorymechanism and also remove the interference stage by stage. The second part isthe convolutional auto-encoder. The third part consists of a series ofconcatenated gated linear units, which are capable of facilitating theinformation flow and gradually increasing the receptive fields. Feedbacklearning is adopted to improve the parameter efficiency and therefore, thenumber of trainable parameters is effectively reduced without sacrificing itsperformance. Numerous experiments are conducted on TIMIT corpus andexperimental results demonstrate that the proposed network can achieveconsistently better performance in terms of both PESQ and STOI scores than twostate-of-the-art time domain-based baselines in different conditions.","Andong Li, Chengshi Zheng, Linjuan Cheng, Renhua Peng, Xiaodong Li",,,11,
Self-Supervised Representation Learning for Vocal Music Context,"  In music and speech, meaning is derived at multiple levels of context.Affect, for example, can be inferred both by a short sound token and by sonicpatterns over a longer temporal window such as an entire recording. In thispaper we focus on inferring meaning from this dichotomy of contexts. We showhow contextual representations of short sung vocal lines can be implicitlylearned from fundamental frequency ($F_0$) and thus be used as a meaningfulfeature space for downstream Music Information Retrieval (MIR) tasks. Wepropose three self-supervised deep learning paradigms which leverage pseudotasklearning of these two levels of context to produce latent representationspaces. We evaluate the usefulness of these representations by embedding unseenvocal contours into each space and conducting downstream classification tasks.Our results show that contextual representation can enhance downstreamclassification by as much as 15 % as compared to using traditional statisticalcontour features.","Camille Noufi and Prateek Verma, Jonathan Berger",,,11,
"Inaudible Adversarial Perturbations for Targeted Attack in Speaker
  Recognition","  Speaker recognition is a popular topic in biometric authentication and manydeep learning approaches have achieved extraordinary performances. However, ithas been shown in both image and speech applications that deep neural networksare vulnerable to adversarial examples. In this study, we aim to exploit thisweakness to perform targeted adversarial attacks against the x-vector basedspeaker recognition system. We propose to generate inaudible adversarialperturbations achieving targeted white-box attacks to speaker recognitionsystem based on the psychoacoustic principle of frequency masking.Specifically, we constrict the perturbation under the masking threshold oforiginal audio, instead of using a common l_p norm to measure theperturbations. Experiments on Aishell-1 corpus show that our approach yields upto 98.5% attack success rate to arbitrary gender speaker targets, whileretaining indistinguishable attribute to listeners. Furthermore, we alsoachieve an effective speaker attack when applying the proposed approach to acompletely irrelevant waveform, such as music.","Qing Wang, Pengcheng Guo, Lei Xie",,,11,
CURE Dataset: Ladder Networks for Audio Event Classification,"  Audio event classification is an important task for several applications suchas surveillance, audio, video and multimedia retrieval etc. There areapproximately 3M people with hearing loss who can't perceive events happeningaround them. This paper establishes the CURE dataset which contains curated setof specific audio events most relevant for people with hearing loss. We proposea ladder network based audio event classifier that utilizes 5s sound recordingsderived from the Freesound project. We adopted the state-of-the-artconvolutional neural network (CNN) embeddings as audio features for this task.We also investigate extreme learning machine (ELM) for event classification. Inthis study, proposed classifiers are compared with support vector machine (SVM)baseline. We propose signal and feature normalization that aims to reduce themismatch between different recordings scenarios. Firstly, CNN is trained onweakly labeled Audioset data. Next, the pre-trained model is adopted as featureextractor for proposed CURE corpus. We incorporate ESC-50 dataset as secondevaluation set. Results and discussions validate the superiority of Laddernetwork over ELM and SVM classifier in terms of robustness and increasedclassification accuracy. While Ladder network is robust to data mismatches,simpler SVM and ELM classifiers are sensitive to such mismatches, where theproposed normalization techniques can play an important role. Experimentalstudies with ESC-50 and CURE corpora elucidate the differences in datasetcomplexity and robustness offered by proposed approaches.","Harishchandra Dubey, Dimitra Emmanouilidou, Ivan J. Tashev",,,11,
"Metric Learning vs Classification for Disentangled Music Representation
  Learning","  Deep representation learning offers a powerful paradigm for mapping inputdata onto an organized embedding space and is useful for many music informationretrieval tasks. Two central methods for representation learning include deepmetric learning and classification, both having the same goal of learning arepresentation that can generalize well across tasks. Along withgeneralization, the emerging concept of disentangled representations is also ofgreat interest, where multiple semantic concepts (e.g., genre, mood,instrumentation) are learned jointly but remain separable in the learnedrepresentation space. In this paper we present a single representation learningframework that elucidates the relationship between metric learning,classification, and disentanglement in a holistic manner. For this, we (1)outline past work on the relationship between metric learning andclassification, (2) extend this relationship to multi-label data by exploringthree different learning approaches and their disentangled versions, and (3)evaluate all models on four tasks (training time, similarity retrieval,auto-tagging, and triplet prediction). We find that classification-based modelsare generally advantageous for training time, similarity retrieval, andauto-tagging, while deep metric learning exhibits better performance fortriplet-prediction. Finally, we show that our proposed approach yieldsstate-of-the-art results for music auto-tagging.","Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, Juhan Nam",,,11,
CGCNN: Complex Gabor Convolutional Neural Network on raw speech,"  Convolutional Neural Networks (CNN) have been used in Automatic SpeechRecognition (ASR) to learn representations directly from the raw signal insteadof hand-crafted acoustic features, providing a richer and lossless inputsignal. Recent researches propose to inject prior acoustic knowledge to thefirst convolutional layer by integrating the shape of the impulse responses inorder to increase both the interpretability of the learnt acoustic model, andits performances. We propose to combine the complex Gabor filter withcomplex-valued deep neural networks to replace usual CNN weights kernels, tofully take advantage of its optimal time-frequency resolution and of thecomplex domain. The conducted experiments on the TIMIT phoneme recognition taskshows that the proposed approach reaches top-of-the-line performances whileremaining interpretable.","Paul-Gauthier No\'e, Titouan Parcollet and Mohamed Morchid",,,11,
"A pairwise approach to simultaneous onset/offset detection for singing
  voice using correntropy","  In this paper, we propose a novelmethod to search for precise locations ofpaired note onset and offset in a singing voice signal. In comparison with theexisting onset detection algorithms,our approach differs in two key respects.First, we employ Correntropy, a generalized correlation function inspired fromReyni's entropy, as a detection function to capture the instantaneous fluxwhile preserving insensitiveness to outliers. Next, a novel peak pickingalgorithm is specially designed for this detection function. By calculating thefitness of a pre-defined inverse hyperbolic kernel to a detection function, itis possible to find an onset and its corresponding offset simultaneously.Experimental results show that the proposed method achieves performancesignificantly better than or comparable to other state-of-the-art techniquesfor onset detection in singing voice.",Sungkyun Chang and Kyogu Lee,,,11,
Quantum Pontryagin Principle under Continuous Measurements and Feedback,"  In this note we develop the theory of the quantum Pontryagin principle forcontinuous measurements and feedback. The analysis is carried out under theassumption of compatible events in the output channel. The plant is a quantumsystem, which generally is in a mixed state, coupled to a continuousmeasurement channel. The Pontryagin Maximum Principle is derived in both theSchr\""{o}dinger picture and Heisenberg picture, in particular in statisticalmoment coordinates. To avoid solving stochastic equations we derive a LQGscheme which is more suitable for control purposes.","Juan I. Mulero, Javier Molina-Vilaplana",,,11,
"A Game-Theoretic Approach to Decision Making for Multiple Vehicles at
  Roundabout","  In this paper, we study the decision making of multiple autonomous vehiclesat a roundabout. The behaviours of the vehicles depend on their aggressiveness,which indicates how much they value speed over safety. We propose a distributeddecision-making process that balances safety and speed of the vehicles. In theproposed process, each vehicle estimates other vehicles' aggressiveness andformulates the interactions among the vehicles as a finite sequential game.Based on the Nash equilibrium of this game, the vehicle predicts othervehicles' behaviours and makes decisions. We perform numerical simulations toillustrate the effectiveness of the proposed process, both for safety (absenceof collisions), and speed (time spent within the roundabout).","Sasinee Pruekprasert, J\'er\'emy Dubut, Xiaoyi Zhang, Chao Huang,
  Masako Kishida",,,11,
"Robust Cubature Kalman Filter for Dynamic State Estimation of
  Synchronous Machines under Unknown Measurement Noise Statistics","  Kalman-type filtering techniques including cubature Kalman filter (CKF) doesnot work well in non-Gaussian environments, especially in the presence ofoutliers. To solve this problem, Huber's M-estimation based robust CKF (RCKF)is proposed for synchronous machines by combining the Huber's M-estimationtheory with the classical CKF, which is capable of coping with thedeterioration in performance and discretization of tracking curves whenmeasurement noise statistics deviatefrom the prior noise statistics. Theproposed RCKF algorithm has good adaptability to unknown measurement noisestatistics characteristics including non-Gaussian measurement noise andoutliers. The simulation results on the WSCC 3-machine 9-bus system and NewEngland 16-machine 68-bus system verify the effectiveness of the proposedmethod and its advantage over the classical CKF.","Yang Li, Jing Li, Junjian Qi, Liang Chen",,,11,
"A new condition for stability of switched linear systems under
  restricted minimum dwell time switching","  We propose matrix commutator based stability characterization fordiscrete-time switched linear systems under restricted switching. Given anadmissible minimum dwell time, we identify sufficient conditions on subsystemssuch that a switched system is stable under all switching signals that obey thegiven restriction. The primary tool for our analysis is commutation relationsbetween the subsystem matrices. Our stability conditions are robust withrespect to small perturbations in the elements of these matrices. In case ofarbitrary switching (i.e., given minimum dwell time = 1), we recover the priorresult [1,Proposition 1] as a special case of our result.",Atreyee Kundu,,,11,
"Modeling and Quantifying the Impact of Wind Power Penetration on Power
  System Coherency","  This paper presents a mathematical analysis of how wind generation impactsthe coherency property of power systems. Coherency arises from time-scaleseparation in the dynamics of synchronous generators, where generator statesinside a coherent area synchronize over a fast time-scale due to strongercoupling, while the areas themselves synchronize over a slower time-scale dueto weaker coupling. This time-scale separation is reflected in the form of aspectral separation in the weighted Laplacian matrix describing the swingdynamics of the generators. However, when wind farms with doubly-fed inductiongenerators (DFIG) are integrated in the system then this Laplacian matrixchanges based on both the level of wind penetration and the location of thewind farms. The modified Laplacian changes the effective slow eigenspace of thegenerators. Depending on penetration level, this change may result in changingthe identities of the coherent areas. We develop a theoretical framework toquantify this modification, and validate our results with numerical simulationsof the IEEE 68-bus system with one and multiple wind farms. We compare ourmodel based results on clustering with results using measurement-basedprincipal component analysis to substantiate our derivations.","Sayak Mukherjee, Aranya Chakrabortty, Saman Babaei",,,11,
Differentiation and Passivity for Control of Brayton-Moser Systems,"  This paper deals with a class of Resistive-Inductive-Capacitive (RLC)circuits and switched RLC (s-RLC) circuits modeled in Brayton Moser framework.For this class of systems, new passivity properties using a Krasovskii's typeLyapunov function as storage function are presented. Consequently, thesupply-rate is a function of the system states, inputs and their firsttime-derivatives. Moreover, after showing the integrability property of theport-variables, two simple control methodologies called output shaping andinput shaping are proposed for regulating the voltage in RLC and s-RLCcircuits. Global asymptotic convergence to the desired operating point istheoretically proved for both proposed control methodologies. Moreover,robustness with respect to load uncertainty is ensured by the input shapingmethodology. The applicability of the proposed methodologies is illustrated bydesigning voltage controllers for DC-DC converters and DC networks.","Krishna Chaitanya Kosaraju, Michele Cucuzzella, Jacquelien M. A.
  Scherpen and Ramkrishna Pasumarthy",,,11,
Reachability analysis of linear hybrid systems via block decomposition,"  Reachability analysis aims at identifying states reachable by a system withina given time horizon. This task is known to be computationally expensive forlinear hybrid systems. Reachability analysis works by iteratively applyingcontinuous and discrete post operators to compute states reachable according tocontinuous and discrete dynamics, respectively. In this paper, we enhance bothof these operators and make sure that most of the involved computations areperformed in low-dimensional state space. In particular, we improve thecontinuous-post operator by performing computations in high-dimensional statespace only for time intervals relevant for the subsequent application of thediscrete-post operator. Furthermore, the new discrete-post operator performslow-dimensional computations by leveraging the structure of the guard andassignment of a considered transition. We illustrate the potential of ourapproach on a number of challenging benchmarks.","Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Kostiantyn Potomkin,
  Christian Schilling",,,11,
"A globally exponentially stable position observer for interior permanent
  magnet synchronous motors","  The design of a position observer for the interior permanent magnetsynchronous motor is a challenging problem that, in spite of many researchefforts, remained open for a long time. In this paper we present the firstglobally exponentially convergent solution to it, assuming that the saliency isnot too large. As expected in all observer tasks, a persistency of excitationcondition is imposed. Conditions on the operation of the motor, under which itis verified, are given. In particular, it is shown that at rotorstandstill---when the system is not observable---it is possible to inject aprobing signal to enforce the persistent excitation condition. {The highperformance of the proposed observer, in standstill and high speed regions, isverified by extensive series of test-runs on an experimental setup.","Romeo Ortega, Bowen Yi, Slobodan Vukosavic, Kwanghee Nam, Jongwon Choi",,,11,
"An indirect computational procedure for receding horizon hybrid optimal
  control","  In this work, solution of the finite horizon hybrid optimal control problemas the central element of the receding horizon optimal control (modelpredictive control) is investigated based on the indirect approach. Theresponse of a hybrid system within the prediction horizon is composed of bothdiscrete-valued sequences and continuous-valued time-trajectories. Given a costfunctional, the optimal continuous trajectories can be calculated given thediscrete sequences by the means of the recent results on the hybrid maximumprinciple. It is shown that these calculations reduce to solving a system ofalgebraic equations in the case of affine hybrid systems. Then, a branch andbound algorithm is proposed which determines both the discrete and continuouscontrol inputs by iterating on the discrete sequences. It is shown that thealgorithm finds the correct solution in a finite number of steps if theselected cost functional satisfies certain conditions. Efficiency of theproposed method is demonstrated during a case study through comparisons withthe main existing method.",Babak Tavassoli,,,11,
Selling Demand Response Using Options,"  Wholesale electricity markets in many jurisdictions use a two-settlementstructure: a day-ahead market for bulk power transactions and a real-timemarket for fine-grain supply-demand balancing. This paper explores tradingdemand response assets within this two-settlement market structure. We considertwo approaches for trading demand response assets: (a) an intermediate spotmarket with contingent pricing, and (b) an over-the-counter options contract.In the first case, we characterize the competitive equilibrium of the spotmarket, and show that it is socially optimal. Economic orthodoxy advocates spotmarkets, but these require expensive infrastructure and regulatory blessing. Inthe second case, we characterize competitive equilibria and compare itsefficiency with the idealized spot market. Options contract are privatebilateral over-the-counter transactions and do not require regulatory approval.We show that the optimal social welfare is, in general, not supported. We thendesign optimal option prices that minimize the social welfare gap. This optimaldesign serves to approximate the ideal spot market for demand response usingoptions with modest loss of efficiency. Our results are validated throughnumerical simulations.","Deepan Muthirayan, Dileep Kalathil, Sen Li, Kameshwar Poolla and
  Pravin Varaiya",,,11,
"Resilient consensus for multi-agent systems subject to differential
  privacy requirements","  We consider multi-agent systems interacting over directed network topologieswhere a subset of agents is adversary/faulty and where the non-faulty agentshave the goal of reaching consensus, while fulfilling a differential privacyrequirement on their initial conditions. To address this problem, we develop anupdate law for the non-faulty agents. Specifically, we propose a modificationof the so-called Mean-Subsequence-Reduced (MSR) algorithm, the DifferentiallyPrivate MSR (DP-MSR) algorithm, and characterize three important properties ofthe algorithm: correctness, accuracy and differential privacy. We show that ifthe network topology is $(2f +1)$-robust, then the algorithm allows thenon-faulty agents to reach consensus despite the presence of up to $f$ faultyagents and we characterize the accuracy of the algorithm. Furthermore, we alsoshow in two important cases that our distributed algorithm can be tuned toguarantees differential privacy of the initial conditions and the differentialprivacy requirement is related to the maximum network degree. The results areillustrated via simulations.",Davide Fiore and Giovanni Russo,,,11,
"Transitivity of Commutativity for Second-Order Linear Time-Varying
  Analog Systems","  After reviewing commutativity of second-order linear time-varying analogsystems, the inverse commutativity conditions are derived for these systems byconsidering non-zero initial conditions. On the base of these conditions, thetransitivity property is studied for second order linear time-varying unrelaxedanalog systems. It is proven that this property is always valid for suchsystems when their initial states are zero; when non-zero initial states arepresent, it is shown that the validity of transitivity does not require anymore conditions and it is still valid. Throughout the study it is assumed thatthe subsystems considered can not be obtained from each other by anyfeed-forward and feed-back structure. The results are well validated by MATLABsimulations.",Mehmet Emir Koksal,,,11,
"Event Detection and Localization in Distribution Grids with Phasor
  Measurement Units","  The recent introduction of synchrophasor technology into power distributionsystems has given impetus to various monitoring, diagnostic, and controlapplications, such as system identification and event detection, which arecrucial for restoring service, preventing outages, and managing equipmenthealth. Drawing on the existing framework for inferring topology andadmittances of a power network from voltage and current phasor measurements,this paper proposes an online algorithm for event detection and localization inunbalanced three-phase distribution systems. Using a convex relaxation and amatrix partitioning technique, the proposed algorithm is capable of identifyingtopology changes and attributing them to specific categories of events. Theperformance of this algorithm is evaluated on a standard test distributionfeeder with synthesized loads, and it is shown that a tripped line can bedetected and localized in an accurate and timely fashion, highlighting itspotential for real-world applications.","Omid Ardakanian, Ye Yuan, Roel Dobbe, Alexandra von Meier, Steven Low,
  Claire Tomlin",,,11,
"Spatiotemporal Arbitrage of Large-Scale Portable Energy Storage for Grid
  Congestion Relief","  Energy storage has great potential in grid congestion relief. By makinglarge-scale energy storage portable through trucking, its capability to addressgrid congestion can be greatly enhanced. This paper explores a business modelof large-scale portable energy storage for spatiotemporal arbitrage over nodeswith congestion. We propose a spatiotemporal arbitrage model to determine theoptimal operation and transportation schedules of portable storage. To validatethe business model, we simulate the schedules of a Tesla Semi full of TeslaPowerpack doing arbitrage over two nodes in California with local transmissioncongestion. The results indicate that the contributions of portable storage tocongestion relief are much greater than that of stationary storage, and thattrucking storage can bring net profit in energy arbitrage applications.","Guannan He, Da Zhang, Xidong Pi, Qixin Chen, Soummya Kar, and Jay
  Whitacre",,,11,
Diffusion multi-rate LMS algorithm for acoustic sensor networks,"  In this paper, we present a diffusion multi-rate least-mean-square (LMS)algorithm, named DMLMS, which is an effective solution for distributedestimation when two or more observation sequences are available with differentsampling rates. Then, we focus on a more practical application in the wirelessacoustic sensor networks (ASN). The filtered-x LMS (FxLMS) algorithm isextended to the distributed multi-rate system and it introduces collaborationbetween nodes following a diffusion strategy. Simulation results show that theeffectiveness of the proposed algorithms.","Lu Lu, Xiaomin Yang, Rongzhu Zhang",,,11,
Convex Relaxations of Chance Constrained AC Optimal Power Flow,"  High penetration of renewable energy sources and the increasing share ofstochastic loads require the explicit representation of uncertainty in toolssuch as the optimal power flow (OPF). Current approaches follow either alinearized approach or an iterative approximation of non-linearities. Thispaper proposes a semidefinite relaxation of a chance constrained AC-OPF whichis able to provide guarantees for global optimality. Using a piecewise affinepolicy, we can ensure tractability, accurately model large power deviations,and determine suitable corrective control policies for active power, reactivepower, and voltage. We state a tractable formulation for two types ofuncertainty sets. Using a scenario-based approach and making no priorassumptions about the probability distribution of the forecast errors, weobtain a robust formulation for a rectangular uncertainty set. Alternatively,assuming a Gaussian distribution of the forecast errors, we propose ananalytical reformulation of the chance constraints suitable for semidefiniteprogramming. We demonstrate the performance of our approach on the IEEE 24 and118 bus system using realistic day-ahead forecast data and obtain tightnear-global optimality guarantees.","Andreas Venzke, Lejla Halilbasic, Uros Markovic, Gabriela Hug, Spyros
  Chatzivasileiadis",,,11,
"Preventing wind turbine tower natural frequency excitation with a
  quasi-LPV model predictive control scheme","  With the ever increasing power rates of wind turbines, more advanced controltechniques are needed to facilitate tall towers that are low-weight and costeffective, but in effect more flexible. Such soft-soft tower configurationsgenerally have their fundamental side-side frequency in the below-ratedoperational domain. Because the turbine rotor practically has or develops amass imbalance over time, a periodic and rotor-speed dependent side-sideexcitation is present during below-rated operation. Persistent operation at thecoinciding tower and rotational frequency degrades the expected structural lifespan. To reduce this effect, earlier work has shown the effectiveness of activetower damping control strategies using collective pitch control. A more passiveapproach is frequency skipping by inclusion of speed exclusion zones, whichavoids prolonged operation near the critical frequency. However, neither of themethods incorporate a convenient way of performing a trade-off between energymaximization and fatigue load minimization. Therefore, this paper introduces aquasi-linear parameter varying model predictive control (qLPV-MPC) scheme,exploiting the beneficial (convex) properties of a qLPV system description. TheqLPV model is obtained by a demodulation transformation, and is subsequentlyaugmented with a simple wind turbine model. Results show the effectiveness ofthe algorithm in synthetic and realistic simulations using the NREL 5-MWreference wind turbine in high-fidelity simulation code. Prolonged rotor speedoperation at the tower side-side natural frequency is prevented, whereas whenthe trade-off is in favor of energy production, the algorithm decides torapidly pass over the natural frequency to attain higher rotor speeds and powerproductions.","Sebastiaan Paul Mulders, Tobias Gybel Hovgaard, Jacob Deleuran
  Grunnet, Jan-Willem van Wingerden",,,11,
"Prioritized Inverse Kinematics: Nonsmoothness, Trajectory Existence,
  Task Convergence, Stability","  In this paper, we study various theoretical properties of a class ofprioritized inverse kinematics (PIK) solutions that can be considered as aclass of (output regulation or tracking) control laws of a dynamical systemwith prioritized multiple outputs. We first develop tools to investigatenonsmoothness of PIK solutions and find a sufficient condition fornonsmoothness. It implies that existence and uniqueness of a joint trajectorysatisfying a PIK solution cannot be guaranteed by the classical theorems. So,we construct an alternative existence and uniqueness theorem that usesstructural information of PIK solutions. Then, we narrow the class of PIKsolutions down to the case that all tasks are designed to follow some desiredtask trajectories and discover a few properties related to task convergence.The study goes further to analyze stability of equilibrium points of thedifferential equation whose right hand side is a PIK solution when all tasksare designed to reach some desired task positions. Finally, we furnish anexample with a two-link manipulator that shows how our findings can be used toanalyze the behavior of a joint trajectory generated from a PIK solution.",Sang-ik An and Dongheui Lee,,,11,
Commutativity of Systems with their Feedback Conjugates,"  After introducing commutativity concept and summarizing the relevantliterature, this work is focused on the commutativity of feedback conjugates.It is already known that a linear time-varying differential system describing asingle input-single output dynamical system is always commutative with itsconstant gain feedback pairs. In this article, it is proven that among thetime-varying feedback conjugates of a linear time-varying system, constantfeedback conjugates are the only commutative feedback pairs and any of thetime-varying feedback conjugates cannot constitutes a commutative pair of alinear time-varying system.",Mehmet Emir Koksal,,,11,
"A Primal Decomposition Method with Suboptimality Bounds for Distributed
  Mixed-Integer Linear Programming","  In this paper we deal with a network of agents seeking to solve in adistributed way Mixed-Integer Linear Programs (MILPs) with a couplingconstraint (modeling a limited shared resource) and local constraints. MILPsare NP-hard problems and several challenges arise in a distributed framework,so that looking for suboptimal solutions is of interest. To achieve this goal,the presence of a linear coupling calls for tailored decomposition approaches.We propose a fully distributed algorithm based on a primal decompositionapproach and a suitable tightening of the coupling constraints. Agentsrepeatedly update local allocation vectors, which converge to an optimalresource allocation of an approximate version of the original problem. Based onsuch allocation vectors, agents are able to (locally) compute a mixed-integersolution, which is guaranteed to be feasible after a sufficiently large time.Asymptotic and finite-time suboptimality bounds are established for thecomputed solution. Numerical simulations highlight the efficacy of the proposedmethodology.","Andrea Camisa, Ivano Notarnicola, Giuseppe Notarstefano",,,11,
"Towards a Better Understanding of (Partial Weighted) MaxSAT Proof
  Systems","  MaxSAT, the optimization version of the well-known SAT problem, has attracteda lot of research interest in the last decade. Motivated by the many importantapplications and inspired by the success of modern SAT solvers, researchershave developed many MaxSAT solvers. Since most research is algorithmic, itssignificance is mostly evaluated empirically. In this paper we want to addressMaxSAT from the more formal point of view of Proof Complexity. With that aim westart providing basic definitions and proving some basic results. Then weanalyze the effect of adding split and virtual, two original inference rules,to MaxSAT resolution. We show that each addition makes the resulting proofsystem stronger, with the virtual rule capturing the recently proposed conceptof circular proof.",Javier Larrosa and Emma Rollon,,,11,
A Verified Packrat Parser Interpreter for Parsing Expression Grammars,"  Parsing expression grammars (PEGs) offer a natural opportunity for buildingverified parser interpreters based on higher-order parsing combinators. PEGsare expressive, unambiguous, and efficient to parse in a top-down recursivedescent style. We use the rich type system of the PVS specification languageand verification system to formalize the metatheory of PEGs and define areference implementation of a recursive parser interpreter for PEGs. In orderto ensure termination of parsing, we define a notion of a well-formed grammar.Rather than relying on an inductive definition of parsing, we use abstractsyntax trees that represent the computational trace of the parser to provide aneffective proof certificate for correct parsing and ensure that parsingproperties including soundness and completeness are maintained. The correctnessproperties are embedded in the types of the operations so that the proofs canbe easily constructed from local proof obligations. Building on the referenceparser interpreter, we define a packrat parser interpreter as well as anextension that is capable of semantic interpretation. Both these parserinterpreters are proved equivalent to the reference one. All of the parsers areexecutable. The proofs are formalized in mathematical terms so that similarparser interpreters can be defined in any specification language with a typesystem similar to PVS.",Clement Blaudeau and Natarajan Shankar,,,11,
On Equivalence of Infinitary Formulas under the Stable Model Semantics,"  Propositional formulas that are equivalent in intuitionistic logic, or in itsextension known as the logic of here-and-there, have the same stable models. Weextend this theorem to propositional formulas with infinitely long conjunctionsand disjunctions and show how to apply this generalization to provingproperties of aggregates in answer set programming. To appear in Theory andPractice of Logic Programming (TPLP).","Amelia Harrison, Vladimir Lifschitz, Miroslaw Truszczynski",,,11,
The Hidden Subgroup Problem for Universal Algebras,"  The Hidden Subgroup Problem (HSP) is a computational problem which includesas special cases integer factorization, the discrete logarithm problem, graphisomorphism, and the shortest vector problem. The celebrated polynomial-timequantum algorithms for factorization and the discrete logarithm are restrictedversions of a generic polynomial-time quantum solution to the HSP for abeliangroups, but despite focused research no full solution has yet been found. Wepropose a generalization of the HSP to include arbitrary algebraic structuresand analyze this new problem on powers of 2-element algebras. We prove acomplete classification of every such power as quantum tractable (i.e.polynomial-time), classically tractable, quantum intractable, and classicallyintractable. In particular, we identify a class of algebras for which thegeneralized HSP exhibits super-polynomial speedup on a quantum computercompared to a classical one.","Matthew Moore, Taylor Walenczyk",,,11,
"Bisimulation and bisimilarity for fuzzy description logics under the
  G\""odel semantics","  Description logics (DLs) are a suitable formalism for representing knowledgeabout domains in which objects are described not only by attributes but also bybinary relations between objects. Fuzzy extensions of DLs can be used for suchdomains when data and knowledge about them are vague and imprecise. One of thepossible ways to specify classes of objects in such domains is to use conceptsin fuzzy DLs. As DLs are variants of modal logics, indiscernibility in DLs ischaracterized by bisimilarity. The bisimilarity relation of an interpretationis the largest auto-bisimulation of that interpretation. In DLs and their fuzzyextensions, such equivalence relations can be used for concept learning. Inthis paper, we define and study fuzzy bisimulation and bisimilarity for fuzzyDLs under the G\""odel semantics, as well as crisp bisimulation and strongbisimilarity for such logics extended with involutive negation. The consideredlogics are fuzzy extensions of the DL $\mathcal{ALC}_{reg}$ (a variant of PDL)with additional features among inverse roles, nominals, (qualified orunqualified) number restrictions, the universal role, local reflexivity of arole and involutive negation. We formulate and prove results on invariance ofconcepts under fuzzy (resp. crisp) bisimulation, conditional invariance offuzzy TBoxex/ABoxes under bisimilarity (resp. strong bisimilarity), and theHennessy-Milner property of fuzzy (resp. crisp) bisimulation for fuzzy DLswithout (resp. with) involutive negation under the G\""odel semantics. Apartfrom these fundamental results, we also provide results on using fuzzybisimulation to separate the expressive powers of fuzzy DLs, as well as resultson using strong bisimilarity to minimize fuzzy interpretations.","Linh Anh Nguyen, Quang-Thuy Ha, Ngoc Thanh Nguyen, Thi Hong Khanh
  Nguyen, Thanh-Luong Tran",,,11,
LTLf Synthesis under Partial Observability: From Theory to Practice,"  LTL synthesis is the problem of synthesizing a reactive system from a formalspecification in Linear Temporal Logic. The extension of allowing for partialobservability, where the system does not have direct access to all relevantinformation about the environment, allows generalizing this problem to a widerset of real-world applications, but the difficulty of implementing such anextension in practice means that it has remained in the realm of theory.Recently, it has been demonstrated that restricting LTL synthesis to systemswith finite executions by using LTL with finite-horizon semantics (LTLf) allowsfor significantly simpler implementations in practice. With the conceptualsimplicity of LTLf, it becomes possible to explore extensions such as partialobservability in practice for the first time. Previous work has analyzed theproblem of LTLf synthesis under partial observability theoretically andsuggested two possible algorithms, one with 3EXPTIME and another with 2EXPTIMEcomplexity. In this work, we first prove a complexity lower bound conjecturedin earlier work. Then, we complement the theoretical analysis by showing howthe two algorithms can be integrated in practice into an established frameworkfor LTLf synthesis. We furthermore identify a third, MSO-based, approachenabled by this framework. Our experimental evaluation reveals very differentresults from what the theory seems to suggest, with the 3EXPTIME algorithmoften outperforming the 2EXPTIME approach. Furthermore, as long as it is ableto overcome an initial memory bottleneck, the MSO-based approach can oftenoutperforms the others.","Lucas M. Tabajara (Rice University), Moshe Y. Vardi (Rice University)",,,11,
A complete equational axiomatisation of partial differentiation,"  We formalise the well-known rules of partial differentiation in a version ofequational logic with function variables and binding constructs. We prove theresulting theory is complete with respect to polynomial interpretations. Theproof makes use of Severi's interpolation theorem that all multivariate Hermiteproblems are solvable. We also present a number of related results, such asdecidability and equational completeness.",Gordon D. Plotkin,,,11,
Why FHilb is Not an Interesting (Co)Differential Category,"  Differential categories provide an axiomatization of the basics ofdifferentiation and categorical models of differential linear logic. Asdifferentiation is an important tool throughout quantum mechanics and quantuminformation, it makes sense to study applications of the theory of differentialcategories to categorical quantum foundations. In categorical quantumfoundations, compact closed categories (and therefore traced symmetric monoidalcategories) are one of the main objects of study, in particular the category offinite-dimensional Hilbert spaces FHilb. In this paper, we will explain why theonly differential category structure on FHilb is the trivial one. This followsfrom a sort of in-compatibility between the trace of FHilb and possibledifferential category structure. That said, there are interesting non-trivialexamples of traced/compact closed differential categories, which we alsodiscuss.  The goal of this paper is to introduce differential categories to the broadercategorical quantum foundation community and hopefully open the door to furtherwork in combining these two fields. While the main result of this paper mayseem somewhat ""negative"" in achieving this goal, we discuss interestingpotential applications of differential categories to categorical quantumfoundations.",Jean-Simon Pacaud Lemay (University of OXford),,,11,
Iteration in ACL2,"  Iterative algorithms are traditionally expressed in ACL2 using recursion. Onthe other hand, Common Lisp provides a construct, loop, which -- like mostprogramming languages -- provides direct support for iteration. We describe anACL2 analogue loop$ of loop that supports efficient ACL2 programming andreasoning with iteration.","Matt Kaufmann (Univ. of Texas at Austin), J Strother Moore (Univ. of
  Texas at Austin)",,,11,
Taming denumerable Markov decision processes with decisiveness,"  Decisiveness has proven to be an elegant concept for denumerable Markovchains: it is general enough to encompass several natural classes ofdenumerable Markov chains, and is a sufficient condition for simple qualitativeand approximate quantitative model checking algorithms to exist. In this paper,we explore how to extend the notion of decisiveness to Markov decisionprocesses. Compared to Markov chains, the extra non-determinism can be resolvedin an adversarial or cooperative way, yielding two natural notions ofdecisiveness. We then explore whether these notions yield model checkingprocedures concerning the infimum and supremum probabilities of reachabilityproperties.","Nathalie Bertrand and Patricia Bouyer and Thomas Brihaye and Paulin
  Fournier",,,11,
"Relational Width of First-Order Expansions of Homogeneous Graphs with
  Bounded Strict Width","  Solving the algebraic dichotomy conjecture for constraint satisfactionproblems over structures first-order definable in countably infinite finitelybounded homogeneous structures requires understanding the applicability oflocal-consistency methods in this setting. We study the amount of consistency(measured by relational width) needed to solve CSP for first-order expansions Sof countably infinite homogeneous graphs that additionally have bounded strictwidth, i.e., for which establishing local consistency of an instance of the CSPnot only decides if there is a solution but also ensures that every solutionmay be obtained from a locally consistent instance by greedily assigning valuesto variables, without backtracking.  Our main result is that the structures S under consideration have relationalwidth exactly (2, L) where L is the maximal size of a forbidden subgraph of ahomogeneous graph under consideration, but not smaller than 3. It beats theupper bound (2m, 3m) where m = max(arity(S)+1, L, 3) and arity(S) is thelargest arity of a relation in S, which follows from a sufficient conditionimplying bounded relational width from the literature. Since L may bearbitrarily large, our result contrasts the collapse of the relational boundedwidth hierarchy for finite structures , whose relational width, if finite, isalways at most (2,3).",Micha{\l} Wrona,,,11,
Reversible Computation in Cyclic Petri Nets,"  Petri nets are a mathematical language for modeling and reasoning aboutdistributed systems. In this paper we propose an approach to Petri nets forembedding reversibility, i.e., the ability of reversing an executed sequence ofoperations at any point during operation. Specifically, we introduce machineryand associated semantics to support the three main forms of reversibilitynamely, backtracking, causal reversing, and out-of-causal-order reversing in avariation of cyclic Petri nets where tokens are persistent and aredistinguished from each other by an identity. Our formalism is influenced byapplications in biochemistry but the methodology can be applied to a wide rangeof problems that feature reversibility. In particular, we demonstrate theapplicability of our approach with a model of the ERK signalling pathway, anexample that inherently features reversible behavior.","Anna Philippou, Kyriaki Psara",,,11,
Dialectica Fuzzy Petri Nets,"  Brown and Gurr have introduced a model of Petri Nets that is based onde~Paiva's Dialectica categories. This model was refined in an unpublishedtechnical report, where Petri nets with multiplicities, instead of {\emelementary} nets (i.e., nets with multiplicities zero and one only) wereconsidered. In this note we expand this modelling to deal with {\em fuzzy}petri nets. The basic idea is to use as the dualizing object in the Dialecticacategories construction, the unit interval that has all the properties of a{\em lineale} structure.",Valeria de Paiva and Apostolos Syropoulos,,,11,
"Querying and Repairing Inconsistent Prioritized Knowledge Bases:
  Complexity Analysis and Links with Abstract Argumentation","  In this paper, we explore the issue of inconsistency handling overprioritized knowledge bases (KBs), which consist of an ontology, a set offacts, and a priority relation between conflicting facts. In the databasesetting, a closely related scenario has been studied and led to the definitionof three different notions of optimal repairs (global, Pareto, and completion)of a prioritized inconsistent database. After transferring the notions ofglobally-, Pareto- and completion-optimal repairs to our setting, we study thedata complexity of the core reasoning tasks: query entailment underinconsistency-tolerant semantics based upon optimal repairs, existence of aunique optimal repair, and enumeration of all optimal repairs. Our resultsprovide a nearly complete picture of the data complexity of these tasks forontologies formulated in common DL-Lite dialects. The second contribution ofour work is to clarify the relationship between optimal repairs and differentnotions of extensions for (set-based) argumentation frameworks. Among ourresults, we show that Pareto-optimal repairs correspond precisely to stableextensions (and often also to preferred extensions), and we propose a novelsemantics for prioritized KBs which is inspired by grounded extensions andenjoys favourable computational properties. Our study also yields some resultsof independent interest concerning preference-based argumentation frameworks.",Meghyn Bienvenu and Camille Bourgaux,,,11,
"Satisfiability and Query Answering in Description Logics with Global and
  Local Cardinality Constraints","  We introduce and investigate the expressive description logic (DL) ALCSCC++,in which the global and local cardinality constraints introduced in previouspapers can be mixed. On the one hand, we prove that this does not increase thecomplexity of satisfiability checking and other standard inference problems. Onthe other hand, the satisfiability problem becomes undecidable if inverse rolesare added to the languages. In addition, even without inverse roles,conjunctive query entailment in this DL turns out to be undecidable. We provethat decidability of querying can be regained if global and local constraintsare not mixed and the global constraints are appropriately restricted. Thelatter result is based on a locally-acyclic model construction, and it reducesquery entailment to ABox consistency in the restricted setting, i.e., to ABoxconsistency w.r.t. restricted cardinality constraints in ALCSCC, for which wecan show an ExpTime upper bound.","Franz Baader, Bartosz Bednarczyk and Sebastian Rudolph",,,11,
Coaxioms: flexible coinductive definitions by inference systems,"  We introduce a generalized notion of inference system to support moreflexible interpretations of recursive definitions. Besides axioms and inferencerules with the usual meaning, we allow also coaxioms, which are, intuitively,axioms which can only be applied ""at infinite depth"" in a proof tree. Coaxiomsallow us to interpret recursive definitions as fixed points which are notnecessarily the least, nor the greatest one, whose existence is guaranteed by asmooth extension of classical results. This notion nicely subsumes standardinference systems and their inductive and coinductive interpretation, thusallowing formal reasoning in cases where the inductive and coinductiveinterpretation do not provide the intended meaning, but are rather mixedtogether.",Francesco Dagnino,,,11,
A classical-logic view of a paraconsistent logic,"  This paper is concerned with the first-order paraconsistent logicLPQ$^{\supset,\mathsf{F}}$. A sequent-style natural deduction proof system forthis logic is given and, for this proof system, both a model-theoreticjustification and a logical justification by means of an embedding intofirst-order classical logic is presented. For no logic that is essentially thesame as LPQ$^{\supset,\mathsf{F}}$, a natural deduction proof system iscurrently available in the literature. The presented embedding provides both aclassical-logic explanation of this logic and a logical justification of itsproof system.",C. A. Middelburg,,,11,
"Efficient Trace Encodings of Bounded Synthesis for Asynchronous
  Distributed Systems","  The manual implementation of distributed systems is an error-prone taskbecause of the asynchronous interplay of components and the environment.Bounded synthesis automatically generates an implementation for thespecification of the distributed system if one exists. So far, boundedsynthesis for distributed systems does not utilize their asynchronous nature.Instead, concurrent behavior of components is encoded by all interleavings andonly then checked against the specification. We close this gap by identifyingtrue concurrency in synthesis of asynchronous distributed systems representedas Petri games. This defines when several interleavings can be subsumed by onetrue concurrent trace. Thereby, fewer and shorter verification problems have tobe solved in each iteration of the bounded synthesis algorithm. For Petrigames, experimental results show that our implementation using true concurrencyoutperforms the implementation based on checking all interleavings.",Jesko Hecking-Harbusch and Niklas O. Metzger,,,11,
Constructing Infinitary Quotient-Inductive Types,"  This paper introduces an expressive class of quotient-inductive types, calledQW-types. We show that in dependent type theory with uniqueness of identityproofs, even the infinitary case of QW-types can be encoded using thecombination of inductive-inductive definitions involving strictly positiveoccurrences of Hofmann-style quotient types, and Abel's size types. The latter,which provide a convenient constructive abstraction of what classically wouldbe accomplished with transfinite ordinals, are used to prove termination of therecursive definitions of the elimination and computation properties of ourencoding of QW-types. The development is formalized using the Agda theoremprover.","Marcelo Fiore, Andrew M. Pitts, and S. C. Steenkamp",,,11,
Minimal witnesses for probabilistic timed automata,"  Witnessing subsystems have proven to be a useful concept in the analysis ofprobabilistic systems, for example as diagnostic information on why a givenproperty holds or as input to refinement algorithms. This paper introduceswitnessing subsystems for reachability problems in probabilistic timed automata(PTA). Using a new operation on difference bounds matrices, it is shown howFarkas certificates of finite-state bisimulation quotients of a PTA can betranslated into witnessing subsystems. We present algorithms for thecomputation of minimal witnessing subsystems under three notions of minimality,which capture the timed behavior from different perspectives, and discuss theircomplexity.","Simon Jantsch, Florian Funke, Christel Baier",,,11,
Survey: Machine Learning in Production Rendering,"  In the past few years, machine learning-based approaches have had some greatsuccess for rendering animated feature films. This survey summarizes several ofthe most dramatic improvements in using deep neural networks over traditionalrendering methods, such as better image quality and lower computationaloverhead. More specifically, this survey covers the fundamental principles ofmachine learning and its applications, such as denoising, path guiding,rendering participating media, and other notoriously difficult light transportsituations. Some of these techniques have already been used in the latestreleased animations while others are still in the continuing development byresearchers in both academia and movie studios. Although learning-basedrendering methods still have some open issues, they have already demonstratedpromising performance in multiple parts of the rendering pipeline, and peopleare continuously making new attempts.",Shilin Zhu,,,11,
"Style-compatible Object Recommendation for Multi-room Indoor Scene
  Synthesis","  Traditional indoor scene synthesis methods often take a two-step approach:object selection and object arrangement. Current state-of-the-art objectselection approaches are based on convolutional neural networks (CNNs) and canproduce realistic scenes for a single room. However, they cannot be directlyextended to synthesize style-compatible scenes for multiple rooms withdifferent functions. To address this issue, we treat the object selectionproblem as combinatorial optimization based on a Labeled LDA (L-LDA) model. Wefirst calculate occurrence probability distribution of object categoriesaccording to a topic model, and then sample objects from each categoryconsidering their function diversity along with style compatibility, whileregarding not only separate rooms, but also associations among rooms. Userstudy shows that our method outperforms the baselines by incorporatingmulti-function and multi-room settings with style constraints, and sometimeseven produces plausible scenes comparable to those produced by professionaldesigners.","Yu He, Yun Cai, Yuan-Chen Guo, Zheng-Ning Liu, Shao-Kui Zhang,
  Song-Hai Zhang, Hong-Bo Fu, Sheng-Yong Chen",,,11,
A Discrete Probabilistic Approach to Dense Flow Visualization,"  Dense flow visualization is a popular visualization paradigm. Traditionally,the various models and methods in this area use a continuous formulation,resting upon the solid foundation of functional analysis. In this work, weexamine a discrete formulation of dense flow visualization. From probabilitytheory, we derive a similarity matrix that measures the similarity betweendifferent points in the flow domain, leading to the discovery of a whole newclass of visualization models. Using this matrix, we propose a novelvisualization approach consisting of the computation of spectral embeddings,i.e., characteristic domain maps, defined by particle mixture probabilities.These embeddings are scalar fields that give insight into the mixing processesof the flow on different scales. The approach of spectral embeddings is alreadywell studied in image segmentation, and we see that spectral embeddings areconnected to Fourier expansions and frequencies. We showcase the utility of ourmethod using different 2D and 3D flows.","Daniel Preu{\ss}, Tino Weinkauf, and Jens Kr\""uger",,,11,
SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform,"  Segmenting arbitrary 3D objects into constituent parts that are structurallymeaningful is a fundamental problem encountered in a wide range of computergraphics applications. Existing methods for 3D shape segmentation suffer fromcomplex geometry processing and heavy computation caused by using low-levelfeatures and fragmented segmentation results due to the lack of globalconsideration. We present an efficient method, called SEG-MAT, based on themedial axis transform (MAT) of the input shape. Specifically, with the richgeometrical and structural information encoded in the MAT, we are able todevelop a simple and principled approach to effectively identify the varioustypes of junctions between different parts of a 3D shape. Extensive evaluationsand comparisons show that our method outperforms the state-of-the-art methodsin terms of segmentation quality and is also one order of magnitude faster.","Cheng Lin, Lingjie Liu, Changjian Li, Leif Kobbelt, Bin Wang, Shiqing
  Xin, Wenping Wang",,,11,
Conversion Between Bezier and Catmull-Rom Splines,"  Splines are one of the main methods of mathematically representingcomplicated shapes, which have become the primary technique in computergraphics for modeling complex surfaces. Among all, Bezier and Catmull-Romsplines are of the most used in the subfields of engineering. In this document,we focus on conversion of splines rather than going through the properties ofthem, i.e, converting the control points of a spline to the control points ofanother spline, which results in approximately the same curve, as the splinebefore conversion represents.","Soroosh Tayebi Arasteh, Adam Kalisz",,,11,
A Survey on Patch-based Synthesis: GPU Implementation and Optimization,"  This thesis surveys the research in patch-based synthesis and algorithms forfinding correspondences between small local regions of images. We additionallyexplore a large kind of applications of this new fast randomized matchingtechnique. One of the algorithms we have studied in particular is PatchMatch,can find similar regions or ""patches"" of an image one to two orders ofmagnitude faster than previous techniques. The algorithmic program is driven byapplying mathematical properties of nearest neighbors in natural images. It isobserved that neighboring correspondences tend to be similar or ""coherent"" anduse this observation in algorithm in order to quickly converge to anapproximate solution. The algorithm is the most general form can find k-nearestneighbor matching, using patches that translate, rotate, or scale, usingarbitrary descriptors, and between two or more images. Speed-ups are obtainedover various techniques in an exceeding range of those areas. We have exploredmany applications of PatchMatch matching algorithm. In computer graphics, wehave explored removing unwanted objects from images, seamlessly moving objectsin images, changing image aspect ratios, and video summarization. In computervision we have explored denoising images, object detection, detecting imageforgeries, and detecting symmetries. We conclude by discussing the restrictionsof our algorithmic program, GPU implementation and areas for future analysis.",Hadi Abdi Khojasteh,,,11,
"Efficient Feature-based Image Registration by Mapping Sparsified
  Surfaces","  With the advancement in the digital camera technology, the use of highresolution images and videos has been widespread in the modern society. Inparticular, image and video frame registration is frequently applied incomputer graphics and film production. However, conventional registrationapproaches usually require long computational time for high resolution imagesand video frames. This hinders the application of the registration approachesin the modern industries. In this work, we first propose a new imagerepresentation method to accelerate the registration process by triangulatingthe images effectively. For each high resolution image or video frame, wecompute an optimal coarse triangulation which captures the important featuresof the image. Then, we apply a surface registration algorithm to obtain aregistration map which is used to compute the registration of the highresolution image. Experimental results suggest that our overall algorithm isefficient and capable to achieve a high compression rate while the accuracy ofthe registration is well retained when compared with the conventionalgrid-based approach. Also, the computational time of the registration issignificantly reduced using our triangulation-based approach.","Chun Pang Yung, Gary P. T. Choi, Ke Chen, Lok Ming Lui",,,11,
Optimized Processing of Localized Collisions in Projective Dynamics,"  We present a method for the efficient processing of contact and collision involumetric elastic models simulated using the Projective Dynamics paradigm. Ourapproach enables interactive simulation of tetrahedral meshes with more thanhalf a million elements, provided that the model satisfies two fundamentalproperties: the region of the model's surface that is susceptible to collisionevents needs to be known in advance, and the simulation degrees of freedomassociated with that surface region should be limited to a small fraction (e.g.5\%) of the total simulation nodes. Despite this conscious delineation ofscope, our hypotheses hold true for common animation subjects, such assimulated models of the human face and parts of the body. In such scenarios, apartial Cholesky factorization can abstract away the behavior of thecollision-safe subset of the face into the Schur Complement matrix with respectto the collision-prone region. We demonstrate how fast and accurate updates ofpenalty-based collision terms can be incorporated into this representation, andsolved with high efficiency on the GPU. We also demonstrate the opportunity toiterate a partial update of the element rotations, akin to a selectiveapplication of the local step, specifically on the smaller collision-proneregion without explicitly paying the cost associated with the rest of thesimulation mesh. We demonstrate efficient and robust interactive simulation indetailed models from animation and medical applications.","Qisi Wang, Yutian Tao, Eric Brandt, Court Cutting, and Eftychios
  Sifakis",,,11,
Modular Primitives for High-Performance Differentiable Rendering,"  We present a modular differentiable renderer design that yields performancesuperior to previous methods by leveraging existing, highly optimized hardwaregraphics pipelines. Our design supports all crucial operations in a moderngraphics pipeline: rasterizing large numbers of triangles, attributeinterpolation, filtered texture lookups, as well as user-programmable shadingand geometry processing, all in high resolutions. Our modular primitives allowcustom, high-performance graphics pipelines to be built directly withinautomatic differentiation frameworks such as PyTorch or TensorFlow. As amotivating application, we formulate facial performance capture as an inverserendering problem and show that it can be solved efficiently using our tools.Our results indicate that this simple and straightforward approach achievesexcellent geometric correspondence between rendered results and referenceimagery.","Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko
  Lehtinen, Timo Aila",,,11,
"A Bayesian Inference Framework for Procedural Material Parameter
  Estimation","  Procedural material models have been gaining traction in many applicationsthanks to their flexibility, compactness, and easy editability. We explore theinverse rendering problem of procedural material parameter estimation fromphotographs, presenting a unified view of the problem in a Bayesian framework.In addition to computing point estimates of the parameters by optimization, ourframework uses a Markov Chain Monte Carlo approach to sample the space ofplausible material parameters, providing a collection of plausible matches thata user can choose from, and efficiently handling both discrete and continuousmodel parameters. To demonstrate the effectiveness of our framework, we fitprocedural models of a range of materials---wall plaster, leather, wood,anisotropic brushed metals and layered metallic paints---to both synthetic andreal target images.","Yu Guo, Milos Hasan, Lingqi Yan, Shuang Zhao",,,11,
AnimePose: Multi-person 3D pose estimation and animation,"  3D animation of humans in action is quite challenging as it involves using ahuge setup with several motion trackers all over the person's body to track themovements of every limb. This is time-consuming and may cause the persondiscomfort in wearing exoskeleton body suits with motion sensors. In this work,we present a trivial yet effective solution to generate 3D animation ofmultiple persons from a 2D video using deep learning. Although significantimprovement has been achieved recently in 3D human pose estimation, most of theprior works work well in case of single person pose estimation and multi-personpose estimation is still a challenging problem. In this work, we firstlypropose a supervised multi-person 3D pose estimation and animation frameworknamely AnimePose for a given input RGB video sequence. The pipeline of theproposed system consists of various modules: i) Person detection andsegmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information forperson localization iv) Person trajectory prediction and human pose tracking.Our proposed system produces comparable results on previous state-of-the-art 3Dmulti-person pose estimation methods on publicly available datasets MuCo-3DHPand MuPoTS-3D datasets and it also outperforms previous state-of-the-art humanpose tracking methods by a significant margin of 11.7% performance gain on MOTAscore on Posetrack 2018 dataset.",Laxman Kumarapu and Prerana Mukherjee,,,11,
Iconify: Converting Photographs into Icons,"  In this paper, we tackle a challenging domain conversion task between photoand icon images. Although icons often originate from real object images (i.e.,photographs), severe abstractions and simplifications are applied to generateicon images by professional graphic designers. Moreover, there is no one-to-onecorrespondence between the two domains, for this reason we cannot use it as theground-truth for learning a direct conversion function. Since generativeadversarial networks (GAN) can undertake the problem of domain conversionwithout any correspondence, we test CycleGAN and UNIT to generate icons fromobjects segmented from photo images. Our experiments with several imagedatasets prove that CycleGAN learns sufficient abstraction and simplificationability to generate icon-like images.","Takuro Karamatsu, Gibran Benitez-Garcia, Keiji Yanai, Seiichi Uchida",,,11,
Complementary Dynamics,"  We present a novel approach to enrich arbitrary rig animations withelastodynamic secondary effects. Unlike previous methods which pit rigdisplacements and physical forces as adversaries against each other, weadvocate that physics should complement artists intentions. We proposeoptimizing for elastodynamic displacements in the subspace orthogonal todisplacements that can be created by the rig. This ensures that the additionaldynamic motions do not undo the rig animation. The complementary space is highdimensional, algebraically constructed without manual oversight, and capable ofrich high-frequency dynamics. Unlike prior tracking methods, we do not requireextra painted weights, segmentation into fixed and free regions or trackingclusters. Our method is agnostic to the physical model and plugs intonon-linear FEM simulations, geometric as-rigid-as-possible energies, ormass-spring models. Our method does not require a particular type of rig andadds secondary effects to skeletal animations, cage-based deformations, wiredeformers, motion capture data, and rigid-body simulations.","Jiayi Eris Zhang and Seungbae Bang and David I.W. Levin and Alec
  Jacobson",,,11,
"DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape
  Reconstruction","  We introduce a differential visual similarity metric to train deep neuralnetworks for 3D reconstruction, aimed at improving reconstruction quality. Themetric compares two 3D shapes by measuring distances between multi-view imagesdifferentiably rendered from the shapes. Importantly, the image-space distanceis also differentiable and measures visual similarity, rather than pixel-wisedistortion. Specifically, the similarity is defined by mean-squared errors overHardNet features computed from probabilistic keypoint maps of the comparedimages. Our differential visual shape similarity metric can be easily pluggedinto various 3D reconstruction networks, replacing their distortion-basedlosses, such as Chamfer or Earth Mover distances, so as to optimize the networkweights to produce reconstructions with better structural fidelity and visualquality. We demonstrate this both objectively, using well-known shape metricsfor retrieval and classification tasks that are independent from our newmetric, and subjectively through a perceptual study.","Jiongchao Jin, Akshay Gadi Patil, Zhang Xiong, Hao Zhang",,,11,
Neural Subdivision,"  This paper introduces Neural Subdivision, a novel framework for data-drivencoarse-to-fine geometry modeling. During inference, our method takes a coarsetriangle mesh as input and recursively subdivides it to a finer geometry byapplying the fixed topological updates of Loop Subdivision, but predictingvertex positions using a neural network conditioned on the local geometry of apatch. This approach enables us to learn complex non-linear subdivisionschemes, beyond simple linear averaging used in classical techniques. One ofour key contributions is a novel self-supervised training setup that onlyrequires a set of high-resolution meshes for learning network weights. For anytraining shape, we stochastically generate diverse low-resolutiondiscretizations of coarse counterparts, while maintaining a bijective mappingthat prescribes the exact target position of every new vertex during thesubdivision process. This leads to a very efficient and accurate loss functionfor conditional mesh generation, and enables us to train a method thatgeneralizes across discretizations and favors preserving the manifold structureof the output. During training we optimize for the same set of network weightsacross all local mesh patches, thus providing an architecture that is notconstrained to a specific input mesh, fixed genus, or category. Our networkencodes patch geometry in a local frame in a rotation- andtranslation-invariant manner. Jointly, these design choices enable our methodto generalize well, and we demonstrate that even when trained on a singlehigh-resolution mesh our method generates reasonable subdivisions for novelshapes.","Hsueh-Ti Derek Liu, Vladimir G. Kim, Siddhartha Chaudhuri, Noam
  Aigerman, Alec Jacobson",,,11,
Virtual Lenses as Embodied Tools for Immersive Analytics,"  Interactive lenses are useful tools for supporting the analysis of data indifferent ways. Most existing lenses are designed for 2D visualization and areoperated using standard mouse and keyboard interaction. On the other hand,research on virtual lenses for novel 3D immersive visualization environments isscarce. Our work aims to narrow this gap in the literature. We focusparticularly on the interaction with lenses. Inspired by natural interactionwith magnifying glasses in the real world, our lenses are designed as graspabletools that can be created and removed as needed, manipulated and parameterizeddepending on the task, and even combined to flexibly create new views on thedata. We implemented our ideas in a system for the visual analysis of 3D sonardata. Informal user feedback from more than a hundred people suggests that thedesigned lens interaction is easy to use for the task of finding a hidden wreckin sonar data.","Sven Kluge and Stefan Gladisch and Uwe Freiherr von Lukas and Oliver
  Staadt and Christian Tominski",,,11,
Structure and Design of HoloGen,"  Increasing popularity of augmented and mixed reality systems has seen asimilar increase of interest in 2D and 3D computer generated holography (CGH).Unlike stereoscopic approaches, CGH can fully represent a light field includingdepth of focus, accommodation and vergence. Along with existingtelecommunications, imaging, projection, lithography, beam shaping and opticaltweezing applications, CGH is an exciting technique applicable to a wide arrayof photonic problems including full 3D representation. Traditionally, theprimary roadblock to acceptance has been the significant numerical processingrequired to generate holograms requiring both significant expertise andsignificant computational power. This article discusses the structure anddesign of HoloGen. HoloGen is an MIT licensed application that may be used togenerate holograms using a wide array of algorithms without expert guidance.HoloGen uses a Cuda C and C++ backend with a C# and Windows PresentationFramework graphical user interface. The article begins by introducing HoloGenbefore providing an in-depth discussion of its design and structure. Particularfocus is given to the communication, data transfer and algorithmic aspects.",Peter J. Christopher and Timothy D. Wilkinson,,,11,
Visualization of Human Spine Biomechanics for Spinal Surgery,"  We propose a visualization application, designed for the exploration of humanspine simulation data. Our goal is to support research in biomechanical spinesimulation and advance efforts to implement simulation-backed analysis insurgical applications. Biomechanical simulation is a state-of-the-art techniquefor analyzing load distributions of spinal structures. Through the inclusion ofpatient-specific data, such simulations may facilitate personalized treatmentand customized surgical interventions. Difficulties in spine modelling andsimulation can be partly attributed to poor result representation, which mayalso be a hindrance when introducing such techniques into a clinicalenvironment. Comparisons of measurements across multiple similar anatomicalstructures and the integration of temporal data make commonly availablediagrams and charts insufficient for an intuitive and systematic display ofresults. Therefore, we facilitate methods such as multiple coordinated views,abstraction and focus and context to display simulation outcomes in a dedicatedtool. By linking the result data with patient-specific anatomy, we makerelevant parameters tangible for clinicians. Furthermore, we introduce newconcepts to show the directions of impact force vectors, which were notaccessible before. We integrated our toolset into a spine segmentation andsimulation pipeline and evaluated our methods with both surgeons andbiomechanical researchers. When comparing our methods against standardrepresentations that are currently in use, we found increases in accuracy andspeed in data exploration tasks. In a qualitative review, domain experts deemedthe tool highly useful when dealing with simulation result data, whichtypically combines time-dependent patient movement and the resulting forcedistributions on spinal structures.","Pepe Eulzer, Sabine Bauer, Francis Kilian, Kai Lawonn",,,11,
Deep No-reference Tone Mapped Image Quality Assessment,"  The process of rendering high dynamic range (HDR) images to be viewed onconventional displays is called tone mapping. However, tone mapping introducesdistortions in the final image which may lead to visual displeasure. Toquantify these distortions, we introduce a novel no-reference qualityassessment technique for these tone mapped images. This technique is composedof two stages. In the first stage, we employ a convolutional neural network(CNN) to generate quality aware maps (also known as distortion maps) from tonemapped images by training it with the ground truth distortion maps. In thesecond stage, we model the normalized image and distortion maps using anAsymmetric Generalized Gaussian Distribution (AGGD). The parameters of the AGGDmodel are then used to estimate the quality score using support vectorregression (SVR). We show that the proposed technique delivers competitiveperformance relative to the state-of-the-art techniques. The novelty of thiswork is its ability to visualize various distortions as quality maps(distortion maps), especially in the no-reference setting, and to use thesemaps as features to estimate the quality score of tone mapped images.","Chandra Sekhar Ravuri (1), Rajesh Sureddi (2), Sathya Veera Reddy
  Dendi (2), Shanmuganathan Raman (1), Sumohana S. Channappayya (2) ((1)
  Department of Electrical Engineering, Indian Institute of Technology
  Gandhinagar, India., (2) Department of Electrical Engineering, Indian
  Institute of Technology Hyderabad, India.)",,,11,
"Scenior: An Immersive Visual Scripting system based on VR Software
  Design Patterns for Experiential Training","  Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumerproduct, and training on simulators is rapidly becoming standard in manyindustrial sectors. However, the available systems are either focusing ongaming context, featuring limited capabilities or they support only contentcreation of virtual environments without any rapid prototyping andmodification. In this project, we propose a code-free, visual scriptingplatform to replicate gamified training scenarios through rapid prototyping andVR software design patterns. We implemented and compared two authoring tools:a) visual scripting and b) VR editor for the rapid reconstruction of VRtraining scenarios. Our visual scripting module is capable to generate trainingapplications utilizing a node-based scripting system whereas the VR editorgives user/developer the ability to customize and populate new VR trainingscenarios directly from the virtual environment. We also introduce actionprototypes, a new software design pattern suitable to replicate behavioraltasks for VR experiences. In addition, we present the training scenegrapharchitecture as the main model to represent training scenarios on a modular,dynamic and highly adaptive acyclic graph based on a structured educationalcurriculum. Finally, a user-based evaluation of the proposed solution indicatedthat users - regardless of their programming expertise - can effectively usethe tools to create and modify training scenarios in VR.","Paul Zikas, George Papagiannakis, Nick Lydatakis, Steve Kateros,
  Stavroula Ntoa, Ilia Adami, Constantine Stephanidis",,,11,
"Formal derivation of Mesh Neural Networks with their Forward-Only
  gradient Propagation","  This paper proposes the Mesh Neural Network (MNN), a novel architecture whichallows neurons to be connected in any topology, to efficiently routeinformation. In MNNs, information is propagated between neurons throughout astate transition function. State and error gradients are then directly computedfrom state updates without backward computation. The MNN architecture and theerror propagation schema is formalized and derived in tensor algebra. Theproposed computational model can fully supply a gradient descent process, andis suitable for very large scale NNs, due to its expressivity and trainingefficiency, with respect to NNs based on back-propagation and computationalgraphs.","Federico A. Galatolo, Mario G.C.A. Cimino, Gigliola Vaglini",,,11,
Training Sensitivity in Graph Isomorphism Network,"  Graph neural network (GNN) is a popular tool to learn the lower-dimensionalrepresentation of a graph. It facilitates the applicability of machine learningtasks on graphs by incorporating domain-specific features. There are variousoptions for underlying procedures (such as optimization functions, activationfunctions, etc.) that can be considered in the implementation of GNN. However,most of the existing tools are confined to one approach without any analysis.Thus, this emerging field lacks a robust implementation ignoring the highlyirregular structure of the real-world graphs. In this paper, we attempt to fillthis gap by studying various alternative functions for a respective moduleusing a diverse set of benchmark datasets. Our empirical results suggest thatthe generally used underlying techniques do not always perform well to capturethe overall structure from a set of graphs.",Md. Khaledur Rahman,,,11,
ADAIL: Adaptive Adversarial Imitation Learning,"  We present the ADaptive Adversarial Imitation Learning (ADAIL) algorithm forlearning adaptive policies that can be transferred between environments ofvarying dynamics, by imitating a small number of demonstrations collected froma single source domain. This is an important problem in robotic learningbecause in real world scenarios 1) reward functions are hard to obtain, 2)learned policies from one domain are difficult to deploy in another due tovarying source to target domain statistics, 3) collecting expert demonstrationsin multiple environments where the dynamics are known and controlled is ofteninfeasible. We address these constraints by building upon recent advances inadversarial imitation learning; we condition our policy on a learned dynamicsembedding and we employ a domain-adversarial loss to learn a dynamics-invariantdiscriminator. The effectiveness of our method is demonstrated on simulatedcontrol tasks with varying environment dynamics and the learned adaptive agentoutperforms several recent baselines.","Yiren Lu, Jonathan Tompson",,,11,
"DAGs with No Fears: A Closer Look at Continuous Optimization for
  Learning Bayesian Networks","  This paper re-examines a continuous optimization framework dubbed NOTEARS forlearning Bayesian networks. We first generalize existing algebraiccharacterizations of acyclicity to a class of matrix polynomials. Next,focusing on a one-parameter-per-edge setting, it is shown that theKarush-Kuhn-Tucker (KKT) optimality conditions for the NOTEARS formulationcannot be satisfied except in a trivial case, which explains a behavior of theassociated algorithm. We then derive the KKT conditions for an equivalentreformulation, show that they are indeed necessary, and relate them to explicitconstraints that certain edges be absent from the graph. If the score functionis convex, these KKT conditions are also sufficient for local minimalitydespite the non-convexity of the constraint. Informed by the KKT conditions, alocal search post-processing algorithm is proposed and shown to substantiallyand universally improve the structural Hamming distance of all testedalgorithms, typically by a factor of 2 or more. Some combinations with localsearch are both more accurate and more efficient than the original NOTEARS.","Dennis Wei, Tian Gao, Yue Yu",,,11,
"Safe Imitation Learning via Fast Bayesian Reward Inference from
  Preferences","  Bayesian reward learning from demonstrations enables rigorous safety anduncertainty analysis when performing imitation learning. However, Bayesianreward learning methods are typically computationally intractable for complexcontrol problems. We propose Bayesian Reward Extrapolation (Bayesian REX), ahighly efficient Bayesian reward learning algorithm that scales tohigh-dimensional imitation learning problems by pre-training a low-dimensionalfeature encoding via self-supervised tasks and then leveraging preferences overdemonstrations to perform fast Bayesian inference. Bayesian REX can learn toplay Atari games from demonstrations, without access to the game score and cangenerate 100,000 samples from the posterior over reward functions in only 5minutes on a personal laptop. Bayesian REX also results in imitation learningperformance that is competitive with or better than state-of-the-art methodsthat only learn point estimates of the reward function. Finally, Bayesian REXenables efficient high-confidence policy evaluation without having access tosamples of the reward function. These high-confidence performance bounds can beused to rank the performance and risk of a variety of evaluation policies andprovide a way to detect reward hacking behaviors.","Daniel S. Brown, Russell Coleman, Ravi Srinivasan, Scott Niekum",,,11,
"A Quantitative Perspective on Values of Domain Knowledge for Machine
  Learning","  With the exploding popularity of machine learning, domain knowledge invarious forms has been playing a crucial role in improving the learningperformance, especially when training data is limited. Nonetheless, there islittle understanding of to what extent domain knowledge can affect a machinelearning task from a quantitative perspective. To increase the transparency andrigorously explain the role of domain knowledge in machine learning, we studythe problem of quantifying the values of domain knowledge in terms of itscontribution to the learning performance in the context of informed machinelearning. We propose a quantification method based on Shapley value that fairlyattributes the overall learning performance improvement to different domainknowledge. We also present Monte-Carlo sampling to approximate the fair valueof domain knowledge with a polynomial time complexity. We run experiments ofinjecting symbolic domain knowledge into semi-supervised learning tasks on bothMNIST and CIFAR10 datasets, providing quantitative values of different symbolicknowledge and rigorously explaining how it affects the machine learningperformance in terms of test accuracy.","Jianyi Yang, Shaolei Ren",,,11,
On Infinite-Width Hypernetworks,"  {\em Hypernetworks} are architectures that produce the weights of atask-specific {\em primary network}. A notable application of hypernetworks inthe recent literature involves learning to output functional representations.In these scenarios, the hypernetwork learns a representation corresponding tothe weights of a shallow MLP, which typically encodes shape or imageinformation. While such representations have seen considerable success inpractice, they remain lacking in the theoretical guarantees in the wide regimeof the standard architectures. In this work, we study wide over-parameterizedhypernetworks. We show that unlike typical architectures, infinitely widehypernetworks do not guarantee convergence to a global minima under gradientdescent. We further show that convexity can be achieved by increasing thedimensionality of the hypernetwork's output, to represent wide MLPs. In thedually infinite-width regime, we identify the functional priors of thesearchitectures by deriving their corresponding GP and NTK kernels, the latter ofwhich we refer to as the {\em hyperkernel}. As part of this study, we make amathematical contribution by deriving tight bounds on high order Taylorexpansion terms of standard fully connected ReLU networks.","Etai Littwin, Tomer Galanti, Lior Wolf, Greg Yang",,,11,
"Towards Maximizing the Representation Gap between In-Domain \&
  Out-of-Distribution Examples","  Among existing uncertainty estimation approaches, Dirichlet Prior Network(DPN) distinctly models different predictive uncertainty types. However, forin-domain examples with high data uncertainties among multiple classes, even aDPN model often produces indistinguishable representations from theout-of-distribution (OOD) examples, compromising their OOD detectionperformance. We address this shortcoming by proposing a novel loss function forDPN to maximize the \textit{representation gap} between in-domain and OODexamples. Experimental results demonstrate that our proposed approachconsistently improves OOD detection performance.",Jay Nandy and Wynne Hsu and Mong Li Lee,,,11,
The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits,"  We consider a decentralized multi-agent Multi Armed Bandit (MAB) setupconsisting of $N$ agents, solving the same MAB instance to minimize individualcumulative regret. In our model, agents collaborate by exchanging messagesthrough pairwise gossip style communications on an arbitrary connected graph.We develop two novel algorithms, where each agent only plays from a subset ofall the arms. Agents use the communication medium to recommend only arm-IDs(not samples), and thus update the set of arms from which they play. Weestablish that, if agents communicate $\Omega(\log(T))$ times through anyconnected pairwise gossip mechanism, then every agent's regret is a factor oforder $N$ smaller compared to the case of no collaborations. Furthermore, weshow that the communication constraints only have a second order effect on theregret of our algorithm. We then analyze this second order term of the regretto derive bounds on the regret-communication tradeoffs. Finally, we empiricallyevaluate our algorithm and conclude that the insights are fundamental and notartifacts of our bounds. We also show a lower bound which gives that the regretscaling obtained by our algorithm cannot be improved even in the absence of anycommunication constraints. Our results thus demonstrate that even a minimallevel of collaboration among agents greatly reduces regret for all agents.","Ronshee Chawla, Abishek Sankararaman, Ayalvadi Ganesh, Sanjay
  Shakkottai",,,11,
"Unsupervised Domain Adaptation Through Transferring both the
  Source-Knowledge and Target-Relatedness Simultaneously","  Unsupervised domain adaptation (UDA) is an emerging research topic in thefield of machine learning and pattern recognition, which aims to help thelearning of unlabeled target domain by transferring knowledge from the sourcedomain. To perform UDA, a variety of methods have been proposed, most of whichconcentrate on the scenario of single source and single target domain (1S1T).However, in real applications, usually single source domain with multipletarget domains are involved (1SmT), which cannot be handled directly by those1S1T models. Unfortunately, although a few related works on 1SmT UDA have beenproposed, nearly none of them model the source domain knowledge and leveragethe target-relatedness jointly. To overcome these shortcomings, we hereinpropose a more general 1SmT UDA model through transferring both theSource-Knowledge and Target-Relatedness, UDA-SKTR for short. In this way, notonly the supervision knowledge from the source domain, but also the potentialrelatedness among the target domains are simultaneously modeled forexploitation in the process of 1SmT UDA. In addition, we construct analternating optimization algorithm to solve the variables of the proposed modelwith convergence guarantee. Finally, through extensive experiments on bothbenchmark and real datasets, we validate the effectiveness and superiority ofthe proposed method.","Qing Tian, Chuang Ma, Meng Cao, Songcan Chen",,,11,
"Architectural configurations, atlas granularity and functional
  connectivity with diagnostic value in Autism Spectrum Disorder","  Currently, the diagnosis of Autism Spectrum Disorder (ASD) is dependent upona subjective, time-consuming evaluation of behavioral tests by an expertclinician. Non-invasive functional MRI (fMRI) characterizes brain connectivityand may be used to inform diagnoses and democratize medicine. However,successful construction of deep learning models from fMRI requires addressingkey choices about the model's architecture, including the number of layers andnumber of neurons per layer. Meanwhile, deriving functional connectivity (FC)features from fMRI requires choosing an atlas with an appropriate level ofgranularity. Once a model has been built, it is vital to determine whichfeatures are predictive of ASD and if similar features are learned across atlasgranularity levels. To identify aptly suited architectural configurations,probability distributions of the configurations of high versus low performingmodels are compared. To determine the effect of atlas granularity, connectivityfeatures are derived from atlases with 3 levels of granularity and importantfeatures are ranked with permutation feature importance. Results show thehighest performing models use between 2-4 hidden layers and 16-64 neurons perlayer, granularity dependent. Connectivity features identified as importantacross all 3 atlas granularity levels include FC to the supplementary motorgyrus and language association cortex, regions associated with deficits insocial and sensory processing in ASD. Importantly, the cerebellum, often notincluded in functional analyses, is also identified as a region whose abnormalconnectivity is highly predictive of ASD. Results of this study identifyimportant regions to include in future studies of ASD, help assist in theselection of network architectures, and help identify appropriate levels ofgranularity to facilitate the development of accurate diagnostic models of ASD.","Cooper J. Mellema, Alex Treacher, Kevin P. Nguyen, Albert Montillo",,,11,
Data Poisoning Attacks Against Federated Learning Systems,"  Federated learning (FL) is an emerging paradigm for distributed training oflarge-scale deep neural networks in which participants' data remains on theirown devices with only model updates being shared with a central server.However, the distributed nature of FL gives rise to new threats caused bypotentially malicious participants. In this paper, we study targeted datapoisoning attacks against FL systems in which a malicious subset of theparticipants aim to poison the global model by sending model updates derivedfrom mislabeled data. We first demonstrate that such data poisoning attacks cancause substantial drops in classification accuracy and recall, even with asmall percentage of malicious participants. We additionally show that theattacks can be targeted, i.e., they have a large negative impact only onclasses that are under attack. We also study attack longevity in early/lateround training, the impact of malicious participant availability, and therelationships between the two. Finally, we propose a defense strategy that canhelp identify malicious participants in FL to circumvent poisoning attacks, anddemonstrate its effectiveness.","Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu",,,11,
Making Online Sketching Hashing Even Faster,"  Data-dependent hashing methods have demonstrated good performance in variousmachine learning applications to learn a low-dimensional representation fromthe original data. However, they still suffer from several obstacles: First,most of existing hashing methods are trained in a batch mode, yieldinginefficiency for training streaming data. Second, the computational cost andthe memory consumption increase extraordinarily in the big data setting, whichperplexes the training procedure. Third, the lack of labeled data hinders theimprovement of the model performance. To address these difficulties, we utilizeonline sketching hashing (OSH) and present a FasteR Online Sketching Hashing(FROSH) algorithm to sketch the data in a more compact form via an independenttransformation. We provide theoretical justification to guarantee that ourproposed FROSH consumes less time and achieves a comparable sketching precisionunder the same memory cost of OSH. We also extend FROSH to its distributedimplementation, namely DFROSH, to further reduce the training time cost ofFROSH while deriving the theoretical bound of the sketching precision. Finally,we conduct extensive experiments on both synthetic and real datasets todemonstrate the attractive merits of FROSH and DFROSH.","Xixian Chen, Haiqin Yang, Shenglin Zhao, Michael R. Lyu, and Irwin
  King",,,11,
"Unsupervised Cross-domain Image Classification by Distance Metric Guided
  Feature Alignment","  Learning deep neural networks that are generalizable across different domainsremains a challenge due to the problem of domain shift. Unsupervised domainadaptation is a promising avenue which transfers knowledge from a source domainto a target domain without using any labels in the target domain. Contemporarytechniques focus on extracting domain-invariant features using domainadversarial training. However, these techniques neglect to learn discriminativeclass boundaries in the latent representation space on a target domain andyield limited adaptation performance. To address this problem, we proposedistance metric guided feature alignment (MetFA) to extract discriminative aswell as domain-invariant features on both source and target domains. Theproposed MetFA method explicitly and directly learns the latent representationwithout using domain adversarial training. Our model integrates classdistribution alignment to transfer semantic knowledge from a source domain to atarget domain. We evaluate the proposed method on fetal ultrasound datasets forcross-device image classification. Experimental results demonstrate that theproposed method outperforms the state-of-the-art and enables modelgeneralization.",Qingjie Meng and Daniel Rueckert and Bernhard Kainz,,,11,
REMIND Your Neural Network to Prevent Catastrophic Forgetting,"  People learn throughout life. However, incrementally updating conventionalneural networks leads to catastrophic forgetting. A common remedy is replay,which is inspired by how the brain consolidates memory. Replay involvesfine-tuning a network on a mixture of new and old instances. While there isneuroscientific evidence that the brain replays compressed memories, existingmethods for convolutional networks replay raw images. Here, we propose REMIND,a brain-inspired approach that enables efficient replay with compressedrepresentations. REMIND is trained in an online manner, meaning it learns oneexample at a time, which is closer to how humans learn. Under the sameconstraints, REMIND outperforms other methods for incremental class learning onthe ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data orderingschemes known to induce catastrophic forgetting. We demonstrate REMIND'sgenerality by pioneering online learning for Visual Question Answering (VQA).","Tyler L. Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya,
  Christopher Kanan",,,11,
The Value of Nullspace Tuning Using Partial Label Information,"  In semi-supervised learning, information from unlabeled examples is used toimprove the model learned from labeled examples. But in some learning problems,partial label information can be inferred from otherwise unlabeled examples andused to further improve the model. In particular, partial label informationexists when subsets of training examples are known to have the same label, eventhough the label itself is missing. By encouraging a model to give the samelabel to all such examples, we can potentially improve its performance. We callthis encouragement \emph{Nullspace Tuning} because the difference vectorbetween any pair of examples with the same label should lie in the nullspace ofa linear model. In this paper, we investigate the benefit of using partiallabel information using a careful comparison framework over well-characterizedpublic datasets. We show that the additional information provided by partiallabels reduces test error over good semi-supervised methods usually by a factorof 2, up to a factor of 5.5 in the best case. We also show that addingNullspace Tuning to the newer and state-of-the-art MixMatch method decreasesits test error by up to a factor of 1.8.","Colin B. Hansen, Vishwesh Nath, Diego A. Mesa, Yuankai Huo, Bennett A.
  Landman, Thomas A. Lasko",,,11,
"Generalization Guarantees for Sparse Kernel Approximation with Entropic
  Optimal Features","  Despite their success, kernel methods suffer from a massive computationalcost in practice. In this paper, in lieu of commonly used kernel expansion withrespect to $N$ inputs, we develop a novel optimal design maximizing the entropyamong kernel features. This procedure results in a kernel expansion withrespect to entropic optimal features (EOF), improving the data representationdramatically due to features dissimilarity. Under mild technical assumptions,our generalization bound shows that with only $O(N^{\frac{1}{4}})$ features(disregarding logarithmic factors), we can achieve the optimal statisticalaccuracy (i.e., $O(1/\sqrt{N})$). The salient feature of our design is itssparsity that significantly reduces the time and space cost. Our numericalexperiments on benchmark datasets verify the superiority of EOF over thestate-of-the-art in kernel approximation.","Liang Ding, Rui Tuo, Shahin Shahrampour",,,11,
A Sharp Analysis of Model-based Reinforcement Learning with Self-Play,"  Model-based algorithms---algorithms that decouple learning of the model andplanning given the model---are widely used in reinforcement learning practiceand theoretically shown to achieve optimal sample efficiency for single-agentreinforcement learning in Markov Decision Processes (MDPs). However, formulti-agent reinforcement learning in Markov games, the current best knownsample complexity for model-based algorithms is rather suboptimal and comparesunfavorably against recent model-free approaches. In this paper, we present asharp analysis of model-based self-play algorithms for multi-agent Markovgames. We design an algorithm \emph{Optimistic Nash Value Iteration} (Nash-VI)for two-player zero-sum Markov games that is able to output an$\epsilon$-approximate Nash policy in $\tilde{\mathcal{O}}(H^3SAB/\epsilon^2)$episodes of game playing, where $S$ is the number of states, $A,B$ are thenumber of actions for the two players respectively, and $H$ is the horizonlength. This is the first algorithm that matches the information-theoreticlower bound $\Omega(H^3S(A+B)/\epsilon^2)$ except for a $\min\{A,B\}$ factor,and compares favorably against the best known model-free algorithm if$\min\{A,B\}=o(H^3)$. In addition, our Nash-VI outputs a single Markov policywith optimality guarantee, while existing sample-efficient model-freealgorithms output a nested mixture of Markov policies that is in generalnon-Markov and rather inconvenient to store and execute. We further adapt ouranalysis to designing a provably efficient task-agnostic algorithm for zero-sumMarkov games, and designing the first line of provably sample-efficientalgorithms for multi-player general-sum Markov games.","Qinghua Liu, Tiancheng Yu, Yu Bai, Chi Jin",,,11,
Towards an Adversarially Robust Normalization Approach,"  Batch Normalization (BatchNorm) is effective for improving the performanceand accelerating the training of deep neural networks. However, it has alsoshown to be a cause of adversarial vulnerability, i.e., networks without it aremore robust to adversarial attacks. In this paper, we investigate how BatchNormcauses this vulnerability and proposed new normalization that is robust toadversarial attacks. We first observe that adversarial images tend to shift thedistribution of BatchNorm input, and this shift makes train-time estimatedpopulation statistics inaccurate. We hypothesize that these inaccuratestatistics make models with BatchNorm more vulnerable to adversarial attacks.We prove our hypothesis by replacing train-time estimated statistics withstatistics calculated from the inference-time batch. We found that theadversarial vulnerability of BatchNorm disappears if we use these statistics.However, without estimated batch statistics, we can not use BatchNorm in thepractice if large batches of input are not available. To mitigate this, wepropose Robust Normalization (RobustNorm); an adversarially robust version ofBatchNorm. We experimentally show that models trained with RobustNorm performbetter in adversarial settings while retaining all the benefits of BatchNorm.Code is available at \url{https://github.com/awaisrauf/RobustNorm}.","Muhammad Awais, Fahad Shamshad, Sung-Ho Bae",,,11,
"A Semi-Supervised Generative Adversarial Network for Prediction of
  Genetic Disease Outcomes","  For most diseases, building large databases of labeled genetic data is anexpensive and time-demanding task. To address this, we introduce geneticGenerative Adversarial Networks (gGAN), a semi-supervised approach based on aninnovative GAN architecture to create large synthetic genetic data setsstarting with a small amount of labeled data and a large amount of unlabeleddata. Our goal is to determine the propensity of a new individual to developthe severe form of the illness from their genetic profile alone. The proposedmodel achieved satisfactory results using real genetic data from differentdatasets and populations, in which the test populations may not have the samegenetic profiles. The proposed model is self-aware and capable of determiningwhether a new genetic profile has enough compatibility with the data on whichthe network was trained and is thus suitable for prediction. The code anddatasets used can be found at https://github.com/caio-davi/gGAN.",Caio Davi and Ulisses Braga-Neto,,,11,
"Fitness Landscape Analysis of Dimensionally-Aware Genetic Programming
  Featuring Feynman Equations","  Genetic programming is an often-used technique for symbolic regression:finding symbolic expressions that match data from an unknown function. To makethe symbolic regression more efficient, one can also use dimensionally-awaregenetic programming that constrains the physical units of the equation.Nevertheless, there is no formal analysis of how much dimensionality awarenesshelps in the regression process. In this paper, we conduct a fitness landscapeanalysis of dimensionallyaware genetic programming search spaces on a subset ofequations from Richard Feynmans well-known lectures. We define aninitialisation procedure and an accompanying set of neighbourhood operators forconducting the local search within the physical unit constraints. Ourexperiments show that the added information about the variable dimensionalitycan efficiently guide the search algorithm. Still, further analysis of thedifferences between the dimensionally-aware and standard genetic programminglandscapes is needed to help in the design of efficient evolutionary operatorsto be used in a dimensionally-aware regression.","Marko Durasevic, Domagoj Jakobovic, Marcella Scoczynski Ribeiro
  Martins, Stjepan Picek, and Markus Wagner",,,11,
"Dynamic Partial Removal: A Neural Network Heuristic for Large
  Neighborhood Search","  This paper presents a novel neural network design that learns the heuristicfor Large Neighborhood Search (LNS). LNS consists of a destroy operator and arepair operator that specify a way to carry out the neighborhood search tosolve the Combinatorial Optimization problems. The proposed approach in thispaper applies a Hierarchical Recurrent Graph Convolutional Network (HRGCN) as aLNS heuristic, namely Dynamic Partial Removal, with the advantage of adaptivedestruction and the potential to search across a large scale, as well as thecontext-awareness in both spatial and temporal perspective. This model isgeneralized as an efficient heuristic approach to different combinatorialoptimization problems, especially to the problems with relatively tightconstraints. We apply this model to vehicle routing problem (VRP) in this paperas an example. The experimental results show that this approach outperforms thetraditional LNS heuristics on the same problem as well. The source code isavailable at\href{https://github.com/water-mirror/DPR}{https://github.com/water-mirror/DPR}.","Mingxiang Chen, Lei Gao, Qichang Chen, Zhixin Liu",,,11,
Towards On-Chip Bayesian Neuromorphic Learning,"  If edge devices are to be deployed to critical applications where theirdecisions could have serious financial, political, or public-healthconsequences, they will need a way to signal when they are not sure how toreact to their environment. For instance, a lost delivery drone could make itsway back to a distribution center or contact the client if it is confused abouthow exactly to make its delivery, rather than taking the action which is ""mostlikely"" correct. This issue is compounded for health care or militaryapplications. However, the brain-realistic temporal credit assignment problemneuromorphic computing algorithms have to solve is difficult. The double roleweights play in backpropagation-based-learning, dictating how the networkreacts to both input and feedback, needs to be decoupled. e-prop 1 is apromising learning algorithm that tackles this with Broadcast Alignment (atechnique where network weights are replaced with random weights duringfeedback) and accumulated local information. We investigate under whatconditions the Bayesian loss term can be expressed in a similar fashion,proposing an algorithm that can be computed with only local information as welland which is thus no more difficult to implement on hardware. This algorithm isexhibited on a store-recall problem, which suggests that it can learn gooduncertainty on decisions to be made over time.","Nathan Wycoff, Prasanna Balaprakash, Fangfang Xia",,,11,
Learn to Design the Heuristics for Vehicle Routing Problem,"  This paper presents an approach to learn the local-search heuristics thatiteratively improves the solution of Vehicle Routing Problem (VRP). Alocal-search heuristics is composed of a destroy operator that destructs acandidate solution, and a following repair operator that rebuilds thedestructed one into a new one. The proposed neural network, as trained throughactor-critic framework, consists of an encoder in form of a modified version ofGraph Attention Network where node embeddings and edge embeddings areintegrated, and a GRU-based decoder rendering a pair of destroy and repairoperators. Experiment results show that it outperforms both the traditionalheuristics algorithms and the existing neural combinatorial optimization forVRP on medium-scale data set, and is able to tackle the large-scale data set(e.g., over 400 nodes) which is a considerable challenge in this area.Moreover, the need for expertise and handcrafted heuristics design iseliminated due to the fact that the proposed network learns to design theheuristics with a better performance. Our implementation is available online.","Lei Gao, Mingxiang Chen, Qichang Chen, Ganzhong Luo, Nuoyi Zhu, Zhixin
  Liu",,,11,
Nature-Inspired Optimization Algorithms: Challenges and Open Problems,"  Many problems in science and engineering can be formulated as optimizationproblems, subject to complex nonlinear constraints. The solutions of highlynonlinear problems usually require sophisticated optimization algorithms, andtraditional algorithms may struggle to deal with such problems. A current trendis to use nature-inspired algorithms due to their flexibility andeffectiveness. However, there are some key issues concerning nature-inspiredcomputation and swarm intelligence. This paper provides an in-depth review ofsome recent nature-inspired algorithms with the emphasis on their searchmechanisms and mathematical foundations. Some challenging issues are identifiedand five open problems are highlighted, concerning the analysis of algorithmicconvergence and stability, parameter tuning, mathematical framework, role ofbenchmarking and scalability. These problems are discussed with the directionsfor future research.",Xin-She Yang,,,11,
"Self-adaptation in non-Elitist Evolutionary Algorithms on Discrete
  Problems with Unknown Structure","  A key challenge to make effective use of evolutionary algorithms is to chooseappropriate settings for their parameters. However, the appropriate parametersetting generally depends on the structure of the optimisation problem, whichis often unknown to the user. Non-deterministic parameter control mechanismsadjust parameters using information obtained from the evolutionary process.Self-adaptation -- where parameter settings are encoded in the chromosomes ofindividuals and evolve through mutation and crossover -- is a popular parametercontrol mechanism in evolutionary strategies. However, there is littletheoretical evidence that self-adaptation is effective, and self-adaptation haslargely been ignored by the discrete evolutionary computation community.  Here we show through a theoretical runtime analysis that a non-elitist,discrete evolutionary algorithm which self-adapts its mutation rate not onlyoutperforms EAs which use static mutation rates on \leadingones, but alsoimproves asymptotically on an EA using a state-of-the-art control mechanism.The structure of this problem depends on a parameter $k$, which is \emph{apriori} unknown to the algorithm, and which is needed to appropriately set afixed mutation rate. The self-adaptive EA achieves the same asymptotic runtimeas if this parameter was known to the algorithm beforehand, which is anasymptotic speedup for this problem compared to all other EAs previouslystudied. An experimental study of how the mutation-rates evolve show that theyrespond adequately to a diverse range of problem structures.  These results suggest that self-adaptation should be adopted more broadly asa parameter control mechanism in discrete, non-elitist evolutionary algorithms.",Brendan Case and Per Kristian Lehre,,,11,
"ClipUp: A Simple and Powerful Optimizer for Distribution-based Policy
  Evolution","  Distribution-based search algorithms are an effective approach forevolutionary reinforcement learning of neural network controllers. In thesealgorithms, gradients of the total reward with respect to the policy parametersare estimated using a population of solutions drawn from a search distribution,and then used for policy optimization with stochastic gradient ascent. A commonchoice in the community is to use the Adam optimization algorithm for obtainingan adaptive behavior during gradient ascent, due to its success in a variety ofsupervised learning settings. As an alternative to Adam, we propose to enhanceclassical momentum-based gradient ascent with two simple techniques: gradientnormalization and update clipping. We argue that the resulting optimizer calledClipUp (short for ""clipped updates"") is a better choice for distribution-basedpolicy evolution because its working principles are simple and easy tounderstand and its hyperparameters can be tuned more intuitively in practice.Moreover, it removes the need to re-tune hyperparameters if the reward scalechanges. Experiments show that ClipUp is competitive with Adam despite itssimplicity and is effective on challenging continuous control benchmarks,including the Humanoid control task based on the Bullet physics simulator.","Nihat Engin Toklu, Pawe{\l} Liskowski, Rupesh Kumar Srivastava",,,11,
"On the Combined Impact of Population Size and Sub-problem Selection in
  MOEA/D","  This paper intends to understand and to improve the working principle ofdecomposition-based multi-objective evolutionary algorithms. We review thedesign of the well-established Moea/d framework to support the smoothintegration of different strategies for sub-problem selection, whileemphasizing the role of the population size and of the number of offspringcreated at each generation. By conducting a comprehensive empirical analysis ona wide range of multi-and many-objective combinatorial NK landscapes, weprovide new insights into the combined effect of those parameters on theanytime performance of the underlying search process. In particular, we showthat even a simple random strategy selecting sub-problems at random outperformsexisting sophisticated strategies. We also study the sensitivity of suchstrategies with respect to the ruggedness and the objective space dimension ofthe target problem.","Geoffrey Pruvost (BONUS), Bilel Derbel (BONUS), Arnaud Liefooghe
  (BONUS), Ke Li, Qingfu Zhang (CUHK)",,,11,
"Runtime Performances of Randomized Search Heuristics for the Dynamic
  Weighted Vertex Cover Problem","  Randomized search heuristics such as evolutionary algorithms are frequentlyapplied to dynamic combinatorial optimization problems. Within this paper, wepresent a dynamic model of the classic Weighted Vertex Cover problem andanalyze the runtime performances of the well-studied algorithms RandomizedLocal Search and (1+1) EA adapted to it, to contribute to the theoreticalunderstanding of evolutionary computing for problems with dynamic changes. Inour investigations, we use an edge-based representation based on the dual formof the Linear Programming formulation for the problem and study the expectedruntime that the adapted algorithms require to maintain a 2-approximatesolution when the given weighted graph is modified by an edge-editing orweight-editing operation. Considering the weights on the vertices may beexponentially large with respect to the size of the graph, the step sizeadaption strategy is incorporated, with or without the 1/5-th rule that isemployed to control the increasing/decreasing rate of the step size. Ourresults show that three of the four algorithms presented in the paper canrecompute 2-approximate solutions for the studied dynamic changes in polynomialexpected runtime, but the (1+1) EA with 1/5-th Rule requires pseudo-polynomialexpected runtime.","Feng Shi, Frank Neumann, Jianxin Wang",,,11,
"Thermal-Aware Compilation of Spiking Neural Networks to Neuromorphic
  Hardware","  Hardware implementation of neuromorphic computing can significantly improveperformance and energy efficiency of machine learning tasks implemented withspiking neural networks (SNNs), making these hardware platforms particularlysuitable for embedded systems and other energy-constrained environments. Weobserve that the long bitlines and wordlines in a crossbar of the hardwarecreate significant current variations when propagating spikes through itssynaptic elements, which are typically designed with non-volatile memory (NVM).Such current variations create a thermal gradient within each crossbar of thehardware, depending on the machine learning workload and the mapping of neuronsand synapses of the workload to these crossbars. \mr{This thermal gradientbecomes significant at scaled technology nodes and it increases the leakagepower in the hardware leading to an increase in the energy consumption.} Wepropose a novel technique to map neurons and synapses of SNN-based machinelearning workloads to neuromorphic hardware. We make two novel contributions.First, we formulate a detailed thermal model for a crossbar in a neuromorphichardware incorporating workload dependency, where the temperature of eachNVM-based synaptic cell is computed considering the thermal contributions fromits neighboring cells. Second, we incorporate this thermal model in the mappingof neurons and synapses of SNN-based workloads using a hill-climbing heuristic.The objective is to reduce the thermal gradient in crossbars. We evaluate ourneuron and synapse mapping technique using 10 machine learning workloads for astate-of-the-art neuromorphic hardware. We demonstrate an average 11.4Kreduction in the average temperature of each crossbar in the hardware, leadingto a 52% reduction in the leakage power consumption (11% lower total energyconsumption) compared to a performance-oriented SNN mapping technique.",Twisha Titirsha and Anup Das,,,11,
"Advanced Cauchy Mutation for Differential Evolution in Numerical
  Optimization","  Among many evolutionary algorithms, differential evolution (DE) has receivedmuch attention over the last two decades. DE is a simple yet powerfulevolutionary algorithm that has been used successfully to optimize variousreal-world problems. Since it was introduced, many researchers have developednew methods for DE, and one of them makes use of a mutation based on the Cauchydistribution to increase the convergence speed of DE. The method monitors theresults of each individual in the selection operator and performs the Cauchymutation on consecutively failed individuals, which generates mutant vectors byperturbing the best individual with the Cauchy distribution. Therefore, themethod can locate the consecutively failed individuals to new positions closeto the best individual. Although this approach is interesting, it fails to takeinto account establishing a balance between exploration and exploitation. Inthis paper, we propose a sigmoid based parameter control that alters thefailure threshold for performing the Cauchy mutation in a time-varyingschedule, which can establish a good ratio between exploration andexploitation. Experiments and comparisons have been done with six conventionaland six advanced DE variants on a set of 30 benchmark problems, which indicatethat the DE variants assisted by the proposed algorithm are highly competitive,especially for multimodal functions.","Tae Jong Choi, Julian Togelius, Yun-Gyung Cheong",,,11,
AutoLR: An Evolutionary Approach to Learning Rate Policies,"  The choice of a proper learning rate is paramount for good Artificial NeuralNetwork training and performance. In the past, one had to rely on experienceand trial-and-error to find an adequate learning rate. Presently, a plethora ofstate of the art automatic methods exist that make the search for a goodlearning rate easier. While these techniques are effective and have yieldedgood results over the years, they are general solutions. This means theoptimization of learning rate for specific network topologies remains largelyunexplored. This work presents AutoLR, a framework that evolves Learning RateSchedulers for a specific Neural Network Architecture using StructuredGrammatical Evolution. The system was used to evolve learning rate policiesthat were compared with a commonly used baseline value for learning rate.Results show that training performed using certain evolved policies is moreefficient than the established baseline and suggest that this approach is aviable means of improving a neural network's performance.","Pedro Carvalho, Nuno Louren\c{c}o, Filipe Assun\c{c}\~ao, Penousal
  Machado",,,11,
"Unified Framework for the Adaptive Operator Selection of Discrete
  Parameters","  We conduct an exhaustive survey of adaptive selection of operators (AOS) inEvolutionary Algorithms (EAs). We simplified the AOS structure by adding morecomponents to the framework to built upon the existing categorisation of AOSmethods. In addition to simplifying, we looked at the commonality among AOSmethods from literature to generalise them. Each component is presented with anumber of alternative choices, each represented with a formula. We make threesets of comparisons. First, the methods from literature are tested on the BBOBtest bed with their default hyper parameters. Second, the hyper parameters ofthese methods are tuned using an offline configurator known as IRACE. Third,for a given set of problems, we use IRACE to select the best combination ofcomponents and tune their hyper parameters.","Mudita Sharma, Manuel Lopez-Ibanez and Dimitar Kazakov",,,11,
"The Heidelberg spiking datasets for the systematic evaluation of spiking
  neural networks","  Spiking neural networks are the basis of versatile and power-efficientinformation processing in the brain. Although we currently lack a detailedunderstanding of how these networks compute, recently developed optimizationtechniques allow us to instantiate increasingly complex functional spikingneural networks in-silico. These methods hold the promise to build moreefficient non-von-Neumann computing hardware and will offer new vistas in thequest of unraveling brain circuit function. To accelerate the development ofsuch methods, objective ways to compare their performance are indispensable.Presently, however, there are no widely accepted means for comparing thecomputational performance of spiking neural networks. To address this issue, weintroduce two spike-based classification datasets, broadly applicable tobenchmark both software and neuromorphic hardware implementations of spikingneural networks. To accomplish this, we developed a general audio-to-spikingconversion procedure inspired by neurophysiology. Further, we applied thisconversion to an existing and a novel speech dataset. The latter is the free,high-fidelity, and word-level aligned Heidelberg digit dataset that we createdspecifically for this study. By training a range of conventional and spikingclassifiers, we show that leveraging spike timing information within thesedatasets is essential for good classification accuracy. These results serve asthe first reference for future performance comparisons of spiking neuralnetworks.","Benjamin Cramer, Yannik Stradmann, Johannes Schemmel and Friedemann
  Zenke",,,11,
"Synaptic Plasticity Dynamics for Deep Continuous Local Learning
  (DECOLLE)","  A growing body of work underlines striking similarities between biologicalneural networks and recurrent, binary neural networks. A relatively smallerbody of work, however, discusses similarities between learning dynamicsemployed in deep artificial neural networks and synaptic plasticity in spikingneural networks. The challenge preventing this is largely caused by thediscrepancy between the dynamical properties of synaptic plasticity and therequirements for gradient backpropagation. Learning algorithms that approximategradient backpropagation using locally synthesized gradients can overcome thischallenge. Here, we show that synthetic gradients enable the derivation of DeepContinuous Local Learning (DECOLLE) in spiking neural networks. DECOLLE iscapable of learning deep spatio-temporal representations from spikes relyingsolely on local information. Synaptic plasticity rules are derivedsystematically from user-defined cost functions and neural dynamics byleveraging existing autodifferentiation methods of machine learning frameworks.We benchmark our approach on the MNIST and the event-based neuromorphicDvsGesture dataset, on which DECOLLE performs comparably to thestate-of-the-art. DECOLLE networks provide continuously learning machines thatare relevant to biology and supportive of event-based, low-power computervision architectures matching the accuracies of conventional computers on taskswhere temporal precision and speed are essential.",Jacques Kaiser and Hesham Mostafa and Emre Neftci,,,11,
Evolving Robust Neural Architectures to Defend from Adversarial Attacks,"  Neural networks are prone to misclassify slightly modified input images.Recently, many defences have been proposed, but none have improved therobustness of neural networks consistently. Here, we propose to use adversarialattacks as a function evaluation to search for neural architectures that canresist such attacks automatically. Experiments on neural architecture searchalgorithms from the literature show that although accurate, they are not ableto find robust architectures. A significant reason for this lies in theirlimited search space. By creating a novel neural architecture search withoptions for dense layers to connect with convolution layers and vice-versa aswell as the addition of concatenation layers in the search, we were able toevolve an architecture that is inherently accurate on adversarial samples.Interestingly, this inherent robustness of the evolved architecture rivalsstate-of-the-art defences such as adversarial training while being trained onlyon the non-adversarial samples. Moreover, the evolved architecture makes use ofsome peculiar traits which might be useful for developing even more robustones. Thus, the results here confirm that more robust architectures exist aswell as opens up a new realm of feasibilities for the development andexploration of neural networks.  Code available at http://bit.ly/RobustArchitectureSearch.",Shashank Kotyan and Danilo Vasconcellos Vargas,,,11,
Transfer learning of chaotic systems,"  Can a neural network trained by the time series of system A be used topredict the evolution of system B? This problem, knowing as transfer learningin a broad sense, is of great importance in machine learning and data mining,yet has not been addressed for chaotic systems. Here we investigate transferlearning of chaotic systems from the perspective of synchronization-based stateinference, in which a reservoir computer trained by chaotic system A is used toinfer the unmeasured variables of chaotic system B, while A is different from Bin either parameter or dynamics. It is found that if systems A and B aredifferent in parameter, the reservoir computer can be well synchronized tosystem B. However, if systems A and B are different in dynamics, the reservoircomputer fails to synchronize with system B in general. Knowledge transferalong a chain of coupled reservoir computers is also studied, and it is foundthat, although the reservoir computers are trained by different systems, theunmeasured variables of the driving system can be successfully inferred by theremote reservoir computer. Finally, by an experiment of chaotic pendulum, weshow that the knowledge learned from the modeling system can be used to predictthe evolution of the experimental system.","Yali Guo, Han Zhang, Liang Wang, Huawei Fan, and Xingang Wang",,,11,
"An Experimental Study of Weight Initialization and Weight Inheritance
  Effects on Neuroevolution","  Weight initialization is critical in being able to successfully trainartificial neural networks (ANNs), and even more so for recurrent neuralnetworks (RNNs) which can easily suffer from vanishing and exploding gradients.In neuroevolution, where evolutionary algorithms are applied to neuralarchitecture search, weights typically need to be initialized at threedifferent times: when initial genomes (ANN architectures) are created at thebeginning of the search, when offspring genomes are generated by crossover, andwhen new nodes or edges are created during mutation. This work explores thedifference between using Xavier, Kaiming, and uniform random weightinitialization methods, as well as novel Lamarckian weight inheritance methodsfor initializing new weights during crossover and mutation operations. Theseare examined using the Evolutionary eXploration of Augmenting Memory Models(EXAMM) neuroevolution algorithm, which is capable of evolving RNNs with avariety of modern memory cells (e.g., LSTM, GRU, MGU, UGRNN and Delta-RNNcells) as well recurrent connections with varying time skips through a highperformance island based distributed evolutionary algorithm. Results show thatwith statistical significance, utilizing the Lamarckian strategies outperformsKaiming, Xavier and uniform random weight initialization, and can speedneuroevolution by requiring less backpropagation epochs to be evaluated foreach generated RNN.","Zimeng Lyu, AbdElRahman ElSaid, Joshua Karns, Mohamed Mkaouer, Travis
  Desell",,,11,
"Memory Organization for Energy-Efficient Learning and Inference in
  Digital Neuromorphic Accelerators","  The energy efficiency of neuromorphic hardware is greatly affected by theenergy of storing, accessing, and updating synaptic parameters. Various methodsof memory organisation targeting energy-efficient digital accelerators havebeen investigated in the past, however, they do not completely encapsulate theenergy costs at a system level. To address this shortcoming and to account forvarious overheads, we synthesize the controller and memory for differentencoding schemes and extract the energy costs from these synthesized blocks.Additionally, we introduce functional encoding for structured connectivity suchas the connectivity in convolutional layers. Functional encoding offers a 58%reduction in the energy to implement a backward pass and weight update in suchlayers compared to existing index-based solutions. We show that for a 2 layerspiking neural network trained to retain a spatio-temporal pattern, bitmap(PB-BMP) based organization can encode the sparser networks more efficiently.This form of encoding delivers a 1.37x improvement in energy efficiency comingat the cost of a 4% degradation in network retention accuracy as measured bythe van Rossum distance.","Clemens JS Schaefer, Patrick Faley, Emre O Neftci, Siddharth Joshi",,,11,
"The Node Weight Dependent Traveling Salesperson Problem: Approximation
  Algorithms and Randomized Search Heuristics","  Several important optimization problems in the area of vehicle routing can beseen as a variant of the classical Traveling Salesperson Problem (TSP). In thearea of evolutionary computation, the traveling thief problem (TTP) has gainedincreasing interest over the last 5 years. In this paper, we investigate theeffect of weights on such problems, in the sense that the cost of travelingincreases with respect to the weights of nodes already visited during a tour.This provides abstractions of important TSP variants such as the TravelingThief Problem and time dependent TSP variants, and allows to study preciselythe increase in difficulty caused by weight dependence. We provide a3.59-approximation for this weight dependent version of TSP with metricdistances and bounded positive weights. Furthermore, we conduct experimentalinvestigations for simple randomized local search with classical mutationoperators and two variants of the state-of-the-art evolutionary algorithm EAXadapted to the weighted TSP. Our results show the impact of the node weights onthe position of the nodes in the resulting tour.","Jakob Bossek, Katrin Casel, Pascal Kerschke and Frank Neumann",,,11,
On the decomposition of generalized semiautomata,Semi-automata are abstractions of electronic devices that are deterministicfinite-state machines having inputs but no outputs. Generalized semiautomataare obtained from stochastic semiautomata by dropping the restrictions imposedby probability. It is well-known that each stochastic semiautomaton can bedecomposed into a sequential product of a dependent source and a deterministicsemiautomaton making partly use of the celebrated theorem of Birkhoff-vonNeumann. It will be shown that each generalized semiautomaton can bepartitioned into a sequential product of a generalized dependent source and adeterministic semiautomaton.,"Merve Nur Cakir, Karl-Heinz Zimmermann",,,11,
"A translation of weighted LTL formulas to weighted B\""uchi automata over
  {\omega}-valuation monoids","  In this paper we introduce a weighted LTL over product {\omega}-valuationmonoids that satisfy specific properties. We also introduce weightedgeneralized B\""uchi automata with {\epsilon}-transitions, as well as weightedB\""uchi automata with {\epsilon}-transitions over product {\omega}-valuationmonoids and prove that these two models are expressively equivalent and alsoequivalent to weighted B\""uchi automata already introduced in the literature.We prove that every formula of a syntactic fragment of our logic can beeffectively translated to a weighted generalized B\""uchi automaton with{\epsilon}-transitions. Finally, we prove that the number of states of theproduced automaton is polynomial in the size of the corresponding formula.",Eleni Mandrali,,,11,
Register Games on Infinite Ordered Data Domains,"  We introduce two-player turn-based zero-sum register games on an infinitelinearly ordered data domain. Register games have a finite set of registersintended to store data values. At each round, Adam picks some data in thedomain, which is tested against the data contained in the registers, using thelinear order. Depending on which test holds (exactly one is required), Evedecides to assign, or not, the data to some of her registers, and the gamedeterministically evolves to a successor vertex depending on her assignmentchoice. Eve wins the game if she has a strategy which depends only on the teststhat hold (and not on the concrete data values of Adam), such that whichevervalues Adam provides, the sequence of visited vertices of the game satisfiessome parity condition. We show the decidability of register games over datadomains N and Q. For Q, they can be solved in ExpTime and finite-memorystrategies always suffice to win. For N, we show that deciding the existence ofa finite-memory strategy is also in ExpTime. We apply these results to solvethe synthesis problem of strategies resolving non-determinism in(non-deterministic) register transducers on data words.","L\'eo Exibard, Emmanuel Filiot, Ayrat Khalimov",,,11,
"Monotone Precision and Recall Measures for Comparing Executions and
  Specifications of Dynamic Systems","  The behavioural comparison of systems is an important concern of softwareengineering research. For example, the areas of specification discovery andspecification mining are concerned with measuring the consistency between acollection of execution traces and a program specification. This problem isalso tackled in process mining with the help of measures that describe thequality of a process specification automatically discovered from executionlogs. Though various measures have been proposed, it was recently demonstratedthat they neither fulfil essential properties, such as monotonicity, nor canthey handle infinite behaviour. In this paper, we address this research problemby introducing a new framework for the definition of behavioural quotients. Weproof that corresponding quotients guarantee desired properties that existingmeasures have failed to support. We demonstrate the application of thequotients for capturing precision and recall measures between a collection ofrecorded executions and a system specification. We use a prototypicalimplementation of these measures to contrast their monotonic assessment withmeasures that have been defined in prior research.","Artem Polyvyanyy, Andreas Solti, Matthias Weidlich, Claudio Di Ciccio,
  Jan Mendling",,,11,
Harnessing LTL With Freeze Quantification,"  Logics and automata models for languages over infinite alphabets, such asFreeze LTL and register automata, respectively, serve the verification ofprocesses or documents with data. They relate tightly to formalisms overnominal sets, where names play the role of data. For example, regularnondeterministic nominal automata (RNNA) are equivalent to a subclass of thestandard register automata model, characterized by a lossiness conditionreferred to as name dropping. This subclass generally enjoys bettercomputational properties than the full class of register automata, for which,e.g., inclusion checking is undecidable. Similarly, satisfiability in fullfreeze LTL is undecidable, and decidable but not primitive recursive if thenumber of registers is limited to at most one. In the present paper, weintroduce a name-dropping variant bar-muTL of Freeze LTL for finite words overan infinite alphabet. We show by reduction to extended regular nondeterministicnominal automata (ERNNA) that even with unboundedly many registers, modelchecking for bar-muTL over RNNA is elementary, in fact in ExpSpace, moreprecisely in parametrized PSpace, effectively with the number of registers asthe parameter.","Daniel Hausmann, Stefan Milius and Lutz Schr\""oder",,,11,
Decidability and k-Regular Sequences,"  In this paper we consider a number of natural decision problems involvingk-regular sequences. Specifically, they arise from - lower and upper bounds ongrowth rate; in particular boundedness, - images, - regularity (recognizabilityby a deterministic finite automaton) of preimages, and - factors, such assquares and palindromes of such sequences. We show that the decision problemsare undecidable.",Daniel Krenn and Jeffrey Shallit,,,11,
Transition Property For Cube-Free Words,"  We study cube-free words over arbitrary non-unary finite alphabets and provethe following structural property: for every pair $(u,v)$ of $d$-ary cube-freewords, if $u$ can be infinitely extended to the right and $v$ can be infinitelyextended to the left respecting the cube-freeness property, then there exists a""transition"" word $w$ over the same alphabet such that $uwv$ is cube free. Thecrucial case is the case of the binary alphabet, analyzed in the central partof the paper.  The obtained ""transition property"", together with the developed technique,allowed us to solve cube-free versions of three old open problems by Restivoand Salemi. Besides, it has some further implications for combinatorics onwords; e.g., it implies the existence of infinite cube-free words of very bigsubword (factor) complexity.",Elena A. Petrova and Arseny M. Shur,,,11,
A Characterization of Morphic Words with Polynomial Growth,"  A morphic word is obtained by iterating a morphism to generate an infiniteword, and then applying a coding. We characterize morphic words with polynomialgrowth in terms of a new type of infinite word called a $\textit{zigzag word}$.A zigzag word is represented by an initial string, followed by a finite list ofterms, each of which repeats for each $n \geq 1$ in one of three ways: it growsforward [$t(1)\ t(2)\ \dotsm\ t(n)]$, backward [$t(n)\ \dotsm\ t(2)\ t(1)$], orjust occurs once [$t$]. Each term can recursively contain subterms with theirown forward and backward repetitions. We show that an infinite word is morphicwith growth $\Theta(n^k)$ iff it is a zigzag word of depth $k$. As corollaries,we obtain that the morphic words with growth $O(n)$ are exactly the ultimatelyperiodic words, and the morphic words with growth $O(n^2)$ are exactly themultilinear words.",Tim Smith,,,11,
"A B\""uchi-Elgot-Trakhtenbrot theorem for automata with MSO graph storage","  We introduce MSO graph storage types, and call a storage type MSO-expressibleif it is isomorphic to some MSO graph storage type. An MSO graph storage typehas MSO-definable sets of graphs as storage configurations and as storagetransformations. We consider sequential automata with MSO graph storage andassociate with each such automaton a string language (in the usual way) and agraph language; a graph is accepted by the automaton if it represents a correctsequence of storage configurations for a given input string. For each MSO graphstorage type, we define an MSO logic which is a subset of the usual MSO logicon graphs. We prove a B\""uchi-Elgot-Trakhtenbrot theorem, both for the stringcase and the graph case. Moreover, we prove that (i) each MSO graphtransduction can be used as storage transformation in an MSO graph storagetype, (ii) every automatic storage type is MSO-expressible, and (iii) thepushdown operator on storage types preserves the property ofMSO-expressibility. Thus, the iterated pushdown storage types areMSO-expressible.",Joost Engelfriet and Heiko Vogler,,,11,
A Faster-Than Relation for Semi-Markov Decision Processes,"  When modeling concurrent or cyber-physical systems, non-functionalrequirements such as time are important to consider. In order to improve thetiming aspects of a model, it is necessary to have some notion of what it meansfor a process to be faster than another, which can guide the stepwiserefinement of the model. To this end we study a faster-than relation forsemi-Markov decision processes and compare it to standard notions for relatingsystems. We consider the compositional aspects of this relation, and show thatthe faster-than relation is not a precongruence with respect to parallelcomposition, hence giving rise to so-called parallel timing anomalies. We takethe first steps toward understanding this problem by identifying decidableconditions sufficient to avoid parallel timing anomalies in the absence ofnon-determinism.","Mathias Ruggaard Pedersen (Aalborg University), Giorgio Bacci (Aalborg
  University), Kim Guldstrand Larsen (Aalborg University)",,,11,
Recurrence in Dense-time AMS Assertions,"  The notion of recurrence over continuous or dense time, as required forexpressing Analog and Mixed-Signal (AMS) behaviours, is fundamentally differentfrom what is offered by the recurrence operators of SystemVerilog Assertions(SVA). This article introduces the formal semantics of recurrence over densetime and provides a methodology for the runtime verification of such propertiesusing interval arithmetic. Our property language extends SVA with densereal-time intervals and predicates containing real-valued signals. We provide atool kit which interfaces with off-the-shelf EDA tools through standard VPI.","Sayandeep Sanyal, Antonio Anastasio Bruto da Costa, Pallab Dasgupta",,,11,
Synthesis of Data Word Transducers,"  In reactive synthesis, the goal is to automatically generate animplementation from a specification of the reactive and non-terminatinginput/output behaviours of a system. Specifications are usually modelled aslogical formulae or automata over infinite sequences of signals($\omega$-words), while implementations are represented as transducers. In theclassical setting, the set of signals is assumed to be finite. In this paper,we do not make such an assumption and consider data $\omega$-words instead,i.e., words over an infinite alphabet. In this context, we study specificationsand implementations respectively given as automata and transducers extendedwith a finite set of registers. We consider different instances, depending onwhether the specification is nondeterministic, universal or deterministic, anddepending on whether the number of registers of the implementation is given(bounded synthesis) or not.  In the unbounded setting, we show undecidability for both universal andnon-deterministic specifications, while decidability is recovered in thedeterministic case. In the bounded setting, undecidability still holds fornon-deterministic specifications, but can be recovered by disallowing testsover input data. The generic technique we use to show the latter result allowsus to reprove some known result, namely decidability of bounded synthesis foruniversal specifications.","L\'eo Exibard, Emmanuel Filiot, Pierre-Alain Reynier",,,11,
The monitoring problem for timed automata,"  We study a variant of the classical membership problem in automata theory,which consists of deciding whether a given input word is accepted by a givenautomaton. We do so under a different perspective, that is, we consider adynamic version of the problem, called monitoring problem, where the automatonis fixed and the input is revealed as in a stream, one symbol at a timefollowing the natural order on positions. The goal here is to design a dynamicdata structure that can be queried about whether the word consisting of symbolsrevealed so far is accepted by the automaton, and that can be efficientlyupdated when the next symbol is revealed. We provide complexity bounds for thismonitoring problem, by considering timed automata that process symbolsinterleaved with timestamps. The main contribution is that monitoring of aone-clock timed automaton, with all its components but the clock constantsfixed, can be done in amortised constant time per input symbol.","Alejandro Grez, Filip Mazowiecki, Micha{\l} Pilipczuk, Gabriele Puppis
  and Cristian Riveros",,,11,
Hardware/Software Co-verification Using Path-based Symbolic Execution,"  Conventional tools for formal hardware/software co-verification use boundedmodel checking techniques to construct a single monolithic propositionalformula. Formulas generated in this way are extremely complex and contain agreat deal of irrelevant logic, hence are difficult to solve even by thestate-of-the-art Satis ability (SAT) solvers. In a typical hardware/softwareco-design the firmware only exercises a fraction of the hardware state-space,and we can use this observation to generate simpler and more concise formulas.In this paper, we present a novel verification algorithm for hardware/softwareco-designs that identify partitions of the firmware and the hardware logicpertaining to the feasible execution paths by means of path-based symbolicsimulation with custom path-pruning, property-guided slicing and incrementalSAT solving. We have implemented this approach in our tool COVERIF. We haveexperimentally compared COVERIF with HW-CBMC, a monolithic BMC basedco-verification tool, and observed an average speed-up of 5X over HW-CBMC forproving safety properties as well as detecting critical co-design bugs in anopen-source Universal Asynchronous Receiver Transmitter design and a large SoCdesign.","Rajdeep Mukherjee, Saurabh Joshi, John O'Leary, Daniel Kroening, Tom
  Melham",,,11,
When is Containment Decidable for Probabilistic Automata?,"  The emptiness and containment problems for probabilistic automata are naturalquantitative generalisations of the classical language emptiness and inclusionproblems for Boolean automata. It is well known that both problems areundecidable. In this paper we provide a more refined view of these problems interms of the degree of ambiguity of probabilistic automata. We show that a gapversion of the emptiness problem (that is known be undecidable in general)becomes decidable for automata of polynomial ambiguity. We complement thispositive result by showing that the emptiness problem remains undecidable evenwhen restricted to automata of linear ambiguity. We then turn to finitelyambiguous automata. Here we show decidability of containment in case one of theautomata is assumed to be unambiguous while the other one is allowed to befinitely ambiguous. Our proof of this last result relies on the decidability ofthe theory of real exponentiation, which has been shown, subject to Schanuel'sConjecture, by Macintyre and Wilkie.","Laure Daviaud, Marcin Jurdzi\'nski, Ranko Lazi\'c, Filip Mazowiecki,
  Guillermo A. P\'erez, James Worrell",,,11,
"A note on the class of languages generated by F-systems over regular
  languages","  An F-system is a computational model that performs a folding operation onstrings of a given language, following directions coded on strings of anothergiven language. This note considers the case in which both given languages areregular, and it shows that such F-system generates linear context-freelanguages. The demonstration is based on constructing a one-turn pushdownautomaton for the generated language.",Jorge C. Lucero,,,11,
Most Permissive Semantics of Boolean Networks,"  As shown in (http://dx.doi.org/10.1101/2020.03.22.998377), the usual updatemodes of Boolean networks (BNs), including synchronous and (generalized)asynchronous, fail to capture behaviors introduced by multivalued refinements.Thus, update modes do not allow a correct abstract reasoning on dynamics ofbiological systems, as they may lead to reject valid BN models.This technicalreport lists the main definitions and properties of the most permissivesemantics of BNs introduced in http://dx.doi.org/10.1101/2020.03.22.998377.This semantics meets with a correct abstraction of any multivalued refinements,with any update mode. It subsumes all the usual updating modes, while enablingnew behaviors achievable by more concrete models. Moreover, it appears thatclassical dynamical analyzes of reachability and attractors have a simplercomputational complexity:- reachability can be assessed in a polynomial numberof iterations. The computation of iterations is in NP in the very general case,and is linear when local functions are monotonic, or with some usualrepresentations of functions of BNs (binary decision diagrams, Petri nets,automata networks, etc.). Thus, reachability is in P with locally-monotonicBNs, and P$^{\text{NP}}$ otherwise (instead of being PSPACE-complete withupdate modes);- deciding wherever a configuration belongs to an attractor is incoNP with locally-monotonic BNs, and coNP$^{\text{coNP}}$ otherwise (instead ofPSPACE-complete with update modes).Furthermore, we demonstrate that thesemantics completely captures any behavior achievable with any multilevel orODE refinement of the BN; and the semantics is minimal with respect to thismodel refinement criteria: to any most permissive trajectory, there exists amultilevel refinement of the BN which can reproduce it.In brief, the mostpermissive semantics of BNs enables a correct abstract reasoning on dynamics ofBNs, with a greater tractability than previously introduced update modes.","Thomas Chatain (MEXICO), Stefan Haar (MEXICO), Juraj Kol{\v{c}}\'ak
  (LSV), Lo\""ic Paulev\'e (LaBRI, BioInfo - LRI)",,,11,
"Wreath/cascade products and related decomposition results for the
  concurrent setting of Mazurkiewicz traces (extended version)","  We develop a new algebraic framework to reason about languages ofMazurkiewicz traces. This framework supports true concurrency and provides anon-trivial generalization of the wreath product operation to the tracesetting. A novel local wreath product principle has been established. The newframework is crucially used to propose a decomposition result for recognizabletrace languages, which is an analogue of the Krohn-Rhodes theorem. We provethis decomposition result in the special case of acyclic architectures andapply it to extend Kamp's theorem to this setting. We also introduce andanalyze distributed automata-theoretic operations called local and globalcascade products. Finally, we show that aperiodic trace languages can becharacterized using global cascade products of localized and distributedtwo-state reset automata.","Bharat Adsul, Paul Gastin, Saptarshi Sarkar, Pascal Weil",,,11,
"Reachability in two-parametric timed automata with one parameter is
  EXPSPACE-complete","  Parametric timed automata (PTA) are an extension of timed automata in whichclocks can be compared against parameters. The reachability problem asks forthe existence of an assignment of the parameters to the non-negative integerssuch that reachability holds in the underlying timed automaton. Thereachability problem for PTA is long known to be undecidable, already overthree parametric clocks.  A few years ago, Bundala and Ouaknine proved that for PTA over two parametricclocks and one parameter the reachability problem is decidable and also showeda lower bound for the complexity class PSPACE^NEXP. Our main result is that thereachability problem for two-parametric timed automata with one parameter isEXPSPACE-complete. Our contribution is two-fold.  For the EXPSPACE lower bound we make use of deep results from complexitytheory, namely a serializability characterization of EXPSPACE (based onBarrington's Theorem) and a logspace translation of numbers in chineseremainder representation to binary representation.  For the EXPSPACE upper bound, we give a careful exponential time reductionfrom PTA over two parametric clocks and one parameter to a slight subclass ofparametric one-counter automata (POCA) over one parameter based on a minoradjustment of a construction due to Bundala and Ouaknine. We provide a seriesof techniques to partition a fictitious run of a POCA into several carefullychosen subruns that allow us to prove that it is sufficient to consider aparameter value of exponential magnitude only. This allows us to show adoubly-exponential upper bound on the value of the only parameter of a PTA overtwo parametric clocks and one parameter. We hope that extensions of ourtechniques lead to finally establishing decidability of the long-standing openproblem of reachability in parametric timed automata with two parametric clocks(and arbitrarily many parameters).","Stefan G\""oller and Mathieu Hilaire",,,11,
How Good Is a Strategy in a Game With Nature?,"  We consider games with two antagonistic players --- \'Elo\""ise (modelling aprogram) and Ab\'elard (modelling a byzantine environment) --- and a third,unpredictable and uncontrollable player, that we call Nature. Motivated by thefact that the usual probabilistic semantics very quickly leads toundecidability when considering either infinite game graphs orimperfect-information, we propose two alternative semantics that leads todecidability where the probabilistic one fails: one based on counting and onebased on topology.",Arnaud Carayol and Olivier Serre,,,11,
What's Live? Understanding Distributed Consensus,"  Distributed consensus algorithms such as Paxos have been studied extensively.They all use a same definition of safety. Liveness is especially important inpractice despite well-known theoretical impossibility results. However, manydifferent liveness properties and assumptions have been stated, and there areno systematic comparisons for better understanding of these properties.  This paper studies and compares different liveness properties stated for over30 well-known consensus algorithms and variants. We build a lattice of livenessproperties combining a lattice of the assumptions used and a lattice of theassertions made, and we compare the strengths and weaknesses of algorithms thatensure these properties.  Our precise specifications and systematic comparisons led to the discovery ofa range of problems in various stated liveness properties, from lackingassumptions or too weak assumptions for which no liveness assertions can hold,to too strong assumptions making it trivial or uninteresting to achieve theassertions.  We also developed TLA+ specifications of these liveness properties. We showthat model checking execution steps using TLC can illustrate liveness patternsfor single-valued Paxos on up to 4 proposers and 4 acceptors in a few hours,but becomes too expensive for multi-valued Paxos or more processes.",Saksham Chand and Yanhong A Liu,,,11,
"Cpp-Taskflow v2: A General-purpose Parallel and Heterogeneous Task
  Programming System at Scale","  The Cpp-Taskflow project addresses the long-standing question: How can wemake it easier for developers to write parallel and heterogeneous programs withhigh performance and simultaneous high productivity? Cpp-Taskflow develops asimple and powerful task programming model to enable efficient implementationsof heterogeneous decomposition strategies. Our programming model empowers userswith both static and dynamic task graph constructions to incorporate a broadrange of computational patterns including hybrid CPU-GPU computing, dynamiccontrol flow, and irregularity. We develop an efficient heterogeneouswork-stealing strategy that adapts worker threads to available task parallelismat any time during the graph execution. We have demonstrated promisingperformance of Cpp-Taskflow on both micro-benchmark and real-worldapplications. As an example, we solved a large machine learning workload by upto 1.5x faster, 1.6x less memory, and 1.7x fewer lines of code than twoindustrial-strength systems, oneTBB and StarPU, on a machine of 40 CPUs and 4GPUs.","Tsung-Wei Huang, Dian-Lun Lin, Yibo Lin, Chun-Xun Lin",,,11,
Online Anomaly Detection in HPC Systems,"  Reliability is a cumbersome problem in High Performance Computing Systems andData Centers evolution. During operation, several types of fault conditions oranomalies can arise, ranging from malfunctioning hardware to improperconfigurations or imperfect software. Currently, system administrator and finalusers have to discover it manually. Clearly this approach does not scale tolarge scale supercomputers and facilities: automated methods to detect faultsand unhealthy conditions is needed. Our method uses a type of neural networkcalled autoncoder trained to learn the normal behavior of a real, in-productionHPC system and it is deployed on the edge of each computing node. We obtain avery good accuracy (values ranging between 90% and 95%) and we also demonstratethat the approach can be deployed on the supercomputer nodes without negativelyaffecting the computing units performance.","Andrea Borghesi, Antonio Libri, Luca Benini, Andrea Bartolini",,,11,
"VirtualFlow: Decoupling Deep Learning Model Execution from Underlying
  Hardware","  State-of-the-art deep learning systems tightly couple model execution withthe underlying hardware. This coupling poses important challenges in a worldwhere the scale of deep learning workloads is growing rapidly: workloads withhigh resource requirements are inaccessible to most users, experimentation onsmaller test beds is impossible, and results are difficult to reproduce acrossdifferent hardware.  We propose VirtualFlow, a novel system approach leveraging virtual nodeprocessing to decouple model execution from the hardware. In each executionstep, the batch is divided and processed with data parallelism on many virtualnodes instead of physical devices (GPUs, TPUs), and the gradients areaggregated and applied to the model after all virtual nodes finish processingtheir data. With multiple virtual nodes mapped to each device, the systemallows users to run models at much larger batch sizes that would otherwiseexceed the memory limits of the underlying physical resources. VirtualFlowsignificantly improves model training reproducibility across differenthardware, and enables models to run on shared clusters with dynamicallychanging resources for better efficiency.  Our implementation of VirtualFlow enables virtual node processing withelasticity for TensorFlow. Evaluation with representative deep learning models(ResNet, BERT, Transformer) demonstrates strong convergence guarantees ondifferent hardware with out-of-the-box hyperparameters, and up to 48% lower jobcompletion times with resource elasticity.","Andrew Or, Haoyu Zhang, Michael J. Freedman",,,11,
A CAD-Based tool for fault tolerant distributed embedded systems,"  Reliability and availability analysis are essential in dependable criticalembedded systems. The classical implementation of dependability for an embeddedsystem relies on merging both fundamental structures with the requireddependability techniques to form one composite structure. The separation of thebasic system components from the dependability components, reduces complexityand improves the design. The goal of this work is to assist implementingreconfiguration-based fault tolerance in safety-critical embedded systemsapplications. The primary intention is to reduce the repair time in order toenhance fault tolerance and produce dependable embedded systems. The proposedsolution is a dedicated CAD-tool designed to generate a reference strategy forthe system manager of a distributed embedded system to control andautomatically reconfigure the processing elements of the system. The proposedtool auto-generates program codes to be executed by a system manager to governthe DES. It also computes different reliability solutions with necessarysupporting calculated parameters and graphs sorted to support the faulttolerance design of the system. The proposed tool can be used to simulatepossible configurations based on the desired degrees of faults and systemreliability. The graphical interface of the tool is unique and hides thecomplexity of the systems underneath. A comparison with a similar tool ispresented.","Mahmoud I. Banat, Belal H. Sababha, Sami Al-Hamdan",,,11,
"SNEAP: A Fast and Efficient Toolchain for Mapping Large-Scale Spiking
  Neural Network onto NoC-based Neuromorphic Platform","  Spiking neural network (SNN), as the third generation of artificial neuralnetworks, has been widely adopted in vision and audio tasks. Nowadays, manyneuromorphic platforms support SNN simulation and adopt Network-on-Chips (NoC)architecture for multi-cores interconnection.  However, interconnection brings huge area overhead to the platform. Moreover,run-time communication on the interconnection has a significant effect on thetotal power consumption and performance of the platform. In this paper, wepropose a toolchain called SNEAP for mapping SNNs to neuromorphic platformswith multi-cores, which aims to reduce the energy and latency brought by spikecommunication on the interconnection.  SNEAP includes two key steps: partitioning the SNN to reduce the spikescommunicated between partitions, and mapping the partitions of SNN to the NoCto reduce average hop of spikes under the constraint of hardware resources.SNEAP can reduce more spikes communicated on the interconnection of NoC andspend less time than other toolchains in the partitioning phase. Moreover, theaverage hop of spikes is reduced more by SNEAP within a time period, whicheffectively reduces the energy and latency on the NoC-based neuromorphicplatform.  The experimental results show that SNEAP can achieve 418x reduction inend-to-end execution time, and reduce energy consumption and spike latency, onaverage, by 23% and 51% respectively, compared with SpiNeMap.","Shiming Li, Shasha Guo, Limeng Zhang, Ziyang Kang, Shiying Wang, Wei
  Shi, Lei Wang, Weixia Xu",,,11,
An Asynchronous Computability Theorem for Fair Adversaries,"  This paper proposes a simple topological characterization of a large class offair adversarial models via affine tasks: sub-complexes of the second iterationof the standard chromatic subdivision. We show that the task computability of amodel in the class is precisely captured by iterations of the correspondingaffine task. Fair adversaries include, but are not restricted to, the models ofwait-freedom, t-resilience, and $k$-concurrency. Our results generalize andimprove all previously derived topological characterizations of the ability ofa model to solve distributed tasks.","Petr Kuznetsov, Thibault Rieutord and Yuan He",,,11,
"Improved Parallel Construction of Wavelet Trees and Rank/Select
  Structures","  Existing parallel algorithms for wavelet tree construction have a workcomplexity of $O(n\log\sigma)$. This paper presents parallel algorithms for theproblem with improved work complexity. Our first algorithm is based on parallelinteger sorting and has either $O(n\log\log n\lceil\log\sigma/\sqrt{\logn\log\log n}\rceil)$ work and polylogarithmic depth, or$O(n\lceil\log\sigma/\sqrt{\log n}\rceil)$ work and sub-linear depth. We alsodescribe another algorithm that has $O(n\lceil\log\sigma/\sqrt{\log n} \rceil)$work and $O(\sigma+\log n)$ depth. We then show how to use similar ideas toconstruct variants of wavelet trees (arbitrary-shaped binary trees and multiarytrees) as well as wavelet matrices in parallel with lower work complexity thanprior algorithms. Finally, we show that the rank and select structures onbinary sequences and multiary sequences, which are stored on wavelet treenodes, can be constructed in parallel with improved work bounds, matching thoseof the best existing sequential algorithms for constructing rank and selectstructures.",Julian Shun,,,11,
"Data Replication for Reducing Computing Time in Distributed Systems with
  Stragglers","  In distributed computing systems with stragglers, various forms of redundancycan improve the average delay performance. We study the optimal replication ofdata in systems where the job execution time is a stochastically decreasing andconvex random variable. We show that in such systems, the optimum assignmentpolicy is the balanced replication of disjoint batches of data. Furthermore,for Exponential and Shifted-Exponential service times, we derive the optimumredundancy levels for minimizing both expected value and the variance of thejob completion time. Our analysis shows that, the optimum redundancy level maynot be the same for the two metrics, thus there is a trade-off between reducingthe expected value of the completion time and reducing its variance.",Amir Behrouzi-Far and Emina Soljanin,,,11,
Mapping Matters: Application Process Mapping on 3-D Processor Topologies,"  Applications' performance is influenced by the mapping of processes tocomputing nodes, the frequency and volume of exchanges among processingelements, the network capacity, and the routing protocol. A poor mapping ofapplication processes degrades performance and wastes resources. Processmapping is frequently ignored as an explicit optimization step since the systemtypically offers a default mapping, users may lack awareness of theirapplications' communication behavior, and the opportunities for improvingperformance through mapping are often unclear. This work studies the impact ofapplication process mapping on several processor topologies. We propose aworkflow that renders mapping as an explicit optimization step for parallelapplications. We apply the workflow to a set of four applications, twelvemapping algorithms, and three direct network topologies. We assess themappings' quality in terms of volume, frequency, and distance of exchangesusing metrics such as dilation (measured in hop$\cdot$Byte). With a paralleltrace-based simulator, we predict the applications' execution on the threetopologies using the twelve mappings. We evaluate the impact of process mappingon the applications' simulated performance in terms of execution andcommunication times and identify the mappings that achieve the highestperformance in both cases. To ensure the correctness of the simulations, wecompare the pre- and post-simulation results. This work emphasizes theimportance of process mapping as an explicit optimization step and offers asolution for parallel applications to exploit the full potential of theallocated resources on a given system.","Jonas H. M\""uller Kornd\""orfer and Mario Bielert and La\'ercio L.
  Pilla and Florina M. Ciorba",,,11,
"ROMANet: Fine-Grained Reuse-Driven Off-Chip Memory Access Management and
  Data Organization for Deep Neural Network Accelerators","  Enabling high energy efficiency is crucial for embedded implementations ofdeep learning. Several studies have shown that the DRAM-based off-chip memoryaccesses are one of the most energy-consuming operations in deep neural network(DNN) accelerators, and thereby limit the designs from achieving efficiencygains at the full potential. DRAM access energy varies depending upon thenumber of accesses required as well as the energy consumed per-access.Therefore, searching for a solution towards the minimum DRAM access energy isan important optimization problem. Towards this, we propose the ROMANetmethodology that aims at reducing the number of memory accesses, by searchingfor the appropriate data partitioning and scheduling for each layer of anetwork using a design space exploration, based on the knowledge of theavailable on-chip memory and the data reuse factors. Moreover, ROMANet alsotargets decreasing the number of DRAM row buffer conflicts and misses, byexploiting the DRAM multi-bank burst feature to improve the energy-per-access.Besides providing the energy benefits, our proposed DRAM data mapping alsoresults in an increased effective DRAM throughput, which is useful forlatency-constraint scenarios. Our experimental results show that the ROMANetsaves DRAM access energy by 12% for the AlexNet, by 36% for the VGG-16, and by46% for the MobileNet, while also improving the DRAM throughput by 10%, ascompared to the state-of-the-art.","Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad
  Shafique",,,11,
"HeAT -- a Distributed and GPU-accelerated Tensor Framework for Data
  Analytics","  To cope with the rapid growth in available data, the efficiency of dataanalysis and machine learning libraries has recently received increasedattention. Although great advancements have been made in traditionalarray-based computations, most are limited by the resources available on asingle computation node. Consequently, novel approaches must be made to exploitdistributed resources, e.g. distributed memory architectures. To this end, weintroduce HeAT, an array-based numerical programming framework for large-scaleparallel processing with an easy-to-use NumPy-like API. HeAT utilizes PyTorchas a node-local eager execution engine and distributes the workload onarbitrarily large high-performance computing systems via MPI. It provides bothlow-level array computations, as well as assorted higher-level algorithms. WithHeAT, it is possible for a NumPy user to take full advantage of their availableresources, significantly lowering the barrier to distributed data analysis.When compared to similar frameworks, HeAT achieves speedups of up to two ordersof magnitude.","Markus G\""otz, Daniel Coquelin, Charlotte Debus, Kai Krajsek, Claudia
  Comito, Philipp Knechtges, Bj\""orn Hagemeier, Michael Tarnawa, Simon
  Hanselmann, Martin Siggel, Achim Basermann, Achim Streit",,,11,
Ramanujan Graphs and the Spectral Gap of Supercomputing Topologies,"  Graph eigenvalues play a fundamental role in controlling structuralproperties, such as bisection bandwidth, diameter, and fault tolerance, whichare critical considerations in the design of supercomputing interconnectionnetworks. This motivates considering graphs with optimal spectral expansion,called Ramanujan graphs, as potential candidates for interconnection networks.In this work, we explore this possibility by comparing Ramanujan graphproperties against those of a wide swath of current and proposed supercomputingtopologies. We derive analytic expressions for the spectral gap, bisectionbandwidth, and diameter of these topologies, some of which were previouslyunknown. We find the spectral gap of existing topologies are well-separatedfrom the optimal achievable by Ramanujan topologies, suggesting the potentialutility of adopting Ramanujan graphs as interconnection networks.","Sinan G. Aksoy, Paul Bruillard, Stephen J. Young, Mark Raugas",,,11,
On the Analysis of Spatially Constrained Power of Two Choice Policies,"  We consider a class of power of two choice based assignment policies forallocating users to servers, where both users and servers are located on atwo-dimensional Euclidean plane. In this framework, we investigate the inherenttradeoff between the communication cost, and load balancing performance ofdifferent allocation policies. To this end, we first design and evaluate aSpatial Power of two (sPOT) policy in which each user is allocated to the leastloaded server among its two geographically nearest servers sequentially. Whenservers are placed on a two-dimensional square grid, sPOT maps to the classicalPower of two (POT) policy on the Delaunay graph associated with the Voronoitessellation of the set of servers. We show that the associated Delaunay graphis 4-regular and provide expressions for asymptotic maximum load using resultsfrom the literature. For uniform placement of servers, we map sPOT to aclassical balls and bins allocation policy with bins corresponding to theVoronoi regions associated with the second order Voronoi diagram of the set ofservers. We provide expressions for the lower bound on the asymptotic expectedmaximum load on the servers and prove that sPOT does not achieve POT loadbalancing benefits. However, experimental results suggest the efficacy of sPOTwith respect to expected communication cost. Finally, we propose twonon-uniform server sampling based POT policies that achieve the best of boththe performance metrics. Experimental results validate the effctiveness of ourproposed policies.","Nitish K. Panigrahy, Prithwish Basu, Don Towsley, Ananthram Swami and
  Kin K. Leung",,,11,
"The Scalable Systems Laboratory: a Platform for Software Innovation for
  HEP","  The Scalable Systems Laboratory (SSL), part of the IRIS-HEP SoftwareInstitute, provides Institute participants and HEP software developersgenerally with a means to transition their R&D from conceptual toys to testbedsto production-scale prototypes. The SSL enables tooling, infrastructure, andservices supporting the innovation of novel analysis and data architectures,development of software elements and tool-chains, reproducible functional andscalability testing of service components, and foundational systems R&D foraccelerated services developed by the Institute. The SSL is constructed with acore team having expertise in scale testing and deployment of services across awide range of cyberinfrastructure. The core team embeds and partners with otherareas in the Institute, and with LHC and other HEP development and operationsteams as appropriate, to define investigations and required service deploymentpatterns. We describe the approach and experiences with early applicationdeployments, including analysis platforms and intelligent data deliverysystems.","Robert Gardner, Lincoln Bryant, Mark Neubauer, Frank Wuerthwein,
  Judith Stephen and Andrew Chien",,,11,
Smart Contract-based Computing ResourcesTrading in Edge Computing,"  In recent years, there is an emerging trend that some computing services aremoving from cloud to the edge of the networks. Compared to cloud computing,edge computing can provide services with faster response, lower expense, andmore security. The massive idle computing resources closing to the edge alsoenhance the deployment of edge services. Instead of using cloud services fromsome primary providers, edge computing provides people with a great chance toactively join the market of computing resources. However, edge computing alsohas some critical impediments that we have to overcome.  In this paper, we design an edge computing service platform that can receiveand distribute the computing resources from the end-users in a decentralizedway. Without centralized trade control, we propose a novel hierarchical smartcontract-based decentralized technique to establish the trading trust amongusers and provide flexible smart contract interfaces to satisfy users. Oursystem also considers and resolves a variety of security and privacy challengeswhen utilizing the encryption and distributed access control mechanism. Weimplement our system and conduct extensive experiments to show the feasibilityand effectiveness of our proposed system.","Jinyue Song, Tianbo Gu, Yunjie Ge, Prasant Mohapatra",,,11,
"ComPar: Optimized Multi-Compiler for Automatic OpenMP S2S
  Parallelization","  Parallelization schemes are essential in order to exploit the full benefitsof multi-core architectures. In said architectures, the most comprehensiveparallelization API is OpenMP. However, the introduction of correct and optimalOpenMP parallelization to applications is not always a simple task, due tocommon parallel management pitfalls, architecture heterogeneity and the currentnecessity for human expertise in order to comprehend many fine details andabstract correlations. To ease this process, many automatic parallelizationcompilers were created over the last decade. Harel et al. [2020] tested severalsource-to-source compilers and concluded that each has its advantages anddisadvantages and no compiler is superior to all other compilers in all tests.This indicates that a fusion of the compilers' best outputs under the besthyper-parameters for the current hardware setups can yield greater speedups. Tocreate such a fusion, one should execute a computationally intensivehyper-parameter sweep, in which the performance of each option is estimated andthe best option is chosen. We created a novel parallelization source-to-sourcemulti-compiler named ComPar, which uses code segmentation-and-fusion withhyper-parameters tuning to achieve the best parallel code possible without anyhuman intervention while maintaining the program's validity. In this paper wepresent ComPar and analyze its results on NAS and PolyBench benchmarks. Weconclude that although the resources ComPar requires to produce parallel codeare greater than other source-to-source parallelization compilers - as itdepends on the number of parameters the user wishes to consider, and theircombinations - ComPar achieves superior performance overall compared to theserial code version and other tested parallelization compilers. ComPar ispublicly available at: https://github.com/Scientific-Computing-Lab-NRCN/compar.","Idan Mosseri, Lee-or Alon, Re'em Harel and Gal Oren",,,11,
"Consistency of Proof-of-Stake Blockchains with Concurrent Honest Slot
  Leaders","  We improve the fundamental security threshold of eventual consensusProof-of-Stake (PoS) blockchain protocols under the longest-chain rule byshowing, for the first time, the positive effect of rounds with concurrenthonest leaders.  Current security analyses reduce consistency to the dynamics of an abstract,round-based block creation process that is determined by three eventsassociated with a round: (i) event $A$: at least one adversarial leader, (ii)event $S$: a single honest leader, and (iii) event $M$: multiple, but honest,leaders. We present an asymptotically optimal consistency analysis assumingthat an honest round is more likely than an adversarial round (i.e., $\Pr[S] +\Pr[M] > \Pr[A]$); this threshold is optimal. This is a first in the literatureand can be applied to both the simple synchronous communication as well ascommunication with bounded delays.  In all existing consistency analyses, event $M$ is either penalized ortreated neutrally. Specifically, the consistency analyses in Ouroboros Praos(Eurocrypt 2018) and Genesis (CCS 2018) assume that $\Pr[S] - \Pr[M] > \Pr[A]$;the analyses in Sleepy Consensus (Asiacrypt 2017) and Snow White (Fin. Crypto2019) assume that $\Pr[S] > \Pr[A]$. Moreover, all existing analyses completelybreak down when $\Pr[S] < \Pr[A]$. These thresholds determine the criticaltrade-off between the honest majority, network delays, and consistency error.  Our new results can be directly applied to improve the security guarantees ofthe existing protocols. We also provide an efficient algorithm to explicitlycalculate these error probabilities in the synchronous setting. Furthermore, wecomplement these results by analyzing the setting where $S$ is rare, evenallowing $\Pr[S] = 0$, under the added assumption that honest players adopt aconsistent chain selection rule.","Aggelos Kiayias, Saad Quader, and Alexander Russell",,,11,
Population protocols with unreliable communication,"  Population protocols are a model of distributed computation intended for thestudy of networks of independent computing agents with dynamic communicationstructure. Each agent has a finite number of states, and communicationopportunities occur nondeterministically, allowing the agents involved tochange their states based on each other's states.  In the present paper we study unreliable models based on population protocolsand their variations from the point of view of expressive power. We model theeffects of message loss. We show that for a general definition of unreliableprotocols with constant-storage agents such protocols can only computepredicates computable by immediate observation population protocols (sometimesalso called one-way protocols). Immediate observation population protocols areinherently tolerant of unreliable communication and keep their expressive powerunder a wide range of fairness conditions. We also prove that a large class ofmessage-based models that are generally more expressive than immediateobservation becomes strictly less expressive than immediate observation in theunreliable case.",Mikhail Raskin,,,11,
"Optimizing Deep Learning Recommender Systems' Training On CPU Cluster
  Architectures","  During the last two years, the goal of many researchers has been to squeezethe last bit of performance out of HPC system for AI tasks. Often thisdiscussion is held in the context of how fast ResNet50 can be trained.Unfortunately, ResNet50 is no longer a representative workload in 2020. Thus,we focus on Recommender Systems which account for most of the AI cycles incloud computing centers. More specifically, we focus on Facebook's DLRMbenchmark. By enabling it to run on latest CPU hardware and software tailoredfor HPC, we are able to achieve more than two-orders of magnitude improvementin performance (110x) on a single socket compared to the reference CPUimplementation, and high scaling efficiency up to 64 sockets, while fittingultra-large datasets. This paper discusses the optimization techniques for thevarious operators in DLRM and which component of the systems are stressed bythese different operators. The presented techniques are applicable to a broaderset of DL workloads that pose the same scaling challenges/characteristics asDLRM.","Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping
  Chen, Mikhail Shiryaev, Alexander Heinecke",,,11,
Heterogeneous Verification of an Autonomous Curiosity Rover,"  The Curiosity rover is one of the most complex systems successfully deployedin a planetary exploration mission to date. It was sent by NASA to explore thesurface of Mars and to identify potential signs of life. Even though it haslimited autonomy on-board, most of its decisions are made by the ground controlteam. This hinders the speed at which the Curiosity reacts to its environment,due to the communication delays between Earth and Mars. Depending on theorbital position of both planets, it can take 4--24 minutes for a message to betransmitted between Earth and Mars. If the Curiosity were controlledautonomously, it would be able to perform its activities much faster and moreflexibly. However, one of the major barriers to increased use of autonomy insuch scenarios is the lack of assurances that the autonomous behaviour willwork as expected. In this paper, we use a Robot Operating System (ROS) model ofthe Curiosity that is simulated in Gazebo and add an autonomous agent that isresponsible for high-level decision-making. Then, we use a mixture of formaland non-formal techniques to verify the distinct system components (ROS nodes).This use of heterogeneous verification techniques is essential to provideguarantees about the nodes at different abstraction levels, and allows us tobring together relevant verification evidence to provide overall assurance.","Rafael C. Cardoso, Marie Farrell, Matt Luckcuck, Angelo Ferrando, and
  Michael Fisher",,,11,
"Towards Characterizing Adversarial Defects of Deep Learning Software
  from the Lens of Uncertainty","  Over the past decade, deep learning (DL) has been successfully applied tomany industrial domain-specific tasks. However, the current state-of-the-art DLsoftware still suffers from quality issues, which raises great concernespecially in the context of safety- and security-critical scenarios.Adversarial examples (AEs) represent a typical and important type of defectsneeded to be urgently addressed, on which a DL software makes incorrectdecisions. Such defects occur through either intentional attack orphysical-world noise perceived by input sensors, potentially hindering furtherindustry deployment. The intrinsic uncertainty nature of deep learningdecisions can be a fundamental reason for its incorrect behavior. Although sometesting, adversarial attack and defense techniques have been recently proposed,it still lacks a systematic study to uncover the relationship between AEs andDL uncertainty. In this paper, we conduct a large-scale study towards bridgingthis gap. We first investigate the capability of multiple uncertainty metricsin differentiating benign examples (BEs) and AEs, which enables to characterizethe uncertainty patterns of input data. Then, we identify and categorize theuncertainty patterns of BEs and AEs, and find that while BEs and AEs generatedby existing methods do follow common uncertainty patterns, some otheruncertainty patterns are largely missed. Based on this, we propose an automatedtesting technique to generate multiple types of uncommon AEs and BEs that arelargely missed by existing techniques. Our further evaluation reveals that theuncommon data generated by our method is hard to be defended by the existingdefense techniques with the average defense success rate reduced by 35\%. Ourresults call for attention and necessity to generate more diverse data forevaluating quality assurance solutions of DL software.","Xiyue Zhang, Xiaofei Xie, Lei Ma, Xiaoning Du, Qiang Hu, Yang Liu,
  Jianjun Zhao, Meng Sun",,,11,
Detecting Latency Degradation Patterns in Service-based Systems,"  Performance in heterogeneous service-based systems shows non-determistictrends. Even for the same request type, latency may vary from one request toanother. These variations can occur due to several reasons on different levelsof the software stack: operating system, network, software libraries,application code or others. Furthermore, a request may involve several RemoteProcedure Calls (RPC), where each call can be subject to performance variation.Performance analysts inspect distributed traces and seek for recurrent patternsin trace attributes, such as RPCs execution time, in order to cluster traces inwhich variations may be induced by the same cause. Clustering ""similar"" tracesis a prerequisite for effective performance debugging. Given the scale of theproblem, such activity can be tedious and expensive. In this paper, we presentan automated approach that detects relevant RPCs execution time patternsassociated to request latency degradation, i.e. latency degradation patterns.The presented approach is based on a genetic search algorithm driven by aninformation retrieval relevance metric and an optimized fitness evaluation.Each latency degradation pattern identifies a cluster of requests subject tolatency degradation with similar patterns in RPCs execution time. We show on amicroservice-based application case study that the proposed approach caneffectively detect clusters identified by artificially injected latencydegradation patterns. Experimental results show that our approach outperformsin terms of F-score a state-of-art approach for latency profile analysis andwidely popular machine learning clustering algorithms. We also show how ourapproach can be easily extended to trace attributes other than RPC executiontime (e.g. HTTP headers, execution node, etc.).","Vittorio Cortellessa, Luca Traini",,,11,
Mutation testing of smart contracts at scale,"  It is crucial that smart contracts are tested thoroughly due to theirimmutable nature. Even small bugs in smart contracts can lead to huge monetarylosses. However, testing is not enough; it is also important to ensure thequality and completeness of the tests. There are already several approachesthat tackle this challenge with mutation testing, but their effectiveness isquestionable since they only considered small contract samples. Hence, weevaluate the quality of smart contract mutation testing at scale. We choose themost promising of the existing (smart contract specific) mutation operators,analyse their effectiveness in terms of killability and highlight severevulnerabilities that can be injected with the mutations. Moreover, we improvethe existing mutation methods by introducing a novel killing condition that isable to detect a deviation in the gas consumption, i.e., in the monetary valuethat is required to perform transactions.  This paper has a replication package athttps://github.com/pieterhartel/Mutation-at-scale","Pieter Hartel, Richard Schumi",,,11,
Vulnerability Coverage as an Adequacy Testing Criterion,"  Mainstream software applications and tools are the configurable platformswith an enormous number of parameters along with their values. Certain settingsand possible interactions between these parameters may harden (or soften) thesecurity and robustness of these applications against some knownvulnerabilities. However, the large number of vulnerabilities reported andassociated with these tools make the exhaustive testing of these toolsinfeasible against these vulnerabilities infeasible. As an instance of generalsoftware testing problem, the research question to address is whether thesystem under test is robust and secure against these vulnerabilities. Thispaper introduces the idea of ``vulnerability coverage,'' a concept toadequately test a given application for a certain classes of vulnerabilities,as reported by the National Vulnerability Database (NVD). The deriving idea isto utilize the Common Vulnerability Scoring System (CVSS) as a means to measurethe fitness of test inputs generated by evolutionary algorithms and thenthrough pattern matching identify vulnerabilities that match the generatedvulnerability vectors and then test the system under test for those identifiedvulnerabilities. We report the performance of two evolutionary algorithms(i.e., Genetic Algorithms and Particle Swarm Optimization) in generating thevulnerability pattern vectors.",Shuvalaxmi Dass and Akbar Siami Namin,,,11,
"How to Improve AI Tools (by Adding in SE Knowledge): Experiments with
  the TimeLIME Defect Reduction Tool","  AI algorithms are being used with increased frequency in SE research andpractice. Such algorithms are usually commissioned and certified using datafrom outside the SE domain. Can we assume that such algorithms can be used''off-the-shelf'' (i.e. with no modifications)? To say that another way, arethere special features of SE problems that suggest a different and better wayto use AI tools?  To answer these questions, this paper reports experiments with TimeLIME, avariant of the LIME explanation algorithm from KDD'16. LIME can offerrecommendations on how to change static code attributes in order to reduce thenumber of defects in the next software release. That version of LIME used aninternal weighting tool to decide what attributes to include/exclude in thoserecommendations. TimeLIME improves on that weighting scheme using the followingSE knowledge: software comes in releases; an implausible change to software issomething that has never been changed in prior releases; so it is better to useplausible changes, i.e. changes with some precedent in the prior releases. Byrestricting recommendations to just the frequently changed attributes, TimeLIMEcan produce (a)~dramatically better explanations of what causes defects and(b)~much better recommendations on how to fix buggy code.  Apart from these specific results about defect reduction and TimeLIME, themore general point of this paper is that our community should be more carefulabout using off-the-shelf AI tools, without first applying SE knowledge. Asshown here, it may not be a complex matter to apply that knowledge. Further,once that SE knowledge is applied, this can result in dramatically bettersystems.","Kewen Peng, Tim Menzies",,,11,
TypeWriter: Neural Type Prediction with Search-based Validation,"  Maintaining large code bases written in dynamically typed languages, such asJavaScript or Python, can be challenging due to the absence of typeannotations: simple data compatibility errors proliferate, IDE support islimited, and APIs are hard to comprehend. Recent work attempts to address thoseissues through either static type inference or probabilistic type prediction.Unfortunately, static type inference for dynamic languages is inherentlylimited, while probabilistic approaches suffer from imprecision. This paperpresents TypeWriter, the first combination of probabilistic type predictionwith search-based refinement of predicted types. TypeWriter's predictor learnsto infer the return and argument types for functions from partially annotatedcode bases by combining the natural language properties of code withprogramming language-level information. To validate predicted types, TypeWriterinvokes a gradual type checker with different combinations of the predictedtypes, while navigating the space of possible type combinations in afeedback-directed manner. We implement the TypeWriter approach for Python andevaluate it on two code corpora: a multi-million line code base at Facebook anda collection of 1,137 popular open-source projects. We show that TypeWriter'stype predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5)predictions for return types, and 0.57 (0.80) for argument types, which clearlyoutperforms prior type prediction models. By combining predictions withsearch-based validation, TypeWriter can fully annotate between 14% to 44% ofthe files in a randomly selected corpus, while ensuring type correctness. Acomparison with a static type inference tool shows that TypeWriter adds manymore non-trivial types. TypeWriter currently suggests types to developers atFacebook and several thousands of types have already been accepted with minimalchanges.","Michael Pradel, Georgios Gousios, Jason Liu, Satish Chandra",,,11,
E-APR: Mapping the Effectiveness of Automated Program Repair,"  Automated Program Repair (APR) is a fast growing area with many newtechniques being developed to tackle one of the most challenging softwareengineering problems. APR techniques have shown promising results, giving ushope that one day it will be possible for software to repair itself. Existingtechniques, however, are only effective at repairing certain kinds of bugs. Forexample, prior studies have shown that the effectiveness of APR techniques iscorrelated with bug complexity, with most techniques producing patches for easybugs. This is a useful explanation that can help researchers improve APRtechniques. In this paper, we extend these explanations towards a more granularlevel, with the aim of assessing the strengths and weaknesses of existing APRtechniques. To this end, we introduce e-APR, which is a new framework forexplaining the effectiveness of APR techniques. E-APR takes as input a set ofbuggy programs, their features and a set of APR techniques, and generates thefootprints of APR techniques, i.e., the regions of the instance space of buggyprograms in which good performance is expected from each technique. In thispaper, we consider features of the whole program, such as the number of methodsand the depth of the inheritance tree, and more specific features of the buggypart of the program, such as the number of Boolean operators in an ifstatement. The e-APR framework performs machine learning and dimensionalityreduction over the feature space to identify the most significant features thathave an impact on the effectiveness of APR. The footprints of APR techniquesare presented both in a visual and numerical way, which enables us to determinetheir strengths and weaknesses and how different the APR techniques are fromeach-other. Finally, e-APR could be integrated to repair infrastructures andrepair bots to choose, given a buggy program, the most suitable APR tool.",Aldeida Aleti and Matias Martinez,,,11,
"Why and How to Balance Alignment and Diversity of Requirements
  Engineering Practices in Automotive","  In large-scale automotive companies, various requirements engineering (RE)practices are used across teams. RE practices manifest in RequirementsInformation Models (RIM) that define what concepts and information should becaptured for requirements. Collaboration of practitioners from different partsof an organization is required to define a suitable RIM that balances supportfor diverse practices in individual teams with the alignment needed for ashared view and team support on system level. There exists no guidance for thischallenging task. This paper presents a mixed methods study to examine the roleof RIMs in balancing alignment and diversity of RE practices in four automotivecompanies. Our analysis is based on data from systems engineering tools, 11semi-structured interviews, and a survey to validate findings and suggestions.We found that balancing alignment and diversity of RE practices is important toconsider when defining RIMs. We further investigated enablers for this balanceand actions that practitioners take to achieve it. From these factors, wederived and evaluated recommendations for managing RIMs in practice that takeinto account the lifecycle of requirements and allow for diverse practicesacross sub-disciplines in early development, while enforcing alignment ofrequirements that are close to release.","Rebekka Wohlrab, Eric Knauss, Patrizio Pelliccione",,,11,
"Research, Develop, Deploy: Building a Full Spectrum Software Engineering
  and Research Department","  At Sandia National Laboratories, the Software Engineering and ResearchDepartment seeks to provide sustainable career pathways for research softwareengineers (RSEs). The conceptual model for our organization follows what wecall a Research, Develop, and Deploy (RDD) workflow pattern, enabling RSEs topartner with research and deployment specialists. We argue that thisinterdisciplinary model allows our department to act as an incubator and anaccelerator for impactful ideas. We describe these tactics and our experiencesas a RSE team in a scientific computing center.","Reed Milewicz, James Willenbring, and Dena Vigil",,,11,
"Session-Based Recommender Systems for Action Selection in GUI Test
  Generation","  Test generation at the graphical user interface (GUI) level has proven to bean effective method to reveal faults. When doing so, a test generator has torepeatably decide what action to execute given the current state of the systemunder test (SUT). This problem of action selection usually involves randomchoice, which is often referred to as monkey testing. Some approaches leverageother techniques to improve the overall effectiveness, but only a few try tocreate human-like actions---or even entire action sequences. We have built anovel session-based recommender system that can guide test generation. Thisallows us to mimic past user behavior, reaching states that require complexinteractions. We present preliminary results from an empirical study, where weuse GitHub as the SUT. These results show that recommender systems appear to bewell-suited for action selection, and that the approach can significantlycontribute to the improvement of GUI-based test generation.",Varun Nayak and Daniel Kraus,,,11,
"Introducing Artificial Intelligence Agents to the Empirical Measurement
  of Design Properties for Aspect Oriented Software Development","  The proponents of Aspect Oriented Software Development (AOSD) methodologyhave done a tremendous amount of work to bring out the positive effects of itsadoption using quantitative assessment. A structured assessment of themethodology requires a well-defined quality model. In this paper, an AI agentbased quality model has been proposed to evaluate the effect of aspectization.The model has been applied on already existing and equivalent versions ofobject oriented and aspect oriented case study application, universityautomation software. Specific metrics for the software design properties havebeen measured using AI agents for the different versions and were used to inferupon the effect on quality. Based on the initial measurement, it was found thataspectization has positively improved all the three quality attributes definedin the quality model. The attributes of evolution and reusability showedsignificant improvement in quality due to the effect of aspectization.",Senthil Velan S,,,11,
"Scalable Online Vetting of Android Apps for Measuring Declared SDK
  Versions and Their Consistency with API Calls","  Android has been the most popular smartphone system with multiple platformversions active in the market. To manage the application's compatibility withone or more platform versions, Android allows apps to declare the supportedplatform SDK versions in their manifest files. In this paper, we conduct asystematic study of this modern software mechanism. Our objective is to measurethe current practice of declared SDK versions (which we term as DSDK versionsafterwards) in real apps, and the (in)consistency between DSDK versions andtheir host apps' API calls. To successfully analyze a modern dataset of 22,687popular apps (with an average app size of 25MB), we design a scalable approachthat operates on the Android bytecode level and employs a lightweight bytecodesearch for app analysis. This approach achieves a good performance suitable foronline vetting in app markets, requiring only around 5 seconds to process anapp on average. Besides shedding light on the characteristics of DSDK in thewild, our study quantitatively measures two side effects of inappropriate DSDKversions: (i) around 35% apps under-set the minimum DSDK versions and couldincur runtime crashes, but fortunately, only 11.3% apps could crash on Android6.0 and above; (ii) around 2% apps, due to under-claiming the targeted DSDKversions, are potentially exploitable by remote code execution, and half ofthem invoke the vulnerable API via embedded third-party libraries. Theseresults indicate the importance and difficulty of declaring correct DSDK, andour work can help developers fulfill this goal.",Daoyuan Wu and Debin Gao and David Lo,,,11,
"On-the-Fly Construction of Composite Events in Scenario-Based Modeling
  using Constraint Solvers","  Scenario-Based Programming is a methodology for modeling and constructingcomplex reactive systems from simple, stand-alone building blocks, calledscenarios. These scenarios are designed to model different traits of thesystem, and can be interwoven together and executed to produce cohesive systembehavior. Existing execution frameworks for scenario-based programs allowscenarios to specify their view of what the system must, may, or must not doonly through very strict interfaces. This limits the methodology's expressivepower and often prevents users from modeling certain complex requirements.Here, we propose to extend Scenario-Based Programming's execution mechanism toallow scenarios to specify how the system should behave using rich logicalconstraints. We then leverage modern constraint solvers (such as SAT or SMTsolvers) to resolve these constraints at every step of running the system,towards yielding the desired overall system behavior. We provide animplementation of our approach and demonstrate its applicability to varioussystems that could not be easily modeled in an executable manner by existingScenario-Based approaches.","Guy Katz, Assaf Marron, Aviran Sadon, Gera Weiss",,,11,
"An Automated Framework for the Extraction of Semantic Legal Metadata
  from Legal Texts","  Semantic legal metadata provides information that helps with understandingand interpreting legal provisions. Such metadata is therefore important for thesystematic analysis of legal requirements. However, manually enhancing a largelegal corpus with semantic metadata is prohibitively expensive. Our work ismotivated by two observations: (1) the existing requirements engineering (RE)literature does not provide a harmonized view on the semantic metadata typesthat are useful for legal requirements analysis; (2) automated support for theextraction of semantic legal metadata is scarce, and it does not exploit thefull potential of artificial intelligence technologies, notably naturallanguage processing (NLP) and machine learning (ML). Our objective is to takesteps toward overcoming these limitations. To do so, we review and reconcilethe semantic legal metadata types proposed in the RE literature. Subsequently,we devise an automated extraction approach for the identified metadata typesusing NLP and ML. We evaluate our approach through two case studies over theLuxembourgish legislation. Our results indicate a high accuracy in thegeneration of metadata annotations. In particular, in the two case studies, wewere able to obtain precision scores of 97.2% and 82.4% and recall scores of94.9% and 92.4%.","Amin Sleimi, Nicolas Sannier, Mehrdad Sabetzadeh, Lionel Briand,
  Marcello Ceci and John Dann",,,11,
Some Aspects of a Software Reliability Problem,"  Obviously, the dynamism of software reliability research has speeded upsignificantly in the last period, and we can state the fact that its intensityis approaching, and in some cases is ahead of the information systems hardwarereliability research intensity. Reliability of software is much more importantthan its other characteristics, such as runtime, and although the absolutereliability of modern software is apparently unattainable, there is still nogenerally accepted measure of reliability of computer programs. The articleanalyzes the reasons for the situation and offers an approach to solving theproblem. The article touches upon the issue of general characteristics ofinformation systems software life cycle. Considered software applicationreliability questions and use of fail-safe ensuring programming. Also presentedbasic types of so-called virus programs that lead to abnormal functioning ofinformation systems. Much attention is given to presenting some known modelsused for software debugging and operating. So, this review paper consists offour sections: information systems software process creation, reliability ofinformation systems software, using of fail-safe programs and estimation ofsoftware reliability according the results of adjusting and normal operation.","Anton Petrov, Elena Popova, Alexander Petrov",,,11,
Analyzing Web Search Behavior for Software Engineering Tasks,"  Web search plays an integral role in software engineering (SE) to help withvarious tasks such as finding documentation, debugging, installation, etc. Inthis work, we present the first large-scale analysis of web search behavior forSE tasks using the search query logs from Bing, a commercial web search engine.First, we use distant supervision techniques to build a machine learningclassifier to extract the SE search queries with an F1 score of 93%. We thenperform an analysis on one million search sessions to understand how softwareengineering related queries and sessions differ from other queries andsessions. Subsequently, we propose a taxonomy of intents to identify thevarious contexts in which web search is used in software engineering. Lastly,we analyze millions of SE queries to understand the distribution, searchmetrics and trends across these SE search intents. Our analysis shows that SErelated queries form a significant portion of the overall web search traffic.Additionally, we found that there are six major intent categories for which websearch is used in software engineering. The techniques and insights can notonly help improve existing tools but can also inspire the development of newtools that aid in finding information for SE related tasks.","Nikitha Rao, Chetan Bansal, Thomas Zimmermann, Ahmed Hassan Awadallah,
  Nachiappan Nagappan",,,11,
"A Systematic Review of Unsupervised Learning Techniques for Software
  Defect Prediction","  Background: Unsupervised machine learners have been increasingly applied tosoftware defect prediction. It is an approach that may be valuable for softwarepractitioners because it reduces the need for labeled training data. Objective:Investigate the use and performance of unsupervised learning techniques insoftware defect prediction. Method: We conducted a systematic literature reviewthat identified 49 studies containing 2456 individual experimental results,which satisfied our inclusion criteria published between January 2000 and March2018. In order to compare prediction performance across these studies in aconsistent way, we (re-)computed the confusion matrices and employed theMatthews Correlation Coefficient (MCC) as our main performance measure.Results: Our meta-analysis shows that unsupervised models are comparable withsupervised models for both within-project and cross-project prediction. Amongthe 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs(FSOMs) perform best. In addition, where we were able to check, we found thatalmost 11% (262/2456) of published results (contained in 16 papers) wereinternally inconsistent and a further 33% (823/2456) provided insufficientdetails for us to check. Conclusion: Although many factors impact theperformance of a classifier, e.g., dataset characteristics, broadly speaking,unsupervised classifiers do not seem to perform worse than the supervisedclassifiers in our review. However, we note a worrying prevalence of (i)demonstrably erroneous experimental results, (ii) undemanding benchmarks and(iii) incomplete reporting. We therefore encourage researchers to becomprehensive in their reporting.","Ning Li, Martin Shepperd, Yuchen Guo",,,11,
Probabilistic Bisimulation for Parameterized Systems (Technical Report),"  Probabilistic bisimulation is a fundamental notion of process equivalence forprobabilistic systems. Among others, it has important applications includingformalizing the anonymity property of several communication protocols. There isa lot of work on verifying probabilistic bisimulation for finite systems. Thisis however not the case for parameterized systems, where the problem is ingeneral undecidable. In this paper we provide a generic framework for reasoningabout probabilistic bisimulation for parameterized systems. Our approach is inthe spirit of software verification, wherein we encode proof rules forprobabilistic bisimulation and use a decidable first-order theory to specifysystems and candidate bisimulation relations, which can then be checkedautomatically against the proof rules. As a case study, we show that ourframework is sufficiently expressive for proving the anonymity property of theparameterized dining cryptographers protocol and the parameterized gradesprotocol, when supplied with a candidate regular bisimulation relation. Both ofthese protocols hitherto could not be verified by existing automatic methods.Moreover, with the help of standard automata learning algorithms, we show thatthe candidate relations can be synthesized fully automatically, making theverification fully automated.","Chih-Duo Hong, Anthony W. Lin, Rupak Majumdar, Philipp R\""ummer",,,11,
"Software Defect Prediction Based On Deep Learning Models: Performance
  Study","  In recent years, defect prediction, one of the major software engineeringproblems, has been in the focus of researchers since it has a pivotal role inestimating software errors and faulty modules. Researchers with the goal ofimproving prediction accuracy have developed many models for software defectprediction. However, there are a number of critical conditions and theoreticalproblems in order to achieve better results. In this paper, two deep learningmodels, Stack Sparse Auto-Encoder (SSAE) and Deep Belief Network (DBN), aredeployed to classify NASA datasets, which are unbalanced and have insufficientsamples. According to the conducted experiment, the accuracy for the datasetswith sufficient samples is enhanced and beside this SSAE model gains betterresults in comparison to DBN model in the majority of evaluation metrics.","Ahmad Hasanpour, Pourya Farzi, Ali Tehrani, Reza Akbari",,,11,
A Task-based Multi-shift QR/QZ Algorithm with Aggressive Early Deflation,"  The QR algorithm is one of the three phases in the process of computing theeigenvalues and the eigenvectors of a dense nonsymmetric matrix. This paperdescribes a task-based QR algorithm for reducing an upper Hessenberg matrix toreal Schur form. The task-based algorithm also supports generalized eigenvalueproblems (QZ algorithm) but this paper focuses more on the standard case. Thetask-based algorithm inherits previous algorithmic improvements, such astightly-coupled multi-shifts and Aggressive Early Deflation (AED), and alsoincorporates several new ideas that significantly improve the performance. Thisincludes the elimination of several synchronization points, the dynamic mergingof previously separate computational steps, the shorting and the prioritizationof the critical path, and the introduction of an experimental GPU support. Thetask-based implementation is demonstrated to be significantly faster thanmulti-threaded LAPACK and ScaLAPACK in both single-node and multi-nodeconfigurations on two different machines based on Intel and AMD CPUs. Theimplementation is built on top of the StarPU runtime system and is part of anopen-source StarNEig library.",Mirko Myllykoski,,,11,
"PBBFMM3D: a parallel black-box algorithm for kernel matrix-vector
  multiplication","  We introduce \texttt{PBBFMM3D}, a parallel black-box method for computingkernel matrix-vector multiplication, where the underlying kernel is anon-oscillatory function in three dimensions. While a naive method requires$\O(N^2)$ computation, \texttt{PBBFMM3D} reduces the cost to $\O(N)$ work. Inparticular, our algorithm requires only the ability to evaluate the kernelfunction, and is thus a black-box method. To further accelerate the computationon shared-memory machines, a parallel algorithm is presented and implementedusing \verb|OpenMP|, which achieved at most $19\times$ speedup on 32 cores inour numerical experiments. A real-world application in geostatistics is alsopresented, where \texttt{PBBFMM3D} is used in computing the truncatedeigen-decomposition (a.k.a., principle component analysis) of a covariancematrix (a.k.a., graph Laplacian).","Ruoxi Wang, Chao Chen, Jonghyun Lee, Eric Darve",,,11,
"COMPLEX-IT: A Case-Based Modeling and Scenario Simulation Platform for
  Social Inquiry","  COMPLEX-IT is a case-based, mixed-methods platform for social inquiry intocomplex data/systems, designed to increase non-expert access to the tools ofcomputational social science (i.e., cluster analysis, artificial intelligence,data visualization, data forecasting, and scenario simulation). In particular,COMPLEX-IT aids social inquiry though a heavy emphasis on learning about thecomplex data/system under study, which it does by (a) identifying andforecasting major and minor clusters/trends; (b) visualizing their complexcausality; and (c) simulating scenarios for potential interventions. COMPLEX-ITis accessible through the web or can be run locally and is powered by R and theShiny web framework.",Corey Schimpf and Brian Castellani,,,11,
"TuckerMPI: A Parallel C++/MPI Software Package for Large-scale Data
  Compression via the Tucker Tensor Decomposition","  Our goal is compression of massive-scale grid-structured data, such as themulti-terabyte output of a high-fidelity computational simulation. For suchdata sets, we have developed a new software package called TuckerMPI, aparallel C++/MPI software package for compressing distributed data. Theapproach is based on treating the data as a tensor, i.e., a multidimensionalarray, and computing its truncated Tucker decomposition, a higher-orderanalogue to the truncated singular value decomposition of a matrix. The resultis a low-rank approximation of the original tensor-structured data. Compressionefficiency is achieved by detecting latent global structure within the data,which we contrast to most compression methods that are focused on localstructure. In this work, we describe TuckerMPI, our implementation of thetruncated Tucker decomposition, including details of the data distribution andin-memory layouts, the parallel and serial implementations of the key kernels,and analysis of the storage, communication, and computational costs. We testthe software on 4.5 terabyte and 6.7 terabyte data sets distributed across 100sof nodes (1000s of MPI processes), achieving compression rates between100-200,000$\times$ which equates to 99-99.999% compression (depending on thedesired accuracy) in substantially less time than it would take to even readthe same dataset from a parallel filesystem. Moreover, we show that our methodalso allows for reconstruction of partial or down-sampled data on a singlenode, without a parallel computer so long as the reconstructed portion is smallenough to fit on a single machine, e.g., in the instance ofreconstructing/visualizing a single down-sampled time step or computing summarystatistics.",Grey Ballard and Alicia Klinvex and Tamara G. Kolda,,,11,
FunGrim: a symbolic library for special functions,"  We present the Mathematical Functions Grimoire (FunGrim), a website anddatabase of formulas and theorems for special functions. We also discuss thesymbolic computation library used as the backend and main development tool forFunGrim, and the Grim formula language used in these projects to representmathematical content semantically.",Fredrik Johansson (LFANT),,,11,
"A highly scalable approach to solving linear systems using two-stage
  multisplitting","  Iterative methods for solving large sparse systems of linear equations arewidely used in many HPC applications. Extreme scaling of these methods can bedifficult, however, since global communication to form dot products istypically required at every iteration.  To try to overcome this limitation we propose a hybrid approach, where thematrix is partitioned into blocks. Within each block, we use a highly optimised(parallel) conventional solver, but we then couple the blocks together usingblock Jacobi or some other multisplitting technique that can be implemented ineither a synchronous or an asynchronous fashion. This allows us to limit theblock size to the point where the conventional iterative methods no longerscale, and to avoid global communication (and possibly synchronisation) acrossall processes.  Our block framework has been built to use PETSc, a popular scientific suitefor solving sparse linear systems, as the synchronous intra-block solver, andwe demonstrate results on up to 32768 cores of a Cray XE6 system. At thisscale, the conventional solvers are still more efficient, though trends suggestthat the hybrid approach may be beneficial at higher core counts.","Nick Brown, J. Mark Bull, Iain Bethune",,,11,
"Hierarchical Jacobi Iteration for Structured Matrices on GPUs using
  Shared Memory","  High fidelity scientific simulations modeling physical phenomena typicallyrequire solving large linear systems of equations which result fromdiscretization of a partial differential equation (PDE) by some numericalmethod. This step often takes a vast amount of computational time to complete,and therefore presents a bottleneck in simulation work. Solving these linearsystems efficiently requires the use of massively parallel hardware with highcomputational throughput, as well as the development of algorithms whichrespect the memory hierarchy of these hardware architectures to achieve highmemory bandwidth.  In this paper, we present an algorithm to accelerate Jacobi iteration forsolving structured problems on graphics processing units (GPUs) using ahierarchical approach in which multiple iterations are performed within on-chipshared memory every cycle. A domain decomposition style procedure is adopted inwhich the problem domain is partitioned into subdomains whose data is copied tothe shared memory of each GPU block. Jacobi iterations are performed internallywithin each block's shared memory, avoiding the need to perform expensiveglobal memory accesses every step. We test our algorithm on the linear systemsarising from discretization of Poisson's equation in 1D and 2D, and observespeedup in convergence using our shared memory approach compared to atraditional Jacobi implementation which only uses global memory on the GPU. Weobserve a x8 speedup in convergence in the 1D problem and a nearly x6 speedupin the 2D case from the use of shared memory compared to a conventional GPUapproach.","Mohammad Shafaet Islam, Qiqi Wang",,,11,
"Characteristics-based Simulink implementation of first-order quasilinear
  partial differential equations","  The paper deals with solving first-order quasilinear partial differentialequations in an online simulation environment, such as Simulink, utilizing thewell-known and well-recommended method of characteristics. Compared to thecommonly applied space discretization methods on static grids, thecharacteristics-based approach provides better numerical stability. Simulinksubsystem implementing the method of characteristics is developed. It employsSimulink's built-in solver and its zero-crossing detection algorithm to performsimultaneous integration of a pool of characteristics as well as to create newcharacteristics dynamically and discard the old ones. Numerical accuracy of thesolution thus obtained is established. The subsystem has been tested on afull-state feedback example and produced better results than the spacediscretization-based ""method of lines"". The implementation is available fordownload and can be used in a wide range of models.","Anton Ponomarev, Julian Hofmann, Lutz Gr\""oll",,,11,
ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians,"  The computation of first and second-order derivatives is a staple in manycomputing applications, ranging from machine learning to scientific computing.We propose an algorithm to automatically differentiate algorithms written in asubset of C99 code and its efficient implementation as a Python script. Wedemonstrate that our algorithm enables automatic, reliable, and efficientdifferentiation of common algorithms used in physical simulation and geometryprocessing.","Deshana Desai, Etai Shuchatowitz, Zhongshi Jiang, Teseo Schneider, and
  Daniele Panozzo",,,11,
"Task-based, GPU-accelerated and Robust Library for Solving Dense
  Nonsymmetric Eigenvalue Problems","  In this paper, we present the StarNEig library for solving dense nonsymmetricstandard and generalized eigenvalue problems. The library is built on top ofthe StarPU runtime system and targets both shared and distributed memorymachines. Some components of the library have support for GPU acceleration. Thelibrary is currently in an early beta state and supports only real matrices.Support for complex matrices is planned for a future release. This paper isaimed at potential users of the library. We describe the design choices andcapabilities of the library, and contrast them to existing software such asScaLAPACK. StarNEig implements a ScaLAPACK compatibility layer which shouldassist new users in the transition to StarNEig. We demonstrate the performanceof the library with a sample of computational experiments.","Mirko Myllykoski, Carl Christian Kjelgaard Mikkelsen",,,11,
"LightSeq: A High Performance Inference Library for Sequence Processing
  and Generation","  LightSeq is a high performance inference library for sequence processing andgeneration implemented in CUDA. To our best knowledge, this is the firstopen-source inference library which fully supports highly efficient computationof modern NLP models such as BERT, GPT, Transformer, etc. This library isefficient, functional and convenient. A demo usage can be found here:https://github.com/bytedance/lightseq/blob/master/example.","Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li",,,11,
Flexible Performant GEMM Kernels on GPUs,"  General Matrix Multiplication or GEMM kernels take center place in highperformance computing and machine learning. Recent NVIDIA GPUs include GEMMaccelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered bythe two-language problem: it requires either low-level programming whichimplies low programmer productivity or using libraries that only offer alimited set of components. Because rephrasing algorithms in terms ofestablished components often introduces overhead, the libraries' lack offlexibility limits the freedom to explore new algorithms. Researchers usingGEMMs can hence not enjoy programming productivity, high performance, andresearch flexibility at once.  In this paper we solve this problem. We present three sets of abstractionsand interfaces to program GEMMs within the scientific Julia programminglanguage. The interfaces and abstractions are co-designed for researchers'needs and Julia's features to achieve sufficient separation of concerns andflexibility to easily extend basic GEMMs in many different ways without payinga performance price. Comparing our GEMMs to state-of-the-art libraries cuBLASand CUTLASS, we demonstrate that our performance is mostly on par with, and insome cases even exceeds, the libraries, without having to write a single lineof code in CUDA C++ or assembly, and without facing flexibility limitations.","Thomas Faingnaert, Tim Besard, Bjorn De Sutter",,,11,
"RationalizeRoots: Software Package for the Rationalization of Square
  Roots","  The computation of Feynman integrals often involves square roots. One way toobtain a solution in terms of multiple polylogarithms is to rationalize thesesquare roots by a suitable variable change. We present a program that can beused to find such transformations. After an introduction to the theoreticalbackground, we explain in detail how to use the program in practice.","Marco Besier, Pascal Wasser, Stefan Weinzierl",,,11,
Deep Learning Framework From Scratch Using Numpy,"  This work is a rigorous development of a complete and general-purpose deeplearning framework from the ground up. The fundamental components of deeplearning - automatic differentiation and gradient methods of optimizingmultivariable scalar functions - are developed from elementary calculus andimplemented in a sensible object-oriented approach using only Python and theNumpy library. Demonstrations of solved problems using the framework, namedArrayFlow, include a computer vision classification task, solving for the shapeof a catenary, and a 2nd order differential equation.",Andrei Nicolae,,,11,
"Matrix Equations, Sparse Solvers: M-M.E.S.S.-2.0.1 -- Philosophy,
  Features and Application for (Parametric) Model","  Matrix equations are omnipresent in (numerical) linear algebra and systemstheory. Especially in model order reduction (MOR) they play a key role in manybalancing based reduction methods for linear dynamical systems. When thesesystems arise from spatial discretizations of evolutionary partial differentialequations, their coefficient matrices are typically large and sparse. Moreover,the numbers of inputs and outputs of these systems are typically far smallerthan the number of spatial degrees of freedom. Then, in many situations thesolutions of the corresponding large-scale matrix equations are observed tohave low (numerical) rank. This feature is exploited by M-M.E.S.S. to findsuccessively larger low-rank factorizations approximating the solutions. Thiscontribution describes the basic philosophy behind the implementation and thefeatures of the package, as well as its application in the model orderreduction of large-scale linear time-invariant (LTI) systems and parametric LTIsystems.","Peter Benner, Martin K\""ohler, Jens Saak",,,11,
Code generation for generally mapped finite elements,"  Many classical finite elements such as the Argyris and Bell elements havelong been absent from high-level PDE software. Building on recent theoreticalwork, we describe how to implement very general finite element transformationsin FInAT and hence into the Firedrake finite element system. Numerical resultsevaluate the new elements, comparing them to existing methods for classicalproblems. For a second order model problem, we find that new elements givesmooth solutions at a mild increase in cost over standard Lagrange elements.For fourth-order problems, however, the newly-enabled methods significantlyoutperform interior penalty formulations. We also give some advanced use cases,solving the nonlinear Cahn-Hilliard equation and some biharmonic eigenvalueproblems (including Chladni plates) using $C^1$ discretizations.",Robert C. Kirby and Lawrence Mitchell,,,11,
"Airline Crew Pairing Optimization Framework for Large Networks with
  Multiple Crew Bases and Hub-and-Spoke Subnetworks","  Crew Pairing Optimization aims at generating a set of flight sequences (crewpairings), covering all flights in an airline's flight schedule, at minimumcost, while satisfying several legality constraints. CPO is criticallyimportant for airlines' business viability, considering that the crew operatingcost is their second-largest expense. It poses an NP-hard combinatorialoptimization problem, to tackle which, the state-of-the-art relies on relaxingthe underlying Integer Programming Problem (IPP) into a Linear ProgrammingProblem (LPP), solving the latter through Column Generation (CG) technique, andintegerization of the resulting LPP solution. However, with the growing scaleand complexity of the flight networks (those with a large number of flights,multiple crew bases and/or multiple hub-and-spoke subnetworks), the utility ofthe conventional CG-practices has become questionable. This paper proposed anAirline Crew Pairing Optimization Framework, AirCROP, whose constitutivemodules include the Legal Crew Pairing Generator, Initial Feasible SolutionGenerator, and an Optimization Engine built on heuristic-basedCG-implementation. In this paper, besides the design of AirCROP's modules,insights into important questions related to how these modules interact, whichthe literature is otherwise silent on, have been shared. These relate to thesensitivity of AirCROP's performance towards: sources of variability overmultiple runs for a given problem, initialization method, and terminationparameters for LPP-solutioning and IPP-solutioning. The efficacy of the AirCROPhas been demonstrated on real-world large-scale and complex flight networks(with over 4200 flights, 15 crew bases, and billion-plus pairings). It is hopedthat with the emergence of such complex flight networks, this paper shall serveas an important milestone for affiliated research and applications.","Divyam Aggarwal, Dhish Kumar Saxena, Thomas B\""ack, Michael Emmerich",,,11,
"A parallel structured divide-and-conquer algorithm for symmetric
  tridiagonal eigenvalue problems","  In this paper, a parallel structured divide-and-conquer (PSDC) eigensolver isproposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallelstructured matrix multiplication algorithm, called PSMMA. Computing theeigenvectors via matrix-matrix multiplications is the most computationallyexpensive part of the divide-and-conquer algorithm, and one of the matricesinvolved in such multiplications is a rank-structured Cauchy-like matrix. Byexploiting this particular property, PSMMA constructs the local matrices byusing generators of Cauchy-like matrices without any communication, and furtherreduces the computation costs by using a structured low-rank approximationalgorithm. Thus, both the communication and computation costs are reduced.Experimental results show that both PSMMA and PSDC are highly scalable andscale to 4096 processes at least. PSDC has better scalability than PHDC thatwas proposed in [J. Comput. Appl. Math. 344 (2018) 512--520] and only scaled to300 processes for the same matrices. Comparing with \texttt{PDSTEDC} inScaLAPACK, PSDC is always faster and achieves $1.4$x--$1.6$x speedup for somematrices with few deflations. PSDC is also comparable with ELPA, with PSDCbeing faster than ELPA when using few processes and a little slower when usingmany processes.","Xia Liao, Shengguo Li, Yutong Lu and Jose E. Roman",,,11,
Automatic Generation of Efficient Linear Algebra Programs,"  The level of abstraction at which application experts reason about linearalgebra computations and the level of abstraction used by developers ofhigh-performance numerical linear algebra libraries do not match. The former isconveniently captured by high-level languages and libraries such as Matlab andEigen, while the latter expresses the kernels included in the BLAS and LAPACKlibraries. Unfortunately, the translation from a high-level computation to anefficient sequence of kernels is a task, far from trivial, that requiresextensive knowledge of both linear algebra and high-performance computing.Internally, almost all high-level languages and libraries use efficientkernels; however, the translation algorithms are too simplistic and thus leadto a suboptimal use of said kernels, with significant performance losses. Inorder to both achieve the productivity that comes with high-level languages,and make use of the efficiency of low level kernels, we are developing Linnea,a code generator for linear algebra problems. As input, Linnea takes ahigh-level description of a linear algebra problem and produces as output anefficient sequence of calls to high-performance kernels. In 25 applicationproblems, the code generated by Linnea always outperforms Matlab, Julia, Eigenand Armadillo, with speedups up to and exceeding 10x.","Henrik Barthels, Christos Psarras, Paolo Bientinesi",,,11,
"Pseudo random number generators: attention for a newly proposed
  generator","  Xorshift128+ is a newly proposed pseudo random number generator (PRNG), whichis now the standard PRNG in a number of platforms. We point out thatthree-dimensional plots of the random points generated by the generator havevisible structures: they concentrate on particular planes in the cube. Weprovide mathematical analysis on this phenomenon. A key-observation is that theexclusive-or is well-approximated by the arithmetic sum or subtraction withrelatively high probability.","Hiroshi Haramoto, Makoto Matsumoto, Mutsuo Saito",,,11,
De-homogenization of optimal multi-scale 3D topologies,"  This paper presents a highly efficient method to obtain high-resolution,near-optimal 3D topologies optimized for minimum compliance on a standard PC.Using an implicit geometry description we derive a single-scale interpretationof optimal multi-scale designs on a very fine mesh (de-homogenization). Byperforming homogenization-based topology optimization, optimal multi-scaledesigns are obtained on a relatively coarse mesh resulting in a lowcomputational cost. As microstructure parameterization we use orthogonal rank-3microstructures, which are known to be optimal for a single loading case.Furthermore, a method to get explicit control of the minimum feature size andcomplexity of the final shapes will be discussed. Numerical examples showexcellent performance of these fine-scale designs resulting in objective valuessimilar to the homogenization-based designs. Comparisons with well-establisheddensity-based topology optimization methods show a reduction in computationalcost of 3 orders of magnitude, paving the way for giga-scale designs on astandard PC.","Jeroen Groen, Florian Stutz, Niels Aage, J. Andreas B{\ae}rentzen and
  Ole Sigmund",,,11,
"DuMu$^\text{x}$ 3 -- an open-source simulator for solving flow and
  transport problems in porous media with a focus on model coupling","  We present version 3 of the open-source simulator for flow and transportprocesses in porous media DuMu$^\text{x}$. DuMu$^\text{x}$ is based on themodular C++ framework Dune (Distributed and Unified Numerics Environment) andis developed as a research code with a focus on modularity and reusability. Wedescribe recent efforts in improving the transparency and efficiency of thedevelopment process and community-building, as well as efforts towards qualityassurance and reproducible research. In addition to a major redesign of manysimulation components in order to facilitate setting up complex simulations inDuMu$^\text{x}$, version 3 introduces a more consistent abstraction of finitevolume schemes. Finally, the new framework for multi-domain simulations isdescribed, and three numerical examples demonstrate its flexibility.","Timo Koch, Dennis Gl\""aser, Kilian Weishaupt, Sina Ackermann, Martin
  Beck, Beatrix Becker, Samuel Burbulla, Holger Class, Edward Coltman, Simon
  Emmert, Thomas Fetzer, Christoph Gr\""uninger, Katharina Heck, Johannes
  Hommel, Theresa Kurz, Melanie Lipp, Farid Mohammadi, Samuel Scherrer, Martin
  Schneider, Gabriele Seitz, Leopold Stadler, Martin Utz, Felix Weinhardt,
  Bernd Flemisch",,,11,
"Momentum-based Accelerated Mirror Descent Stochastic Approximation for
  Robust Topology Optimization under Stochastic Loads","  Robust topology optimization (RTO) improves the robustness of designs withrespect to random sources in real-world structures, yet an accurate sensitivityanalysis requires the solution of many systems of equations at eachoptimization step, leading to a high computational cost. To open up the fullpotential of RTO under a variety of random sources, this paper presents amomentum-based accelerated mirror descent stochastic approximation (AC-MDSA)approach to efficiently solve RTO problems involving various types of loaduncertainties. The proposed framework can perform high-quality design updateswith highly noisy stochastic gradients. We reduce the sample size to two(minimum for unbiased variance estimation) and show only two samples aresufficient for evaluating stochastic gradients to obtain robust designs, thusdrastically reducing the computational cost. We derive the AC-MDSA updateformula based on $\ell_1$-norm with entropy function, which is tailored to thegeometry of the feasible domain. To accelerate and stabilize the algorithm, weintegrate a momentum-based acceleration scheme, which also alleviates the stepsize sensitivity. Several 2D and 3D examples with various sizes are presentedto demonstrate the effectiveness and efficiency of the proposed AC-MDSAframework to handle RTO involving various types of loading uncertainties.",Weichen Li and Xiaojia Shelly Zhang,,,11,
"Benchmark Dataset for Mid-Price Forecasting of Limit Order Book Data
  with Machine Learning Methods","  Managing the prediction of metrics in high-frequency financial markets is achallenging task. An efficient way is by monitoring the dynamics of a limitorder book to identify the information edge. This paper describes the firstpublicly available benchmark dataset of high-frequency limit order markets formid-price prediction. We extracted normalized data representations of timeseries data for five stocks from the NASDAQ Nordic stock market for a timeperiod of ten consecutive days, leading to a dataset of ~4,000,000 time seriessamples in total. A day-based anchored cross-validation experimental protocolis also provided that can be used as a benchmark for comparing the performanceof state-of-the-art methodologies. Performance of baseline approaches are alsoprovided to facilitate experimental comparisons. We expect that such alarge-scale dataset can serve as a testbed for devising novel solutions ofexpert systems for high-frequency limit order book data analysis.","Adamantios Ntakaris, Martin Magris, Juho Kanniainen, Moncef Gabbouj,
  Alexandros Iosifidis",,,11,
"On the treatment of boundary conditions for bond-based peridynamic
  models","  In this paper, we propose two approaches to apply boundary conditions forbond-based peridynamic models. There has been in recent years a renewedinterest in the class of so-called non-local models, which include peridynamicmodels, for the simulation of structural mechanics problems as an alternativeapproach to classical local continuum models. However, a major issue, which isoften disregarded when dealing with this class of models, is concerned with themanner by which boundary conditions should be prescribed. Our point of viewhere is that classical boundary conditions, since applied on surfaces of solidbodies, are naturally associated with local models. The paper describes twomethods to incorporate classical Dirichlet and Neumann boundary conditions intobond-based peridynamics. The first method consists in artificially extendingthe domain with a thin boundary layer over which the displacement field isrequired to behave as an odd function with respect to the boundary points. Thesecond method resorts to the idea that peridynamic models and local modelsshould be compatible in the limit that the so-called horizon vanishes. Theapproach consists then in decreasing the horizon from a constant value in theinterior of the domain to zero at the boundary so that one can directly applythe classical boundary conditions. We present the continuous and discreteformulations of the two methods and assess their performance on severalnumerical experiments dealing with the simulation of a one-dimensional bar.",Serge Prudhomme and Patrick Diehl,,,11,
A state-of-knowledge review on the Endurance Time Method,"  Endurance time method is a time history dynamic analysis in which structuresare subjected to predesigned intensifying excitations. This method provides atool for response prediction that correlates structural responses to theintensity of earthquakes with a considerably less computational demand ascompared to conventional time history analysis. The endurance time method isbeing used in different areas of earthquake engineering such asperformance-based assessment and design, life-cycle cost-based design,value-based design, seismic safety, seismic assessment, and multicomponentseismic analysis. Successful implementation of the endurance time method reliesheavily on the quality of endurance time excitations. In this paper, a reviewof the endurance time method from conceptual development to its practicalapplications is provided. Different types of endurance time excitations aredescribed. Features related to the existing endurance time excitations are alsopresented. Particular attention is given to different applications of theendurance time method in the field of earthquake engineering.","Homayoon E. Estekanchi, Mohammadreza Mashayekhi, Hassan Vafai, Goodarz
  Ahmadi, S. Ali Mirfarhadi, Mojtaba Harati",,,11,
"A numerical approach for hybrid reliability analysis of structures under
  mixed uncertainties using the uncertainty theory","  This paper presents a novel numerical method for the hybrid reliabilityanalysis by using the uncertainty theory. Aleatory uncertainty and epistemicuncertainty are considered simultaneously in this method. Epistemic uncertaintyis characterized by the uncertainty theory, and the effect of epistemicuncertainty is quantified by the sub-additive uncertain measure. Then, underthe framework of the chance theory which can be interpreted as the combinationof the probability theory and the uncertainty theory, a general uncertaintyquantification model is established to deal with the hybrid reliabilityanalysis problem, then the corresponding reliability metric is defined. Afterthat, to improve the feasibility of the proposed model, by utilizing the polarcoordinate transformation based dimension reduction method, a numericalanalysis method for the hybrid reliability model are provided. At last, severalapplication cases are presented to prove the effectiveness of the proposedmethod for the reliability analysis under hybrid uncertainty. The comparisonsbetween the results of the proposed method and the Monte Carlo simulation alsoillustrate the merit of this method.",Lei Zhang,,,11,
"Topology optimization of nonlinear periodically microstructured
  materials for tailored homogenized constitutive properties","  A topology optimization method is presented for the design of periodicmicrostructured materials with prescribed homogenized nonlinear constitutiveproperties over finite strain ranges. The mechanical model assumes linearelastic isotropic materials, geometric nonlinearity at finite strain, and aquasi-static response. The optimization problem is solved by a nonlinearprogramming method and the sensitivities computed via the adjoint method.Two-dimensional structures identified using this optimization method areadditively manufactured and their uniaxial tensile strain response comparedwith the numerically predicted behavior. The optimization approach hereinenables the design and development of lattice-like materials with prescribednonlinear effective properties, for use in myriad potential applications,ranging from stress wave and vibration mitigation to soft robotics.","Reza Behrou, Maroun Abi Ghanem, Brianna C. Macnider, Vimarsh Verma,
  Ryan Alvey, Jinho Hong, Ashley F. Emery, Hyunsun Alicia Kim, Nicholas
  Boechler",,,11,
"Exploring market power using deep reinforcement learning for intelligent
  bidding strategies","  Decentralized electricity markets are often dominated by a small set ofgenerator companies who control the majority of the capacity. In this paper, weexplore the effect of the total controlled electricity capacity by a single, orgroup, of generator companies can have on the average electricity price. Wedemonstrate this through the use of ElecSim, a simulation of a country-wideenergy market. We develop a strategic agent, representing a generation company,which uses a deep deterministic policy gradient reinforcement learningalgorithm to bid in a uniform pricing electricity market. A uniform pricingmarket is one where all players are paid the highest accepted price. ElecSim isparameterized to the United Kingdom for the year 2018. This work can helpinform policy on how to best regulate a market to ensure that the price ofelectricity remains competitive.  We find that capacity has an impact on the average electricity price in asingle year. If any single generator company, or a collaborating group ofgenerator companies, control more than ${\sim}$11$\%$ of generation capacityand bid strategically, prices begin to increase by ${\sim}$25$\%$. The value of${\sim}$25\% and ${\sim}$11\% may vary between market structures and countries.For instance, different load profiles may favour a particular type of generatoror a different distribution of generation capacity. Once the capacitycontrolled by a generator company, which bids strategically, is higher than${\sim}$35\%, prices increase exponentially. We observe that the use of amarket cap of approximately double the average market price has the effect ofsignificantly decreasing this effect and maintaining a competitive market. Afair and competitive electricity market provides value to consumers and enablesa more competitive economy through the utilisation of electricity by bothindustry and consumers.","Alexander J. M. Kell, Matthew Forshaw, A. Stephen McGough",,,11,
"Localized Nonlinear Solution Strategies for Efficient Simulation of
  Unconventional Reservoirs","  Accurate and efficient numerical simulation of unconventional reservoirs ischallenging. Long periods of transient flow and steep potential gradients occurdue to the extreme conductivity contrast between matrix and fracture. Detailednear-well/near-fracture models are necessary to provide sufficient resolution,but they are computationally impractical for field cases with multiple fracturestages. Previous works in the literature of unconventional simulations mainlyfocus on gridding level that adapts to wells and fractures. Limited researchhas been conducted on nonlinear strategies that exploit locality acrosstimesteps and nonlinear iterations. To perform localized computations, ana-priori strategy is essential to first determine the active subset ofsimulation cells for the subsequent iteration. The active set flags the cellsthat will be updated, and then the corresponding localized linear system issolved. This work develops localization methods that are readily applicable tocomplex fracture networks and flow physics in unconventional reservoirs. Byutilizing the diffusive nature of pressure updates, an adaptive algorithm isproposed to make adequate estimates for the active domains. In addition, wedevelop a localized solver based on nonlinear domain decomposition (DD).Comparing to a standard DD method, domain partitions are dynamicallyconstructed. The new solver provides effective partitioning that adapts to flowdynamics and Newton updates. We evaluate the developed methods using severalcomplex problems with discrete fracture networks. The results show that largedegrees of solution locality present across timesteps and iterations. Comparingto a standard Newton solver, the new solvers enable superior computationalperformance. Moreover, Newton convergence behavior is preserved, without anyimpact on solution accuracy.",Jiamin Jiang,,,11,
Level-set topology optimization considering nonlinear thermoelasticity,"  At elevated temperature environments, elastic structures experience a changeof the stress-free state of the body that can strongly influence the optimaltopology of the structure. This work presents level-set based topologyoptimization of structures undergoing large deformations due to thermal andmechanical loads. The nonlinear analysis model is constructed bymultiplicatively decomposing thermal and mechanical effects and introducing anintermediate stress-free state between the undeformed and deformed coordinates.By incorporating the thermoelastic nonlinearity into the level-set topologyoptimization scheme, wider design spaces can be explored with the considerationof both mechanical and thermal loads. Four numerical examples are presentedthat demonstrate how temperature changes affect the optimal design oflarge-deforming structures. In particular, we show how optimization canmanipulate the material layout in order to create a counteracting effectbetween thermal and mechanical loads, even up to a degree that buckling andsnap-through are suppressed. Hence the consideration of large deformations inconjunction with thermoelasticity opens many new possibilities for controllingand manipulating the thermo-mechanical response via topology optimization.","Hayoung Chung, Oded Amir, H. Alicia Kim",,,11,
"A micropolar peridynamics model with non-unified horizon for damage of
  solids with different non-local effects","  Most peridynamics models adopt regular point distribution and unifiedhorizon, limiting their flexibility and engineering applications. In this work,a micropolar peridynamics approach with non-unified horizon (NHPD) is proposed.This approach is implemented in a conventional finite element framework, usingelement-based discretization. By modifying the dual horizon approach into thepre-processing part, point dependent horizon and non-unified beam-like bondsare built. By implementing a domain correction strategy, the equivalence ofstrain energy density is assured. Then, a novel energy density-based failurecriterion is presented which directly bridges the critical stretch to themechanical strength. The numerical results indicate the weak mesh dependency ofNHPD and the effectiveness of the new failure criterion. Moreover, it is proventhat damage of solid with different non-local effects can lead to similarresults by only adjusting the mechanical strength.",Yiming Zhang and Xueqing Yang and Xiaoying Zhuang,,,11,
An accurate methodology for surface tension modeling in OpenFOAM,"  In this paper a numerical methodology for surface tension modeling ispresented, with an emphasis on the implementation in the OpenFOAM framework.The methodology relies on a combination of (i) a well-balanced approach basedon the Ghost Fluid Method (GFM), including the jump of density and pressuredirectly in the numerical discretization of the pressure equation, and (ii)Height Functions to evaluate the interface curvature, implemented, to theauthors' knowledge, for the first time in OpenFOAM. The method is able tosignificantly reduce spurious currents (almost to machine accuracy) for astationary droplet, showing second order convergence both for the curvature andthe interface shape. Accurate results are also obtained for additional testcases such as translating droplets, capillary oscillations and rising bubbles,for which numerical results are comparable to what obtained by other numericalcodes in the same conditions. Finally, the Height Functions method is extendedto include the treatment of contact angles, both for sessile droplets anddroplets suspended under the effect of gravity, showing a very good agreementwith the theoretical prediction. The code works in parallel mode and details onthe actual implementation in OpenFOAM are included to facilitate thereproducibility of the results.","Abd Essamade Saufi, Olivier Desjardins, Alberto Cuoci",,,11,
"A hybridizable discontinuous Galerkin method for electromagnetics with a
  view on subsurface applications","  Two Hybridizable Discontinuous Galerkin (HDG) schemes for the solution ofMaxwell's equations in the time domain are presented. The first method is basedon an electromagnetic diffusion equation, while the second is based onFaraday's and Maxwell--Amp\`ere's laws. Both formulations include the diffusiveterm depending on the conductivity of the medium. The three-dimensionalformulation of the electromagnetic diffusion equation in the framework of HDGmethods, the introduction of the conduction current term and the choice of theelectric field as hybrid variable in a mixed formulation are the key points ofthe current study. Numerical results are provided for validation purposes andconvergence studies of spatial and temporal discretizations are carried out.The test cases include both simulation in dielectric and conductive media.","Luca Berardocco, Martin Kronbichler and Volker Gravemeier",,,11,
"Linear-frictional contact model for 3D discrete element simulations of
  granular systems","  The linear-frictional contact model is the most commonly used contactmechanism for discrete element (DEM) simulations of granular materials. Linearsprings with a frictional slider are used for modeling interactions indirections normal and tangential to the contact surface. Although the model issimple in two dimensions, its implementation in 3D faces certain subtlechallenges, and the particle interactions that occur within a single time-steprequire careful modeling with a robust algorithm. The paper details a 3Dalgorithm that accounts for the changing direction of the tangential forcewithin a time-step, the transition from elastic to slip behavior within atime-step, possible contact sliding during only part of a time-step, andtwirling and rotation of the tangential force during a time-step. Without threeof these adjustments, errors are introduced in the incremental stiffness of anassembly. Without the fourth adjustment, the resulting stress tensor is notonly incorrect, it is no longer a tensor. The algorithm also computes the workincrements during a time-step, both elastic and dissipative.","Matthew R. Kuhn, Kiichi Suzuki, Ali Daouadji",,,11,
"A Physics-Guided Neural Network Framework for Elastic Plates: Comparison
  of Governing Equations-Based and Energy-Based Approaches","  One of the obstacles hindering the scaling-up of the initial successes ofmachine learning in practical engineering applications is the dependence of theaccuracy on the size of the database that ""drives"" the algorithms.Incorporating the already-known physical laws into the training process cansignificantly reduce the size of the required database. In this study, weestablish a neural network-based computational framework to characterize thefinite deformation of elastic plates, which in classic theories is described bythe F\""oppl--von K\'arm\'an (FvK) equations with a set of boundary conditions(BCs). A neural network is constructed by taking the spatial coordinates as theinput and the displacement field as the output to approximate the exactsolution of the FvK equations. The physical information (PDEs, BCs, andpotential energies) is then incorporated into the loss function, and a pseudodataset is sampled without knowing the exact solution to finally train theneural network. The prediction accuracy of the modeling framework is carefullyexamined by applying it to four different loading cases: in-plane tension withnon-uniformly distributed stretching forces, in-plane central-hole tension,out-of-plane deflection, and buckling under compression. Two ways offormulating the loss function are compared, one based on the PDEs and BCs, andthe other based on the total potential energy of the plate. Through thecomparison with the finite element simulation results, it is found that ourcomputational framework is capable of characterizing the elastic deformation ofplates with a satisfactory accuracy. Compared with incorporating the PDEs andBCs in the loss, using the total potential energy is a better way in terms oftraining accuracy and efficiency.",Wei Li and Martin Z. Bazant and Juner Zhu,,,11,
"On phase change and latent heat models in metal additive manufacturing
  process simulation","  This work proposes an extension of phase change and latent heat models forthe simulation of metal powder bed fusion additive manufacturing processes onthe macroscale and compares different models with respect to accuracy andnumerical efficiency. Specifically, a systematic formulation of phase fractionvariables is proposed relying either on temperature- or enthalpy-basedinterpolation schemes. Moreover, two well-known schemes for the numericaltreatment of latent heat, namely the apparent capacity and the so-called heatintegration scheme, are critically reviewed and compared with respect tonumerical efficiency and overall accuracy. Eventually, a novel variant of theheat integration scheme is proposed that allows to directly control efficiencyand accuracy by means of a user-defined tolerance. Depending on the chosentolerance, it is shown that this novel approach offers increased numericalefficiency for a given level of accuracy or improved accuracy for a given levelof numerical efficiency as compared to the apparent capacity and the originalheat integration scheme. The investigation and comparison of all consideredschemes is based on a series of numerical test cases that are representativefor application scenarios in metal powder bed fusion additive manufacturing.","Sebastian D. Proell, Wolfgang A. Wall, Christoph Meier",,,11,
"Topology Optimization and 3D-printing of Large Deformation Compliant
  Mechanisms for Straining Biological Tissues","  This paper presents a synthesis approach in a density-based topologyoptimization setting to design large deformation compliant mechanisms forinducing desired strains in biological tissues. The modelling is based ongeometrical nonlinearity together with a suitably chosen hypereleastic materialmodel, wherein the mechanical equilibrium equations are solved using the totalLagrangian finite element formulation. An objective based on least-square errorwith respect to target strains is formulated and minimized with the given setof constraints and the appropriate surroundings of the tissues. To circumventnumerical instabilities arising due to large deformation in low stiffnessdesign regions during topology optimization, a strain-energy basedinterpolation scheme is employed. The approach uses an extended robustformulation i.e. the eroded, intermediate and dilated projections for thedesign description as well as variation in tissue stiffness. Efficacy of thesynthesis approach is demonstrated by designing various compliant mechanismsfor providing different target strains in biological tissue constructs.Optimized compliant mechanisms are 3D-printed and their performances arerecorded in a simplified experiment and compared with simulation resultsobtained by a commercial software.","P. Kumar, C. Schmidleithner, N. B. Larsen and O. Sigmund",,,11,
"A combined XFEM phase-field computational model for crack growth without
  remeshing","  This paper presents an adaptive strategy for phase-field simulations withtransition to fracture. The phase-field equations are solved only in smallsubdomains around crack tips to determine propagation, while an XFEMdiscretization is used in the rest of the domain to represent sharp cracks,enabling to use a coarser discretization and therefore reducing thecomputational cost. Crack-tip subdomains move as cracks propagate in a fullyautomatic process. The same computational mesh is used during all thesimulation, with an $h$-refined approximation in the elements in the crack-tipsubdomains. Continuity of the displacement between the refined subdomains andthe XFEM region is imposed in weak form via Nitsche's method. The robustness ofthe strategy is shown for some numerical examples in 2D and 3D, includingbranching and coalescence tests.","Alba Muix\'i, Onofre Marco, Antonio Rodr\'iguez-Ferran, Sonia
  Fern\'andez-M\'endez",,,11,
"Bending models of lipid bilayer membranes: spontaneous curvature and
  area-difference elasticity","  We preset a computational study of bending models for the curvatureelasticity of lipid bilayer membranes that are relevant for simulations ofvesicles and red blood cells. We compute bending energy and forces ontriangulated meshes and evaluate and extend four well established schemes fortheir approximation: Kantor and Nelson 1987, Phys. Rev. A 36, 4020, J\""ulicher1996, J. Phys. II France 6, 1797, Gompper and Kroll 1996, J. Phys. I France 6,1305, and Meyer et. al. 2003 in Visualization and Mathematics III, Springer,p35, termed A, B, C, D. We present a comparative study of these four schemes onthe minimal bending model and propose extensions for schemes B, C and D. Theseextensions incorporate the reference state and non-local energy to account forthe spontaneous curvature, bilayer coupling, and area-difference elasticitymodels. Our results indicate that the proposed extensions enhance the models toaccount for shape transformation including budding/vesiculation as well as fornon-axisymmetric shapes. We find that the extended scheme B is superior to therest in terms of accuracy, and robustness as well as simplicity ofimplementation. We demonstrate the capabilities of this scheme on severalbenchmark problems including the budding-vesiculating process and thereproduction of the phase diagram of vesicles.",Xin Bian and Sergey Litvinov and Petros Koumoutsakos,,,11,
Validation of counting methods in bibliometrics,"  The discussion about counting methods in bibliometrics is often reduced tothe choice between full and fractional counting. However, several studiesdocument that this distinction is too simple. The aim of the present study isto give an overview of counting methods in the bibliometric literature and toprovide insight into their properties and use. A mix of methods is used. In thepreliminary results, a literature review covering 1970-2018 identified 29original counting methods. Seventeen were introduced in the period 2010-2018.Twenty-one of the 29 counting methods are rank-dependent and fractionalizedmeaning that the authors of a publications share 1 credit but do not receiveequal shares, for example harmonic counting. The internal and externalvalidation of the counting methods are assessed. Three criteria forwell-constructed bibliometric indicators - adequacy, sensitivity, andhomogeneity - are used to assess the internal validity. Regarding the externalvalidation of the counting methods, it is investigated whether the intentionsin the studies that introduced the 29 counting methods comply with thesubsequent use of the counting methods. This study has the potential to give asolid foundation for the use of and discussion about counting methods.",Marianne Gauffriau,,,11,
"Daily growth rate of scientific production on Covid-19. Analysis in
  databases and open access repositories","  The scientific community is facing one of its greatest challenges in solvinga global health problem: COVID-19 pandemic. This situation has generated anunprecedented volume of publications. What is the volume, in terms ofpublications, of research on COVID-19? The general objective of this researchwork is to obtain a global vision of the daily growth of scientific productionon COVID-19 in different databases (Dimensions, Web of Science Core Collection,Scopus-Elsevier, Pubmed and eight repositories). In relation to the resultsobtained, Dimensions indexes a total of 9435 publications (69% with peer reviewand 2677 preprints) well above Scopus (1568) and WoS (718). This is a classicbiliometric phenomenon of exponential growth (R2 = 0.92). The global growthrate is 500 publications and the production doubles every 15 days. In the caseof Pubmed the weekly growth is around 1000 publications. Of the eightrepositories analysed, Pubmed Central, Medrxiv and SSRN are the leaders.Despite their enormous contribution, the journals continue to be the core ofscientific communication. Finally, it has been established that three out ofevery four publications on the COVID-19 are available in open access. Theinformation explosion demands a serious and coordinated response frominformation professionals, which places us at the centre of the informationpandemic.",Daniel Torres-Salinas,,,11,
Delayed Recognition; the Co-citation Perspective,"  A Sleeping Beauty is a publication that is apparently unrecognized for someperiod of time before experiencing sudden recognition by citation. Variousreasons, including resistance to new ideas, have been attributed to suchdelayed recognition. We examine this phenomenon in the special case ofco-citations, which represent new ideas generated through the combination ofexisting ones. Using relatively stringent selection criteria derived from thework of others, we analyze a very large dataset of over 940 million uniqueco-cited article pairs, and identified 1,196 cases of delayed co-citations. Wefurther classify these 1,196 cases with respect to amplitude, rate of citation,and disciplinary origin and discuss alternative approaches towards identifyingsuch instances.","Wenxi Zhao, Dmitriy Korobskiy, and George Chacko",,,11,
Funding information in Web of Science: An updated overview,"  Despite the limitations of funding acknowledgment (FA) data in Web of Science(WoS), studies using FA information have increased rapidly over the lastseveral years. Considering this WoS'recent practice of updating funding data,this paper further investigates the characteristics and distribution of FA datain four WoS journal citation indexes. The research reveals that FA informationcoverage variances persist cross all four citation indexes by time coverage,language and document type. Our evidence suggests an improvement in FAinformation collection in humanity and social science research. Departing fromprevious studies, we argue that FA text (FT) alone no longer seems anappropriate field to retrieve and analyze funding information, since asubstantial number of documents only report funding agency or grant numberinformation in respective fields. Articles written in Chinese have a higher FApresence rate than other non-English WoS publications. This updated studyconcludes with a discussion of new findings and practical guidance for thefuture retrieval and analysis of funded research.","Weishu Liu, Li Tang, Guangyuan Hu",,,11,
"Scholarly journal publishing in transition: from restricted to open
  access","  While the business models used in most segments of the media industry havebeen profoundly changed by the Internet surprisingly little has been changed inthe publishing of scholarly peer reviewed journals. Electronic delivery hasbecome the norm, but the same publishers as before are dominating the market,selling content to subscribers. This article asks the question why Open Access(OA) to the output of mainly publicly funded research hasn't yet become themainstream business model. OA implies a reversal of business logic from readerspaying for content to authors paying fro dissemination via universa freeaccess. The current situation is analyzed using Porter's five forces model. Theanalysis demonstrates a lack of competitive pressure in this industry, leadingto so high profit levels of the leading publishers that they have yet to feel astrong need to change the way they operate.","Bo-Christer Bj\""ork",,,11,
"Are papers addressing certain diseases perceived where these diseases
  are prevalent? The proposal to use Twitter data as social-spatial sensors","  We propose to use Twitter data as social-spatial sensors. This study dealswith the question whether research papers on certain diseases are perceived bypeople in regions (worldwide) that are especially concerned by the diseases.Since (some) Twitter data contain location information, it is possible tospatially map the activity of Twitter users referring to certain papers (e.g.,dealing with tuberculosis). The resulting maps reveal whether heavy activity onTwitter is correlated with large numbers of people having certain diseases. Inthis study, we focus on tuberculosis, human immunodeficiency virus (HIV), andmalaria, since the World Health Organization ranks these diseases as the topthree causes of death worldwide by a single infectious agent. The results ofthe social-spatial Twitter maps (and additionally performed regression models)reveal the usefulness of the proposed sensor approach. One receives animpression of how research papers on the diseases have been perceived by peoplein regions that are especially concerned by the diseases. Our studydemonstrates a promising approach for using Twitter data for researchevaluation purposes beyond simple counting of tweets.","Lutz Bornmann, Robin Haunschild, and Vanash M. Patel",,,11,
"Identifying Historical Travelogues in Large Text Corpora Using Machine
  Learning","  Travelogues represent an important and intensively studied source forscholars in the humanities, as they provide insights into people, cultures, andplaces of the past. However, existing studies rarely utilize more than a dozenprimary sources, since the human capacities of working with a large number ofhistorical sources are naturally limited. In this paper, we define the notionof travelogue and report upon an interdisciplinary method that, using machinelearning as well as domain knowledge, can effectively identify Germantravelogues in the digitized inventory of the Austrian National Library with F1scores between 0.94 and 1.00. We applied our method on a corpus of 161,522German volumes and identified 345 travelogues that could not be identifiedusing traditional search methods, resulting in the most extensive collection ofearly modern German travelogues ever created. To our knowledge, this is thefirst time such a method was implemented for the bibliographic indexing of atext corpus on this scale, improving and extending the traditional methods inthe humanities. Overall, we consider our technique to be an important firststep in a broader effort of developing a novel mixed-method approach for thelarge-scale serial analysis of travelogues.","Jan R\""orden (1), Doris Gruber (2), Martin Krickl (3), Bernhard
  Haslhofer (1) ((1) AIT Austrian Insitute of Technology, (2) Austrian Academy
  of Sciences, (3) Austrian National Library)",,,11,
"Should citations be field-normalized in evaluative bibliometrics? An
  empirical analysis based on propensity score matching","  Field-normalization of citations is bibliometric standard. Despite theobserved differences in citation counts between fields, the question remainshow strong fields influence citation rates beyond the effect of attributes orfactors possibly influencing citations (FICs). We considered several FICs suchas number of pages and number of co-authors in this study. We wondered whetherthere is a separate field-effect besides other effects (e.g., from numbers ofpages and co-authors). To find an answer on the question in this study, weapplied inverse-probability of treatment weighting (IPW). Using Web of Sciencedata (a sample of 308,231 articles), we investigated whether mean differencesamong subject categories in citation rates still remain, even if the subjectcategories are made comparable in the field-related attributes (e.g.,comparable of co-authors, comparable number of pages) by IPW. In a diagnosticstep of our statistical analyses, we considered propensity scores as covariatesin regression analyses to examine whether the differences between the fields inFICs vanish. The results revealed that the differences did not completelyvanish but were strongly reduced. We received similar results when wecalculated mean value differences of the fields after IPW representing thecausal or unconfounded field effects on citations. However, field differencesin citation rates remain. The results point out that field-normalization seemsto be a prerequisite for citation analysis and cannot be replaced by theconsideration of any set of FICs in citation analyses.","Lutz Bornmann, Robin Haunschild, Ruediger Mutz",,,11,
Open Access all you wanted to know and never dared to ask,"  This editorial presents the various forms of open access, discusses theirpros and cons from the perspective of the Journal of Object Technology and itseditors in chiefs, and illustrates how JOT implements a platinum open accessmodel. The regular reader will also notice that this editorial features a newtemplate for the journal that will be used from now on.",Alfonso Pierantonio and Benoit Combemale and Mark van den Brand,,,11,
The rise of science in low-carbon energy technologies,"  Successfully combating climate change will require substantial technologicalimprovements in Low-Carbon Energy Technologies (LCETs), but designing efficientallocation of R\&D budgets requires a better understanding of how LCETs rely onscientific knowledge. Using data covering almost all US patents and scientificarticles that are cited by them over the past two centuries, we describe theevolution of knowledge bases of ten key LCETs and show how technologicalinterdependencies have changed over time. The composition of low-carbon energyinnovations shifted over time, from Hydro and Wind energy in the 19th and early20th century, to Nuclear fission after World War II, and more recently to SolarPV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels(including energy from waste) have 35-65\% of their citations directed towardscientific papers, while this ratio is less than 10\% for Wind, Solar thermal,Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citingscience and the share of citations that are to scientific papers has beenincreasing for all technology types. The analysis of the scientific knowledgebase of each LCET reveals three fairly separate clusters, with nuclear energytechnologies, Biofuels and Waste, and all the other LCETs. Our detaileddescription of knowledge requirements for each LCET helps to design of targetedinnovation policies.","Kerstin H\""otte, Anton Pichler, Fran\c{c}ois Lafond",,,11,
"Predicting the popularity of scientific publications by an age-based
  diffusion model","  Predicting the popularity of scientific publications has attracted manyattentions from various disciplines. In this paper, we focus on the popularityprediction problem of scientific papers, and propose an age-based diffusion(AD) model to identify which paper will receive more citations in the nearfuture and will be popular. The AD model is a mimic of the attention diffusionprocess along the citation networks. The experimental study shows that the ADmodel can achieve better prediction accuracy than other networkbased methods.For some newly published papers that have not accumulated many citations butwill be popular in the near future, the AD model can substantially improvetheir rankings. This is really critical, because identifying the future highlycited papers from massive numbers of new papers published each month wouldprovide very valuable references for researchers.",Yanbo Zhou and Qu Li and Xuhua Yang and Hongbing cheng,,,11,
"On the Performance of Hybrid Search Strategies for Systematic Literature
  Reviews in Software Engineering","  Context: When conducting a Systematic Literature Review (SLR), researchersusually face the challenge of designing a search strategy that appropriatelybalances result quality and review effort. Using digital library (or database)searches or snowballing alone may not be enough to achieve high-qualityresults. On the other hand, using both digital library searches and snowballingtogether may increase the overall review effort.  Objective: The goal of this research is to propose and evaluate hybrid searchstrategies that selectively combine database searches with snowballing.  Method: We propose four hybrid search strategies combining database searchesin digital libraries with iterative, parallel, or sequential backward andforward snowballing. We simulated the strategies over three existing SLRs in SEthat adopted both database searches and snowballing. We compared the outcome ofdigital library searches, snowballing, and hybrid strategies using precision,recall, and F-measure to investigate the performance of each strategy.  Results: Our results show that, for the analyzed SLRs, combining databasesearches from the Scopus digital library with parallel or sequentialsnowballing achieved the most appropriate balance of precision and recall.  Conclusion: We put forward that, depending on the goals of the SLR and theavailable resources, using a hybrid search strategy involving a representativedigital library and parallel or sequential snowballing tends to represent anappropriate alternative to be used when searching for evidence in SLRs.","Erica Mour\~ao, Jo\~ao Felipe Pimentel, Leonardo Murta, Marcos
  Kalinowski, Emilia Mendes, Claes Wohlin",,,11,
Growth and dynamics of Econophysics: A bibliometric and network analysis,"  Digitization of publications, advancement in communication technology, andthe availability of bibliographic data have made it easier for the researchersto study the growth and dynamics of any discipline. We present a study on""Econophysics"" metadata extracted from the Web of Science managed by theClarivate Analytics from 2000-2019. The study highlights the growth anddynamics of the discipline by measures of a number of publications, citationson publications, other disciplines contribution, institutions participation,country-wise spread, etc. We investigate the impact of self-citations oncitations with every five-year interval. Also, we find the contribution ofother disciplines by analyzing the cited references. Results emerged frommicro, meso and macro-level analysis of collaborations show that thedistributions among authors collaboration and affiliations of authors follow apower law. Thus, very few authors keep producing most of the papers and arefrom a few institutions. We find that China is leading in the production of anumber of authors and a number of papers; however, shares more of nationalcollaboration rather than international, whereas the USA shares moreinternational collaboration. Finally, we demonstrate the evolution of theauthor's collaborations and affiliations networks from 2000-2019. Overall theanalysis reveals the ""small-world"" property of the network with average pathlength 5. As a consequence of our analysis, this study can serve as in-depthknowledge to understand the growth and dynamics of the Econophysics networkboth qualitatively and quantitatively.",Kiran Sharma and Parul Khurana,,,11,
The practice of self-citations: a longitudinal study,"  In this article, we discuss the outcomes of an experiment where we analysedwhether and to what extent the introduction, in 2012, of the new researchassessment exercise in Italy (a.k.a. Italian Scientific Habilitation) affectedself-citation behaviours in the Italian research community. The ItalianScientific Habilitation attests to the scientific maturity of researchers andin Italy, as in many other countries, is a requirement for accessing to aprofessorship. To this end, we obtained from ScienceDirect 35,673 articlespublished from 1957 and 2016 by the participants to the 2012 Italian ScientificHabilitation, that resulted in the extraction of 1,379,050 citations retrievedthrough Semantic Publishing technologies. Our analysis showed an overallincrement in author self-citations (i.e. where the citing article and the citedarticle share at least one author) in several of the 24 academic disciplinesconsidered. However, we depicted a stronger causal relation between suchincrement and the rules introduced by the 2012 Italian Scientific Habilitationin 10 out of 24 disciplines analysed.","Silvio Peroni, Paolo Ciancarini, Aldo Gangemi, Andrea Giovanni
  Nuzzolese, Francesco Poggi, Valentina Presutti",,,11,
Exploring Direct Citations between Citing Publications,"  This paper defines and explores the direct citations between citingpublications (DCCPs) of a publication. We construct an ego-centered citationnetwork for each paper that contains all of its citing papers and itself, aswell as the citation relationships among them. By utilizing a large-scalescholarly dataset from the computer science field in the Microsoft AcademicGraph (MAG-CS) dataset, we find that DCCPs exist universally in medium andhighly cited papers. For those papers that have DCCPs, DCCPs do occurfrequently; highly cited papers tend to contain more DCCPs than others.Meanwhile, the number of DCCPs of papers published in different years does notvary dramatically. The current paper also discusses the relationship betweenDCCPs and some indirect citation relationships (e.g., co-citation andbibliographic coupling).","Yong Huang, Yi Bu, Ying Ding, Wei Lu",,,11,
"Chemistry research in India in a global perspective- A scientometrics
  profile","  Papers from India are cited 14.68 times on average compared to cites perpaper of 45.34 for Singapore, 30.47 for USA, 23.12 for China, 26.51 for the UK,21.77 for South Korea and 24.77 for Germany. Less than 39% of papers from Indiaare found in quartile 1 (high impact factor) journals, compared to 53.6% forChina and 53.8% for South Korea. Percent share of papers in quartile 1 journalsfrom India is lower than that for the world for all of chemistry and for eachone of the eight categories, viz. analytical, applied, inorganic & nuclear,medicinal, multidisciplinary, organic, physical and electrochemistry whetherone considers data for the entire five-year period or for 2015 alone. About 20%of Indian chemistry papers are in collaboration with international coauthors.Researchers from only 160 Indian institutions have published at least 100papers (compared to 362 in USA and 399 in China) and these include 67 state, 14central and 11 private universities, 27 institutions under the Ministry ofHuman Resource Development, 20 CSIR laboratories, seven Department of AtomicEnergy institutions, and seven Department of Science & Technology institutions.About 40% of all Indian chemistry papers have come from public universities.Only three Indian institutions, viz Bhabha Atomic Research Centre, IndianInstitute of Science and Indian Institute of Chemical Technology, havepublished more than 2,000 papers. None of the Indian universities has performedas well as leading Asian universities. Amrita Vishwa Vidyapeetham, a smallinstitution with less than 200 papers, has performed reasonably well.","Muthu Madhan, Subbiah Gunasekaran, Rani M T, Subbiah Arunachalam and T
  A Abinandanan",,,11,
"Bibliometrics-based heuristics: What is their definition and how can
  they be studied?","  When scientists study the phenomena they are interested in, they apply soundmethods and base their work on theoretical considerations. In contrast, whenthe fruits of their research is being evaluated, basic scientific standards donot seem to matter. Instead, simplistic bibliometric indicators (i.e.,publications and citation counts) are, paradoxically, both widely used andcriticized without any methodological and theoretical framework that wouldserve to ground both use and critique. Yet, Bornmann and Marewski [1] proposedsuch a framework recently. They developed bibliometrics-based heuristics (BBHs)based on the fast-and-frugal heuristics approach [2] to decision making, inorder to conceptually understand and empirically investigate the quantitativeevaluation of research as well as to effectively train end-users ofbibliometrics (e.g., science managers, scientists). Heuristics are decisionstrategies that use part of the available information and ignore the rest. Byexploiting the statistical structure of task environments, they can aid to makeaccurate, fast, effortless, and cost-efficient decisions without thattrade-offs are incurred. Because of their simplicity, heuristics are easy tounderstand and communicate, enhancing the transparency of decision processes.In this commentary, we explain several BBHs and discuss how such heuristics canbe employed in practice (using the evaluation of applicants for fundingprograms as one example). Furthermore, we outline why heuristics can performwell, and how they and their fit to task environments can be studied. Inpointing to the potential of research on BBHs and to the risks that come withan under-researched, mindless usage of bibliometrics, this commentarycontributes to make research evaluation more scientific.","Lutz Bornmann, Sven Hug",,,11,
"A tale of two databases: The use of Web of Science and Scopus in
  academic papers","  Web of Science and Scopus are two world-leading and competing citationdatabases. By using the Science Citation Index Expanded and Social SciencesCitation Index, this paper conducts a comparative, dynamic, and empirical studyfocusing on the use of Web of Science (WoS) and Scopus in academic paperspublished during 2004 and 2018. This brief communication reveals that althoughboth Web of Science and Scopus are increasingly used in academic papers, Scopusas a new-comer is really challenging the dominating role of WoS. Researchersfrom more and more countries/regions and knowledge domains are involved in theuse of these two databases. Even though the main producers of related papersare developed economies, some developing economies such as China, Brazil andIran also act important roles but with different patterns in the use of thesetwo databases. Both two databases are widely used in meta-analysis relatedstudies especially for researchers in China. Health/medical science relateddomains and the traditional Information Science & Library Science field standout in the use of citation databases.",Junwen Zhu and Weishu Liu,,,11,
"A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility","  Data makes science possible. Sharing data improves visibility, and makes theresearch process transparent. This increases trust in the work, and allows forindependent reproduction of results. However, a large proportion of data frompublished research is often only available to the original authors. Despite theobvious benefits of sharing data, and scientists' advocating for the importanceof sharing data, most advice on sharing data discusses its broader benefits,rather than the practical considerations of sharing. This paper providespractical, actionable advice on how to actually share data alongside research.The key message is sharing data falls on a continuum, and entering it shouldcome with minimal barriers.","Nicholas J Tierney, Karthik Ram",,,11,
"A Decade of In-text Citation Analysis based on Natural Language
  Processing and Machine Learning Techniques: An overview of empirical studies","  Citation analysis is one of the most frequently used methods in researchevaluation. We are seeing significant growth in citation analysis throughbibliometric metadata, primarily due to the availability of citation databasessuch as the Web of Science, Scopus, Google Scholar, Microsoft Academic, andDimensions. Due to better access to full-text publication corpora in recentyears, information scientists have gone far beyond traditional bibliometrics bytapping into advancements in full-text data processing techniques to measurethe impact of scientific publications in contextual terms. This has led totechnical developments in citation context and content analysis, citationclassifications, citation sentiment analysis, citation summarisation, andcitation-based recommendation. This article aims to narratively review thestudies on these developments. Its primary focus is on publications that haveused natural language processing and machine learning techniques to analysecitations.","Sehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani, Salem Alelyani,
  Raheel Nawaz and Lutz Bornmann",,,11,
"Viewport-Aware Deep Reinforcement Learning Approach for 360$^o$ Video
  Caching","  360$^o$ video is an essential component of VR/AR/MR systems that providesimmersive experience to the users. However, 360$^o$ video is associated withhigh bandwidth requirements. The required bandwidth can be reduced byexploiting the fact that users are interested in viewing only a part of thevideo scene and that users request viewports that overlap with each other.Motivated by the findings of recent works where the benefits of caching videotiles at edge servers instead of caching entire 360$^o$ videos were shown, inthis paper, we introduce the concept of virtual viewports that have the samenumber of tiles with the original viewports. The tiles forming these viewportsare the most popular ones for each video and are determined by the users'requests. Then, we propose a proactive caching scheme that assumes unknownvideos' and viewports' popularity. Our scheme determines which videos to cacheas well as which is the optimal virtual viewport per video. Virtual viewportspermit to lower the dimensionality of the cache optimization problem. To solvethe problem, we first formulate the content placement of 360$^o$ videos in edgecache networks as a Markov Decision Process (MDP), and then we determine theoptimal caching placement using the Deep Q-Network (DQN) algorithm. Theproposed solution aims at maximizing the overall quality of the 360$^o$ videosdelivered to the end-users by caching the most popular 360$^o$ videos at basequality along with a virtual viewport in high quality. We extensively evaluatethe performance of the proposed system and compare it with that of knownsystems such as LFU, LRU, FIFO, over both synthetic and real 360$^o$ videotraces. The results reveal the large benefits coming from proactive caching ofvirtual viewports instead of the original ones in terms of the overall qualityof the rendered viewports, the cache hit ratio, and the servicing cost.",Pantelis Maniotis and Nikolaos Thomos,,,11,
"A multi-level approach with visual information for encrypted H.265/HEVC
  videos","  High-efficiency video coding (HEVC) encryption has been proposed to encryptsyntax elements for the purpose of video encryption. To achieve high videosecurity, to the best of our knowledge, almost all of the existing HEVCencryption algorithms mainly encrypt the whole video, such that the userwithout permissions cannot obtain any viewable information. However, theseencryption algorithms cannot meet the needs of customers who need part of theinformation but not the full information in the video. In many cases, such asprofessional paid videos or video meetings, users would like to observe somevisible information in the encrypted video of the original video to satisfytheir requirements in daily life. Aiming at this demand, this paper proposes amulti-level encryption scheme that is composed of lightweight encryption,medium encryption and heavyweight encryption, where each encryption level canobtain a different amount of visual information. It is found that bothencrypting the luma intraprediction model (IPM) and scrambling the syntaxelement of the DCT coefficient sign can achieve the performance of a distortedvideo in which there is still residual visual information, while encryptingboth of them can implement the intensity of encryption and one cannot gain anyvisual information. The experimental results meet our expectationsappropriately, indicating that there is a different amount of visualinformation in each encryption level. Meanwhile, users can flexibly choose theencryption level according to their various requirements.","Wenying Wen, Rongxin Tu, Yushu Zhang, Yuming Fang, Yong Yang",,,11,
"Sequential Reinforced 360-Degree Video Adaptive Streaming with
  Cross-user Attentive Network","  In the tile-based 360-degree video streaming, predicting user's futureviewpoints and developing adaptive bitrate (ABR) algorithms are essential foroptimizing user's quality of experience (QoE). Traditional single-user basedviewpoint prediction methods fail to achieve good performance in long-termprediction, and the recently proposed reinforcement learning (RL) based ABRschemes applied in traditional video streaming can not be directly applied inthe tile-based 360-degree video streaming due to the exponential action space.Therefore, we propose a sequential reinforced 360-degree video streaming schemewith cross-user attentive network. Firstly, considering different users mayhave the similar viewing preference on the same video, we propose a cross-userattentive network (CUAN), boosting the performance of long-term viewpointprediction by selectively utilizing cross-user information. Secondly, wepropose a sequential RL-based (360SRL) ABR approach, transforming action spacesize of each decision step from exponential to linear via introducing asequential decision structure. We evaluate the proposed CUAN and 360SRL usingtrace-driven experiments and experimental results demonstrate that CUAN and360SRL outperform existing viewpoint prediction and ABR approaches with anoticeable margin.","Jun Fu, Zhibo Chen, Xiaoming Chen, Weiping Li",,,11,
"ASMD: an automatic framework for compiling multimodal datasets with
  audio and scores","  This paper describes an open-source Python framework for handling datasetsfor music processing tasks, built with the aim of improving the reproducibilityof research projects in music computing and assessing the generalizationabilities of machine learning models. The framework enables the automaticdownload and installation of several commonly used datasets for multimodalmusic processing. Specifically, we provide a Python API to access the datasetsthrough Boolean set operations based on particular attributes, such asintersections and unions of composers, instruments, and so on. The framework isdesigned to ease the inclusion of new datasets and the respective ground-truthannotations so that one can build, convert, and extend one's own collection aswell as distribute it by means of a compliant format to take advantage of theAPI. All code and ground-truth are released under suitable open licenses.","Federico Simonetta, Stavros Ntalampiras, Federico Avanzini",,,11,
Deep Learning-Based Video Coding: A Review and A Case Study,"  The past decade has witnessed great success of deep learning technology inmany disciplines, especially in computer vision and image processing. However,deep learning-based video coding remains in its infancy. This paper reviews therepresentative works about using deep learning for image/video coding, whichhas been an actively developing research area since the year of 2015. We dividethe related works into two categories: new coding schemes that are builtprimarily upon deep networks (deep schemes), and deep network-based codingtools (deep tools) that shall be used within traditional coding schemes ortogether with traditional coding tools. For deep schemes, pixel probabilitymodeling and auto-encoder are the two approaches, that can be viewed aspredictive coding scheme and transform coding scheme, respectively. For deeptools, there have been several proposed techniques using deep learning toperform intra-picture prediction, inter-picture prediction, cross-channelprediction, probability distribution prediction, transform, post- or in-loopfiltering, down- and up-sampling, as well as encoding optimizations. In thehope of advocating the research of deep learning-based video coding, we presenta case study of our developed prototype video codec, namely Deep Learning VideoCoding (DLVC). DLVC features two deep tools that are both based onconvolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)and CNN-based block adaptive resolution coding (CNN-BARC). Both tools helpimprove the compression efficiency by a significant margin. With the two deeptools as well as other non-deep coding tools, DLVC is able to achieve onaverage 39.6\% and 33.0\% bits saving than HEVC, under random-access andlow-delay configurations, respectively. The source code of DLVC has beenreleased for future researches.","Dong Liu, Yue Li, Jianping Lin, Houqiang Li, Feng Wu",,,11,
"Towards 5G: Joint Optimization of Video Segment Cache, Transcoding and
  Resource Allocation for Adaptive Video Streaming in a Muti-access Edge
  Computing Network","  The cache and transcoding of the multi-access edge computing (MEC) server andwireless resource allocation in eNodeB interact and determine the quality ofexperience (QoE) of dynamic adaptive streaming over HTTP (DASH) clients in MECnetworks. However, the relationship among the three factors has not beenexplored, which has led to limited improvement in clients' QoE. Therefore, wepropose a joint optimization framework of video segment cache and transcodingin MEC servers and resource allocation to improve the QoE of DASH clients.Based on the established framework, we develop a MEC cache management mechanismthat consists of the MEC cache partition, video segment deletion, and MEC cachespace transfer. Then, a joint optimization algorithm that combines videosegment cache and transcoding in the MEC server and resource allocation isproposed. In the algorithm, the clients' channel state and the playback statusand cooperation among MEC servers are employed to estimate the client'spriority, video segment presentation switch and continuous playback time.Considering the above four factors, we develop a utility function model ofclients' QoE. Then, we formulate a mixed-integer nonlinear programmingmathematical model to maximize the total utility of DASH clients, where thevideo segment cache and transcoding strategy and resource allocation strategyare jointly optimized. To solve this problem, we propose a low-complexityheuristic algorithm that decomposes the original problem into multiplesubproblems. The simulation results show that our proposed algorithmsefficiently improve client's throughput, received video quality and hit ratioof video segments while decreasing the playback rebuffering time, video segmentpresentation switch and system backhaul traffic.","Xinyu Huang, Lijun He, Liejun Wang, Fan Li",,,11,
Temporally Guided Music-to-Body-Movement Generation,"  This paper presents a neural network model to generate virtual violinist's3-D skeleton movements from music audio. Improved from the conventionalrecurrent neural network models for generating 2-D skeleton data in previousworks, the proposed model incorporates an encoder-decoder architecture, as wellas the self-attention mechanism to model the complicated dynamics in bodymovement sequences. To facilitate the optimization of self-attention model,beat tracking is applied to determine effective sizes and boundaries of thetraining examples. The decoder is accompanied with a refining network and abowing attack inference mechanism to emphasize the right-hand behavior andbowing attack timing. Both objective and subjective evaluations reveal that theproposed model outperforms the state-of-the-art methods. To the best of ourknowledge, this work represents the first attempt to generate 3-D violinists'body movements considering key features in musical body movement.",Hsuan-Kai Kao and Li Su,,,11,
Exploring the Role of Visual Content in Fake News Detection,"  The increasing popularity of social media promotes the proliferation of fakenews, which has caused significant negative societal effects. Therefore, fakenews detection on social media has recently become an emerging research area ofgreat concern. With the development of multimedia technology, fake newsattempts to utilize multimedia content with images or videos to attract andmislead consumers for rapid dissemination, which makes visual content animportant part of fake news. Despite the importance of visual content, ourunderstanding of the role of visual content in fake news detection is stilllimited. This chapter presents a comprehensive review of the visual content infake news, including the basic concepts, effective visual features,representative detection methods and challenging issues of multimedia fake newsdetection. This chapter can help readers to understand the role of visualcontent in fake news detection, and effectively utilize visual content toassist in detecting multimedia fake news.","Juan Cao, Peng Qi, Qiang Sheng, Tianyun Yang, Junbo Guo, Jintao Li",,,11,
"Comparing emotional states induced by 360$^{\circ}$ videos via
  head-mounted display and computer screen","  In recent years 360$^{\circ}$ videos have been becoming more popular. Fortraditional media presentations, e.g., on a computer screen, a wide range ofassessment methods are available. Different constructs, such as perceivedquality or the induced emotional state of viewers, can be reliably assessed bysubjective scales. Many of the subjective methods have only been validatedusing stimuli presented on a computer screen. This paper is using 360$^{\circ}$videos to induce varying emotional states. Videos were presented 1) via ahead-mounted display (HMD) and 2) via a traditional computer screen.Furthermore, participants were asked to rate their emotional state 1) inretrospect on the self-assessment manikin scale and 2) continuously on a2-dimensional arousal-valence plane. In a repeated measures design, allparticipants (N = 18) used both presentation systems and both rating systems.Results indicate that there is a statistically significant difference ininduced presence due to the presentation system. Furthermore, there was nostatistically significant difference in ratings gathered with the twopresentation systems. Finally, it was found that for arousal measures, astatistically significant difference could be found for the different ratingmethods, potentially indicating an underestimation of arousal ratings gatheredin retrospect for screen presentation. In the future, rating methods such as a2-dimensional arousal-valence plane could offer the advantage of enabling areliable measurement of emotional states while being more embedded in theexperience itself, enabling a more precise capturing of the emotional states.","Jan-Niklas Voigt-Antons, Eero Lehtonen, Andres Pinilla Palacios,
  Danish Ali, Tanja Koji\'c, Sebastian M\""oller",,,11,
"Short Video-based Advertisements Evaluation System: Self-Organizing
  Learning Approach","  With the rising of short video apps, such as TikTok, Snapchat and Kwai,advertisement in short-term user-generated videos (UGVs) has become a trendingform of advertising. Prediction of user behavior without specific user profileis required by advertisers, as they expect to acquire advertisement performancein advance in the scenario of cold start. Current recommender system do nottake raw videos as input; additionally, most previous work of Multi-ModalMachine Learning may not deal with unconstrained videos like UGVs. In thispaper, we proposed a novel end-to-end self-organizing framework for userbehavior prediction. Our model is able to learn the optimal topology of neuralnetwork architecture, as well as optimal weights, through training data. Weevaluate our proposed method on our in-house dataset. The experimental resultsreveal that our model achieves the best performance in all our experiments.","Yunjie Zhang, Fei Tao, Xudong Liu, Runze Su, Xiaorong Mei, Weicong
  Ding, Zhichen Zhao, Lei Yuan, Ji Liu",,,11,
"Building a Manga Dataset ""Manga109"" with Annotations for Multimedia
  Applications","  Manga, or comics, which are a type of multimodal artwork, have been leftbehind in the recent trend of deep learning applications because of the lack ofa proper dataset. Hence, we built Manga109, a dataset consisting of a varietyof 109 Japanese comic books (94 authors and 21,142 pages) and made it publiclyavailable by obtaining author permissions for academic use. We carefullyannotated the frames, speech texts, character faces, and character bodies; thetotal number of annotations exceeds 500k. This dataset provides numerous mangaimages and annotations, which will be beneficial for use in machine learningalgorithms and their evaluation. In addition to academic use, we obtainedfurther permission for a subset of the dataset for industrial use. In thisarticle, we describe the details of the dataset and present a few examples ofmultimedia processing applications (detection, retrieval, and generation) thatapply existing deep learning methods and are made possible by the dataset.","Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru Ogawa, Yusuke
  Matsui, Koki Tsubota, Hikaru Ikuta",,,11,
Stacked Convolutional Deep Encoding Network for Video-Text Retrieval,"  Existing dominant approaches for cross-modal video-text retrieval task are tolearn a joint embedding space to measure the cross-modal similarity. However,these methods rarely explore long-range dependency inside video frames ortextual words leading to insufficient textual and visual details. In thispaper, we propose a stacked convolutional deep encoding network for video-textretrieval task, which considers to simultaneously encode long-range andshort-range dependency in the videos and texts. Specifically, a multi-scaledilated convolutional (MSDC) block within our approach is able to encodeshort-range temporal cues between video frames or text words by adoptingdifferent scales of kernel size and dilation size of convolutional layer. Astacked structure is designed to expand the receptive fields by repeatedlyadopting the MSDC block, which further captures the long-range relationsbetween these cues. Moreover, to obtain more robust textual representations, wefully utilize the powerful language model named Transformer in two stages:pretraining phrase and fine-tuning phrase. Extensive experiments on twodifferent benchmark datasets (MSR-VTT, MSVD) show that our proposed methodoutperforms other state-of-the-art approaches.","Rui Zhao, Kecheng Zheng, Zheng-jun Zha",,,11,
Performance of AV1 Real-Time Mode,"  With COVID-19, the interest for digital interactions has raised, putting inturn real-time (or low-latency) codecs into a new light. Most of the codecresearch has been traditionally focusing on coding efficiency, while verylittle literature exist on real-time codecs. It is shown how the speed at whichcontent is made available impacts both latency and throughput. The authorsintroduce a new test set up, integrating a paced reader, which allows to runcodec in the same condition as real-time media capture. Quality measurementsusing VMAF, as well as multiple speed measurements are made on encoding of HDand full HD video sequences, both at 25 fps and 50 fps to compare therespective performances of several implementations of the H.264, H.265, VP8,VP9 and AV1 codecs.",Ludovic Roux and Alexandre Gouaillard,,,11,
"Exploring the Contextual Dynamics of Multimodal Emotion Recognition in
  Videos","  Emotional expressions form a key part of user behavior on today's digitalplatforms. While multimodal emotion recognition techniques are gaining researchattention, there is a lack of deeper understanding on how visual and non-visualfeatures can be used in better recognizing emotions for certain contexts, butnot others. This study analyzes the interplay between the effects of multimodalemotion features derived from facial expressions, tone and text in conjunctionwith two key contextual factors: 1) the gender of the speaker, and 2) theduration of the emotional episode. Using a large dataset of more than 2500manually annotated videos from YouTube, we found that while multimodal featuresconsistently outperformed bimodal and unimodal features, their performancesvaried significantly for different emotions, gender and duration contexts.Multimodal features were found to perform particularly better for male thanfemale speakers in recognizing most emotions except for fear. Furthermore,multimodal features performed particularly better for shorter than for longervideos in recognizing neutral, happiness, and surprise, but not sadness, anger,disgust and fear. These findings offer new insights towards the development ofmore context-aware emotion recognition and empathetic systems.","Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang",,,11,
"A Modified Fourier-Mellin Approach for Source Device Identification on
  Stabilized Videos","  To decide whether a digital video has been captured by a given device,multimedia forensic tools usually exploit characteristic noise traces left bythe camera sensor on the acquired frames. This analysis requires that the noisepattern characterizing the camera and the noise pattern extracted from videoframes under analysis are geometrically aligned. However, in many practicalscenarios this does not occur, thus a re-alignment or synchronization has to beperformed. Current solutions often require time consuming search of therealignment transformation parameters. In this paper, we propose to overcomethis limitation by searching scaling and rotation parameters in the frequencydomain. The proposed algorithm tested on real videos from a well-knownstate-of-the-art dataset shows promising results.","Sara Mandelli, Fabrizio Argenti, Paolo Bestagini, Massimo Iuliani,
  Alessandro Piva, Stefano Tubaro",,,11,
"An Automated and Robust Image Watermarking Scheme Based on Deep Neural
  Networks","  Digital image watermarking is the process of embedding and extracting awatermark covertly on a cover-image. To dynamically adapt image watermarkingalgorithms, deep learning-based image watermarking schemes have attractedincreased attention during recent years. However, existing deep learning-basedwatermarking methods neither fully apply the fitting ability to learn andautomate the embedding and extracting algorithms, nor achieve the properties ofrobustness and blindness simultaneously. In this paper, a robust and blindimage watermarking scheme based on deep learning neural networks is proposed.To minimize the requirement of domain knowledge, the fitting ability of deepneural networks is exploited to learn and generalize an automated imagewatermarking algorithm. A deep learning architecture is specially designed forimage watermarking tasks, which will be trained in an unsupervised manner toavoid human intervention and annotation. To facilitate flexible applications,the robustness of the proposed scheme is achieved without requiring any priorknowledge or adversarial examples of possible attacks. A challenging case ofwatermark extraction from phone camera-captured images demonstrates therobustness and practicality of the proposal. The experiments, evaluation, andapplication cases confirm the superiority of the proposed scheme.","Xin Zhong, Pei-Chi Huang, Spyridon Mastorakis, Frank Y. Shih",,,11,
"Semantics Preserving Hierarchy based Retrieval of Indian heritage
  monuments","  Monument classification can be performed on the basis of their appearance andshape from coarse to fine categories. Although there is much semanticinformation present in the monuments which is reflected in the eras they werebuilt, its type or purpose, the dynasty which established it, etc.Particularly, Indian subcontinent exhibits a huge deal of variation in terms ofarchitectural styles owing to its rich cultural heritage. In this paper, wepropose a framework that utilizes hierarchy to preserve semantic informationwhile performing image classification or image retrieval. We encode the learntdeep embeddings to construct a dictionary of images and then utilize are-ranking framework on the the retrieved results using DeLF features. Thesemantic information preserved in these embeddings helps to classify unknownmonuments at higher level of granularity in hierarchy. We have curated a large,novel Indian heritage monuments dataset comprising of images of historical,cultural and religious importance with subtypes of eras, dynasties andarchitectural styles. We demonstrate the performance of the proposed frameworkin image classification and retrieval tasks and compare it with other competingmethods on this dataset.","Ronak Gupta, Prerana Mukherjee, Brejesh Lall, Varshul Gupta",,,11,
Continuous QoE Prediction Based on WaveNet,"  Continuous QoE prediction is crucial in the purpose of maximizing viewersatisfaction, by which video service providers could improve the revenue.Continuously predicting QoE is challenging since it requires QoE models thatare capable of capturing the complex dependencies among QoE influence factors.The existing approaches that utilize Long-Short-Term-Memory (LSTM) networksuccessfully model such long-term dependencies, providing the superior QoEprediction performance. However, the inherent drawback of sequential computingof LSTM will result in high computational cost in training and predictiontasks. Recently, WaveNet, a deep neural network for generating raw audiowaveform, has been introduced. Immediately, it gains a great attention since itsuccessfully leverages the characteristic of parallel computing of causalconvolution and dilated convolution to deal with time-series data (e.g., audiosignal). Being inspired by the success of WaveNet, in this paper, we proposeWaveNet-based QoE model for continuous QoE prediction in video streamingservices. The model is trained and tested upon on two publicly availabledatabases, namely, LFOVIA Video QoE and LIVE Mobile Stall Video II. Theexperimental results demonstrate that the proposed model outperforms thebaselines models in terms of processing time, while maintaining sufficientaccuracy.","Phan Xuan Tan, Tho Nguyen Duc, Chanh Minh Tran, Eiji Kamioka",,,11,
"Effect of Language Proficiency on Subjective Evaluation of Noise
  Suppression Algorithms","  Speech communication systems based on Voice-over-IP technology are frequentlyused by native as well as non-native speakers of a target language, e.g. ininternational phone calls or telemeetings. Frequently, such calls also occur ina noisy environment, making noise suppression modules necessary to increaseperceived quality of experience. Whereas standard tests for assessing perceivedquality make use of native listeners, we assume that noise-reduced speech andresidual noise may affect native and non-native listeners of a target languagein different ways. To test this assumption, we report results of two subjectivetests conducted with English and German native listeners who judge the qualityof speech samples recorded by native English, German, and Mandarin speakers,which are degraded with different background noise levels and noise suppressioneffects. The experiments were conducted following the standardized ITU-T Rec.P.835 approach, however implemented in a crowdsourcing setting according toITU-T Rec. P.808. Our results show a significant influence of language onspeech signal ratings and, consequently, on the overall perceived quality inspecific conditions.","Babak Naderi, Gabriel Mittag, Rafael Zequeira Jim\a'enez, Sebastian
  M\""oller",,,11,
Content Format and Quality of Experience in Virtual Reality,"  In this paper, we investigate three forms of virtual reality contentproduction and consumption. Namely, 360 stereoscopic video, the combination ofa 3D environment with a video billboard for dynamic elements, and a full 3Drendered scene. On one hand, video based techniques facilitate the acquisitionof content, but they can limit the experience of the user since the content iscaptured from a fixed point of view. On the other hand, 3D content allows forpoint of view translation, but real-time photorealistic rendering is nottrivial and comes at high production and processing costs. We also compare thetwo extremes with an approach that combines dynamic video elements with a 3Dvirtual environment. We discuss the advantages and disadvantages of thesesystems, and present the result of a user study with 24 participants. In thestudy, we evaluated the quality of experience, including presence, simulationsickness and participants' assessment of content quality, of three versions ofa cinematic segment with two actors. We found that, in this context, mixingvideo and 3D content produced the best experience.","Henrique Galvan Debarba, Mario Montagud, Sylvain Chagu\'e, Javier
  Lajara, Ignacio Lacosta, Sergi Fernandez Langa, Caecilia Charbonnier",,,11,
"Low Overhead Online Data Flow Tracking for Intermittently Powered
  Non-volatile FPGAs","  Energy harvesting is an attractive way to power future IoT devices since itcan eliminate the need for battery or power cables. However, harvested energyis intrinsically unstable. While FPGAs have been widely adopted in variousembedded systems, it is hard to survive unstable power since all the memorycomponents in FPGA are based on volatile SRAMs. The emerging non-volatilememory based FPGAs provide promising potentials to keep configuration data onthe chip during power outages. Few works have considered implementing efficientruntime intermediate data checkpoint on non-volatile FPGAs. To realizeaccumulative computation under intermittent power on FPGA, this paper proposesa low-cost design framework, Data-Flow-Tracking FPGA (DFT-FPGA), which utilizesbinary counters to track intermediate data flow. Instead of keeping all on-chipintermediate data, DFT-FPGA only targets on necessary data that is labeled byoff-line analysis and identified by an online tracking system. The evaluationshows that compared with state-of-the-art techniques, DFT-FPGA can realizeaccumulative computing with less off-line workload and significantly reduceonline roll-back time and resource utilization.","Xinyi Zhang, Clay Patterson, Yongpan Liu, Chengmo Yang, Chun Jason
  Xue, Jingtong Hu",,,11,
"Enabling High-Capacity, Latency-Tolerant, and Highly-Concurrent GPU
  Register Files via Software/Hardware Cooperation","  Graphics Processing Units (GPUs) employ large register files to accommodateall active threads and accelerate context switching. Unfortunately, registerfiles are a scalability bottleneck for future GPUs due to long access latency,high power consumption, and large silicon area provisioning. Prior workproposes hierarchical register file to reduce the register file powerconsumption by caching registers in a smaller register file cache.Unfortunately, this approach does not improve register access latency due tothe low hit rate in the register file cache.  In this paper, we propose the Latency-Tolerant Register File (LTRF)architecture to achieve low latency in a two-level hierarchical structure whilekeeping power consumption low. We observe that compile-time interval analysisenables us to divide GPU program execution into intervals with an accurateestimate of a warp's aggregate register working-set within each interval. Thekey idea of LTRF is to prefetch the estimated register working-set from themain register file to the register file cache under software control, at thebeginning of each interval, and overlap the prefetch latency with the executionof other warps. We observe that register bank conflicts while prefetching theregisters could greatly reduce the effectiveness of LTRF. Therefore, we devisea compile-time register renumbering technique to reduce the likelihood ofregister bank conflicts. Our experimental results show that LTRF enableshigh-capacity yet long-latency main GPU register files, paving the way forvarious optimizations. As an example optimization, we implement the mainregister file with emerging high-density high-latency memory technologies,enabling 8X larger capacity and improving overall GPU performance by 34%.","Mohammad Sadrosadati, Amirhossein Mirhosseini, Ali Hajiabadi, Seyed
  Borna Ehsani, Hajar Falahati, Hamid Sarbazi-Azad, Mario Drumond, Babak
  Falsafi, Rachata Ausavarungnirun, Onur Mutlu",,,11,
Load Driven Branch Predictor (LDBP),"  Branch instructions dependent on hard-to-predict load data are the leadingbranch misprediction contributors. Current state-of-the-art history-basedbranch predictors have poor prediction accuracy for these branches. Priorresearch backs this observation by showing that increasing the size of a256-KBit history-based branch predictor to its 1-MBit variant has just a 10%reduction in branch mispredictions.  We present the novel Load Driven Branch Predictor(LDBP) specificallytargeting hard-to-predict branches dependent on a load instruction. Thoughrandom load data determines the outcome for these branches, the load addressfor most of these data has a predictable pattern. This is an observabletemplate in data structures like arrays and maps. Our predictor model exploitsthis behavior to trigger future loads associated with branches ahead of timeand use its data to predict the branch's outcome. The predictable loads aretracked, and the precomputed outcomes of the branch instruction are bufferedfor making predictions. Our experimental results show that compared to astandalone 256-Kbit IMLI predictor, when LDBP is augmented with a 150-KbitIMLI, it reduces the average branch mispredictions by 20% and improves averageIPC by 13.1% for benchmarks from SPEC CINT2006 and GAP benchmark suite.","Akash Sridhar, Nursultan Kabylkas, Jose Renau",,,11,
"Revisiting RowHammer: An Experimental Analysis of Modern DRAM Devices
  and Mitigation Techniques","  In order to shed more light on how RowHammer affects modern and futuredevices at the circuit-level, we first present an experimental characterizationof RowHammer on 1580 DRAM chips (408x DDR3, 652x DDR4, and 520x LPDDR4) from300 DRAM modules (60x DDR3, 110x DDR4, and 130x LPDDR4) with RowHammerprotection mechanisms disabled, spanning multiple different technology nodesfrom across each of the three major DRAM manufacturers. Our studiesdefinitively show that newer DRAM chips are more vulnerable to RowHammer: asdevice feature size reduces, the number of activations needed to induce aRowHammer bit flip also reduces, to as few as 9.6k (4.8k to two rows each) inthe most vulnerable chip we tested.  We evaluate five state-of-the-art RowHammer mitigation mechanisms usingcycle-accurate simulation in the context of real data taken from our chips tostudy how the mitigation mechanisms scale with chip vulnerability. We find thatexisting mechanisms either are not scalable or suffer from prohibitively largeperformance overheads in projected future devices given our observed trends ofRowHammer vulnerability. Thus, it is critical to research more effectivesolutions to RowHammer.","Jeremie S. Kim and Minesh Patel and A. Giray Yaglikci and Hasan Hassan
  and Roknoddin Azizi and Lois Orosa and Onur Mutlu",,,11,
"Decision Tree Based Hardware Power Monitoring for Run Time Dynamic Power
  Management in FPGA","  Fine-grained runtime power management techniques could be promising solutionsfor power reduction. Therefore, it is essential to establish accurate powermonitoring schemes to obtain dynamic power variation in a short period (i.e.,tens or hundreds of clock cycles). In this paper, we leverage adecision-tree-based power modeling approach to establish fine-grained hardwarepower monitoring on FPGA platforms. A generic and complete design flow isdeveloped to implement the decision tree power model which is capable ofprecisely estimating dynamic power in a fine-grained manner. A flexiblearchitecture of the hardware power monitoring is proposed, which can beinstrumented in any RTL design for runtime power estimation, dispensing withthe need for extra power measurement devices. Experimental results of applyingthe proposed model to benchmarks with different resource types reveal anaverage error up to 4% for dynamic power estimation. Moreover, the overheads ofarea, power and performance incurred by the power monitoring circuitry areextremely low. Finally, we apply our power monitoring technique to the powermanagement using phase shedding with an on-chip multi-phase regulator as aproof of concept and the results demonstrate 14% efficiency enhancement for thepower supply of the FPGA internal logic.","Zhe Lin, Wei Zhang, Sharad Sinha",,,11,
"Analytical Modeling the Multi-Core Shared Cache Behavior with
  Considerations of Data-Sharing and Coherence","  To mitigate the ever worsening ""Power wall"" and ""Memory wall"" problems,multi-core architectures with multilevel cache hierarchies have been widelyaccepted in modern processors. However, the complexity of the architecturesmakes modeling of shared caches extremely complex. In this paper, we propose adata-sharing aware analytical model for estimating the miss rates of thedownstream shared cache under multi-core scenarios. Moreover, the proposedmodel can also be integrated with upstream cache analytical models with theconsideration of multi-core private cache coherent effects. This integrationavoids time consuming full simulations of the cache architecture that requiredby conventional approaches. We validate our analytical model against gem5simulation results under 13 applications from PARSEC 2.1 benchmark suites.Compared to the results from gem5 simulations under 8 hardware configurationsincluding dual-core and quad-core architectures, the average absolute error ofthe predicted shared L2 cache miss rates is less than 2% for allconfigurations. After integrated with the refined upstream model with coherencemisses, the overall average absolute error in 4 hardware configurations isdegraded to 8.03% due to the error accumulations. The proposed coherence modelcan achieve similar accuracies of state of the art approach with only one tenthtime overhead. As an application case of the integrated model, we also evaluatethe miss rates of 57 different multi-core and multi-level cache configurations.","Ming Ling, Xiaoqian Lu, Guangmin Wang, Jiancong Ge",,,11,
"Secure Internal Communication of a Trustzone-Enabled Heterogeneous Soc
  Lightweight Encryption","  Security in TrustZone-enabled heterogeneous system-on-chip (SoC) is gainingincreasing attention for several years. Mainly because this type of SoC can befound in more and more applications in servers or in the cloud. The inside-SoCcommunication layer is one of the main element of heterogeneous SoC; indeed allthe data goes through it. Monitoring and controlling inside-SoC communicationsenables to fend off attacks before system corruption. In this article, we studythe feasibility of encrypted data exchange between the secure software executedin a trusted execution environment (TEE) and the secure logic part of anheterogeneous SoC. Experiment are done with a Xilinx Zynq-7010 SoC and twolightweight stream ciphers. We show that using lightweight stream ciphers is anefficient solution without excessive overheads.","El Mehdi Benhani (LHC), Cuauhtemoc Mancillas Lopez (CINVESTAV-IPN),
  Lilian Bossuet (LHC)",,,11,
"CUTIE: Beyond PetaOp/s/W Ternary DNN Inference Acceleration with
  Better-than-Binary Energy Efficiency","  We present a 3.1 POp/s/W fully digital hardware accelerator for ternaryneural networks. CUTIE, the Completely Unrolled Ternary Inference Engine,focuses on minimizing non-computational energy and switching activity so thatdynamic power spent on storing (locally or globally) intermediate results isminimized. This is achieved by 1) a data path architecture completely unrolledin the feature map and filter dimensions to reduce switching activity byfavoring silencing over iterative computation and maximizing data re-use, 2)targeting ternary neural networks which, in contrast to binary NNs, allow forsparse weights which reduce switching activity, and 3) introducing an optimizedtraining method for higher sparsity of the filter weights, resulting in afurther reduction of the switching activity. Compared with state-of-the-artaccelerators, CUTIE achieves greater or equal accuracy while decreasing theoverall core inference energy cost by a factor of 4.8x-21x.","Moritz Scherer, Georg Rutishauser, Lukas Cavigelli, Luca Benini",,,11,
"Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for
  Sensory Processing","  Temporal Neural Networks (TNNs) use time as a resource to represent andprocess information, mimicking the behavior of the mammalian neocortex. Thiswork focuses on implementing TNNs using off-the-shelf digital CMOS technology.A microarchitecture framework is introduced with a hierarchy of building blocksincluding: multi-neuron columns, multi-column layers, and multi-layer TNNs. Wepresent the direct CMOS gate-level implementation of the multi-neuron columnmodel as the key building block for TNNs. Post-synthesis results are obtainedusing Synopsys tools and the 45 nm CMOS standard cell library. The TNNmicroarchitecture framework is embodied in a set of characteristic equationsfor assessing the total gate count, die area, compute time, and powerconsumption for any TNN design. We develop a multi-layer TNN prototype of 32Mgates. In 7 nm CMOS process, it consumes only 1.54 mm^2 die area and 7.26 mWpower and can process 28x28 images at 107M FPS (9.34 ns per image). We evaluatethe prototype's performance and complexity relative to a recentstate-of-the-art TNN model.","Harideep Nair, John Paul Shen, James E. Smith",,,11,
"ArSMART: An Improved SMART NoC Design Supporting Arbitrary-Turn
  Transmission","  SMART NoC, which transmits unconflicted flits to distant processing elements(PEs) in one cycle through the express bypass, is a high-performance NoC designproposed recently. However, if contention occurs, flits with low priority wouldnot only be buffered but also could not fully utilize bypass. Although thereexist several routing algorithms that decrease contentions by rounding busyrouters and links, they cannot be directly applicable to SMART since it lacksthe support for arbitrary-turn (i.e., the number and direction of turns arefree of constraints) routing. Thus, in this article, to minimize contentionsand further utilize bypass, we propose an improved SMART NoC, called ArSMART,in which arbitrary-turn transmission is enabled. Specifically, ArSMART dividesthe whole NoC into multiple clusters where the route computation is conductedby the cluster controller and the data forwarding is performed by thebufferless reconfigurable router. Since the long-range transmission in SMARTNoC needs to bypass the intermediate arbitration, to enable this feature, wedirectly configure the input and output ports connection rather than applyhop-by-hop table-based arbitration. To further explore the higher communicationcapabilities, effective adaptive routing algorithms that are compatible withArSMART are proposed. The route computation overhead, one of the main concernsfor adaptive routing algorithms, is hidden by our carefully designed controlmechanism. Compared with the state-of-the-art SMART NoC, the experimentalresults demonstrate an average reduction of 40.7% in application schedulelength and 29.7% in energy consumption.","Hui Chen, Peng Chen, Jun Zhou, Duong H. K. Luan, and Weichen Liu",,,11,
"Exploiting Inter- and Intra-Memory Asymmetries for Data Mapping in
  Hybrid Tiered-Memories","  Modern computing systems are embracing hybrid memory comprising of DRAM andnon-volatile memory (NVM) to combine the best properties of both memorytechnologies, achieving low latency, high reliability, and high density. Aprominent characteristic of DRAM-NVM hybrid memory is that it has NVM accesslatency much higher than DRAM access latency. We call this inter-memoryasymmetry. We observe that parasitic components on a long bitline are a majorsource of high latency in both DRAM and NVM, and a significant factorcontributing to high-voltage operations in NVM, which impact their reliability.We propose an architectural change, where each long bitline in DRAM and NVM issplit into two segments by an isolation transistor. One segment can be accessedwith lower latency and operating voltage than the other. By introducing tiers,we enable non-uniform accesses within each memory type (which we callintra-memory asymmetry), leading to performance and reliability trade-offs inDRAM-NVM hybrid memory. We extend existing NVM-DRAM OS in three ways. First, weexploit both inter- and intra-memory asymmetries to allocate and migrate memorypages between the tiers in DRAM and NVM. Second, we improve the OS's pageallocation decisions by predicting the access intensity of a newly-referencedmemory page in a program and placing it to a matching tier during its initialallocation. This minimizes page migrations during program execution, loweringthe performance overhead. Third, we propose a solution to migrate pages betweenthe tiers of the same memory without transferring data over the memory channel,minimizing channel occupancy and improving performance. Our overall approach,which we call MNEME, to enable and exploit asymmetries in DRAM-NVM hybridtiered memory improves both performance and reliability for both single-coreand multi-programmed workloads.","Shihao Song, Anup Das, Nagarajan Kandasamy",,,11,
"A distributed memory, local configuration technique for re-configurable
  logic designs","  The use and location of memory in integrated circuits plays a key factor intheir performance. Memory requires large physical area, access times limitoverall system performance and connectivity can result in large fan-out. ModernFPGA systems and ASICs contain an area of memory used to set the operation ofthe device from a series of commands set by a host. Implementing these settingsregisters requires a level of care otherwise the resulting implementation canresult in a number of large fan-out nets that consume valuable resourcescomplicating the placement of timing critical pathways. This paper presents anarchitecture for implementing and programming these settings registers in adistributed method across an FPGA and how the presented architecture works inboth clock-domain crossing and dynamic partial re-configuration applications.The design is compared to that of a `global' settings register architecture. Weimplement the architectures using Intel FPGAs Quartus Prime software targetingan Intel FPGA Cyclone V. It is shown that the distributed memory architecturehas a smaller resource cost (as small as 25% of the ALMs and 20% of theregisters) compared to the global memory architectures.",Alexander E. Beasley,,,11,
Hardware Memory Management for Future Mobile Hybrid Memory Systems,"  The current mobile applications have rapidly growing memory footprints,posing a great challenge for memory system design. Insufficient DRAM mainmemory will incur frequent data swaps between memory and storage, a processthat hurts performance, consumes energy and deteriorates the write endurance oftypical flash storage devices. Alternately, a larger DRAM has higher leakagepower and drains the battery faster. Further, DRAM scaling trends make furthergrowth of DRAMin the mobile space prohibitive due to cost. Emergingnon-volatile memory (NVM) has the potential to alleviate these issues due toits higher capacity per cost than DRAM and mini-mal static power. Recently, awide spectrum of NVM technologies, including phase-change memories (PCM),memristor, and 3D XPoint have emerged. Despite the mentioned advantages, NVMhas longer access latency compared to DRAMand NVM writes can incur higherlatencies and wear costs. Therefore integration of these new memorytechnologies in the memory hierarchy requires a fundamental rearchitect-ing oftraditional system designs. In this work, we propose a hardware-acceleratedmemory manager (HMMU) that addresses both types of memory in a flat spaceaddress space. We design a set of data placement and data migration policieswithin this memory manager, such that we may exploit the advantages of eachmemory technology. By augmenting the system with this HMMU, we reduce theoverall memory latency while also reducing writes to the NVM. Experimentalresults show that our design achieves a 39% reduction in energy consumptionwith only a 12% performance degradation versus an all-DRAM baseline that islikely untenable in the future.","Fei Wen, Mian Qin, Paul Gratz, Narasimha Reddy",,,11,
"A high-performance MEMRISTOR-based Smith-Waterman DNA sequence alignment
  Using FPNI structure",This paper aims to present a new re-configuration sequencing method fordifference of read lengths that may take place as input data in which iscrucial drawbacks lay impact on DNA sequencing methods.,"Mahdi Taheri, Hamed Zandevakili and Ali Mahani",,,11,
"Fast Modeling L2 Cache Reuse Distance Histograms Using Combined Locality
  Information from Software Traces","  To mitigate the performance gap between CPU and the main memory, multi-levelcache architectures are widely used in modern processors. Therefore, modelingthe behaviors of the downstream caches becomes a critical part of the processorperformance evaluation in the early stage of Design Space Exploration (DSE). Inthis paper, we propose a fast and accurate L2 cache reuse distance histogrammodel, which can be used to predict the behaviors of the multi-level cachearchitectures where the L1 cache uses the LRU replacement policy and the L2cache uses LRU/Random replacement policies. We use the profiled L1 reusedistance histogram and two newly proposed metrics, namely the RST table and theHit-RDH, that describing more detailed information of the software traces asthe inputs. For a given L1 cache configuration, the profiling results can bereused for different configurations of the L2 cache. The output of our model isthe L2 cache reuse distance histogram, based on which the L2 cache miss ratescan be evaluated. We compare the L2 cache miss rates with the results from gem5cycle-accurate simulations of 15 benchmarks chosen from SPEC CPU 2006 and 9benchmarks from SPEC CPU 2017. The average absolute error is less than 5%,while the evaluation time for each L2 configuration can be sped up almost 30Xfor four L2 cache candidates.","Ming Ling, Jiancong Ge, Guangmin Wang",,,11,
The Cost of Software-Based Memory Management Without Virtual Memory,"  Virtual memory has been a standard hardware feature for more than threedecades. At the price of increased hardware complexity, it has simplifiedsoftware and promised strong isolation among colocated processes. In moderncomputing systems, however, the costs of virtual memory have increasedsignificantly. With large memory workloads, virtualized environments, datacenter computing, and chips with multiple DMA devices, virtual memory candegrade performance and increase power usage. We therefore explore theimplications of building applications and operating systems without relying onhardware support for address translation. Primarily, we investigate theimplications of removing the abstraction of large contiguous memory segments.Our experiments show that the overhead to remove this reliance is surprisinglysmall for real programs. We expect this small overhead to be worth the benefitof reducing the complexity and energy usage of address translation. In fact, insome cases, performance can even improve when address translation is avoided.","Drew Zagieboylo, G. Edward Suh, Andrew C. Myers",,,11,
Stochastic Rounding: Algorithms and Hardware Accelerator,"  Algorithms and a hardware accelerator for performing stochastic rounding (SR)are presented. The main goal is to augment the ARM M4F based multi-coreprocessor SpiNNaker2 with a more flexible rounding functionality than isavailable in the ARM processor itself. The motivation of adding such anaccelerator in hardware is based on our previous results showing improvementsin numerical accuracy of ODE solvers in fixed-point arithmetic with SR,compared to standard round to nearest or bit truncation rounding modes.Furthermore, performing SR purely in software can be expensive, due torequirement of a pseudorandom number generator (PRNG), multiple masking andshifting instructions, and an addition operation. Also, saturation of therounded values is included, since rounding is usually followed by saturation,which is especially important in fixed-point arithmetic due to a narrow dynamicrange of representable values. The main intended use of the accelerator is toround fixed-point multiplier outputs, which are returned unrounded by the ARMprocessor in a wider fixed-point format than the arguments.",Mantas Mikaitis,,,11,
"Hardware Acceleration of Sparse and Irregular Tensor Computations of ML
  Models: A Survey and Insights","  Machine learning (ML) models are widely used in many domains including mediaprocessing and generation, computer vision, medical diagnosis, embeddedsystems, high-performance and scientific computing, and recommendation systems.For efficiently processing these computational- and memory-intensiveapplications, tensors of these over-parameterized models are compressed byleveraging sparsity, size reduction, and quantization of tensors. Unstructuredsparsity and tensors with varying dimensions yield irregular-shapedcomputation, communication, and memory access patterns; processing them onhardware accelerators in a conventional manner does not inherently leverageacceleration opportunities. This paper provides a comprehensive survey on howto efficiently execute sparse and irregular tensor computations of ML models onhardware accelerators. In particular, it discusses additional enhancementmodules in architecture design and software support; categorizes differenthardware designs and acceleration techniques and analyzes them in terms ofhardware and execution costs; highlights further opportunities in terms ofhardware/software/algorithm co-design optimizations and joint optimizationsamong described hardware and software enhancement modules. The takeaways fromthis paper include: understanding the key challenges in accelerating sparse,irregular-shaped, and quantized tensors; understanding enhancements inacceleration systems for supporting their efficient computations; analyzingtrade-offs in opting for a specific type of design enhancement; understandinghow to map and compile models with sparse tensors on the accelerators;understanding recent design trends for efficient accelerations and furtheropportunities.","Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral
  Shrivastava, Baoxin Li",,,11,
A portable and Linux capable RISC-V computer system in Verilog HDL,"  RISC-V is an open and royalty free instruction set architecture which hasbeen developed at the University of California, Berkeley. The processors usingRISC-V can be designed and released freely. Because of this, various processorcores and system on chips (SoCs) have been released so far. However, there area few public RISC-V computer systems that are portable and can boot Linuxoperating systems. In this paper, we describe a portable and Linux capableRISC-V computer system targeting FPGAs in Verilog HDL. This system can beimplemented on an FPGA with fewer hardware resources, and can be implemented onlow cost FPGAs or customized by introducing an accelerator. This paper alsodescribes the knowledge obtained through the development of this RISC-Vcomputer system.","Junya Miura, Hiromu Miyazaki, Kenji Kise",,,11,
"Manticore: A 4096-core RISC-V Chiplet Architecture for Ultra-efficient
  Floating-point Computing","  Data-parallel problems, commonly found in data analytics, machine learning,and scientific computing demand ever growing floating-point operations persecond under tight area- and energy-efficiency constraints.Application-specific architectures and accelerators, while efficient at a giventask, are hard to adjust to algorithmic changes. In this work, we presentManticore, a general-purpose, ultra-efficient, RISC-V, chiplet-basedarchitecture for data-parallel floating-point workloads. We have manufactured a9$\text{mm}^2$ prototype of the chiplet's computational core in Globalfoundries22nm FD-SOI process and demonstrate more than 2.5$\times$ improvement in energyefficiency on floating-point intensive workloads compared to high performancecompute engines (CPUs and GPUs), despite their more advanced FinFET process.The prototype contains two 64-bit, application-class RISC-V Ariane managementcores that run a full-fledged Linux OS. The compute capability at high energyand area efficiency is provided by Snitch clusters. Each cluster contains eightsmall (20kGE) 32-bit integer RISC-V cores, each controlling a largedouble-precision floating-point unit (120kGE). Each core supports two customRISC-V ISA extensions: FREP and SSR. The SSR extension elides explicit load andstore instructions by encoding them as register reads and writes. The FREPextension mostly decouples the integer core from the FPU by allowing a sequencebuffer to issue instructions to the FPU independently. Both extensions allowthe tiny, single-issue, integer core to saturate the instruction bandwidth ofthe FPU and achieve FPU utilization above 90%, with more than 80% of core areadedicated to the FPU.","Florian Zaruba, Fabian Schuiki and Luca Benini",,,11,
"Exploiting Oxide Based Resistive RAM Variability for Bayesian Neural
  Network Hardware Design","  Uncertainty plays a key role in real-time machine learning. As a significantshift from standard deep networks, which does not consider any uncertaintyformulation during its training or inference, Bayesian deep networks are beingcurrently investigated where the network is envisaged as an ensemble ofplausible models learnt by the Bayes' formulation in response to uncertaintiesin sensory data. Bayesian deep networks consider each synaptic weight as asample drawn from a probability distribution with learnt mean and variance.This paper elaborates on a hardware design that exploits cycle-to-cyclevariability of oxide based Resistive Random Access Memories (RRAMs) as a meansto realize such a probabilistic sampling function, instead of viewing it as adisadvantage.","Akul Malhotra, Sen Lu, Kezhou Yang, Abhronil Sengupta",,,11,
"Temporal State Machines: Using temporal memory to stitch time-based
  graph computations","  Race logic, an arrival-time-coded logic family, has demonstrated energy andperformance improvements for applications ranging from dynamic programming tomachine learning. However, the ad hoc mappings of algorithms into hardwareresult in custom architectures making them difficult to generalize. Wesystematize the development of race logic by associating it with themathematical field called tropical algebra. This association between themathematical primitives of tropical algebra and generalized race logiccomputations guides the design of temporally coded tropical circuits. It alsoserves as a framework for expressing high level timing-based algorithms. Thisabstraction, when combined with temporal memory, allows for the systematicgeneralization of race logic by making it possible to partition feed-forwardcomputations into stages and organizing them into a state machine. We leverageanalog memristor-based temporal memories to design a such a state machine thatoperates purely on time-coded wavefronts. We implement a version of Dijkstra'salgorithm to evaluate this temporal state machine. This demonstration shows thepromise of expanding the expressibility of temporal computing to enable it todeliver significant energy and throughput advantages.","Advait Madhavan, Matthew Daniels, Mark Stiles",,,11,
Perceptrons from Memristors,"  Memristors, resistors with memory whose outputs depend on the history oftheir inputs, have been used with success in neuromorphic architectures,particularly as synapses and non-volatile memories. However, to the best of ourknowledge, no model for a network in which both the synapses and the neuronsare implemented using memristors has been proposed so far. In the present workwe introduce models for single and multilayer perceptrons based exclusively onmemristors. We adapt the delta rule to the memristor-based single-layerperceptron and the backpropagation algorithm to the memristor-based multilayerperceptron. Our results show that both perform as expected for perceptrons,including satisfying Minsky-Papert's theorem. As a consequence of the UniversalApproximation Theorem, they also show that memristors are universal functionapproximators. By using memristors for both the neurons and the synapses, ourmodels pave the way for novel memristor-based neural network architectures andalgorithms. A neural network based on memristors could show advantages in termsof energy conservation and open up possibilities for other learning systems tobe adapted to a memristor-based paradigm, both in the classical and quantumlearning realms.","Francisco Silva, Mikel Sanz, Jo\~ao Seixas, Enrique Solano, and Yasser
  Omar",,,11,
Procedural generation using quantum computation,"  Quantum computation is an emerging technology that promises to be a powerfultool in many areas. Though some years likely still remain until significantquantum advantage is demonstrated, the development of the technology has led toa range of valuable resources. These include publicly available prototypequantum hardware, advanced simulators for small quantum programs andprogramming frameworks to test and develop quantum software. In thisprovocation paper we seek to demonstrate that these resources are sufficient toprovide the first useful results in the field of procedural generation. This isdone by introducing a proof-of-principle method: a quantum generalization of ablurring process, in which quantum interference is used to provide a uniqueeffect. Through this we hope to show that further developments in thetechnology are not required before it becomes useful for procedural generation.Rather, fruitful experimentation with this new technology can begin now.",James R. Wootton,,,11,
"Computing with bricks and mortar: Classification of waveforms with a
  doped concrete blocks","  We present results showing the capability of concrete-based informationprocessing substrate in the signal classification task in accordance with inmaterio computing paradigm. As the Reservoir Computing is a suitable model fordescribing embedded in materio computation, we propose that this type ofpresented basic construction unit can be used as a source for ""reservoir ofstates"" necessary for simple tuning of the readout layer. In that perspective,buildings constructed from computing concrete could function as a highlyparallel information processor for smart architecture. We present an electricalcharacterization of the set of samples with different additive concentrationsfollowed by a dynamical analysis of selected specimens showing fingerprints ofmemfractive properties. Moreover, on the basis of obtained parameters,classification of the signal waveform shapes can be performed in scenariosexplicitly tuned for a given device terminal.","Dawid Przyczyna, Maciej Suchecki, Andrew Adamatzky, Konrad
  Szaci{\l}owski",,,11,
On memfractance of plants and fungi,"  The key feature of a memristor is that the resistance is a function of itsprevious resistance, thereby the behaviour of the device is influenced bychanging the way in which potential is applied across it. Ultimately,information can be encoded on memristors, which can then be used to implement anumber of circuit topologies. Biological substrates have already been shown toexhibit some memristive properties. It is, therefore, logical that allbiological media will follow this trend to some degree. In this paper wedemonstrate that a range of yet untested specimens exhibit memristiveproperties, including mediums such as water and dampened wood shavings on whichwe can cultivate biological specimens. We propose that memristance is not abinary property {0,1}, but rather a continuum on the scale [0,1]. The resultsimply that there is great potential for hybrid electronic systems that combinetraditional electronic typologies with naturally occurring specimens.","Alexander E. Beasley and Mohammed-Salah Abdelouahab and Ren\'e Lozi
  and Anna L. Powell and Andrew Adamatzky",,,11,
"Mobility-aware Beam Steering in Metasurface-based Programmable Wireless
  Environments","  Programmable wireless environments (PWEs) utilize electromagneticmetasurfaces to transform wireless propagation into a software-controlledresource. In this work we study the effects of user device mobility on theefficiency of PWEs. An analytical model is proposed, which describes thepotential misalignment between user-emitted waves and the active PWEconfiguration, and can constitute the basis for studying queuing problems inPWEs. Subsequently, a novel, beam steering approach is proposed which caneffectively mitigate the misalignment effects. Ray-tracing-based simulationsevaluate the proposed scheme.","Christos Liaskos, Shuai Nie, Ageliki Tsioliaridou, Andreas
  Pitsillides, Sotiris Ioannidis, Ian Akyildiz",,,11,
A Noise Filter for Dynamic Vision Sensors using Self-adjusting Threshold,"  Neuromorphic event-based dynamic vision sensors (DVS) have much fastersampling rates and a higher dynamic range than frame-based imagers. However,they are sensitive to background activity (BA) events which are unwanted. wepropose a new criterion with little computation overhead for defining realevents and BA events by utilizing the global space and time information ratherthan the local information by Gaussian convolution, which can be also used as afilter. We denote the filter as GF. We demonstrate GF on three datasets, eachrecorded by a different DVS with different output size. The experimentalresults show that our filter produces the clearest frames compared withbaseline filters and run fast.","Shasha Guo, Ziyang Kang, Lei Wang, Limeng Zhang, Xiaofan Chen, Shiming
  Li, Weixia Xu",,,11,
Realization of p_valued Deutsch quantum gates,"  In this report reversible Toffoli and quantum Deutsch gates are extended tothe p_valued domain. Their structural parameters are determined and theirbehavior is proven. Both conjunctive and disjunctive control strategies withpositive and mixed polarities are introduced for the first time in a p_valueddomain. The design is based on elementary Muthukrishnan_Stroud quantum gates,hence the realizability of the extended gates in the context of ion trapsshould be possible.",Claudio Moraga,,,11,
Resonant Energy Recycling SRAM Architecture,"  Although we may be at the end of Moore's law, lowering chip power consumptionis still the primary driving force for the designers. To enable low-poweroperation, we propose a resonant energy recovery static random access memory(SRAM). We propose the first series resonance scheme to reduce the dynamicpower consumption of the SRAM operation. Besides, we identified the requirementof supply boosting of the write buffers for proper resonant operation. Weevaluated the resonant 144KB SRAM cache through SPICE and test chip using acommercial 28nm CMOS technology. The experimental results show that theresonant SRAM can save up to 30% dynamic power at 1GHz operating frequencycompared to the state-of-the-art design.","Riadul Islam, Biprangshu Saha, Ignatius Bezzam",,,11,
Capacitorless Model of a VO2 Oscillator,"  We implement a capacitorless model of a VO2 oscillator by introducing intothe circuit of a field-effect transistor and a VO2 thermal sensor, whichprovide negative current feedback with a time delay. We compare the dynamics ofcurrent and voltage oscillations on a switch in a circuit with a capacitor andwithout a capacitor. The oscillation period in the capacitorless model iscontrolled in a narrow range by changing the distance between the switch andthe sensor. The capacitorless model provides the possibility of significantminiaturization of the oscillator circuit, and it is important for theimplementation of large arrays of oscillators in oscillatory neural networks tosolve the problem of classification and pattern recognition.","M. A. Belyaev, A. A. Velichko",,,11,
Fungal sensing skin,"  A fungal skin is a thin flexible sheet of a living homogeneous mycelium madeby a filamentous fungus. The skin could be used in future living architecturesof adaptive buildings and as a sensing living skin for softself-growing/adaptive robots. In experimental laboratory studies we demonstratethat the fungal skin is capable for recognising mechanical and opticalstimulation. The skin reacts differently to loading of a weight, removal of theweight, and switching illumination on and off. These are the first experimentalevidences that fungal materials can be used not only as mechanical `skeletons'in architecture and robotics but also as intelligent skins capable forrecognition of external stimuli and sensorial fusion.","Andrew Adamatzky, Antoni Gandia, Alessandro Chiolerio",,,11,
"Spin-Hall MTJ Cells for Intra-Column Competition in Hierarchical
  Temporal Memory","  We propose a dedicated winner-take-all circuit to efficiently implement theintra-column competition between cells in Hierarchical Temporal Memory which isa crucial part of the algorithm. All inputs and outputs are charge-based forcompatibility with standard CMOS. The circuit incorporates memristors forcompetitive advantage to emulate a column with a cell in a predictive state.The circuit can also detect columns 'bursting' by passive averaging andcomparison of the cell outputs. The proposed spintronic devices and circuit arethoroughly described and a series of simulations are used to predict theperformance. The simulations indicate that the circuit can complete anine-cell, nine-input competition operation in under 15 ns at a cost of about25 pJ.","Andrew W. Stephan, Steven J. Koester",,,11,
"AIDX: Adaptive Inference Scheme to Mitigate State-Drift in Memristive
  VMM Accelerators","  An adaptive inference method for crossbar (AIDX) is presented based on anoptimization scheme for adjusting the duration and amplitude of input voltagepulses. AIDX minimizes the long-term effects of memristance drift on artificialneural network accuracy. The sub-threshold behavior of memristor has beenmodeled and verified by comparing with fabricated device data. The proposedmethod has been evaluated by testing on different network structures andapplications, e.g., image reconstruction and classification tasks. The resultsshowed an average of 60% improvement in convolutional neural network (CNN)performance on CIFAR10 dataset after 10000 inference operations as well as78.6% error reduction in image reconstruction.","Tony Liu, Amirali Amirsoleimani, Fabien Alibart, Serge Ecoffey,
  Dominique Drouin, and Roman Genov",,,11,
"Parameter Estimation in a Noisy 1D Environment via Two Absorbing
  Receivers","  This paper investigates the estimation of different parameters, e.g.,propagation distance and flow velocity, by utilizing two fully-absorbingreceivers (RXs) in a one-dimensional (1D) environment. The time-varying numberof absorbed molecules at each RX and the number of absorbed molecules in a timeinterval as time approaches infinity are derived. Noisy molecules in thisenvironment, that are released by sources in addition to the transmitter, arealso considered. A novel estimation method, namely difference estimation (DE),is proposed to eliminate the effect of noise by using the difference ofreceived signals at the two RXs. For DE, the Cramer-Rao lower bound (CRLB) onthe variance of estimation is derived. Independent maximum likelihoodestimation is also considered at each RX as a benchmark to show the performanceadvantage of DE. Aided by particle-based simulation, the derived analyticalresults are verified. Furthermore, numerical results show that DE attains theCRLB and is less sensitive to the change of noise than independent estimationat each RX.","Xinyu Huang, Yuting Fang, Adam Noel, Nan Yang",,,11,
Nonparametric Regression Quantum Neural Networks,"  In two pervious papers \cite{dndiep3}, \cite{dndiep4}, the first authorconstructed the least square quantum neural networks (LS-QNN), and ploynomialinterpolation quantum neural networks ( PI-QNN), parametrico-stattistical QNNlike: leanr regrassion quantum neural networks (LR-QNN), polynomial regressionquantum neural networks (PR-QNN), chi-squared quantum neural netowrks($\chi^2$-QNN). We observed that the method works also in the cases by usingnonparametric statistics. In this paper we analyze and implement thenonparametric tests on QNN such as: linear nonparametric regression quantumneural networks (LNR-QNN), polynomial nonparametric regression quantum neuralnetworks (PNR-QNN). The implementation is constructed through the Gauss-JordanElimination quantum neural networks (GJE-QNN).The training rule is to use thehigh probability confidence regions or intervals.","Do Ngoc Diep, Koji Nagata, and Tadao Nakamura",,,11,
"Low-Cost Performance-Efficient Field-Programmable Pin-Constrained
  Digital Microfluidic Biochip","  Digital microfluidic biochips (DMFBs) are revolutionary biomedical devicestowards diagnostics and point-of-care applications; the chips provide thecapability of performing wide ranges of biochemistry and laboratory procedures,offering various opportunities among which to mention are automation,miniaturization and cost-affordability of bioassays. There have been variousdigital microfluidic biochips architectures; the application-specific chips aremainly suited towards executing a predefined set of bioassays whereas the moreflexible general-purpose chips allow executing wide ranges of bioassays on thesame architecture. Though more flexible in terms of performing variousbioassays the general-purpose chips require more complicated designs comparedwith application-specific counterparts necessitating larger and more costlydesigns. This paper attempts to propose a general-purpose field-programmablepin-constrained DMFB design with improved characteristics in termsarea-consumption, manufacturing cost and performance.","Alireza Abdoli, Sedigheh Farhadtoosky, Ali Jahanian",,,11,
On Boolean gates in fungal colony,"  A fungal colony maintains its integrity via flow of cytoplasm along myceliumnetwork. This flow, together with possible coordination of mycelium tipspropagation, is controlled by calcium waves and associated waves of electricalpotential changes. We propose that these excitation waves can be employed toimplement a computation in the mycelium networks. We use FitzHugh-Nagumo modelto imitate propagation of excitation in a single colony of Aspergillus niger.Boolean values are encoded by spikes of extracellular potential. We representbinary inputs by electrical impulses on a pair of selected electrodes and werecord responses of the colony from sixteen electrodes. We derive sets oftwo-inputs-on-output logical gates implementable the fungal colony and analysedistributions of the gates.","Andrew Adamatzky, Martin Tegelaar, Han A. B. Wosten, Anna L. Powell,
  Alexander E. Beasley, Richard Mayne",,,11,
"IMAC: In-memory multi-bit Multiplication andACcumulation in 6T SRAM
  Array","  `In-memory computing' is being widely explored as a novel computing paradigmto mitigate the well known memory bottleneck. This emerging paradigm aims atembedding some aspects of computations inside the memory array, therebyavoiding frequent and expensive movement of data between the compute unit andthe storage memory. In-memory computing with respect to Silicon memories hasbeen widely explored on various memory bit-cells. Embedding computation insidethe 6 transistor (6T) SRAM array is of special interest since it is the mostwidely used on-chip memory. In this paper, we present a novel in-memorymultiplication followed by accumulation operation capable of performingparallel dot products within 6T SRAM without any changes to the standardbitcell. We, further, study the effect of circuit non-idealities and processvariations on the accuracy of the LeNet-5 and VGG neural network architecturesagainst the MNIST and CIFAR-10 datasets, respectively. The proposed in-memorydot-product mechanism achieves 88.8% and 99% accuracy for the CIFAR-10 andMNIST, respectively. Compared to the standard von Neumann system, the proposedsystem is 6.24x better in energy consumption and 9.42x better in delay.","Mustafa Ali, Akhilesh Jaiswal, Sangamesh Kodge, Amogh Agrawal,
  Indranil Chakraborty, and Kaushik Roy",,,11,
"Control of criticality and computation in spiking neuromorphic networks
  with plasticity","  The critical state is assumed to be optimal for any computation in recurrentneural networks, because criticality maximizes a number of abstractcomputational properties. We challenge this assumption by evaluating theperformance of a spiking recurrent neural network on a set of tasks of varyingcomplexity at - and away from critical network dynamics. To that end, wedeveloped a spiking network with synaptic plasticity on a neuromorphic chip. Weshow that the distance to criticality can be easily adapted by changing theinput strength, and then demonstrate a clear relation between criticality,task-performance and information-theoretic fingerprint. Whereas theinformation-theoretic measures all show that network capacity is maximal atcriticality, this is not the case for performance on specific tasks: Only thecomplex, memory-intensive tasks profit from criticality, whereas the simpletasks suffer from it. Thereby, we challenge the general assumption thatcriticality would be beneficial for any task, and provide instead anunderstanding of how the collective network state should be tuned to taskrequirement to achieve optimal performance.","Benjamin Cramer, David St\""ockel, Markus Kreft, Michael Wibral,
  Johannes Schemmel, Karlheinz Meier, Viola Priesemann",,,11,
Adaptable and Verifiable BDI Reasoning,"  Long-term autonomy requires autonomous systems to adapt as their capabilitiesno longer perform as expected. To achieve this, a system must first be capableof detecting such changes. In this position paper, we describe a systemarchitecture for BDI autonomous agents capable of adapting to changes in adynamic environment and outline the required research. Specifically, wedescribe an agent-maintained self-model with accompanying theories of durativeactions and learning new action descriptions in BDI systems.","Peter Stringer (University of Liverpool), Rafael C. Cardoso
  (University of Liverpool), Xiaowei Huang (University of Liverpool), Louise A.
  Dennis (University of Liverpool)",,,11,
"Social Choice with Changing Preferences: Representation Theorems and
  Long-Run Policies",We study group decision making with changing preferences as a Markov DecisionProcess. We are motivated by the increasing prevalence of automateddecision-making systems when making choices for groups of people over time. Ourmain contribution is to show how classic representation theorems from socialchoice theory can be adapted to characterize optimal policies in this dynamicsetting. We provide an axiomatic characterization of MDP reward functions thatagree with the Utilitarianism social welfare functionals of social choicetheory. We also provide discussion of cases when the implementation of socialchoice-theoretic axioms may fail to lead to long-run optimal outcomes.,"Kshitij Kulkarni, Sven Neth",,,11,
"Objective Social Choice: Using Auxiliary Information to Improve Voting
  Outcomes","  How should one combine noisy information from diverse sources to make aninference about an objective ground truth? This frequently recurring, normativequestion lies at the core of statistics, machine learning, policy-making, andeveryday life. It has been called ""combining forecasts"", ""meta-analysis"",""ensembling"", and the ""MLE approach to voting"", among other names. Past studiestypically assume that noisy votes are identically and independently distributed(i.i.d.), but this assumption is often unrealistic. Instead, we assume thatvotes are independent but not necessarily identically distributed and that ourensembling algorithm has access to certain auxiliary information related to theunderlying model governing the noise in each vote. In our present work, we: (1)define our problem and argue that it reflects common and socially relevant realworld scenarios, (2) propose a multi-arm bandit noise model and count-basedauxiliary information set, (3) derive maximum likelihood aggregation rules forranked and cardinal votes under our noise model, (4) propose, alternatively, tolearn an aggregation rule using an order-invariant neural network, and (5)empirically compare our rules to common voting rules and naiveexperience-weighted modifications. We find that our rules successfully useauxiliary information to outperform the naive baselines.",Silviu Pitis and Michael R. Zhang,,,11,
Electing the Executive Branch,"  The executive branch, or government, is typically not elected directly by thepeople, but rather formed by another elected body or person such as theparliament or the president. As a result, its members are not directlyaccountable to the people, individually or as a group. We consider a scenarioin which the members of the government are elected directly by the people, andwish to achieve proportionality while doing so.  We propose a formal model consisting of $k$ offices, each with its owndisjoint set of candidates, and a set of voters who provide approval ballotsfor all offices. We wish to identify good aggregation rules that assign onecandidate to each office.  As using a simple majority vote for each office independently might result indisregarding minority preferences altogether, here we consider an adaptation ofthe greedy variant of Proportional Approval Voting (GreedyPAV) to our setting,and demonstrate -- through computer-based simulations -- how voting for alloffices together using this rule overcomes this weakness. We note that theapproach is applicable also to a party that employs direct democracy, whereparty members elect the party's representatives in a coalition government.",Ehud Shapiro and Nimrod Talmon,,,11,
"The Curse of Shared Knowledge: Recursive Belief Reasoning in a
  Coordination Game with Imperfect Information","  Common knowledge is a necessary condition for safe group coordination. Whencommon knowledge can not be obtained, humans routinely use their ability toattribute beliefs and intentions in order to infer what is known. But suchshared knowledge attributions are limited in depth and therefore prone tocoordination failures, because any finite-order knowledge attribution allowsfor an even higher order attribution that may change what is known by whom. Inthree separate experiments we investigate to which degree human participants(N=802) are able to recognize the difference between common knowledge andnth-order shared knowledge. We use a new two-person coordination game withimperfect information that is able to cast the recursive game structure andhigher-order uncertainties into a simple, everyday-like setting. Our resultsshow that participants have a very hard time accepting the fact that commonknowledge is not reducible to shared knowledge. Instead, participants try tocoordinate even at the shallowest depths of shared knowledge and in spite ofhuge payoff penalties.","Thomas Bolander, Robin Engelhardt, Thomas S. Nicolet",,,11,
"Approximation Algorithms for Distributed Multi-Robot Coverage in
  Non-Convex Environments","  In this paper, we revisit the distributed coverage control problem withmultiple robots on both metric graphs and in non-convex continuousenvironments. Traditionally, the solutions provided for this problem convergeto a locally optimal solution with no guarantees on the quality of thesolution. We consider sub-additive sensing functions, which capture thescenarios where sensing an event requires the robot to visit the eventlocation. For these sensing functions, we provide the first constant factorapproximation algorithms for the distributed coverage problem. Theapproximation results require twice the conventional communication range in theexisting coverage algorithms. However, we show through extensive simulationresults that the proposed approximation algorithms outperform several existingalgorithms in convex, non-convex continuous, and discrete environments evenwith the conventional communication ranges. Moreover, the proposed algorithmsmatch the state-of-the-art centralized algorithms in the solution quality.","Armin Sadeghi, Ahmad Bilal Asghar, Stephen L. Smith",,,11,
Graph Learning Under Partial Observability,"  Many optimization, inference and learning tasks can be accomplishedefficiently by means of decentralized processing algorithms where the networktopology (i.e., the graph) plays a critical role in enabling the interactionsamong neighboring nodes. There is a large body of literature examining theeffect of the graph structure on the performance of decentralized processingstrategies. In this article, we examine the inverse problem and consider thereverse question: How much information does observing the behavior at the nodesof a graph convey about the underlying topology? For large-scale networks, thedifficulty in addressing such inverse problems is compounded by the fact thatusually only a limited fraction of the nodes can be probed, giving rise to asecond important question: Despite the presence of unobserved nodes, canpartial observations still be sufficient to discover the graph linking theprobed nodes? The article surveys recent advances on this challenging learningproblem and related questions.","Vincenzo Matta, Augusto Santos, Ali H. Sayed",,,11,
Robust Stochastic Bayesian Games for Behavior Space Coverage,"  A key challenge in multi-agent systems is the design of intelligent agentssolving real-world tasks in close interaction with other agents (e.g. humans),thereby being confronted with a variety of behavioral variations and limitedknowledge about the true behaviors of observed agents. The practicability ofexisting works addressing this challenge is being limited due to using finitesets of hypothesis for behavior prediction, the lack of a hypothesis designprocess ensuring coverage over all behavioral variations andsample-inefficiency when modeling continuous behavioral variations. In thiswork, we present an approach to this challenge based on a new framework ofRobust Stochastic Bayesian Games (RSBGs). An RSBG defines hypothesis sets bypartitioning the physically feasible, continuous behavior space of the otheragents. It combines the optimality criteria of the Robust Markov DecisionProcess (RMDP) and the Stochastic Bayesian Game (SBG) to exponentially reducethe sample complexity for planning with hypothesis sets defined over continuousbehavior spaces. Our approach outperforms the baseline algorithms in twoexperiments modeling time-varying intents and large multidimensional behaviorspaces, while achieving the same performance as a planner with knowledge of thetrue behaviors of other agents.",Julian Bernhard and Alois Knoll,,,11,
"Consensus of second order multi-agents with actuator saturation and
  asynchronous time-delays","  This article presents the consensus of a saturated second order multi-agentsystem with non-switching dynamics that can be represented by a directed graph.The system is affected by data processing (input delay) and communicationtime-delays that are assumed to be asynchronous. The agents have saturationnonlinearities, each of them is approximated into separate linear and nonlinearelements. Nonlinear elements are represented by describing functions.Describing functions and stability of linear elements are used to estimate theexistence of limit cycles in the system with multiple control laws. Stabilityanalysis of the linear element is performed using Lyapunov-Krasovskii functionsand frequency domain analysis. A comparison of pros and cons of both theanalyses with respect to time-delay ranges, applicability and computationcomplexity is presented. Simulation and corresponding hardware implementationresults are demonstrated to support theoretical results.","Venkata Karteek Yanumula, Indrani Kar and Somanath Majhi",,,11,
"Towards a Systematic Computational Framework for Modeling Multi-Agent
  Decision-Making at Micro Level for Smart Vehicles in a Smart World","  We propose a multi-agent based computational framework for modelingdecision-making and strategic interaction at micro level for smart vehicles ina smart world. The concepts of Markov game and best response dynamics areheavily leveraged. Our aim is to make the framework conceptually sound andcomputationally practical for a range of realistic applications, includingmicro path planning for autonomous vehicles. To this end, we first convert thewould-be stochastic game problem into a closely related deterministic one byintroducing risk premium in the utility function for each individual agent. Weshow how the sub-game perfect Nash equilibrium of the simplified deterministicgame can be solved by an algorithm based on best response dynamics. In order tobetter model human driving behaviors with bounded rationality, we seek tofurther simplify the solution concept by replacing the Nash equilibriumcondition with a heuristic and adaptive optimization with finite look-aheadanticipation. In addition, the algorithm corresponding to the new solutionconcept drastically improves the computational efficiency. To demonstrate howour approach can be applied to realistic traffic settings, we conduct asimulation experiment: to derive merging and yielding behaviors on adouble-lane highway with an unexpected barrier. Despite assumption differencesinvolved in the two solution concepts, the derived numerical solutions showthat the endogenized driving behaviors are very similar. We also brieflycomment on how the proposed framework can be further extended in a number ofdirections in our forthcoming work, such as behavioral calibration using realtraffic video data, computational mechanism design for traffic policyoptimization, and so on.","Qi Dai, Xunnong Xu, Wen Guo, Suzhou Huang, Dimitar Filev",,,11,
Asynchronous Gradient-Push,"  We consider a multi-agent framework for distributed optimization where eachagent has access to a local smooth strongly convex function, and the collectivegoal is to achieve consensus on the parameters that minimize the sum of theagents' local functions. We propose an algorithm wherein each agent operatesasynchronously and independently of the other agents. When the local functionsare strongly-convex with Lipschitz-continuous gradients, we show that theiterates at each agent converge to a neighborhood of the global minimum, wherethe neighborhood size depends on the degree of asynchrony in the multi-agentnetwork. When the agents work at the same rate, convergence to the globalminimizer is achieved. Numerical experiments demonstrate that AsynchronousGradient-Push can minimize the global objective faster than state-of-the-artsynchronous first-order methods, is more robust to failing or stalling agents,and scales better with the network size.",Mahmoud Assran and Michael Rabbat,,,11,
"Putting Ridesharing to the Test: Efficient and Scalable Solutions and
  the Power of Dynamic Vehicle Relocation","  We perform a systematic evaluation of a diverse set of algorithms for theridesharing problem which is, to the best of our knowledge, one of the largestand most comprehensive to date. In particular, we evaluate 12 differentalgorithms over 12 metrics related to global efficiency, complexity, passenger,driver, and platform incentives. Our evaluation setting is specificallydesigned to resemble reality as closely as possible. We achieve this by (a)using actual data from the NYC's yellow taxi trip records, both for modelingcustomer requests, and taxis (b) following closely the pricing model employedby ridesharing platforms and (c) running our simulations to the scale of theactual problem faced by the ridesharing platforms.  Our results provide a clear-cut recommendation to ridesharing platforms onwhich solutions can be employed in practice and demonstrate the large potentialfor efficiency gains. Moreover, we show that simple, lightweight relocationschemes -- which can be used as independent components to any ridesharingalgorithm -- can significantly improve Quality of Service metrics by up to 50%.As a highlight of our findings, we identify a scalable, on-device heuristicthat offers an efficient, end-to-end solution for the Dynamic Ridesharing andFleet Relocation problem.","Panayiotis Danassis, Marija Sakota, Aris Filos-Ratsikas, Boi Faltings",,,11,
Resilient Distributed Diffusion in Networks with Adversaries,"  In this paper, we study resilient distributed diffusion for multi-taskestimation in the presence of adversaries where networked agents must estimatedistinct but correlated states of interest by processing streaming data. Weshow that in general diffusion strategies are not resilient to malicious agentsthat do not adhere to the diffusion-based information processing rules. Inparticular, by exploiting the adaptive weights used for diffusing information,we develop time-dependent attack models that drive normal agents to converge tostates selected by the attacker. We show that an attacker that has completeknowledge of the system can always drive its targeted agents to its desiredestimates. Moreover, an attacker that does not have complete knowledge of thesystem including streaming data of targeted agents or the parameters they usein diffusion algorithms, can still be successful in deploying an attack byapproximating the needed information. The attack models can be used for bothstationary and non-stationary state estimation.In addition, we present andanalyze a resilient distributed diffusion algorithm that is resilient to anydata falsification attack in which the number of compromised agents in thelocal neighborhood of a normal agent is bounded. The proposed algorithmguarantees that all normal agents converge to their true target states ifappropriate parameters are selected. We also analyze trade-off between theresilience of distributed diffusion and its performance in terms ofsteady-state mean-square-deviation (MSD) from the correct estimates. Finally,we evaluate the proposed attack models and resilient distributed diffusionalgorithm using stationary and non-stationary multi-target localization.","Jiani Li, Waseem Abbas, Xenofon Koutsoukos",,,11,
Self-Replication and Self-Assembly for Manufacturing,"  It has been argued that a central objective of nanotechnology is to makeproducts inexpensively, and that self-replication is an effective approach tovery low-cost manufacturing. The research presented here is intended to be astep towards this vision. We describe a computational simulation of nanoscalemachines floating in a virtual liquid. The machines can bond together to formstrands (chains) that self-replicate and self-assemble into user-specifiedmeshes. There are four types of machines and the sequence of machine types in astrand determines the shape of the mesh they will build. A strand may be in anunfolded state, in which the bonds are straight, or in a folded state, in whichthe bond angles depend on the types of machines. By choosing the sequence ofmachine types in a strand, the user can specify a variety of polygonal shapes.A simulation typically begins with an initial unfolded seed strand in a soup ofunbonded machines. The seed strand replicates by bonding with free machines inthe soup. The child strands fold into the encoded polygonal shape, and then thepolygons drift together and bond to form a mesh. We demonstrate that a varietyof polygonal meshes can be manufactured in the simulation, by simply changingthe sequence of machine types in the seed.","Robert Ewaschuk, Peter D. Turney",,,11,
"SMIX($\lambda$): Enhancing Centralized Value Functions for Cooperative
  Multi-Agent Reinforcement Learning","  Learning a stable and generalizable centralized value function (CVF) is acrucial but challenging task in multi-agent reinforcement learning (MARL), asit has to deal with the issue that the joint action space increasesexponentially with the number of agents in such scenarios. This paper proposesan approach, named SMIX(${\lambda}$), to address the issue using an efficientoff-policy centralized training method within a flexible learner search space.As importance sampling for such off-policy training is both computationallycostly and numerically unstable, we proposed to use the ${\lambda}$-return as aproxy to compute the TD error. With this new loss function objective, we adopta modified QMIX network structure as the base to train our model. By furtherconnecting it with the ${Q(\lambda)}$ approach from an unified expectationcorrection viewpoint, we show that the proposed SMIX(${\lambda}$) is equivalentto ${Q(\lambda)}$ and hence shares its convergence properties, while withoutbeing suffered from the aforementioned curse of dimensionality problem inherentin MARL. Experiments on the StarCraft Multi-Agent Challenge (SMAC) benchmarkdemonstrate that our approach not only outperforms several state-of-the-artMARL methods by a large margin, but also can be used as a general tool toimprove the overall performance of other CTDE-type algorithms by enhancingtheir CVFs.","Xinghu Yao, Chao Wen, Yuhui Wang and Xiaoyang Tan",,,11,
"Learning Optimal Temperature Region for Solving Mixed Integer Functional
  DCOPs","  Distributed Constraint Optimization Problems (DCOPs) are an importantframework for modeling coordinated decision-making problems in multi-agentsystems with a set of discrete variables. Later works have extended DCOPs tomodel problems with a set of continuous variables, named Functional DCOPs(F-DCOPs). In this paper, we combine both of these frameworks into the MixedInteger Functional DCOP (MIF-DCOP) framework that can deal with problemsregardless of their variables' type. We then propose a novel algorithm $-$Distributed Parallel Simulated Annealing (DPSA), where agents cooperativelylearn the optimal parameter configuration for the algorithm while also solvingthe given problem using the learned knowledge. Finally, we empirically evaluateour approach in DCOP, F-DCOP, and MIF-DCOP settings and show that DPSA producessolutions of significantly better quality than the state-of-the-art non-exactalgorithms in their corresponding settings.","Saaduddin Mahmud, Md. Mosaddek Khan, Moumita Choudhury, Long
  Tran-Thanh and Nicholas R. Jennings",,,11,
Resilient Distributed Diffusion for Multi-task Estimation,"  Distributed diffusion is a powerful algorithm for multi-task state estimationwhich enables networked agents to interact with neighbors to process input dataand diffuse information across the network. Compared to a centralized approach,diffusion offers multiple advantages that include robustness to node and linkfailures. In this paper, we consider distributed diffusion for multi-taskestimation where networked agents must estimate distinct but correlated statesof interest by processing streaming data. By exploiting the adaptive weightsused for diffusing information, we develop attack models that drive normalagents to converge to states selected by the attacker. The attack models can beused for both stationary and non-stationary state estimation. In addition, wedevelop a resilient distributed diffusion algorithm under the assumption thatthe number of compromised nodes in the neighborhood of each normal node isbounded by $F$ and we show that resilience may be obtained at the cost ofperformance degradation. Finally, we evaluate the proposed attack models andresilient distributed diffusion algorithm using stationary and non-stationarymulti-target localization.",Jiani Li and Xenofon Koutsoukos,,,11,
"Action Semantics Network: Considering the Effects of Actions in
  Multiagent Systems","  In multiagent systems (MASs), each agent makes individual decisions but allof them contribute globally to the system evolution. Learning in MASs isdifficult since each agent's selection of actions must take place in thepresence of other co-learning agents. Moreover, the environmental stochasticityand uncertainties increase exponentially with the increase in the number ofagents. Previous works borrow various multiagent coordination mechanisms intodeep learning architecture to facilitate multiagent coordination. However, noneof them explicitly consider action semantics between agents that differentactions have different influences on other agents. In this paper, we propose anovel network architecture, named Action Semantics Network (ASN), thatexplicitly represents such action semantics between agents. ASN characterizesdifferent actions' influence on other agents using neural networks based on theaction semantics between them. ASN can be easily combined with existing deepreinforcement learning (DRL) algorithms to boost their performance.Experimental results on StarCraft II micromanagement and Neural MMO show ASNsignificantly improves the performance of state-of-the-art DRL approachescompared with several network architectures.","Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing
  Hu, Yingfeng Chen, Changjie Fan, Yang Gao",,,11,
"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical
  Perspective","  Following the remarkable success of the AlphaGO series, 2019 was a boomingyear that witnessed significant advances in multi-agent reinforcement learning(MARL) techniques. MARL corresponds to the learning problem in a multi-agentsystem in which multiple agents learn simultaneously. MARL is aninterdisciplinary domain with a long history that includes game theory, machinelearning, stochastic control, psychology, and optimisation. Although MARL hasachieved considerable empirical success in solving real-world games, there is alack of a self-contained overview in the literature that elaborates the gametheoretical foundations of modern MARL methods and summarises the recentadvances. In fact, the majority of existing surveys are outdated and do notfully cover the recent developments since 2010. In this work, we provide amonograph on MARL that covers both the fundamentals and the latest developmentsin the research frontier. The goal of our monograph is to provide aself-contained assessment of the current state-of-the-art MARL techniques froma game theoretical perspective. We expect this work to serve as a steppingstone for both new researchers who are about to enter this fast-growing domainand existing domain experts who want to obtain a panoramic view and identifynew directions based on recent advances.","Yaodong Yang, Jun Wang",,,11,
Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning,"  Exploration in multi-agent reinforcement learning is a challenging problem,especially in environments with sparse rewards. We propose a general method forefficient exploration by sharing experience amongst agents. Our proposedalgorithm, called Shared Experience Actor-Critic (SEAC), applies experiencesharing in an actor-critic framework. We evaluate SEAC in a collection ofsparse-reward multi-agent environments and find that it consistentlyoutperforms two baselines and two state-of-the-art algorithms by learning infewer steps and converging to higher returns. In some harder environments,experience sharing makes the difference between learning to solve the task andnot learning at all.","Filippos Christianos, Lukas Sch\""afer, Stefano V. Albrecht",,,11,
A Linux Kernel Scheduler Extension for Multi-core Systems,"  The Linux kernel is mostly designed for multi-programed environments, buthigh-performance applications have other requirements. Such applications arerun standalone, and usually rely on runtime systems to distribute theapplication's workload on worker threads, one per core. However, due to currentOSes limitations, it is not feasible to track whether workers are actuallyrunning or blocked due to, for instance, a requested resource. For I/Ointensive applications, this leads to a significant performance degradationgiven that the core of a blocked thread becomes idle until it is able to runagain. In this paper, we present the proof-of-concept of a Linux kernelextension denoted User-Monitored Threads (UMT) which tackles this problem. Ourextension allows a user-space process to be notified of when the selectedthreads become blocked or unblocked, making it possible for a runtime toschedule additional work on the idle core. We implemented the extension on theLinux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programmingmodel to take advantage of it. The whole prototype was tested on twoapplications which, on the tested hardware and the appropriate conditions,reported speedups of almost 2x.","Aleix Roca, Samuel Rodr\'iguez, Albert Segura, Kevin Marquet,
  Vicen\c{c} Beltran",,,11,
DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks,"  Real-time scheduling and locking protocols are fundamental facilities toconstruct time-critical systems. For parallel real-time tasks, predictablelocking protocols are required when concurrent sub-jobs mutually exclusiveaccess to shared resources. This paper for the first time studies thedistributed synchronization framework of parallel real-time tasks, where bothtasks and global resources are partitioned to designated processors, andrequests to each global resource are conducted on the processor on which theresource is partitioned. We extend the Distributed Priority Ceiling Protocol(DPCP) for parallel tasks under federated scheduling, with which we proved thata request can be blocked by at most one lower-priority request. We develop taskand resource partitioning heuristics and propose analysis techniques to safelybound the task response times. Numerical evaluation (with heavy tasks on 8-,16-, and 32-core processors) indicates that the proposed methods improve theschedulability significantly compared to the state-of-the-art locking protocolsunder federated scheduling.","Maolin Yang, Zewei Chen, Xu Jiang, Nan Guan, Hang Lei",,,11,
On Failure Diagnosis of the Storage Stack,"  Diagnosing storage system failures is challenging even for professionals. Oneexample is the ""When Solid State Drives Are Not That Solid"" incident occurredat Algolia data center, where Samsung SSDs were mistakenly blamed for failurescaused by a Linux kernel bug. With the system complexity keeps increasing, suchobscure failures will likely occur more often. As one step to address thechallenge, we present our on-going efforts called X-Ray. Different fromtraditional methods that focus on either the software or the hardware, X-Rayleverages virtualization to collects events across layers, and correlates themto generate a correlation tree. Moreover, by applying simple rules, X-Ray canhighlight critical nodes automatically. Preliminary results based on 5 failurecases shows that X-Ray can effectively narrow down the search space forfailures.","Duo Zhang, Om Rameshwar Gatla, Runzhou Han, Mai Zheng",,,11,
A File System For Write-Once Media,"  A file system standard for use with write-once media such as digital compactdisks is proposed. The file system is designed to work with any operatingsystem and a variety of physical media. Although the implementation is simple,it provides a a full-featured and high-performance alternative to conventionalfile systems on traditional, multiple-write media such as magnetic disks.",Simson L. Garfinkel and J. Spencer Love,,,11,
HeRTA: Heaviside Real-Time Analysis,"  We investigate the mathematical properties of event bound functions as theyare used in the worst-case response time analysis and utilization tests. Wefigure out the differences and similarities between the two approaches. Basedon this analysis, we derive a more general form do describe events and eventbounds. This new unified approach gives clear new insights in the investigationof real-time systems, simplifies the models and will support algebraic proofsin future work. In the end, we present a unified analysis which allows thealgebraic definition of any scheduler. Introducing such functions to thereal-time scheduling theory will lead two a more systematic way to integratenew concepts and applications to the theory. Last but not least, we show howthe response time analysis in dynamic scheduling can be improved.",Frank Slomka and Mohammadreza Sadeghi,,,11,
"LINTS^RT: A Learning-driven Testbed for Intelligent Scheduling in
  Embedded Systems","  Due to the increasing complexity seen in both workloads and hardwareresources in state-of-the-art embedded systems, developing efficient real-timeschedulers and the corresponding schedulability tests becomes ratherchallenging. Although close to optimal schedulability performance can beachieved for supporting simple system models in practice, adding any smallcomplexity element into the problem context such as non-preemption or resourceheterogeneity would cause significant pessimism, which may not be eliminated byany existing scheduling technique. In this paper, we present LINTS^RT, alearning-based testbed for intelligent real-time scheduling, which has thepotential to handle various complexities seen in practice. The design ofLINTS^RT is fundamentally motivated by AlphaGo Zero for playing the board gameGo, and specifically addresses several critical challenges due to the real-timescheduling context. We first present a clean design of LINTS^RT for supportingthe basic case: scheduling sporadic workloads on a homogeneous multiprocessor,and then demonstrate how to easily extend the framework to handle furthercomplexities such as non-preemption and resource heterogeneity. Bothapplication and OS-level implementation and evaluation demonstrate thatLINTS^RT is able to achieve significantly higher runtime schedulability underdifferent settings compared to perhaps the most commonly applied schedulers,global EDF, and RM. To our knowledge, this work is the first attempt to designand implement an extensible learning-based testbed for autonomously makingreal-time scheduling decisions.","Zelun Kong, Yaswanth Yadlapalli, Soroush Bateni, Junfeng Guo, Cong Liu",,,11,
Optimal Virtual Cluster-based Multiprocessor Scheduling,"  Scheduling of constrained deadline sporadic task systems on multiprocessorplatforms is an area which has received much attention in the recent past. Itis widely believed that finding an optimal scheduler is hard, and thereforemost studies have focused on developing algorithms with good processorutilization bounds. These algorithms can be broadly classified into twocategories: partitioned scheduling in which tasks are statically assigned toindividual processors, and global scheduling in which each task is allowed toexecute on any processor in the platform. In this paper we consider a third,more general, approach called cluster-based scheduling. In this approach eachtask is statically assigned to a processor cluster, tasks in each cluster areglobally scheduled among themselves, and clusters in turn are scheduled on themultiprocessor platform. We develop techniques to support such cluster-basedscheduling algorithms, and also consider properties that minimize totalprocessor utilization of individual clusters. In the last part of this paper,we develop new virtual cluster-based scheduling algorithms. For implicitdeadline sporadic task systems, we develop an optimal scheduling algorithm thatis neither Pfair nor ERfair. We also show that the processor utilization boundof US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neitherpartitioned nor global strategies dominate over the other, cluster-basedscheduling is a natural direction for research towards achieving improvedprocessor utilization bounds.","Arvind Easwaran, Insik Shin, Insup Lee",,,11,
Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,"  Mixed-criticality real-time scheduling has been developed to improve resourceutilization while guaranteeing safe execution of critical applications. Thesestudies use optimistic resource reservation for all the applications to improveutilization, but prioritize critical applications when the reservations becomeinsufficient at runtime. Many of them however share an impractical assumptionthat all the critical applications will simultaneously demand additionalresources. As a consequence, they under-utilize resources by penalizing all thelow-criticality applications. In this paper we overcome this shortcoming usinga novel mechanism that comprises a parameter to model the expected number ofcritical applications simultaneously demanding more resources, and an executionstrategy based on the parameter to improve resource utilization. Since mostmixed-criticality systems in practice are component-based, we design ourmechanism such that the component boundaries provide the isolation necessary tosupport the execution of low-criticality applications, and at the same timeprotect the critical ones. We also develop schedulability tests for theproposed mechanism under both a flat as well as a hierarchical schedulingframework. Finally, through simulations, we compare the performance of theproposed approach with existing studies in terms of schedulability and thecapability to support low-criticality applications.","Xiaozhe Gu, Arvind Easwaran, Kieu-My Phan, Insik Shin",,,11,
"MigrOS: Transparent Operating Systems Live Migration Support for
  Containerised RDMA-applications","  Major data centre providers are introducing RDMA-based networks for theirtenants, as well as for operating the underlying infrastructure. In comparisonto traditional socket-based network stacks, RDMA-based networks offer higherthroughput, lower latency and reduced CPU overhead. However, transparentcheckpoint and migration operations become much more difficult. The key reasonis that the OS is removed from the critical path of communication. As a result,some of the communication state itself resides in the NIC hardware and is nomore under the direct control of the OS. This control includes especially thesupport for virtualisation of communication which is needed for live migrationof communication partners. In this paper, we propose the basic principlesrequired to implement a migration-capable RDMA-based network. We recommend somechanges at the software level and small changes at the hardware level. As aproof of concept, we integrate the proposed changes into SoftRoCE, anopen-source kernel-level implementation of the RoCE protocol. We claim thatthese changes introduce no runtime overhead when migration does not happen.Finally, we develop a proof-of-concept implementation for migratingcontainerised applications that use RDMA-based networks.","Maksym Planeta, Jan Bierbaum, Leo Sahaya Daphne Antony, Torsten
  Hoefler, Hermann H\""artig",,,11,
"PIMOD: A Tool for Configuring Single-Board Computer Operating System
  Images","  Computer systems used in the field of humanitarian technology are often basedon general-purpose single-board computers, such as Raspberry Pis. While thesesystems offer great flexibility for developers and users, configuration anddeployment either introduces overhead by executing scripts on multiple devicesor requires deeper technical understanding when building operating systemimages for such small computers from scratch. In this paper, we present PIMOD,a software tool for configuring operating system images for single-boardcomputer systems. We propose a simple yet comprehensive configuration language.In a configuration profile, called Pifile, a small set of commands is used todescribe the configuration of an operating system image. Virtualizationtechniques are used during the execution of the profile in order to bedistribution and platform independent. Commands can be issued in the guestoperating system, providing access to the distribution specific tools, e.g., toconfigure hardware parameters. The implementation of PIMOD is made public undera free and open source license. PIMOD is evaluated in terms of user benefits,performance compared to on-system configuration, and applicability acrossdifferent hardware platforms and operating systems.","Jonas H\""ochst, Alvar Penning, Patrick Lampe, Bernd Freisleben",,,11,
"Period Adaptation for Continuous Security Monitoring in Multicore
  Real-Time Systems","  We propose a design-time framework (named HYDRA-C) for integrating securitytasks into partitioned real-time systems (RTS) running on multicore platforms.Our goal is to opportunistically execute security monitoring mechanisms in a'continuous' manner -- i.e., as often as possible, across cores, to ensure thatsecurity tasks run with as few interruptions as possible. Our framework willallow designers to integrate security mechanisms without perturbing existingreal-time (RT) task properties or execution order. We demonstrate the frameworkusing a proof-of-concept implementation with intrusion detection mechanisms assecurity tasks. We develop and use both, (a) a custom intrusion detectionsystem (IDS), as well as (b) Tripwire -- an open source data integrity checkingtool. These are implemented on a realistic rover platform designed using an ARMmulticore chip. We compare the performance of HYDRA-C with a state-of-the-artRT security integration approach for multicore-based RTS and find that ourmethod can, on average, detect intrusions 19.05% faster without impacting theperformance of RT tasks.","Monowar Hasan, Sibin Mohan, Rodolfo Pellizzoni, Rakesh B. Bobba",,,11,
"SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main
  Memory","  Several emerging technologies for byte-addressable non-volatile memory (NVM)have been considered to replace DRAM as the main memory in computer systemsduring the last years. The disadvantage of a lower write endurance, compared toDRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM(FeRAM) has been addressed in the literature. As a solution, in-memorywear-leveling techniques have been proposed, which aim to balance thewear-level over all memory cells to achieve an increased memory lifetime.Generally, to apply such advanced aging-aware wear-leveling techniques proposedin the literature, additional special hardware is introduced into the memorysystem to provide the necessary information about the cell age and thus enableaging-aware wear-leveling decisions.  This paper proposes software-only aging-aware wear-leveling based on commonCPU features and does not rely on any additional hardware support from thememory subsystem. Specifically, we exploit the memory management unit (MMU),performance counters, and interrupts to approximate the memory write counts asan aging indicator. Although the software-only approach may lead to slightlyworse wear-leveling, it is applicable on commonly available hardware. Weachieve page-level coarse-grained wear-leveling by approximating the currentcell age through statistical sampling and performing physical memory remappingthrough the MMU. This method results in non-uniform memory usage patternswithin a memory page. Hence, we further propose a fine-grained wear-leveling inthe stack region of C / C++ compiled software.  By applying both wear-leveling techniques, we achieve up to $78.43\%$ of theideal memory lifetime, which is a lifetime improvement of more than a factor of$900$ compared to the lifetime without any wear-leveling.","Christian Hakert, Kuan-Hsun Chen, Paul R. Genssler, Georg von der
  Br\""uggen, Lars Bauer, Hussam Amrouch, Jian-Jia Chen, J\""org Henkel",,,11,
"Look Mum, no VM Exits! (Almost)","  Multi-core CPUs are a standard component in many modern embedded systems.Their virtualisation extensions enable the isolation of services, and gainpopularity to implement mixed-criticality or otherwise split systems. Wepresent Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that usesnovel architectural approaches to combine Linux, a powerful general-purposesystem, with strictly isolated special-purpose components. Our design goalsfavour simplicity over features, establish a minimal code base, and minimisehypervisor activity.  Direct assignment of hardware to guests, together with a deferredinitialisation scheme, offloads any complex hardware handling and bootstrappingissues from the hypervisor to the general purpose OS. The hypervisorestablishes isolated domains that directly access physical resources withoutthe need for emulation or paravirtualisation. This retains, with negligiblesystem overhead, Linux's feature-richness in uncritical parts, while frugalsafety and real-time critical workloads execute in isolated, safe domains.","Ralf Ramsauer, Jan Kiszka, Daniel Lohmann, Wolfgang Mauerer",,,11,
Secure Memory Management on Modern Hardware,"  Almost all modern hardware, from phone SoCs to high-end servers withaccelerators, contain memory translation and protection hardware like IOMMUs,firewalls, and lookup tables which make it impossible to reason about, andenforce protection and isolation based solely on the processor's MMUs. This hasled to numerous bugs and security vulnerabilities in today's system software.  In this paper we regain the ability to reason about and enforce accesscontrol using the proven concept of a reference monitor mediating accesses tomemory resources. We present a fine-grained, realistic memory protection modelthat makes this traditional concept applicable today, and bring system softwarein line with the complexity of modern, heterogeneous hardware.  Our design is applicable to any operating system, regardless of architecture.We show that it not only enforces the integrity properties of a system, butdoes so with no inherent performance overhead and it is even amenable toautomation through code generation from trusted hardware specifications.","Reto Achermann, Nora Hossle, Lukas Humbel, Daniel Schwyn, David Cock,
  Timothy Roscoe",,,11,
FastDrain: Removing Page Victimization Overheads in NVMe Storage Stack,"  Host-side page victimizations can easily overflow the SSD internal buffer,which interferes I/O services of diverse user applications thereby degradinguser-level experiences. To address this, we propose FastDrain, a co-design ofOS kernel and flash firmware to avoid the buffer overflow, caused by pagevictimizations. Specifically, FastDrain can detect a triggering point where anear-future page victimization introduces an overflow of the SSD internalbuffer. Our new flash firmware then speculatively scrubs the buffer space toaccommodate the requests caused by the page victimization. In parallel, our newOS kernel design controls the traffic of page victimizations by considering thetarget device buffer status, which can further reduce the risk of bufferoverflow. To secure more buffer spaces, we also design a latency-aware FTL,which dumps the dirty data only to the fast flash pages. Our evaluation resultsreveal that FastDrain reduces the 99th response time of user applications by84%, compared to a conventional system.","Jie Zhang, Miryeong Kwon, Sanghyun Han, Nam Sung Kim, Mahmut Kandemir,
  Myoungsoo Jung",,,11,
"$\mu$Tiles: Efficient Intra-Process Privilege Enforcement of Memory
  Regions","  With the alarming rate of security advisories and privacy concerns onconnected devices, there is an urgent need for strong isolation guarantees inresource-constrained devices that demand very lightweight solutions. However,the status quo is that Unix-like operating systems do not offer privilegeseparation inside a process. Lack of practical fine-grainedcompartmentalization inside a shared address space leads to private dataleakage through applications' untrusted dependencies and compromised threads.To this end, we propose $\mu$Tiles, a lightweight kernel abstraction and set ofsecurity primitives based on mutual distrust for intra-process privilegeseparation, memory protection, and secure multithreading. $\mu$Tiles takesadvantage of hardware support for virtual memory tagging (e.g., ARM memorydomains) to achieve significant performance gain while eliminating varioushardware limitations. Our results (based on OpenSSL, the Apache HTTP server,and LevelDB) show that $\mu$Tiles is extremely lightweight (adds $\approx 10KB$to kernel image) for IoT use cases. It adds negligible runtime overhead($\approx 0.5\%-3.5\%$) and is easy to integrate with existing applications forproviding strong privilege separation.","Zahra Tarkhani, Anil Madhavapeddy",,,11,
High Velocity Kernel File Systems with Bento,"  High development velocity is critical for modern systems. This is especiallytrue for Linux file systems which are seeing increased pressure from newstorage devices and new demands on storage systems. However, high velocityLinux kernel development is challenging due to the ease of introducing bugs,the difficulty of testing and debugging, and the lack of support forredeployment without service disruption. Existing approaches to high-velocitydevelopment of file systems for Linux have major downsides, such as the highperformance penalty for FUSE file systems, slowing the deployment cycle for newfile system functionality.  We propose Bento, a framework for high velocity development of Linux kernelfile systems. It enables file systems written in safe Rust to be installed inthe Linux kernel, with errors largely sandboxed to the file system. Bento filesystems can be replaced with no disruption to running applications, allowingdaily or weekly upgrades in a cloud server setting. Bento also supportsuserspace debugging. We implement a simple file system using Bento and showthat it performs similarly to VFS-native ext4 on a variety of benchmarks andoutperforms a FUSE version by 10x-60x on Filebench. We also show that we candynamically add file provenance tracking to a running kernel file system withonly 10ms of service interruption.","Samantha Miller (1), Kaiyuan Zhang (1), Mengqi Chen (1), Ryan Jennings
  (1), Ang Chen (2), Danyang Zhuo (3), Tom Anderson (1) ((1) University of
  Washington, (2) Rice University, (3) Duke University)",,,11,
"Scheduling of Real-Time Tasks with Multiple Critical Sections in
  Multiprocessor Systems","  The performance of multiprocessor synchronization and locking protocols is akey factor to utilize the computation power of multiprocessor systems underreal-time constraints. While multiple protocols have been developed in the pastdecades, their performance highly depends on the task partition andprioritization. The recently proposed Dependency Graph Approach showed itsadvantages and attracted a lot of interest. It is, however, restricted to tasksets where each task has at most one critical section. In this paper, we removethis restriction and demonstrate how to utilize algorithms for the classicaljob shop scheduling problem to construct a dependency graph for tasks withmultiple critical sections. To show the applicability, we discuss theimplementation in Litmus^{RT} and report the overheads. Moreover, we provideextensive numerical evaluations under different configurations, which in manysituations show significant improvement compared to the state-of-the-art.","Jian-Jia Chen, Junjie Shi, Georg von der Br\""uggen, Niklas Ueter",,,11,
DEAP Cache: Deep Eviction Admission and Prefetching for Cache,"  Recent approaches for learning policies to improve caching, target just oneout of the prefetching, admission and eviction processes. In contrast, wepropose an end to end pipeline to learn all three policies using machinelearning. We also take inspiration from the success of pretraining on largecorpora to learn specialized embeddings for the task. We model prefetching as asequence prediction task based on past misses. Following previous workssuggesting that frequency and recency are the two orthogonal fundamentalattributes for caching, we use an online reinforcement learning technique tolearn the optimal policy distribution between two orthogonal evictionstrategies based on them. While previous approaches used the past as anindicator of the future, we instead explicitly model the future frequency andrecency in a multi-task fashion with prefetching, leveraging the abilities ofdeep networks to capture futuristic trends and use them for learning evictionand admission. We also model the distribution of the data in an online fashionusing Kernel Density Estimation in our approach, to deal with the problem ofcaching non-stationary data. We present our approach as a ""proof of concept"" oflearning all three components of cache strategies using machine learning andleave improving practical deployment for future work.","Ayush Mangal, Jitesh Jain, Keerat Kaur Guliani, Omkar Bhalerao",,,11,
Study of Firecracker MicroVM,"  Firecracker is a virtualization technology that makes use of Kernel VirtualMachine (KVM). Firecracker belongs to a new virtualization class named themicro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweightMicroVMs in non-virtualized environments in a fraction of a second, at the sametime offering the security and workload isolation provided by traditional VMsand also the resource efficiency that comes along with containers \cite{b1}.Firecracker aims to provide a slimmed-down MicroVM, comprised of approximately50K lines of code in Rust and with a reduced attack surface for guest VMs. Thisreport will examine the internals of Firecracker and understand why Firecrackeris the next big thing going forward in virtualization and cloud computing.",Madhur Jain,,,11,
"Offsite Autotuning Approach -- Performance Model Driven Autotuning
  Applied to Parallel Explicit ODE Methods","  Autotuning techniques are a promising approach to minimize the otherwisetedious manual effort of optimizing scientific applications for a specifictarget platform. Ideally, an autotuning approach is capable of reliablyidentifying the most efficient implementation variant(s) for a new targetsystem or new characteristics of the input by applying suitable programtransformations and analytic models. In this work, we introduce Offsite, anoffline autotuning approach which automates this selection process atinstallation time by rating implementation variants based on an analyticperformance model without requiring time-consuming runtime experiments. Fromabstract multilevel YAML description languages, Offsite automatically derivesoptimized, platform-specific and problem-specific code of possibleimplementation variants and applies the performance model to theseimplementation variants.  We apply Offsite to parallel numerical methods for ordinary differentialequations (ODEs). In particular, we investigate tuning a specific class ofexplicit ODE solvers (PIRK methods) for various initial value problems (IVPs)on shared-memory systems. Our experiments demonstrate that Offsite is able toreliably identify a set of the most efficient implementation variants for giventest configurations (ODE solver, IVP, platform) and is capable of effectivelyhandling important autotuning scenarios.","Johannes Seiferth, Matthias Korch, and Thomas Rauber",,,11,
"The MAP/M/s+G Call Center Model with General Patience Times: Stationary
  Solutions and First Passage Times","  We study the MAP/M/s+G queuing model with MAP (Markovian Arrival Process)arrivals, exponentially distributed service times, infinite waiting room, andgenerally distributed patience times. Using sample-path arguments, we proposeto obtain the steady-state distribution of the virtual waiting time andsubsequently the other relevant performance metrics of interest for theMAP/M/s+G queue by means of finding the steady-state solution of a properlyconstructed Continuous Feedback Fluid Queue (CFFQ). The proposed method isexact when the patience time is a discrete random variable and isasymptotically exact when it is continuous/hybrid for which case discretizationof the patience time distribution and subsequently the steady-state solution ofa Multi-Regime Markov Fluid Queue (MRMFQ) is required. Besides the steady-statedistribution, we also propose a new method to approximately obtain the firstpassage time distribution for the virtual and actual waiting times in the$MAP/M/s+G$ queue. Again, using sample-path arguments, finding the desireddistribution is also shown to reduce to obtaining the steady-state solution ofa larger dimensionality CFFQ where the deterministic time horizon is to beapproximated by Erlang or Concentrated Matrix Exponential (CME) distributions.Numerical results are presented to validate the effectiveness of the proposedmethod.",Omer Gursoy and Kamal Adli Mehr and Nail Akar,,,11,
"Analysis of an M/G/1 system for the optimization of the RTG performances
  in the delivery of containers in Abidjan Terminal","  In front of the major challenges to increase its productivity whilesatisfying its customer, it is today important to establish in advance theoperational performances of the RTG Abidjan Terminal. In this article, by usingan M/G/1 retrial queue system, we obtained the average number of parkeddelivery trucks and as well as their waiting time. Finally, we used Matlab torepresent them graphically then analyze the RTG performances according to thetraffic rate.","Bakary Kone, Salimata Gueye Diagne, Dethie Dione, Coumba Diallo",,,11,
"HPC AI500: The Methodology, Tools, Roofline Performance Models, and
  Metrics for Benchmarking HPC AI Systems","  The recent years witness a trend of applying large-scale distributed deeplearning in both business and scientific computing areas, whose goal is tospeed up the training time to achieve a state-of-the-art quality. The HPCcommunity feels a great interest in building the HPC AI systems that arededicated to running those workloads. The HPC AI benchmarks accelerate theprocess. Unfortunately, benchmarking HPC AI systems at scale raises seriouschallenges. None of previous HPC AI benchmarks achieve the goal of beingequivalent, relevant, representative, affordable, and repeatable. This paperpresents a comprehensive methodology, tools, Roofline performance models, andinnovative metrics for benchmarking, optimizing, and ranking HPC AI systems,which we call HPC AI500 V2.0. We abstract the HPC AI system into nineindependent layers, and present explicit benchmarking rules and procedures toassure equivalence of each layer, repeatability, and replicability. On thebasis of AIBench -- by far the most comprehensive AI benchmarks suite, wepresent and build two HPC AI benchmarks from both business and scientificcomputing: Image Classification, and Extreme Weather Analytics, achieving bothrepresentativeness and affordability. To rank the performance andenergy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPSper watt, which impose a penalty on failing to achieve the target quality. Wepropose using convolution and GEMM -- the two most intensively-used kernelfunctions to measure the upper bound performance of the HPC AI systems, andpresent HPC AI roofline models for guiding performance optimizations. Theevaluations show our methodology, benchmarks, performance models, and metricscan measure, optimize, and rank the HPC AI systems in a scalable, simple, andaffordable way. HPC AI500 V2.0 are publicly available fromhttp://www.benchcouncil.org/benchhub/hpc-ai500-benchmark.","Zihan Jiang, Lei Wang, Xingwang Xiong, Wanling Gao, Chunjie Luo, Fei
  Tang, Chuanxin Lan, Hongxiao Li, and Jianfeng Zhan",,,11,
"A Prompt Report on the Performance of Intel Optane DC Persistent Memory
  Module","  In this prompt report, we present the basic performance evaluation of IntelOptane Data Center Persistent Memory Module (Optane DCPMM), which is the firstcommercially-available, byte-addressable non-volatile memory modules releasedin April 2019. Since at the moment of writing only a few reports on itsperformance were published, this letter is intended to complement otherperformance studies. Through experiments using our own measurement tools, weobtained that the latency of random read-only access was approximately 374 ns.That of random writeback-involving access was 391 ns. The bandwidths ofread-only and writeback-involving access for interleaved memory modules wereapproximately 38 GB/s and 3 GB/s, respectively.",Takahiro Hirofuchi and Ryousei Takano,,,11,
"Resource Allocation in One-dimensional Distributed Service Networks with
  Applications","  We consider assignment policies that allocate resources to users, where bothresources and users are located on a one-dimensional line. First, we considerunidirectional assignment policies that allocate resources only to userslocated to their left. We propose the Move to Right (MTR) policy, which scansfrom left to right assigning nearest rightmost available resource to a user,and contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. Whileboth policies among all unidirectional policies, minimize the expected distancetraveled by a request (request distance), MTR is fairer. Moreover, we show thatwhen user and resource locations are modeled by statistical point processes,and resources are allowed to satisfy more than one user, the spatial systemunder unidirectional policies can be mapped into bulk service queueing systems,thus allowing the application of many queueing theory results that yield closedform expressions. As we consider a case where different resources can satisfydifferent numbers of users, we also generate new results for bulk servicequeues. We also consider bidirectional policies where there are no directionalrestrictions on resource allocation and develop an algorithm for computing theoptimal assignment which is more efficient than known algorithms in theliterature when there are more resources than users. Numerical evaluation ofperformance of unidirectional and bidirectional allocation schemes yieldsdesign guidelines beneficial for resource placement. \np{Finally, we present aheuristic algorithm, which leverages the optimal dynamic programming scheme forone-dimensional inputs to obtain approximate solutions to the optimalassignment problem for the two-dimensional scenario and empirically yieldsrequest distances within a constant factor of the optimal solution.","Nitish K. Panigrahy, Prithwish Basu, Philippe Nain, Don Towsley,
  Ananthram Swami, Kevin S. Chan and Kin K. Leung",,,11,
"Delay and Price Differentiation in Cloud Computing: A Service Model,
  Supporting Architectures, and Performance","  Many cloud service providers (CSPs) provide on-demand service at a price witha small delay. We propose a QoS-differentiated model where multiple SLAsdeliver both on-demand service for latency-critical users and delayed servicesfor delay-tolerant users at lower prices. Two architectures are considered tofulfill SLAs. The first is based on priority queues. The second simplyseparates servers into multiple modules, each for one SLA. As an ecosystem, weshow that the proposed framework is dominant-strategy incentive compatible.Although the first architecture appears more prevalent in the literature, weprove the superiority of the second architecture, under which we furtherleverage queueing theory to determine the optimal SLA delays and prices.Finally, the viability of the proposed framework is validated through numericalcomparison with the on-demand service and it exhibits a revenue improvement inexcess of 200%. Our results can help CSPs design optimal delay-differentiatedservices and choose appropriate serving architectures.","Xiaohu Wu, Francesco De Pellegrini and Giuliano Casale",,,11,
Investigating Applications on the A64FX,"  The A64FX processor from Fujitsu, being designed for computational simulationand machine learning applications, has the potential for unprecedentedperformance in HPC systems. In this paper, we evaluate the A64FX bybenchmarking against a range of production HPC platforms that cover a number ofprocessor technologies. We investigate the performance of complex scientificapplications across multiple nodes, as well as single node and mini-kernelbenchmarks. This paper finds that the performance of the A64FX processor acrossour chosen benchmarks often significantly exceeds other platforms, even withoutspecific application optimisations for the processor instruction set orhardware. However, this is not true for all the benchmarks we have undertaken.Furthermore, the specific configuration of applications can have an impact onthe runtime and performance experienced.","Adrian Jackson, Mich\`ele Weiland, Nick Brown, Andrew Turner, Mark
  Parsons",,,11,
"Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for
  Emerging Storage Models","  With data durability, high access speed, low power efficiency and byteaddressability, NVMe and SSD, which are acknowledged representatives ofemerging storage technologies, have been applied broadly in many areas.However, one key issue with high-performance adoption of these technologies ishow to properly define intelligent cache layers such that the performance gapbetween emerging technologies and main memory can be well bridged. To this end,we propose Phoebe, a reuse-aware reinforcement learning framework for theoptimal online caching that is applicable for a wide range of emerging storagemodels. By continuous interacting with the cache environment and the datastream, Phoebe is capable to extract critical temporal data dependency andrelative positional information from a single trace, becoming ever smarter overtime. To reduce training overhead during online learning, we utilize periodicaltraining to amortize costs. Phoebe is evaluated on a set of Microsoft cloudstorage workloads. Experiment results show that Phoebe is able to close the gapof cache miss rate from LRU and a state-of-the-art online learning based cachepolicy to the Belady's optimal policy by 70.3% and 52.6%, respectively.","Nan Wu, Pengcheng Li",,,11,
Resource Allocation in One-dimensional Distributed Service Networks,"  We consider assignment policies that allocate resources to users, where bothresources and users are located on a one-dimensional line. First, we considerunidirectional assignment policies that allocate resources only to userslocated to their left. We propose the Move to Right (MTR) policy, which scansfrom left to right assigning nearest rightmost available resource to a user,and contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. Whileboth these policies are optimal among all unidirectional policies, we show thatthey are equivalent with respect to the expected distance traveled by a request(request distance), although MTR is fairer. Moreover, we show that when userand resource locations are modeled by statistical point processes, andresources are allowed to satisfy more than one user, the spatial system underunidirectional policies can be mapped into bulk service queuing systems, thusallowing the application of a plethora of queuing theory results that yieldclosed form expressions. As we consider a case where different resources cansatisfy different numbers of users, we also generate new results for bulkservice queues. We also consider bidirectional policies where there are nodirectional restrictions on resource allocation and develop an algorithm forcomputing the optimal assignment which is more efficient than known algorithmsin the literature when there are more resources than users. Finally, numericalevaluation of performance of unidirectional and bidirectional allocationschemes yields design guidelines beneficial for resource placement.","Nitish K. Panigrahy, Prithwish Basu, Philippe Nain, Don Towsley,
  Ananthram Swami, Kevin S. Chan and Kin K. Leung",,,11,
Flattening the Curve: Insights From Queueing Theory,"  The worldwide outbreak of the coronavirus was first identified in 2019 inWuhan, China. Since then, the disease has spread worldwide. As it currentlyspreading in the United States, policy makers, public health officials andcitizens are racing to understand the impact of this virus on the United Stateshealthcare system. They fear that the rapid influx of patients will overwhelmthe healthcare system leading to unnecessary fatalities. Most countries andstates in America have introduced mitigation strategies, such as socialdistancing, to decrease the rate of newly infected people, i.e. flattening thecurve.In this paper, we analyze the time evolution of the number of peoplehospitalized due to the coronavirus using the methods of queueing theory. Giventhat the rate of new infections varies over time as the pandemic evolves, wemodel the number of coronavirus patients as a dynamical system based on thetheory of infinite server queues with non-stationary Poisson arrival rates.With this model we are able to quantify how flattening the curve affects thepeak demand for hospital resources. This allows us to characterize howaggressively society must flatten the curve in order to avoid overwhelming thecapacity of healthcare system. We also demonstrate how flattening the curveimpacts the elapsed time between the peak rate of hospitalizations and the timeof the peak demand for the hospital resources. Finally, we present empiricalevidence from China, South Korea, Italy and the United States that supports theinsights from the model.","Sergio Palomo and Jamol Pender and William Massey and Robert C.
  Hampshire",,,11,
"A review of analytical performance modeling and its role in computer
  engineering and science","  This article is a review of analytical performance modeling for computersystems. It discusses the motivation for this area of research, examines keyissues, introduces some ideas, illustrates how it is applied, and points out arole that it can play in developing Computer Science.",Y.C. Tay,,,11,
A Fast Analytical Model of Fully Associative Caches,"  While the cost of computation is an easy to understand local property, thecost of data movement on cached architectures depends on global state, does notcompose, and is hard to predict. As a result, programmers often fail toconsider the cost of data movement. Existing cache models and simulatorsprovide the missing information but are computationally expensive. We present alightweight cache model for fully associative caches with least recently used(LRU) replacement policy that gives fast and accurate results. We count thecache misses without explicit enumeration of all memory accesses by usingsymbolic counting techniques twice: 1) to derive the stack distance for eachmemory access and 2) to count the memory accesses with stack distance largerthan the cache size. While this technique seems infeasible in theory, due tonon-linearities after the first round of counting, we show that the countingproblems are sufficiently linear in practice. Our cache model often computesthe results within seconds and contrary to simulation the execution time ismostly problem size independent. Our evaluation measures modeling errors below0.6% on real hardware. By providing accurate data placement information weenable memory hierarchy aware software development.","Tobias Gysi, Tobias Grosser, Laurin Brandner, and Torsten Hoefler",,,11,
"The Multi-Source Preemptive M/PH/1/1 Queue with Packet Errors: Exact
  Distribution of the Age of Information and Its Peak","  Age of Information (AoI) and Peak AoI (PAoI) and their analytical models haverecently drawn substantial amount of attention in information theory andwireless communications disciplines, in the context of qualitative assessmentof information freshness in status update systems. We take a queueing-theoreticapproach and study a probabilistically preemptive bufferless $M/PH/1/1$queueing system with arrivals stemming from $N$ separate information sources,with the aim of modeling a generic status update system. In this model, a newinformation packet arrival from source $m$ is allowed to preempt a packet fromsource $n$ in service, with a probability depending on $n$ and $m$. To make themodel even more general than the existing ones, for each of the informationsources, we assume a distinct PH-type service time distribution and a distinctpacket error probability. Subsequently, we obtain the exact distributions ofthe AoI and PAoI for each of the information sources using matrix-analyticalalgorithms and in particular the theory of Markov fluid queues and sample patharguments. This is in contrast with existing methods that rely on StochasticHybrid Systems (SHS) which obtain only the average values and in less generalsettings. Numerical examples are provided to validate the proposed approach aswell as to give engineering insight on the impact of preemption probabilitieson certain AoI and PAoI performance figures.",Ozancan Dogan and Nail Akar,,,11,
"Performance Analysis of Modified SRPT in Multiple-Processor Multitask
  Scheduling","  In this paper we study the multiple-processor multitask scheduling problem inboth deterministic and stochastic models. We consider and analyze ModifiedShortest Remaining Processing Time (M-SRPT) scheduling algorithm, a simplemodification of SRPT, which always schedules jobs according to SRPT wheneverpossible, while processes tasks in an arbitrary order. The M-SRPT algorithm isproved to achieve a competitive ratio of $\Theta(\log \alpha +\beta)$ forminimizing response time, where $\alpha$ denotes the ratio between maximum jobworkload and minimum job workload, $\beta$ represents the ratio between maximumnon-preemptive task workload and minimum job workload. In addition, thecompetitive ratio achieved is shown to be optimal (up to a constant factor),when there are constant number of machines. We further consider the problemunder Poisson arrival and general workload distribution (\ie, $M/GI/N$ system),and show that M-SRPT achieves asymptotic optimal mean response time when thetraffic intensity $\rho$ approaches $1$, if job size distribution has finitesupport. Beyond bounded job workload, the asymptotic optimality of M-SRPT alsoholds for unbounded job size distributions with certain probabilisticassumptions, for example, $M/M/N$ system with upper bounded task workload. Anbyproduct of our analysis is a tight characterization of the heavy trafficbehavior of work-conserving algorithms in single-task job scheduling. We provethat the average response time in $GI/GI/1$ scales with $1/(1-\rho)$, if thejob size distribution has finite support, which generalizes the growth rate in[Lin, Wierman and Zwart, 2011] to general arrival processes and allwork-conserving algorithms.",Wenxin Li,,,11,
Automatically Harnessing Sparse Acceleration,"  Sparse linear algebra is central to many scientific programs, yet compilersfail to optimize it well. High-performance libraries are available, butadoption costs are significant. Moreover, libraries tie programs intovendor-specific software and hardware ecosystems, creating non-portable code.  In this paper, we develop a new approach based on our specification Languagefor implementers of Linear Algebra Computations (LiLAC). Rather than requiringthe application developer to (re)write every program for a given library, theburden is shifted to a one-off description by the library implementer. TheLiLAC-enabled compiler uses this to insert appropriate library routines withoutsource code changes.  LiLAC provides automatic data marshaling, maintaining state between calls andminimizing data transfers. Appropriate places for library insertion aredetected in compiler intermediate representation, independent of sourcelanguages.  We evaluated on large-scale scientific applications written in FORTRAN;standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Acrossheterogeneous platforms, applications and data sets we show speedups of1.1$\times$ to over 10$\times$ without user intervention.","Philip Ginsbach, Bruce Collie, Michael F.P. O'Boyle",,,11,
"On the Asymptotic Optimality of Work-Conserving Disciplines in
  Completion Time Minimization","  In this paper, we prove that under mild stochastic assumptions,work-conserving disciplines are asymptotic optimal for minimizing totalcompletion time. As a byproduct of our analysis, we obtain tight upper bound onthe competitive ratios of work-conserving disciplines on minimizing the metricof flow time.","Wenxin Li, Ness Shroff",,,11,
"nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86
  Systems","  We present nanoBench, a tool for evaluating small microbenchmarks usinghardware performance counters on Intel and AMD x86 systems. Most existing toolsand libraries are intended to either benchmark entire programs, or programsegments in the context of their execution within a larger program. Incontrast, nanoBench is specifically designed to evaluate small, isolated piecesof code. Such code is common in microbenchmark-based hardware analysistechniques.  Unlike previous tools, nanoBench can execute microbenchmarks directly inkernel space. This allows to benchmark privileged instructions, and it enablesmore accurate measurements. The reading of the performance counters isimplemented with minimal overhead avoiding functions calls and branches. As aconsequence, nanoBench is precise enough to measure individual memory accesses.  We illustrate the utility of nanoBench at the hand of two case studies.First, we briefly discuss how nanoBench has been used to determine the latency,throughput, and port usage of more than 13,000 instruction variants on recentx86 processors. Second, we show how to generate microbenchmarks to preciselycharacterize the cache architectures of eleven Intel Core microarchitectures.This includes the most comprehensive analysis of the employed cache replacementpolicies to date.",Andreas Abel and Jan Reineke,,,11,
Scheduling in the Presence of Data Intensive Compute Jobs,"  We study the performance of non-adaptive scheduling policies in computingsystems with multiple servers. Compute jobs are mostly regular, with modestservice requirements. However, there are sporadic data intensive jobs, whoseexpected service time is much higher than that of the regular jobs. Forthismodel, we are interested in the effect of scheduling policieson the averagetime a job spends in the system. To this end, we introduce two performanceindicators in a simplified, only-arrival system. We believe that theseperformance indicators are good predictors of the relative performance of thepolicies in the queuing system, which is supported by simulations results.",Amir Behrouzi-Far and Emina Soljanin,,,11,
"Analytic Performance Modeling and Analysis of Detailed Neuron
  Simulations","  Big science initiatives are trying to reconstruct and model the brain byattempting to simulate brain tissue at larger scales and with increasingly morebiological detail than previously thought possible. The exponential growth ofparallel computer performance has been supporting these developments, and atthe same time maintainers of neuroscientific simulation code have strived tooptimally and efficiently exploit new hardware features. Current state of theart software for the simulation of biological networks has so far beendeveloped using performance engineering practices, but a thorough analysis andmodeling of the computational and performance characteristics, especially inthe case of morphologically detailed neuron simulations, is lacking. Othercomputational sciences have successfully used analytic performance engineeringand modeling methods to gain insight on the computational properties ofsimulation kernels, aid developers in performance optimizations and eventuallydrive co-design efforts, but to our knowledge a model-based performanceanalysis of neuron simulations has not yet been conducted.  We present a detailed study of the shared-memory performance ofmorphologically detailed neuron simulations based on the Execution-Cache-Memory(ECM) performance model. We demonstrate that this model can deliver accuratepredictions of the runtime of almost all the kernels that constitute the neuronmodels under investigation. The gained insight is used to identify the maingoverning mechanisms underlying performance bottlenecks in the simulation. Theimplications of this analysis on the optimization of neural simulation softwareand eventually co-design of future hardware architectures are discussed. Inthis sense, our work represents a valuable conceptual and quantitativecontribution to understanding the performance properties of biological networkssimulations.","Francesco Cremonesi, Georg Hager, Gerhard Wellein, Felix Sch\""urmann",,,11,
