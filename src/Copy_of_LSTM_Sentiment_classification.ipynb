{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of LSTM_Sentiment_classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uA4W5SPv9_S"
      },
      "source": [
        "# import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRhoAaCmv9_U"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "#import category_encoders as ce\n",
        "import re\n",
        "#import joblib\n",
        "\n",
        "#from cx_Oracle import makedsn\n",
        "#from cx_Oracle import connect\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, classification_report\n",
        "sns.set_style(\"darkgrid\")\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as op\n",
        "\n",
        "\n",
        "import time\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVtoAi4KwPzM"
      },
      "source": [
        "\n",
        "def get_default_device():\n",
        "  '''Pick GPU if available, else CPU'''\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULuAONTdw7Xk"
      },
      "source": [
        "device = get_default_device()\n",
        "\n",
        "def to_device(data, device):\n",
        "  '''Move tensor(s) to chosen device'''\n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(x, device) for x in data]\n",
        "  return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "  '''Wrap a dataloader to move data to a device'''\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl=dl\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    '''Yield a batch of data after moving it to device'''\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    '''Number of batches'''\n",
        "\n",
        "    return len(self.dl)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50DRPv05v9_Y"
      },
      "source": [
        "# import os\n",
        "\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJip8p1v9_g"
      },
      "source": [
        "# get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFDv77zn30hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d531a7af-c9e3-4b67-91b6-f857f1a0ac05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L283R4Mq4jJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2297737-ba37-4a3c-f172-e1a9851dc332"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/full_test.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/Colab Notebooks/full_test.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va3QC64Yv9_j"
      },
      "source": [
        "with open('/content/drive/My Drive/Colab Notebooks/full_train.txt', 'r') as f:\n",
        "    reviews_train = f.read()\n",
        "    \n",
        "with open('/content/drive/My Drive/Colab Notebooks/full_test.txt', 'r') as f:\n",
        "    reviews_test = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ffpZ0vv9_s"
      },
      "source": [
        "# data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kjtIvZ1v9_w"
      },
      "source": [
        "reviews_train = reviews_train.lower()\n",
        "reviews_test = reviews_test.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0HrqTfuv9_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937f7a43-5fc4-4f4e-8d27-97cc338bfa88"
      },
      "source": [
        "from string import punctuation\n",
        "print(punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4NDCUi5v9_1"
      },
      "source": [
        "\n",
        "reviews_train_clean = ''.join([c for c in reviews_train if c not in punctuation])\n",
        "reviews_test_clean = ''.join([c for c in reviews_test if c not in punctuation])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2jIwtmv9_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5a2d75-b3a2-45a3-90be-2a713808bace"
      },
      "source": [
        "reviews_train_split = reviews_train_clean.split('\\n')\n",
        "print ('Number of train reviews :', len(reviews_train_split))\n",
        "\n",
        "reviews_test_split = reviews_test_clean.split('\\n')\n",
        "print ('Number of test reviews :', len(reviews_test_split))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews : 25001\n",
            "Number of test reviews : 25001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRiFEkoBv9_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c875362-f7ad-4f96-a8d6-fa18b2e2ac00"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "train_text = ' '.join(reviews_train_split)\n",
        "test_text = ' '.join(reviews_test_split)\n",
        "# create a list of words\n",
        "words_train = train_text.split()\n",
        "words_test = test_text.split()\n",
        "words = words_train + words_test\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "print(total_words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11512912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDnWfeF9v9_8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKixB6LTv9_-"
      },
      "source": [
        "def load_word2vec():\n",
        "    \"\"\" Load Word2Vec Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 3 million embeddings, each lengh 300\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
        "    vocab = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Loaded vocab size %i\" % len(vocab))\n",
        "    return wv_from_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl4bcbG7v-AA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce33435f-db08-4441-fef1-8660ca6e6d91"
      },
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_word2vec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=====================================-------------] 74.2% 1234.5/1662.8MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-cOdmg5v-AC"
      },
      "source": [
        "with open('/content/drive/My Drive/Colab Notebooks/google_news_word_2_vec', 'wb') as f:\n",
        "    pickle.dump(wv_from_bin, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWQTYD31v-AE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8efffea8-e1fd-4049-ce8e-6b3b0b875b2e"
      },
      "source": [
        "embed_size = len(wv_from_bin.word_vec('the'))\n",
        "embeddings_matrix = [np.zeros((1, embed_size))]\n",
        "#word_2_int = {'<pad>':0}\n",
        "\n",
        "\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "vocab_to_int['<pad>'] = 0\n",
        "list_unknown_words = []\n",
        "\n",
        "#pdb.set_trace()\n",
        "\n",
        "for word in vocab_to_int.keys():\n",
        "    #word_2_int[word] = len(word_2_int)\n",
        "    if word in wv_from_bin.vocab.keys():\n",
        "        \n",
        "        #embeddings_matrix = np.concatenate((embeddings_matrix, wv_from_bin.word_vec(word).reshape(1,-1)), axis = 0)\n",
        "        embeddings_matrix.append(wv_from_bin.word_vec(word).reshape(1,-1))\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        list_unknown_words.append(word)\n",
        "        #embeddings_matrix = np.concatenate((embeddings_matrix, (np.random.rand(1, embed_size) - 0.5) / embed_size), axis = 0)\n",
        "        embeddings_matrix.append((np.random.rand(1, embed_size) - 0.5) / embed_size)\n",
        "print(len(list_unknown_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANbwqeiqv-AI"
      },
      "source": [
        "embeddings_matrix = np.asarray(embeddings_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjASZ9jgv-AK"
      },
      "source": [
        "embeddings_matrix = np.squeeze(embeddings_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zac0G-1av-AN"
      },
      "source": [
        "reviews_train_int = []\n",
        "for review in reviews_train_split:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_train_int.append(r)\n",
        "    \n",
        "reviews_test_int = []\n",
        "for review in reviews_test_split:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_test_int.append(r)\n",
        "    \n",
        "#print (reviews_train_int[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHoA4EUMv-AQ"
      },
      "source": [
        "reviews_train_len = [len(x) for x in reviews_train_int]\n",
        "reviews_test_len = [len(x) for x in reviews_test_int]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rFODoQ2v-AU"
      },
      "source": [
        "encoded_train_labels = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "encoded_test_labels = [1 if i < 12500 else 0 for i in range(25000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUSkdJQDv-AW"
      },
      "source": [
        "reviews_train_int = [ reviews_train_int[i] for i, l in enumerate(reviews_train_len) if l>0 ]\n",
        "encoded_train_labels = [encoded_train_labels[i] for i, l in enumerate(reviews_train_len) if l> 0 ]\n",
        "\n",
        "reviews_test_int = [ reviews_test_int[i] for i, l in enumerate(reviews_test_len) if l>0 ]\n",
        "encoded_test_labels = [encoded_test_labels[i] for i, l in enumerate(reviews_test_len) if l> 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8qk3n9v-AX"
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            zeroes = list(np.zeros(seq_length-review_len))\n",
        "            new = zeroes+review\n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUSE21Pyv-AZ"
      },
      "source": [
        "features_train = pad_features(reviews_train_int, 200)\n",
        "features_test = pad_features(reviews_test_int, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfXyMrAsv-Ae"
      },
      "source": [
        "X_train,X_test, y_train, y_test = train_test_split(\n",
        "    np.asarray(features_train), np.asarray(encoded_train_labels), test_size=0.25, random_state=123)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87KVQbtdv-A-"
      },
      "source": [
        "# data visualization of outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd46Yefov-A_"
      },
      "source": [
        "# set up embedding class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGygvG4pv-A_"
      },
      "source": [
        "class ModelEmbeddings(nn.Module): \n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab, pad_token_idx, pre_trained_matrix):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        self.source = None\n",
        "        #self.target = None\n",
        "\n",
        "        #src_pad_token_idx = vocab.src['<pad>']\n",
        "        #tgt_pad_token_idx = vocab.tgt['<pad>']\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO - Initialize the following variables:\n",
        "        ###     self.source (Embedding Layer for source language)\n",
        "        ###     self.target (Embedding Layer for target langauge)\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###     1. `vocab` object contains two vocabularies:\n",
        "        ###            `vocab.src` for source\n",
        "        ###            `vocab.tgt` for target\n",
        "        ###     2. You can get the length of a specific vocabulary by running:\n",
        "        ###             `len(vocab.<specific_vocabulary>)`\n",
        "        ###     3. Remember to include the padding token for the specific vocabulary\n",
        "        ###        when creating your Embedding.\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     Embedding Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "        \n",
        "        '''\n",
        "        self.source = nn.Embedding(len(vocab), self.embed_size, padding_idx=pad_token_idx)\n",
        "        embeds = torch.from_numpy(pre_trained_matrix)\n",
        "        self.source.weight = embeds\n",
        "        '''\n",
        "        \n",
        "        pre_trained_tensor = torch.from_numpy(pre_trained_matrix).to(torch.float64)\n",
        "        self.source = nn.Embedding(pre_trained_tensor.size(0), pre_trained_tensor.size(1), padding_idx=pad_token_idx)\n",
        "        self.source.weight = nn.Parameter(pre_trained_tensor)\n",
        "        \n",
        "        #weight = torch.from_numpy(pre_trained_matrix).to(torch.float64)\n",
        "        #self.source = nn.Embedding.from_pretrained(weight)\n",
        "        \n",
        "        ### END YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQ4RG-pv-BB"
      },
      "source": [
        "# set up Neural Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268rdb1Wv-BB"
      },
      "source": [
        "# Here we define our model as a class\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, hidden_dim, batch_size, vocab, pad_token_idx, pre_trained_matrix, num_layers=1,\n",
        "                 output_dim=1, dropout_rate=0.2):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.model_embeddings = ModelEmbeddings(embed_size, vocab, pad_token_idx, pre_trained_matrix)\n",
        "        \n",
        "        #define dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate, inplace = False)\n",
        "        # Define the LSTM layer\n",
        "        \n",
        "        \n",
        "        self.lstm_layer = nn.LSTM(input_size=embed_size, hidden_size=hidden_dim, dropout=dropout_rate, bidirectional=True)\n",
        "        \n",
        "        # define projection layers\n",
        "        self.att_proj_layer = nn.Linear(in_features=hidden_dim*2, out_features=hidden_dim, bias=False)\n",
        "        self.h_projection = nn.Linear(in_features=hidden_dim*2, out_features=hidden_dim, bias=False)\n",
        "        \n",
        "        \n",
        "        self.combined_output_projection = nn.Linear(in_features=hidden_dim*3, out_features=output_dim)\n",
        "        # Define the output layer\n",
        "        #self.linear_layer = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=False)\n",
        "        \n",
        "        '''\n",
        "        def init_hidden(self):\n",
        "            # This is what we'll initialise our hidden state as\n",
        "            return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                    torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "        '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    def generate_sent_masks(self, enc_hiddens, source_lengths):\n",
        "        \"\"\" Generate sentence masks for encoder hidden states.\n",
        "\n",
        "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "                                     src_len = max source length, h = hidden size. \n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
        "\n",
        "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
        "                                    where src_len = max source length, h = hidden size.\n",
        "        \"\"\"\n",
        "        #pdb.set_trace()\n",
        "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float).to(device)\n",
        "        for e_id, src_len in enumerate(source_lengths):\n",
        "            enc_masks[e_id, src_len:] = 1\n",
        "        return enc_masks\n",
        "    \n",
        "    def forward(self, source_padded, source_lengths):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        '''\n",
        "        pad_embs = F.embedding(source_padded, self.model_embeddings.source.weight)\n",
        "        pack_embs = nn.utils.rnn.pack_padded_sequence(pad_embs, source_lengths, batch_first=False)\n",
        "        enc_hiddens_pack, (last_hidden, last_cell) = self.lstm(pack_embs)\n",
        "        enc_hiddens_pad = \n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        ''' \n",
        "        #pdb.set_trace()\n",
        "        pad_embs = F.embedding(source_padded, self.model_embeddings.source.weight)\n",
        "        pack_embs = nn.utils.rnn.pack_padded_sequence(pad_embs, source_lengths, batch_first=False, enforce_sorted=False)\n",
        "        \n",
        "        enc_hiddens_pack, (last_hidden, last_cell) = self.lstm_layer(pack_embs.float())\n",
        "        \n",
        "        h_cat = torch.cat((last_hidden[0], last_hidden[1]), 1)\n",
        "        \n",
        "        init_h = self.h_projection(h_cat)\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        enc_hiddens_pad = nn.utils.rnn.pad_packed_sequence(enc_hiddens_pack, batch_first=True, total_length=source_padded.size(0))\n",
        "        enc_hiddens = enc_hiddens_pad[0]\n",
        "        #pdb.set_trace()\n",
        "        #create mask of paddings\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        \n",
        "        #apply attention proj\n",
        "        enc_hiddens_proj = self.att_proj_layer(enc_hiddens)\n",
        "        \n",
        "        #appy attention\n",
        "        #multi_e_t = torch.bmm(enc_hiddens_proj, last_hidden.permute(1,2,0), out=None)\n",
        "        multi_e_t = torch.bmm(enc_hiddens_proj, init_h.unsqueeze(2), out=None)\n",
        "        e_t = multi_e_t.squeeze()\n",
        "        \n",
        "        #set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n",
        "        \n",
        "        m = nn.Softmax(dim=1)\n",
        "        alpha_t = m(e_t)\n",
        "        a_t = torch.bmm(enc_hiddens.permute(0, 2, 1), alpha_t.unsqueeze(2)).squeeze()\n",
        "        #U_t = torch.cat((a_t, last_hidden.squeeze()), 1)\n",
        "        U_t = torch.cat((a_t, init_h.squeeze()), 1)\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "        O_t = self.dropout(torch.tanh(V_t))\n",
        "        \n",
        "        \n",
        "        #use the hidden forward and backward states from the last time step\n",
        "        #for the future could implement attention instead\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        #h_cat = torch.cat((last_hidden[0], last_hidden[1]), 1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #h_d = self.dropout(h_cat)\n",
        "        \n",
        "#         h_d = self.dropout(last_hidden)\n",
        "\n",
        "#         y_pred = self.linear_layer(h_d)\n",
        "        \n",
        "#         y_pred = torch.squeeze(y_pred)\n",
        "        #pdb.set_trace()\n",
        "        #m = nn.Softmax(dim=1)\n",
        "        #O_t = m(O_t)\n",
        "        return O_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u7mk_MPv-BC"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCT6TBvFv-BD"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "def create_minibatches(X, y, batch_size): \n",
        "    #pdb.set_trace()\n",
        "    mini_batches = [] \n",
        "    data = np.hstack((X, y)) \n",
        "    np.random.shuffle(data) \n",
        "    n_minibatches = data.shape[0] // batch_size \n",
        "    i = 0\n",
        "    #pdb.set_trace()\n",
        "    for i in range(n_minibatches + 1): \n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    if data.shape[0] % batch_size != 0: \n",
        "        mini_batch = data[i * batch_size:data.shape[0]] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    #pdb.set_trace()\n",
        "    return mini_batches \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(startTime, model, x, y, X_test, y_test, output_path, data_set, batch_size=32, n_epochs=10, lr=0.0001):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    #loss_func = torch.nn.BCELoss()\n",
        "    loss_train_vec = []\n",
        "    loss_test_vec = []\n",
        "    \n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch+1,n_epochs))\n",
        "        result = train_for_epoch(model, x, y, optimizer, loss_func, data_set, batch_size, epoch)\n",
        "        \n",
        "        if epoch == 0:\n",
        "            best_result = result\n",
        "        \n",
        "        if result <= best_result:\n",
        "            best_result = result\n",
        "            print(\"New best result! Saving model.\")\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "         \n",
        "            \n",
        "        # model.eval()\n",
        "        \n",
        "        # src_len_test = []\n",
        "        # for failures in X_test:\n",
        "        #     src_len_test.append(sum(failures == data_set[\"<pad>\"]))\n",
        "        # src_len_test = np.asarray([src_len_test], dtype=int)\n",
        "        # src_len_test = X_test.shape[1] - src_len_test\n",
        "        # src_len_test = torch.from_numpy(src_len_test).to(torch.int64)\n",
        "        # src_len_test = torch.squeeze(src_len_test)\n",
        "\n",
        "        # test_x = torch.from_numpy(X_test).to(int)\n",
        "        # test_x = torch.t(test_x)\n",
        "        \n",
        "        # test_y = torch.squeeze(torch.from_numpy(y_test).long())\n",
        "        # print(80* \"=\")\n",
        "        # print(\"getting output from the model\")\n",
        "        # final_out = model(test_x.to(device), src_len_test.to(device))\n",
        "        # print(80* \"=\")\n",
        "        # loss = torch.nn.CrossEntropyLoss()\n",
        "        # test_loss = loss(final_out.to(device), test_y.to(device))\n",
        "        # print(\"test loss:{}\".format(test_loss))\n",
        "        \n",
        "        # print(\"time elapsed %d seconds\" % (time.time() - startTime))\n",
        "        # loss_test_vec.append(test_loss.cpu().data.numpy())\n",
        "        # loss_train_vec.append(result)\n",
        "    \n",
        "    return loss_train_vec, loss_test_vec\n",
        "        \n",
        "\n",
        "def train_for_epoch(model, x, y, optimizer, loss_func, data_set, batch_size, epoch):\n",
        "    model.train()\n",
        "    n_minibatches = math.floor(len(x) / batch_size)\n",
        "    #pdb.set_trace()\n",
        "    loss_meter = AverageMeter()\n",
        "    #accuracy_meter = AverageMeter()\n",
        "    batch = 1\n",
        "    print_every = 10\n",
        "    num_examples = 0\n",
        "    for row in create_minibatches(x, y, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        \n",
        "        #train_x = torch.from_numpy(row[0]).float()\n",
        "        train_x_np = row[0]\n",
        "        train_y = torch.squeeze(torch.from_numpy(row[1]).long())\n",
        "        #train_y = torch.from_numpy(row[1]).long()\n",
        "        #train_y = torch.from_numpy(row[1]).long()\n",
        "        \n",
        "        #source_lengths = []\n",
        "        \n",
        "        #for failures in train_x_np:\n",
        "            #source_lengths.append(sum(failures == -2))\n",
        "            \n",
        "        source_lengths = [sum(failures == data_set[\"<pad>\"]) for failures in train_x_np]\n",
        "        source_lengths = np.asarray([source_lengths], dtype=int)\n",
        "        source_lengths = train_x_np.shape[1] - source_lengths\n",
        "        source_lengths = torch.from_numpy(source_lengths).to(torch.int64)\n",
        "        source_lengths = torch.squeeze(source_lengths)\n",
        "        \n",
        "        \n",
        "        train_x = torch.from_numpy(row[0]).to(int)\n",
        "        train_x = torch.t(train_x)\n",
        "\n",
        "        #pdb.set_trace()\n",
        "        #output = model(DeviceDataLoader(train_x, device), DeviceDataLoader(source_lengths, device))\n",
        "        output = model(train_x.to(device), source_lengths.to(device))\n",
        "        #pdb.set_trace()\n",
        "        #loss = loss_func(DeviceDataLoader(output, device), DeviceDataLoader(train_y, device))\n",
        "        loss = loss_func(output.to(device), train_y.to(device))\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_meter.update(loss.item())\n",
        "        #print(batch)\n",
        "        num_examples+=train_x.size(1)\n",
        "        if batch % print_every == 0:\n",
        "#             print(\"Epoch:\",epoch+1, \", iter:\", batch, \", loss:\", loss.detach().numpy(), \", cum_examples:\",\n",
        "#                   num_examples,',', round((batch/n_minibatches)*100), '% Completed')\n",
        "            print(\"Epoch:{:d}, iter:{:d}, loss:{:.5f}, cum_examples:{:d}, {:.2f}% Completed \".format(epoch+1, batch,loss.cpu().detach().numpy(),num_examples,\n",
        "                                               (batch/n_minibatches)*100))\n",
        "#             print(\"Epoch:{}, iter:{}, loss:{}, cum_examples:{}, {}% Completed \".format(epoch+1, batch,loss.detach().numpy,num_examples,\n",
        "#                                            (batch/n_minibatches)*100))\n",
        "        batch += 1\n",
        "        \n",
        "       \n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    #print (\"Average Accuracy Loss: {}\".format(accuracy_meter.avg))\n",
        "    \n",
        "    return loss_meter.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DELJUsEcv-BH"
      },
      "source": [
        "# run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngEzA9lev-BH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76da36ec-2c6c-41cd-927b-b66f2462f557"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model = NeuralNet(embed_size=embed_size, hidden_dim=200, batch_size=64, vocab=vocab_to_int,\n",
        "                  pad_token_idx=vocab_to_int[\"<pad>\"], pre_trained_matrix=embeddings_matrix, num_layers=1,\n",
        "                  output_dim=2, dropout_rate=0.2)\n",
        "\n",
        "to_device(model, device)\n",
        "\n",
        "\n",
        "output_dir = \"/content/drive/My Drive/Colab Notebooks/results/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "startTime=time.time()\n",
        "\n",
        "train_loss, test_loss = train(startTime, model, X_train, y_train, X_test, y_test, output_path, vocab_to_int, model.batch_size\n",
        "                              ,n_epochs=10, lr=0.0001)\n",
        "\n",
        "print(\"training took %d seconds\" % (time.time() - startTime))\n",
        "\n",
        "# print(80* \"=\")\n",
        "# print(\"TESTING\")\n",
        "# print(80* \"=\")\n",
        "# print(\"Restoring the best model wieghts found on the set\")\n",
        "# model.load_state_dict(torch.load(output_path))\n",
        "# print(\"Final evation on the test set: \")\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# final_out = model(torch.from_numpy(X_test).float())\n",
        "# final_test = torch.from_numpy(y_test).float()\n",
        "# loss = torch.nn.CrossEntropyLoss()\n",
        "# final_loss = loss(final_out, final_test)\n",
        "# print('Cross Entropy Loss: ',final_loss.detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 out of 10\n",
            "Epoch:1, iter:10, loss:0.70546, cum_examples:640, 3.42% Completed \n",
            "Epoch:1, iter:20, loss:0.69566, cum_examples:1280, 6.85% Completed \n",
            "Epoch:1, iter:30, loss:0.69318, cum_examples:1920, 10.27% Completed \n",
            "Epoch:1, iter:40, loss:0.69012, cum_examples:2560, 13.70% Completed \n",
            "Epoch:1, iter:50, loss:0.69279, cum_examples:3200, 17.12% Completed \n",
            "Epoch:1, iter:60, loss:0.68840, cum_examples:3840, 20.55% Completed \n",
            "Epoch:1, iter:70, loss:0.68919, cum_examples:4480, 23.97% Completed \n",
            "Epoch:1, iter:80, loss:0.68800, cum_examples:5120, 27.40% Completed \n",
            "Epoch:1, iter:90, loss:0.68934, cum_examples:5760, 30.82% Completed \n",
            "Epoch:1, iter:100, loss:0.68819, cum_examples:6400, 34.25% Completed \n",
            "Epoch:1, iter:110, loss:0.68469, cum_examples:7040, 37.67% Completed \n",
            "Epoch:1, iter:120, loss:0.68112, cum_examples:7680, 41.10% Completed \n",
            "Epoch:1, iter:130, loss:0.66920, cum_examples:8320, 44.52% Completed \n",
            "Epoch:1, iter:140, loss:0.65155, cum_examples:8960, 47.95% Completed \n",
            "Epoch:1, iter:150, loss:0.67127, cum_examples:9600, 51.37% Completed \n",
            "Epoch:1, iter:160, loss:0.64103, cum_examples:10240, 54.79% Completed \n",
            "Epoch:1, iter:170, loss:0.64087, cum_examples:10880, 58.22% Completed \n",
            "Epoch:1, iter:180, loss:0.53319, cum_examples:11520, 61.64% Completed \n",
            "Epoch:1, iter:190, loss:0.58898, cum_examples:12160, 65.07% Completed \n",
            "Epoch:1, iter:200, loss:0.57872, cum_examples:12800, 68.49% Completed \n",
            "Epoch:1, iter:210, loss:0.51917, cum_examples:13440, 71.92% Completed \n",
            "Epoch:1, iter:220, loss:0.54008, cum_examples:14080, 75.34% Completed \n",
            "Epoch:1, iter:230, loss:0.56851, cum_examples:14720, 78.77% Completed \n",
            "Epoch:1, iter:240, loss:0.51001, cum_examples:15360, 82.19% Completed \n",
            "Epoch:1, iter:250, loss:0.59725, cum_examples:16000, 85.62% Completed \n",
            "Epoch:1, iter:260, loss:0.49293, cum_examples:16640, 89.04% Completed \n",
            "Epoch:1, iter:270, loss:0.49403, cum_examples:17280, 92.47% Completed \n",
            "Epoch:1, iter:280, loss:0.50386, cum_examples:17920, 95.89% Completed \n",
            "Epoch:1, iter:290, loss:0.51831, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.625983500663115\n",
            "New best result! Saving model.\n",
            "Epoch 2 out of 10\n",
            "Epoch:2, iter:10, loss:0.51442, cum_examples:640, 3.42% Completed \n",
            "Epoch:2, iter:20, loss:0.59559, cum_examples:1280, 6.85% Completed \n",
            "Epoch:2, iter:30, loss:0.47470, cum_examples:1920, 10.27% Completed \n",
            "Epoch:2, iter:40, loss:0.48906, cum_examples:2560, 13.70% Completed \n",
            "Epoch:2, iter:50, loss:0.40093, cum_examples:3200, 17.12% Completed \n",
            "Epoch:2, iter:60, loss:0.64272, cum_examples:3840, 20.55% Completed \n",
            "Epoch:2, iter:70, loss:0.51411, cum_examples:4480, 23.97% Completed \n",
            "Epoch:2, iter:80, loss:0.43174, cum_examples:5120, 27.40% Completed \n",
            "Epoch:2, iter:90, loss:0.57328, cum_examples:5760, 30.82% Completed \n",
            "Epoch:2, iter:100, loss:0.53509, cum_examples:6400, 34.25% Completed \n",
            "Epoch:2, iter:110, loss:0.52980, cum_examples:7040, 37.67% Completed \n",
            "Epoch:2, iter:120, loss:0.45945, cum_examples:7680, 41.10% Completed \n",
            "Epoch:2, iter:130, loss:0.59471, cum_examples:8320, 44.52% Completed \n",
            "Epoch:2, iter:140, loss:0.46200, cum_examples:8960, 47.95% Completed \n",
            "Epoch:2, iter:150, loss:0.54061, cum_examples:9600, 51.37% Completed \n",
            "Epoch:2, iter:160, loss:0.66093, cum_examples:10240, 54.79% Completed \n",
            "Epoch:2, iter:170, loss:0.42678, cum_examples:10880, 58.22% Completed \n",
            "Epoch:2, iter:180, loss:0.43221, cum_examples:11520, 61.64% Completed \n",
            "Epoch:2, iter:190, loss:0.39410, cum_examples:12160, 65.07% Completed \n",
            "Epoch:2, iter:200, loss:0.51852, cum_examples:12800, 68.49% Completed \n",
            "Epoch:2, iter:210, loss:0.41080, cum_examples:13440, 71.92% Completed \n",
            "Epoch:2, iter:220, loss:0.51838, cum_examples:14080, 75.34% Completed \n",
            "Epoch:2, iter:230, loss:0.58553, cum_examples:14720, 78.77% Completed \n",
            "Epoch:2, iter:240, loss:0.53047, cum_examples:15360, 82.19% Completed \n",
            "Epoch:2, iter:250, loss:0.51018, cum_examples:16000, 85.62% Completed \n",
            "Epoch:2, iter:260, loss:0.53773, cum_examples:16640, 89.04% Completed \n",
            "Epoch:2, iter:270, loss:0.59230, cum_examples:17280, 92.47% Completed \n",
            "Epoch:2, iter:280, loss:0.35855, cum_examples:17920, 95.89% Completed \n",
            "Epoch:2, iter:290, loss:0.40410, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.48389438260980205\n",
            "New best result! Saving model.\n",
            "Epoch 3 out of 10\n",
            "Epoch:3, iter:10, loss:0.46687, cum_examples:640, 3.42% Completed \n",
            "Epoch:3, iter:20, loss:0.50113, cum_examples:1280, 6.85% Completed \n",
            "Epoch:3, iter:30, loss:0.50505, cum_examples:1920, 10.27% Completed \n",
            "Epoch:3, iter:40, loss:0.37317, cum_examples:2560, 13.70% Completed \n",
            "Epoch:3, iter:50, loss:0.40542, cum_examples:3200, 17.12% Completed \n",
            "Epoch:3, iter:60, loss:0.54773, cum_examples:3840, 20.55% Completed \n",
            "Epoch:3, iter:70, loss:0.50641, cum_examples:4480, 23.97% Completed \n",
            "Epoch:3, iter:80, loss:0.44932, cum_examples:5120, 27.40% Completed \n",
            "Epoch:3, iter:90, loss:0.49067, cum_examples:5760, 30.82% Completed \n",
            "Epoch:3, iter:100, loss:0.40466, cum_examples:6400, 34.25% Completed \n",
            "Epoch:3, iter:110, loss:0.42936, cum_examples:7040, 37.67% Completed \n",
            "Epoch:3, iter:120, loss:0.42367, cum_examples:7680, 41.10% Completed \n",
            "Epoch:3, iter:130, loss:0.48123, cum_examples:8320, 44.52% Completed \n",
            "Epoch:3, iter:140, loss:0.35880, cum_examples:8960, 47.95% Completed \n",
            "Epoch:3, iter:150, loss:0.37615, cum_examples:9600, 51.37% Completed \n",
            "Epoch:3, iter:160, loss:0.57849, cum_examples:10240, 54.79% Completed \n",
            "Epoch:3, iter:170, loss:0.30793, cum_examples:10880, 58.22% Completed \n",
            "Epoch:3, iter:180, loss:0.32516, cum_examples:11520, 61.64% Completed \n",
            "Epoch:3, iter:190, loss:0.47428, cum_examples:12160, 65.07% Completed \n",
            "Epoch:3, iter:200, loss:0.41761, cum_examples:12800, 68.49% Completed \n",
            "Epoch:3, iter:210, loss:0.48587, cum_examples:13440, 71.92% Completed \n",
            "Epoch:3, iter:220, loss:0.43880, cum_examples:14080, 75.34% Completed \n",
            "Epoch:3, iter:230, loss:0.41095, cum_examples:14720, 78.77% Completed \n",
            "Epoch:3, iter:240, loss:0.40861, cum_examples:15360, 82.19% Completed \n",
            "Epoch:3, iter:250, loss:0.40240, cum_examples:16000, 85.62% Completed \n",
            "Epoch:3, iter:260, loss:0.39416, cum_examples:16640, 89.04% Completed \n",
            "Epoch:3, iter:270, loss:0.50367, cum_examples:17280, 92.47% Completed \n",
            "Epoch:3, iter:280, loss:0.48157, cum_examples:17920, 95.89% Completed \n",
            "Epoch:3, iter:290, loss:0.38201, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.4395229872189412\n",
            "New best result! Saving model.\n",
            "Epoch 4 out of 10\n",
            "Epoch:4, iter:10, loss:0.36088, cum_examples:640, 3.42% Completed \n",
            "Epoch:4, iter:20, loss:0.52585, cum_examples:1280, 6.85% Completed \n",
            "Epoch:4, iter:30, loss:0.35978, cum_examples:1920, 10.27% Completed \n",
            "Epoch:4, iter:40, loss:0.36210, cum_examples:2560, 13.70% Completed \n",
            "Epoch:4, iter:50, loss:0.41240, cum_examples:3200, 17.12% Completed \n",
            "Epoch:4, iter:60, loss:0.38574, cum_examples:3840, 20.55% Completed \n",
            "Epoch:4, iter:70, loss:0.31033, cum_examples:4480, 23.97% Completed \n",
            "Epoch:4, iter:80, loss:0.38725, cum_examples:5120, 27.40% Completed \n",
            "Epoch:4, iter:90, loss:0.39968, cum_examples:5760, 30.82% Completed \n",
            "Epoch:4, iter:100, loss:0.35728, cum_examples:6400, 34.25% Completed \n",
            "Epoch:4, iter:110, loss:0.42858, cum_examples:7040, 37.67% Completed \n",
            "Epoch:4, iter:120, loss:0.47453, cum_examples:7680, 41.10% Completed \n",
            "Epoch:4, iter:130, loss:0.49151, cum_examples:8320, 44.52% Completed \n",
            "Epoch:4, iter:140, loss:0.31740, cum_examples:8960, 47.95% Completed \n",
            "Epoch:4, iter:150, loss:0.38821, cum_examples:9600, 51.37% Completed \n",
            "Epoch:4, iter:160, loss:0.40559, cum_examples:10240, 54.79% Completed \n",
            "Epoch:4, iter:170, loss:0.43371, cum_examples:10880, 58.22% Completed \n",
            "Epoch:4, iter:180, loss:0.43890, cum_examples:11520, 61.64% Completed \n",
            "Epoch:4, iter:190, loss:0.32731, cum_examples:12160, 65.07% Completed \n",
            "Epoch:4, iter:200, loss:0.44687, cum_examples:12800, 68.49% Completed \n",
            "Epoch:4, iter:210, loss:0.38222, cum_examples:13440, 71.92% Completed \n",
            "Epoch:4, iter:220, loss:0.44883, cum_examples:14080, 75.34% Completed \n",
            "Epoch:4, iter:230, loss:0.46562, cum_examples:14720, 78.77% Completed \n",
            "Epoch:4, iter:240, loss:0.33822, cum_examples:15360, 82.19% Completed \n",
            "Epoch:4, iter:250, loss:0.44499, cum_examples:16000, 85.62% Completed \n",
            "Epoch:4, iter:260, loss:0.34730, cum_examples:16640, 89.04% Completed \n",
            "Epoch:4, iter:270, loss:0.34825, cum_examples:17280, 92.47% Completed \n",
            "Epoch:4, iter:280, loss:0.42252, cum_examples:17920, 95.89% Completed \n",
            "Epoch:4, iter:290, loss:0.38798, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.39860708555396723\n",
            "New best result! Saving model.\n",
            "Epoch 5 out of 10\n",
            "Epoch:5, iter:10, loss:0.42283, cum_examples:640, 3.42% Completed \n",
            "Epoch:5, iter:20, loss:0.34098, cum_examples:1280, 6.85% Completed \n",
            "Epoch:5, iter:30, loss:0.36225, cum_examples:1920, 10.27% Completed \n",
            "Epoch:5, iter:40, loss:0.39710, cum_examples:2560, 13.70% Completed \n",
            "Epoch:5, iter:50, loss:0.24506, cum_examples:3200, 17.12% Completed \n",
            "Epoch:5, iter:60, loss:0.33296, cum_examples:3840, 20.55% Completed \n",
            "Epoch:5, iter:70, loss:0.46908, cum_examples:4480, 23.97% Completed \n",
            "Epoch:5, iter:80, loss:0.32758, cum_examples:5120, 27.40% Completed \n",
            "Epoch:5, iter:90, loss:0.33542, cum_examples:5760, 30.82% Completed \n",
            "Epoch:5, iter:100, loss:0.37320, cum_examples:6400, 34.25% Completed \n",
            "Epoch:5, iter:110, loss:0.36624, cum_examples:7040, 37.67% Completed \n",
            "Epoch:5, iter:120, loss:0.33916, cum_examples:7680, 41.10% Completed \n",
            "Epoch:5, iter:130, loss:0.32410, cum_examples:8320, 44.52% Completed \n",
            "Epoch:5, iter:140, loss:0.37098, cum_examples:8960, 47.95% Completed \n",
            "Epoch:5, iter:150, loss:0.36715, cum_examples:9600, 51.37% Completed \n",
            "Epoch:5, iter:160, loss:0.36761, cum_examples:10240, 54.79% Completed \n",
            "Epoch:5, iter:170, loss:0.34697, cum_examples:10880, 58.22% Completed \n",
            "Epoch:5, iter:180, loss:0.41488, cum_examples:11520, 61.64% Completed \n",
            "Epoch:5, iter:190, loss:0.28100, cum_examples:12160, 65.07% Completed \n",
            "Epoch:5, iter:200, loss:0.40215, cum_examples:12800, 68.49% Completed \n",
            "Epoch:5, iter:210, loss:0.29686, cum_examples:13440, 71.92% Completed \n",
            "Epoch:5, iter:220, loss:0.36097, cum_examples:14080, 75.34% Completed \n",
            "Epoch:5, iter:230, loss:0.44347, cum_examples:14720, 78.77% Completed \n",
            "Epoch:5, iter:240, loss:0.33746, cum_examples:15360, 82.19% Completed \n",
            "Epoch:5, iter:250, loss:0.41248, cum_examples:16000, 85.62% Completed \n",
            "Epoch:5, iter:260, loss:0.43235, cum_examples:16640, 89.04% Completed \n",
            "Epoch:5, iter:270, loss:0.41601, cum_examples:17280, 92.47% Completed \n",
            "Epoch:5, iter:280, loss:0.30521, cum_examples:17920, 95.89% Completed \n",
            "Epoch:5, iter:290, loss:0.38817, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.37097921161627284\n",
            "New best result! Saving model.\n",
            "Epoch 6 out of 10\n",
            "Epoch:6, iter:10, loss:0.43438, cum_examples:640, 3.42% Completed \n",
            "Epoch:6, iter:20, loss:0.25563, cum_examples:1280, 6.85% Completed \n",
            "Epoch:6, iter:30, loss:0.36246, cum_examples:1920, 10.27% Completed \n",
            "Epoch:6, iter:40, loss:0.39453, cum_examples:2560, 13.70% Completed \n",
            "Epoch:6, iter:50, loss:0.38610, cum_examples:3200, 17.12% Completed \n",
            "Epoch:6, iter:60, loss:0.28981, cum_examples:3840, 20.55% Completed \n",
            "Epoch:6, iter:70, loss:0.31787, cum_examples:4480, 23.97% Completed \n",
            "Epoch:6, iter:80, loss:0.38802, cum_examples:5120, 27.40% Completed \n",
            "Epoch:6, iter:90, loss:0.46870, cum_examples:5760, 30.82% Completed \n",
            "Epoch:6, iter:100, loss:0.31902, cum_examples:6400, 34.25% Completed \n",
            "Epoch:6, iter:110, loss:0.26914, cum_examples:7040, 37.67% Completed \n",
            "Epoch:6, iter:120, loss:0.30071, cum_examples:7680, 41.10% Completed \n",
            "Epoch:6, iter:130, loss:0.31063, cum_examples:8320, 44.52% Completed \n",
            "Epoch:6, iter:140, loss:0.31366, cum_examples:8960, 47.95% Completed \n",
            "Epoch:6, iter:150, loss:0.35810, cum_examples:9600, 51.37% Completed \n",
            "Epoch:6, iter:160, loss:0.38229, cum_examples:10240, 54.79% Completed \n",
            "Epoch:6, iter:170, loss:0.40429, cum_examples:10880, 58.22% Completed \n",
            "Epoch:6, iter:180, loss:0.31355, cum_examples:11520, 61.64% Completed \n",
            "Epoch:6, iter:190, loss:0.33878, cum_examples:12160, 65.07% Completed \n",
            "Epoch:6, iter:200, loss:0.37429, cum_examples:12800, 68.49% Completed \n",
            "Epoch:6, iter:210, loss:0.33815, cum_examples:13440, 71.92% Completed \n",
            "Epoch:6, iter:220, loss:0.29099, cum_examples:14080, 75.34% Completed \n",
            "Epoch:6, iter:230, loss:0.29244, cum_examples:14720, 78.77% Completed \n",
            "Epoch:6, iter:240, loss:0.29324, cum_examples:15360, 82.19% Completed \n",
            "Epoch:6, iter:250, loss:0.41669, cum_examples:16000, 85.62% Completed \n",
            "Epoch:6, iter:260, loss:0.35814, cum_examples:16640, 89.04% Completed \n",
            "Epoch:6, iter:270, loss:0.36533, cum_examples:17280, 92.47% Completed \n",
            "Epoch:6, iter:280, loss:0.29672, cum_examples:17920, 95.89% Completed \n",
            "Epoch:6, iter:290, loss:0.32637, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.35156003603724395\n",
            "New best result! Saving model.\n",
            "Epoch 7 out of 10\n",
            "Epoch:7, iter:10, loss:0.33682, cum_examples:640, 3.42% Completed \n",
            "Epoch:7, iter:20, loss:0.29001, cum_examples:1280, 6.85% Completed \n",
            "Epoch:7, iter:30, loss:0.34586, cum_examples:1920, 10.27% Completed \n",
            "Epoch:7, iter:40, loss:0.32321, cum_examples:2560, 13.70% Completed \n",
            "Epoch:7, iter:50, loss:0.44432, cum_examples:3200, 17.12% Completed \n",
            "Epoch:7, iter:60, loss:0.27824, cum_examples:3840, 20.55% Completed \n",
            "Epoch:7, iter:70, loss:0.31361, cum_examples:4480, 23.97% Completed \n",
            "Epoch:7, iter:80, loss:0.28206, cum_examples:5120, 27.40% Completed \n",
            "Epoch:7, iter:90, loss:0.28964, cum_examples:5760, 30.82% Completed \n",
            "Epoch:7, iter:100, loss:0.33501, cum_examples:6400, 34.25% Completed \n",
            "Epoch:7, iter:110, loss:0.33025, cum_examples:7040, 37.67% Completed \n",
            "Epoch:7, iter:120, loss:0.32021, cum_examples:7680, 41.10% Completed \n",
            "Epoch:7, iter:130, loss:0.25933, cum_examples:8320, 44.52% Completed \n",
            "Epoch:7, iter:140, loss:0.29990, cum_examples:8960, 47.95% Completed \n",
            "Epoch:7, iter:150, loss:0.36629, cum_examples:9600, 51.37% Completed \n",
            "Epoch:7, iter:160, loss:0.28314, cum_examples:10240, 54.79% Completed \n",
            "Epoch:7, iter:170, loss:0.37113, cum_examples:10880, 58.22% Completed \n",
            "Epoch:7, iter:180, loss:0.29263, cum_examples:11520, 61.64% Completed \n",
            "Epoch:7, iter:190, loss:0.32910, cum_examples:12160, 65.07% Completed \n",
            "Epoch:7, iter:200, loss:0.27561, cum_examples:12800, 68.49% Completed \n",
            "Epoch:7, iter:210, loss:0.45279, cum_examples:13440, 71.92% Completed \n",
            "Epoch:7, iter:220, loss:0.28230, cum_examples:14080, 75.34% Completed \n",
            "Epoch:7, iter:230, loss:0.37680, cum_examples:14720, 78.77% Completed \n",
            "Epoch:7, iter:240, loss:0.23026, cum_examples:15360, 82.19% Completed \n",
            "Epoch:7, iter:250, loss:0.31302, cum_examples:16000, 85.62% Completed \n",
            "Epoch:7, iter:260, loss:0.47592, cum_examples:16640, 89.04% Completed \n",
            "Epoch:7, iter:270, loss:0.43540, cum_examples:17280, 92.47% Completed \n",
            "Epoch:7, iter:280, loss:0.44382, cum_examples:17920, 95.89% Completed \n",
            "Epoch:7, iter:290, loss:0.28669, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.33741216919049116\n",
            "New best result! Saving model.\n",
            "Epoch 8 out of 10\n",
            "Epoch:8, iter:10, loss:0.37131, cum_examples:640, 3.42% Completed \n",
            "Epoch:8, iter:20, loss:0.37998, cum_examples:1280, 6.85% Completed \n",
            "Epoch:8, iter:30, loss:0.31648, cum_examples:1920, 10.27% Completed \n",
            "Epoch:8, iter:40, loss:0.29030, cum_examples:2560, 13.70% Completed \n",
            "Epoch:8, iter:50, loss:0.25011, cum_examples:3200, 17.12% Completed \n",
            "Epoch:8, iter:60, loss:0.34589, cum_examples:3840, 20.55% Completed \n",
            "Epoch:8, iter:70, loss:0.22763, cum_examples:4480, 23.97% Completed \n",
            "Epoch:8, iter:80, loss:0.32194, cum_examples:5120, 27.40% Completed \n",
            "Epoch:8, iter:90, loss:0.31865, cum_examples:5760, 30.82% Completed \n",
            "Epoch:8, iter:100, loss:0.33369, cum_examples:6400, 34.25% Completed \n",
            "Epoch:8, iter:110, loss:0.31820, cum_examples:7040, 37.67% Completed \n",
            "Epoch:8, iter:120, loss:0.25799, cum_examples:7680, 41.10% Completed \n",
            "Epoch:8, iter:130, loss:0.29911, cum_examples:8320, 44.52% Completed \n",
            "Epoch:8, iter:140, loss:0.35922, cum_examples:8960, 47.95% Completed \n",
            "Epoch:8, iter:150, loss:0.30067, cum_examples:9600, 51.37% Completed \n",
            "Epoch:8, iter:160, loss:0.21545, cum_examples:10240, 54.79% Completed \n",
            "Epoch:8, iter:170, loss:0.36320, cum_examples:10880, 58.22% Completed \n",
            "Epoch:8, iter:180, loss:0.42638, cum_examples:11520, 61.64% Completed \n",
            "Epoch:8, iter:190, loss:0.40227, cum_examples:12160, 65.07% Completed \n",
            "Epoch:8, iter:200, loss:0.34188, cum_examples:12800, 68.49% Completed \n",
            "Epoch:8, iter:210, loss:0.30973, cum_examples:13440, 71.92% Completed \n",
            "Epoch:8, iter:220, loss:0.37423, cum_examples:14080, 75.34% Completed \n",
            "Epoch:8, iter:230, loss:0.30880, cum_examples:14720, 78.77% Completed \n",
            "Epoch:8, iter:240, loss:0.40026, cum_examples:15360, 82.19% Completed \n",
            "Epoch:8, iter:250, loss:0.26681, cum_examples:16000, 85.62% Completed \n",
            "Epoch:8, iter:260, loss:0.36404, cum_examples:16640, 89.04% Completed \n",
            "Epoch:8, iter:270, loss:0.28064, cum_examples:17280, 92.47% Completed \n",
            "Epoch:8, iter:280, loss:0.33862, cum_examples:17920, 95.89% Completed \n",
            "Epoch:8, iter:290, loss:0.39980, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.324761168486407\n",
            "New best result! Saving model.\n",
            "Epoch 9 out of 10\n",
            "Epoch:9, iter:10, loss:0.34943, cum_examples:640, 3.42% Completed \n",
            "Epoch:9, iter:20, loss:0.22527, cum_examples:1280, 6.85% Completed \n",
            "Epoch:9, iter:30, loss:0.22750, cum_examples:1920, 10.27% Completed \n",
            "Epoch:9, iter:40, loss:0.35726, cum_examples:2560, 13.70% Completed \n",
            "Epoch:9, iter:50, loss:0.29321, cum_examples:3200, 17.12% Completed \n",
            "Epoch:9, iter:60, loss:0.29806, cum_examples:3840, 20.55% Completed \n",
            "Epoch:9, iter:70, loss:0.37344, cum_examples:4480, 23.97% Completed \n",
            "Epoch:9, iter:80, loss:0.32513, cum_examples:5120, 27.40% Completed \n",
            "Epoch:9, iter:90, loss:0.29224, cum_examples:5760, 30.82% Completed \n",
            "Epoch:9, iter:100, loss:0.37495, cum_examples:6400, 34.25% Completed \n",
            "Epoch:9, iter:110, loss:0.20205, cum_examples:7040, 37.67% Completed \n",
            "Epoch:9, iter:120, loss:0.35977, cum_examples:7680, 41.10% Completed \n",
            "Epoch:9, iter:130, loss:0.23294, cum_examples:8320, 44.52% Completed \n",
            "Epoch:9, iter:140, loss:0.31612, cum_examples:8960, 47.95% Completed \n",
            "Epoch:9, iter:150, loss:0.41422, cum_examples:9600, 51.37% Completed \n",
            "Epoch:9, iter:160, loss:0.31100, cum_examples:10240, 54.79% Completed \n",
            "Epoch:9, iter:170, loss:0.34073, cum_examples:10880, 58.22% Completed \n",
            "Epoch:9, iter:180, loss:0.28594, cum_examples:11520, 61.64% Completed \n",
            "Epoch:9, iter:190, loss:0.38663, cum_examples:12160, 65.07% Completed \n",
            "Epoch:9, iter:200, loss:0.24803, cum_examples:12800, 68.49% Completed \n",
            "Epoch:9, iter:210, loss:0.33327, cum_examples:13440, 71.92% Completed \n",
            "Epoch:9, iter:220, loss:0.31045, cum_examples:14080, 75.34% Completed \n",
            "Epoch:9, iter:230, loss:0.30140, cum_examples:14720, 78.77% Completed \n",
            "Epoch:9, iter:240, loss:0.34935, cum_examples:15360, 82.19% Completed \n",
            "Epoch:9, iter:250, loss:0.32228, cum_examples:16000, 85.62% Completed \n",
            "Epoch:9, iter:260, loss:0.34476, cum_examples:16640, 89.04% Completed \n",
            "Epoch:9, iter:270, loss:0.25024, cum_examples:17280, 92.47% Completed \n",
            "Epoch:9, iter:280, loss:0.28882, cum_examples:17920, 95.89% Completed \n",
            "Epoch:9, iter:290, loss:0.32385, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.3078536086646067\n",
            "New best result! Saving model.\n",
            "Epoch 10 out of 10\n",
            "Epoch:10, iter:10, loss:0.39206, cum_examples:640, 3.42% Completed \n",
            "Epoch:10, iter:20, loss:0.26534, cum_examples:1280, 6.85% Completed \n",
            "Epoch:10, iter:30, loss:0.23507, cum_examples:1920, 10.27% Completed \n",
            "Epoch:10, iter:40, loss:0.25621, cum_examples:2560, 13.70% Completed \n",
            "Epoch:10, iter:50, loss:0.30179, cum_examples:3200, 17.12% Completed \n",
            "Epoch:10, iter:60, loss:0.25187, cum_examples:3840, 20.55% Completed \n",
            "Epoch:10, iter:70, loss:0.26022, cum_examples:4480, 23.97% Completed \n",
            "Epoch:10, iter:80, loss:0.19256, cum_examples:5120, 27.40% Completed \n",
            "Epoch:10, iter:90, loss:0.25927, cum_examples:5760, 30.82% Completed \n",
            "Epoch:10, iter:100, loss:0.25883, cum_examples:6400, 34.25% Completed \n",
            "Epoch:10, iter:110, loss:0.30159, cum_examples:7040, 37.67% Completed \n",
            "Epoch:10, iter:120, loss:0.31970, cum_examples:7680, 41.10% Completed \n",
            "Epoch:10, iter:130, loss:0.29120, cum_examples:8320, 44.52% Completed \n",
            "Epoch:10, iter:140, loss:0.38617, cum_examples:8960, 47.95% Completed \n",
            "Epoch:10, iter:150, loss:0.30652, cum_examples:9600, 51.37% Completed \n",
            "Epoch:10, iter:160, loss:0.28331, cum_examples:10240, 54.79% Completed \n",
            "Epoch:10, iter:170, loss:0.24093, cum_examples:10880, 58.22% Completed \n",
            "Epoch:10, iter:180, loss:0.28820, cum_examples:11520, 61.64% Completed \n",
            "Epoch:10, iter:190, loss:0.26766, cum_examples:12160, 65.07% Completed \n",
            "Epoch:10, iter:200, loss:0.38621, cum_examples:12800, 68.49% Completed \n",
            "Epoch:10, iter:210, loss:0.35810, cum_examples:13440, 71.92% Completed \n",
            "Epoch:10, iter:220, loss:0.22587, cum_examples:14080, 75.34% Completed \n",
            "Epoch:10, iter:230, loss:0.29317, cum_examples:14720, 78.77% Completed \n",
            "Epoch:10, iter:240, loss:0.34384, cum_examples:15360, 82.19% Completed \n",
            "Epoch:10, iter:250, loss:0.32990, cum_examples:16000, 85.62% Completed \n",
            "Epoch:10, iter:260, loss:0.36628, cum_examples:16640, 89.04% Completed \n",
            "Epoch:10, iter:270, loss:0.22791, cum_examples:17280, 92.47% Completed \n",
            "Epoch:10, iter:280, loss:0.40456, cum_examples:17920, 95.89% Completed \n",
            "Epoch:10, iter:290, loss:0.28665, cum_examples:18560, 99.32% Completed \n",
            "Average Train Loss: 0.3042033949271351\n",
            "New best result! Saving model.\n",
            "training took 377 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09v6l6fOv-BJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4070448-0f3a-4537-d10f-9d645d049918"
      },
      "source": [
        "print(train_loss)\n",
        "#print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6czHJ5kzv-BK"
      },
      "source": [
        "epochs = np.linspace(1,10,num=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDhJUrIvv-BL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "e09def71-e379-498f-ce32-e951bc7e7261"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "\n",
        "x = np.linspace(0, 10, 1000)\n",
        "ax.plot( epochs, train_loss, color='green');\n",
        "#ax.plot( epochs, test_loss, color='red');\n",
        "ax.legend(['train', 'validation'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ab1d79388210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#ax.plot( epochs, test_loss, color='red');\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (0,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIn/A+itHbJNNLunAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RtF8FqWv-BO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysc8U97zv-BP"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX4eVliLv-BQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1fb7d476-186d-47d4-a223-c903842fcb0b"
      },
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "print(\"Restoring the best model wieghts found on the set\")\n",
        "#output_path = \"results/20191205_214527model.weights\"\n",
        "model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evation on the test set: \")\n",
        "to_device(model, device='cpu')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "src_len_test = []\n",
        "\n",
        "for failures in X_test:\n",
        "    src_len_test.append(sum(failures == vocab_to_int[\"<pad>\"]))\n",
        "\n",
        "src_len_test = np.asarray([src_len_test], dtype=int)\n",
        "src_len_test = X_test.shape[1] - src_len_test\n",
        "\n",
        "src_len_test = torch.from_numpy(src_len_test).to(torch.int64)\n",
        "src_len_test = torch.squeeze(src_len_test)\n",
        "\n",
        "test_x = torch.from_numpy(X_test).to(int)\n",
        "\n",
        "test_x = torch.t(test_x)\n",
        "\n",
        "test_y = torch.squeeze(torch.from_numpy(y_test).long())\n",
        "print(80 * \"=\")\n",
        "print(\"getting output from the model\")\n",
        "final_out = model(test_x, src_len_test)\n",
        "print(80 * \"=\")\n",
        "# final_test = torch.from_numpy(y_test).float()\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "final_loss = loss(final_out, test_y)\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "print('Test Cross Entropy Loss: ', final_loss.detach().numpy())\n",
        "print('Test Accuracy: {:f}'.format(metrics.accuracy_score(y_test, torch.argmax(m(final_out), dim=1).data.numpy())))\n",
        "\n",
        "# top_vec_test = torch.topk(m(final_out), 5, dim = 1)[1].data.numpy()\n",
        "\n",
        "# #eval_vec = np.zeros(len(out))\n",
        "# eval_list_test = []\n",
        "\n",
        "# for real, top in zip(test_y.data.numpy(), top_vec_test):\n",
        "#     #pdb.set_trace()\n",
        "#     #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "#     eval_list_test.append(np.isin(real, top))\n",
        "\n",
        "\n",
        "# #pdb.set_trace()\n",
        "# print(\"Test Accuracy for top 5 {}\".format(sum(eval_list_test)/len(eval_list_test)))\n",
        "print(classification_report(y_test, torch.argmax(m(final_out), dim=1).data.numpy().reshape(-1, 1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model wieghts found on the set\n",
            "Final evation on the test set: \n",
            "================================================================================\n",
            "getting output from the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bGqX2tSv-BR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6JzRL8Ov-BU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOuJ6vO_v-BW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZwcqHAqv-BZ"
      },
      "source": [
        "# test metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d5WJn7Bv-BZ",
        "outputId": "1b235965-7fed-4067-f8aa-124e00c2dd4f"
      },
      "source": [
        "# model = NeuralNet(embed_size=10, hidden_dim=200, batch_size=128, vocab=data_set.mapping,\n",
        "#                   pad_token_idx=data_set.mapping[\"<pad>\"], pre_trained_matrix=failure_text_embeddings, num_layers=1,\n",
        "#                   output_dim=len(encoder.categories_[0]), dropout_rate=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(80* \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80* \"=\")\n",
        "print(\"Restoring the best model wieghts found on the set\")\n",
        "#output_path = \"results/20191114_145657model.weights\"\n",
        "model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evation on the test set: \")\n",
        "model.eval()\n",
        "\n",
        "\n",
        "src_len_test = []\n",
        "\n",
        "for failures in X_test:\n",
        "    src_len_test.append(sum(failures == data_set.mapping[\"<pad>\"]))\n",
        "\n",
        "src_len_test = np.asarray([src_len_test], dtype=int)\n",
        "src_len_test = X_test.shape[1] - src_len_test\n",
        "\n",
        "\n",
        "src_len_test = torch.from_numpy(src_len_test).to(torch.int64)\n",
        "src_len_test = torch.squeeze(src_len_test)\n",
        "\n",
        "test_x = torch.from_numpy(X_test).to(int)\n",
        "\n",
        "test_x = torch.t(test_x)\n",
        "\n",
        "test_y = torch.squeeze(torch.from_numpy(y_test).long())\n",
        "print(80* \"=\")\n",
        "print(\"getting output from the model\")\n",
        "final_out = model(test_x, src_len_test)\n",
        "print(80* \"=\")\n",
        "#final_test = torch.from_numpy(y_test).float()\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "final_loss = loss(final_out, test_y)\n",
        "\n",
        "\n",
        "m = nn.Softmax(dim=1)\n",
        "print('Test Cross Entropy Loss: ',final_loss.detach().numpy())\n",
        "print('Test Accuracy: {:f}'.format(metrics.accuracy_score(y_test, torch.argmax(m(final_out), dim=1).data.numpy())))\n",
        "\n",
        "\n",
        "\n",
        "# top_vec_test = torch.topk(m(final_out), 5, dim = 1)[1].data.numpy()\n",
        "\n",
        "# #eval_vec = np.zeros(len(out))\n",
        "# eval_list_test = []\n",
        "\n",
        "# for real, top in zip(test_y.data.numpy(), top_vec_test):\n",
        "#     #pdb.set_trace()\n",
        "#     #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "#     eval_list_test.append(np.isin(real, top))\n",
        "\n",
        "    \n",
        "# #pdb.set_trace()\n",
        "# print(\"Test Accuracy for top 5 {}\".format(sum(eval_list_test)/len(eval_list_test)))\n",
        "print(classification_report(y_test, torch.argmax(m(final_out), dim=1).data.numpy().reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model wieghts found on the set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "unexpected EOF, expected 436048800 more bytes. The file might be corrupted.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2569be732eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring the best model wieghts found on the set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#output_path = \"results/20191114_145657model.weights\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final evation on the test set: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/av/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/av/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 436048800 more bytes. The file might be corrupted."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tiyjg6vv-Bc"
      },
      "source": [
        "diff_test = np.absolute(torch.argmax(m(final_out), dim=1).data.numpy().reshape(-1,1) - y_test)\n",
        "y_test_incorrect = y_test[diff_test != 0]\n",
        "y_test_correct = y_test[diff_test == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdaH8ptwv-Be"
      },
      "source": [
        "test_vals_err = pd.DataFrame(y_test_incorrect, columns=['y_test_incorrect'])\n",
        "te = test_vals_err['y_test_incorrect'].value_counts()\n",
        "print(te)\n",
        "print(te/test_vals_err.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JULR2Okv-Bf"
      },
      "source": [
        "test_vals_corr = pd.DataFrame(y_test_correct, columns=['y_test_correct'])\n",
        "tc = test_vals_corr['y_test_correct'].value_counts()\n",
        "print(tc)\n",
        "print(tc/test_vals_corr.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS7sMb4sv-Bh"
      },
      "source": [
        "np.concatenate((encoder.transform(encoder.categories_[0].reshape(-1,1)), encoder.categories_[0].reshape(-1,1)), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_L33fmqv-Bi"
      },
      "source": [
        "print(counts_T)\n",
        "print(counts_T/data['T'].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6G6A2mKv-Bj"
      },
      "source": [
        "print(classification_report(y_test, torch.argmax(m(final_out), dim=1).data.numpy().reshape(-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgZAg1MSv-Bl"
      },
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "print('Test Cross Entropy Loss: ',final_loss.detach().numpy())\n",
        "print('Test Accuracy: {:f}'.format(metrics.accuracy_score(y_test, torch.argmax(m(final_out), dim=1).data.numpy())))\n",
        "\n",
        "\n",
        "\n",
        "top_vec_test = torch.topk(m(final_out), 5, dim = 1)[1].data.numpy()\n",
        "\n",
        "#eval_vec = np.zeros(len(out))\n",
        "eval_list_test = []\n",
        "\n",
        "for real, top in zip(test_y.data.numpy(), top_vec_test):\n",
        "    #pdb.set_trace()\n",
        "    #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "    eval_list_test.append(np.isin(real, top))\n",
        "\n",
        "    \n",
        "#pdb.set_trace()\n",
        "print(\"Test Accuracy for top 5 {}\".format(sum(eval_list_test)/len(eval_list_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC4QqwJSv-Bm"
      },
      "source": [
        "top_vec_test = torch.topk(m(final_out), 3, dim = 1)[1].data.numpy()\n",
        "\n",
        "#eval_vec = np.zeros(len(out))\n",
        "eval_list_test = []\n",
        "\n",
        "for real, top in zip(train_y.data.numpy(), top_vec_test):\n",
        "    #pdb.set_trace()\n",
        "    #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "    eval_list_test.append(np.isin(real, top))\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "#pdb.set_trace()\n",
        "print(\"Test Accuracy for top 3 {}\".format(sum(eval_list_test)/len(eval_list_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2NNCQdzv-Bn"
      },
      "source": [
        "top_vals = torch.argmax(m(final_out), dim=1).data.numpy()\n",
        "test_list = []\n",
        "for actual, top_test in zip(train_y.data.numpy(), top_vals):\n",
        "    test_list.append(np.isin(actual, top_test))\n",
        "\n",
        "print(\"Test Accuracy for top 1 {}\".format(sum(test_list)/len(test_list)))\n",
        "metrics.accuracy_score(y_test, top_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ErG59CTv-Bp"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Niez0Lv-Bq"
      },
      "source": [
        "top_vals.reshape(-1,1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzQ4jcKMv-Bs"
      },
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "print('Test Accuracy: {:f}'.format(metrics.accuracy_score(y_test, torch.argmax(m(final_out), dim=1).data.numpy())))\n",
        "\n",
        "\n",
        "top_vec_test = torch.topk(m(final_out), 5, dim = 1)[1].data.numpy()\n",
        "\n",
        "#eval_vec = np.zeros(len(out))\n",
        "eval_list_test = []\n",
        "\n",
        "for real, top in zip(test_y.data.numpy(), top_vec_test):\n",
        "    #pdb.set_trace()\n",
        "    #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "    eval_list_test.append(np.isin(real, top))\n",
        "\n",
        "    \n",
        "#pdb.set_trace()\n",
        "print(\"Test Accuracy for top 5 {}\".format(sum(eval_list_test)/len(eval_list_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCFwkhpav-Bu"
      },
      "source": [
        "# train metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYR0UPuFv-Bu"
      },
      "source": [
        "train_x_np = X_train\n",
        "train_y = torch.squeeze(torch.from_numpy(y_train).long())\n",
        "\n",
        "\n",
        "source_lengths = [sum(failures == data_set.mapping[\"<pad>\"]) for failures in train_x_np]\n",
        "source_lengths = np.asarray([source_lengths], dtype=int)\n",
        "source_lengths = train_x_np.shape[1] - source_lengths\n",
        "source_lengths = torch.from_numpy(source_lengths).to(torch.int64)\n",
        "source_lengths = torch.squeeze(source_lengths)\n",
        "\n",
        "\n",
        "train_x = torch.from_numpy(train_x_np).to(torch.int64)\n",
        "train_x = torch.t(train_x)\n",
        "\n",
        "out_train = model(train_x, source_lengths)\n",
        "\n",
        "top_vec = torch.topk(m(out_train), 2, dim = 1)[1].data.numpy()\n",
        "\n",
        "#eval_vec = np.zeros(len(out))\n",
        "eval_list = []\n",
        "\n",
        "for real_train, top_train in zip(train_y.data.numpy(), top_vec):\n",
        "    #pdb.set_trace()\n",
        "    #np.concatenate((eval_vec, (np.isin(real, top3))), axis = 0)\n",
        "    eval_list.append(np.isin(real_train, top_train))\n",
        "\n",
        "    \n",
        "#pdb.set_trace()\n",
        "print('Train Cross Entropy Loss: ',loss(out_train, train_y).detach().numpy())\n",
        "print('Train Accuracy: {:f}'.format(metrics.accuracy_score(y_train, torch.argmax(m(out_train), dim=1).data.numpy())))\n",
        "print(\"Train Accuracy for top 2 {}\".format(sum(eval_list)/len(eval_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEWMcYrvv-Bv"
      },
      "source": [
        "print('Train Cross Entropy Loss: ',loss(out_train, train_y).detach().numpy())\n",
        "print('Train Accuracy: {:f}'.format(metrics.accuracy_score(y_train, torch.argmax(m(out_train), dim=1).data.numpy())))\n",
        "print(\"Train Accuracy for top 3 {}\".format(sum(eval_list)/len(eval_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X62j3dqnv-Bx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVJEdckTv-Bz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAEgAgBGv-B0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyUeEEAmv-B2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7zmm605v-B3"
      },
      "source": [
        "final_out[2,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8v2GVjdv-B4"
      },
      "source": [
        "test_y[2:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q70nq4NKv-B5"
      },
      "source": [
        "loss(final_out[2:3,:], test_y[2:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUpNeJKEv-B7"
      },
      "source": [
        "final_out[1,20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ66KOk6v-B-"
      },
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "\n",
        "m(final_out[2:3,:])\n",
        "\n",
        "res = np.zeros(114)\n",
        "res[20] = 1\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00C2GFCrv-B_"
      },
      "source": [
        "final_out[2:3,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYAq4PU4v-CA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMaggqxGv-CB"
      },
      "source": [
        "torch.argmax(m(final_out[0:1, :]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvg8QqVev-CD"
      },
      "source": [
        "m(final_out[0:1, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHKvPdv5v-CE"
      },
      "source": [
        "torch.argmax(m(final_out), dim=1).data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhxmvM1Gv-CF"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A3kumLuv-CG"
      },
      "source": [
        "metrics.accuracy_score(y_test, torch.argmax(m(final_out), dim=1).data.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VvBv96Dv-CJ"
      },
      "source": [
        "metrics.accuracy_score(torch.squeeze(torch.from_numpy(test_y).long()), torch.argmax(m(final_out), dim=1).data.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNdgW_NZv-CP"
      },
      "source": [
        "classification_report(y_true, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh-PomD6v-CQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcimKw7Tv-CR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVgGYSdVv-CS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ecJ_0tv-CT"
      },
      "source": [
        "failure_text_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBadp1GFv-CT"
      },
      "source": [
        "y_train[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqZPJZpBv-CX"
      },
      "source": [
        "encoder.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDRwP8t5v-CY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9MVUV_mv-Ca"
      },
      "source": [
        "vec = np.array([1,2,3,4,5,-2,-2,-2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4XqutH0v-Cc"
      },
      "source": [
        "vec[0:0+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8t-ELBfv-Cf"
      },
      "source": [
        "sum(vec == -2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0rBw3vav-Cf"
      },
      "source": [
        "new_dict = {\"feeling\":0,\"negative\":1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u_TCJN0v-Ch"
      },
      "source": [
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lY8nlvqv-Ci"
      },
      "source": [
        "new_dict.update({\"<pad>\":2})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJRTpimqv-Ck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}